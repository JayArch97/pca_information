function extractQuestions(text) {
  const questionBlocks = Array.from(
    text.matchAll(/Question\s+#(\d+)\s+((?:.|\n)+?)(?=Question\s+#\d+|$)/g)
  );

  const results = [];

  for (const match of questionBlocks) {
    const questionNumber = match[1];
    const block = match[2];

    const qaMatch = block.match(
      /(?:Topic\s+\d+\s+)?(.+?)\s*A\.\s+(.+?)\s*B\.\s+(.+?)\s*C\.\s+(.+?)\s*D\.\s+(.+?)\s*Correct Answer:\s+([A-D])/s
    );

    if (qaMatch) {
      results.push({
        number: parseInt(questionNumber, 10),
        question: qaMatch[1].replace(/\s+/g, ' ').trim(),
        a: qaMatch[2].replace(/\s+/g, ' ').trim(),
        b: qaMatch[3].replace(/\s+/g, ' ').trim(),
        c: qaMatch[4].replace(/\s+/g, ' ').trim(),
        d: qaMatch[5].replace(/\s+/g, ' ').trim(),
        correct: qaMatch[6],
      });
    }
  }
  console.log(results)
  return results;
}

let test  = extractQuestions(`

Question #1

Topic 1

Your company has decided to make a major revision of their API in order to create better experiences for their developers. They need to keep the

old version of the API available and deployable, while allowing new customers and testers to try out the new API. They want to keep the same SSL

and DNS records in place to serve both APIs.

What should they do?

A. Configure a new load balancer for the new version of the API

B. Reconfigure old clients to use a new endpoint for the new API

C. Have the old API forward tra c to the new API based on the path

D. Use separate backend pools for each API path behind the load balancer

Correct Answer: D

Community vote distribution

D (100%)

? ?  shandy  Highly Voted ?  3áyears, 7ámonths ago

D is the answer because HTTP(S) load balancer can direct traffic reaching a single IP to different backends based on the incoming URL. A is
not correct because configuring a new load balancer would require a new or different SSL and DNS records which conflicts with the
requirements to keep the same SSL and DNS records. B is not correct because it goes against the requirements. The company wants to
keep the old API available while new customers and testers try the new API. C is not correct because it is not a requirement to
decommission the implementation behind the old API. Moreover, it introduces unnecessary risk in case bugs or incompatibilities are
discovered in the new API.

upvoted 79 times

? ?  AzureDP900 8ámonths, 2áweeks ago

D is right

upvoted 1 times

? ?  AWS56  Highly Voted ?  3áyears, 7ámonths ago

agreed, The answer is D

upvoted 19 times

? ?  Hemant100  Most Recent ?  1ámonth, 1áweek ago

I'm studying for the Google Professional Cloud Architect certification exam. I was only able to go through a few of the questions on this
website; if anyone has the rest, please send them to my email address: sawhemant41@gmail.com. I'm more likely to pass if I practise
more questions. Thank you ahead of time!

upvoted 3 times

? ?  dream2517_Jack 1ámonth, 2áweeks ago

I am preparing for Google Professional Cloud Architect exam. I was able to access only limited questions here, if anyone has the entire
questions please share them to my email address: dream2517@gmail.com address. Thankyou in advance!

upvoted 1 times

? ?  b_max 3ámonths, 2áweeks ago

Selected Answer: D

Debe ser la D
upvoted 1 times

? ?  DB3118 3ámonths, 2áweeks ago

Selected Answer: D

D is most suitable

upvoted 1 times

? ?  caroleravel 4ámonths, 3áweeks ago

I will take the exam in a week. Does anyone have access to all the questions? if so you can write to me at this address:
carole.ravel@outlook.fr

upvoted 1 times

? ?  zorrosam 2ámonths, 1áweek ago

did you get access to the full bundle of questions? me not

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

2/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  simonab23 5ámonths ago

Selected Answer: D

The question states that the company needs to keep the old version of the API available and deployable, while allowing new customers
and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs.
Using a separate backend pool for each API path behind the load balancer will allow the company to keep the old version of the API
available and deployable, while allowing new customers and testers to try out the new API. This is because the load balancer will be able
to route traffic to the appropriate backend pool based on the path.

upvoted 3 times

? ?  simonab23 5ámonths ago

D - The question states that the company needs to keep the old version of the API available and deployable, while allowing new customers
and testers to try out the new API. They want to keep the same SSL and DNS records in place to serve both APIs.
Using a separate backend pool for each API path behind the load balancer will allow the company to keep the old version of the API
available and deployable, while allowing new customers and testers to try out the new API. This is because the load balancer will be able
to route traffic to the appropriate backend pool based on the path.

upvoted 2 times

? ?  AShrujit 6ámonths ago

D for me

upvoted 1 times

? ?  Jaldhi24 6ámonths, 1áweek ago

Selected Answer: D

D is right

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

D. Use separate backend pools for each API path behind the load balancer

To keep the old version of the API available and deployable while allowing new customers and testers to try out the new API, while also
keeping the same SSL and DNS records in place, your company can use separate backend pools for each API path behind the load
balancer. This will allow them to route traffic to the appropriate backend pool based on the path, allowing them to serve both the old and
new versions of the API using the same load balancer and DNS records.

Other options, such as configuring a new load balancer for the new version of the API or reconfiguring old clients to use a new endpoint
for the new API, may not allow them to keep the same SSL and DNS records in place. Forwarding traffic to the new API based on the path
may not provide a reliable way to separate the two APIs.

upvoted 2 times

? ?  Ahmed1984_ 6ámonths, 1áweek ago

Selected Answer: D

D is the answer
upvoted 1 times

? ?  i_am_robot 6ámonths, 2áweeks ago

D. Use separate backend pools for each API path behind the load balancer

In order to keep both versions of the API available and deployable while allowing new customers and testers to try out the new API, it
would be a good idea to use separate backend pools for each API path behind a load balancer. This would allow the company to keep the
same SSL and DNS records in place, while routing traffic to the appropriate backend pool based on the path.

Option A, configuring a new load balancer for the new version of the API, would not allow the company to keep the old version of the API
available and deployable. Option B, reconfiguring old clients to use a new endpoint for the new API, would require significant effort and
could cause disruptions for existing clients. Option C, having the old API forward traffic to the new API based on the path, would not allow
the company to keep both APIs available and deployable at the same time.

upvoted 1 times

? ?  ABHIVISH 6ámonths, 2áweeks ago

I am preparing for Google Professional Cloud Architect exam. I was able to access only limited questions here, if anyone has the entire
questions please share them to my email address: abhishek.vishwakarma@gmail.com . Thank you in advance!

upvoted 1 times

? ?  [Removed] 6ámonths, 2áweeks ago

I am preparing for Google Professional Cloud Architect exam. I was able to access only limited questions here, if anyone has the entire
questions please share them to my email address: shivisharma237@gmail.com . Thank you in advance!

upvoted 1 times

? ?  GCP_Student1 6ámonths, 2áweeks ago

D is correct answer,

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

3/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Topic 1

Your company plans to migrate a multi-petabyte data set to the cloud. The data set must be available 24hrs a day. Your business analysts have

experience only with using a SQL interface.

How should you store the data to optimize it for ease of analysis?

A. Load data into Google BigQuery

B. Insert data into Google Cloud SQL

C. Put  at  les into Google Cloud Storage

D. Stream data into Google Cloud Datastore

Correct Answer: A

BigQuery is Google's serverless, highly scalable, low cost enterprise data warehouse designed to make all your data analysts productive.

Because there is no infrastructure to manage, you can focus on analyzing data to  nd meaningful insights using familiar SQL and you don't

need a database administrator.

BigQuery enables you to analyze all your data by creating a logical data warehouse over managed, columnar storage as well as data from object

storage, and spreadsheets.

Reference:

https://cloud.google.com/bigquery/

Community vote distribution

A (95%)

5%

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

This question could go either way for A or B. But Big Query was designed with this in mind, according to numerous Google presentation
and videos. Cloud Datastore is a NoSQL database (https://cloud.google.com/datastore/docs/concepts/overview)
Cloud Storage does not have an SQL interface. The previous two sentences eliminate options C and D. So I'd pick "A".

upvoted 28 times

? ?  kinghin 1áyear, 5ámonths ago

B is not correct because Cloud SQL storage limit doesn't fit the requirement.

upvoted 10 times

? ?  zr79 8ámonths, 2áweeks ago

Cloud SQL does not scale to that magnitude also Cloud SQL is not meant for OLAP
Answer is BigQuery

upvoted 2 times

? ?  0xE8D4A51000 8ámonths, 1áweek ago

IMHO, it should be A only. The reason is that they want to perform analysis on the data and BigQuery excels in that over Cloud SQL. You
can run SQL queries in both but I BigQuery has better analytical tools. It can do ad-hoc analysis like Cloud SQL using Cloud Standard
SQL and it can do geo-spatial and ML analysis via its Cloud Standard SQL interface.

upvoted 1 times

? ?  0xE8D4A51000 8ámonths, 1áweek ago

Also the question does not say whether the data is relational or not. So we cannot assume it is only relational. Therefore, for
maximum flexibility BQ is the correct option also. Note that Cloud SQL storage capacity is now at 64TB

upvoted 3 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 14 times

? ?  clouddude  Highly Voted ?  3áyears, 1ámonth ago

I'll go with A because BQ (and BT) are usually meant for analytics.
B isn't correct because Cloud SQL does not scale to that volume.
C isn't correct because Cloud Storage does not provide a standard SQL mechanism.
D could be right but it sounds off because of the analytics requirement.

upvoted 14 times

? ?  Kicod  Most Recent ?  4ámonths ago

Selected Answer: A

A is the correct one

upvoted 1 times

? ?  simonab23 5ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

4/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

A - The question states that the data set must be available 24hrs a day and that your business analysts have experience only with using a
SQL interface.
Loading data into Google BigQuery will allow your business analysts to access the data using a SQL interface. It will also allow the data to
be available 24hrs a day.

upvoted 2 times

? ?  simonab23 5ámonths ago

A - The question states that the data set must be available 24hrs a day and that your business analysts have experience only with using a
SQL interface.
Loading data into Google BigQuery will allow your business analysts to access the data using a SQL interface. It will also allow the data to
be available 24hrs a day.

upvoted 2 times

? ?  cooljayforever 5ámonths, 2áweeks ago

Selected Answer: A

Bigquery for data analytics

upvoted 1 times

? ?  AShrujit 6ámonths ago

A for me

upvoted 1 times

? ?  Jaldhi24 6ámonths, 1áweek ago

Selected Answer: A

A is the correct one.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago
A. Load data into Google BigQuery

To optimize the storage of the multi-petabyte data set for ease of analysis by business analysts who have experience only with using a SQL
interface, you should load the data into Google BigQuery. BigQuery is a fully-managed, cloud-native data warehouse that allows you to
perform fast SQL queries on large amounts of data. By loading the data into BigQuery, you can provide your business analysts with a
familiar SQL interface for querying the data, making it easier for them to analyze the data set.

Other options, such as inserting data into Google Cloud SQL, putting flat files into Google Cloud Storage, or streaming data into Google
Cloud Datastore, may not provide the necessary SQL interface or query performance for efficient analysis of the data set.

upvoted 1 times

? ?  examch 6ámonths, 1áweek ago

A is the answer, with keywords, migrate multi-petabyte dataset, data should be available 24 hours, business analyst with SQL experience,
optimize for analysis. BigQuery is the right answer.

upvoted 1 times

? ?  i_am_robot 6ámonths, 2áweeks ago

A. Load data into Google BigQuery

BigQuery is a fully managed, cloud-native data warehousing solution that makes it easy to analyze large and complex datasets. It is
optimized for analyzing large amounts of data quickly, and can handle petabyte-scale datasets with ease. It also has a SQL-like interface
that is familiar to business analysts, making it easy for them to query and analyze the data. Additionally, BigQuery is highly scalable and
can handle high query concurrency, making it a good choice for storing data that must be available 24/7.

Option B, inserting data into Google Cloud SQL, is not a good choice for a multi-petabyte dataset because Cloud SQL is not designed to
handle such large volumes of data. Option C, putting flat files into Cloud Storage, is also not a good choice because it is not optimized for
querying and analyzing data. Option D, streaming data into Cloud Datastore, is not a good choice because Cloud Datastore is a NoSQL
database and does not have a SQL-like interface.

upvoted 2 times

? ?  GCP_Student1 6ámonths, 2áweeks ago

A is the correct one.

upvoted 1 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: A

BigQuery is the multi petabyte data ware house solution provided with SQL support. Hence A is correct

upvoted 1 times

? ?  andreavale 7ámonths, 3áweeks ago

Selected Answer: A

A it's ok

upvoted 1 times

? ?  ckorbet 8ámonths ago

Selected Answer: A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

5/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is the correct one

upvoted 1 times

? ?  0xE8D4A51000 8ámonths, 1áweek ago

Selected Answer: A

IMHO, it should be A only. The reason is that they want to perform analysis on the data and BigQuery excels in that over Cloud SQL. You
can run SQL queries in both but I BigQuery has better analytical tools. It can do ad-hoc analysis like Cloud SQL using Cloud Standard SQL
and it can do geo-spatial and ML analysis via its Cloud Standard SQL interface.
Also the question does not say whether the data is relational or not. So we cannot assume it is only relational. Therefore, for maximum
flexibility BQ is the correct option also. Note that Cloud SQL storage capacity is now at 64TB so it can't handle multi-PB of data. The answer
is A only.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago
Bigquery is right option A is right

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

6/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Topic 1

The operations manager asks you for a list of recommended practices that she should consider when migrating a J2EE application to the cloud.

Which three practices should you recommend? (Choose three.)

A. Port the application code to run on Google App Engine

B. Integrate Cloud Data ow into the application to capture real-time metrics

C. Instrument the application with a monitoring tool like Stackdriver Debugger

D. Select an automation framework to reliably provision the cloud infrastructure

E. Deploy a continuous integration tool with automated testing in a staging environment

F. Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable

Correct Answer: ADE

References:

https://cloud.google.com/appengine/docs/standard/java/tools/uploadinganapp

https://cloud.google.com/appengine/docs/standard/java/building-app/cloud-sql

Community vote distribution

CDE (37%)

ADE (35%)

ACE (29%)

? ?  rishab86  Highly Voted ?  2áyears ago
CDE seems to be correct to me.

upvoted 60 times

? ?  Sur_Nikki 1ámonth, 2áweeks ago

Me too

upvoted 1 times

? ?  tsestini 3ámonths, 1áweek ago
A is a service, not a practice.

upvoted 2 times

? ?  rishab86 1áyear, 9ámonths ago
ACE seems to correct now

upvoted 7 times

? ?  rishab86 1áyear, 9ámonths ago

ACD sorry

upvoted 3 times

? ?  hareesh123 1áyear, 8ámonths ago

CDE looks like correct .
Porting a J2EE application to App Engine will not work as its is - there are three arpproach for migration -

There are three major types of migrations:

Lift and shift
Improve and move
Rip and replace

So Option A can be discarded .
So the answer is CDE .

upvoted 16 times

? ?  hareesh123 1áyear, 8ámonths ago

https://cloud.google.com/architecture/migration-to-gcp-getting-started

upvoted 1 times

? ?  J19G 1áyear, 8ámonths ago

Sorry ACE, A 9takes care of Infra)

upvoted 8 times

? ?  Tim_Chiang 1áyear ago

Nope, App Engine is PaaS not IaaS, still need that infra.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

7/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  jay9114 1áyear ago

I agree. If we select A (App Engine) then we shouldn't need an automation framework to provision the cloud infrastructure (D).

upvoted 2 times

? ?  zr79 8ámonths, 2áweeks ago

automation framework here is IaaC like Terraform or Deployment Manager

upvoted 2 times

? ?  NapoleonBorntoparty  Highly Voted ?  2áyears ago

This is talking about the APPLICATION not the infrastructure, therefore I believe we should focus on the APP-side of things:
1. port the app to app engine for content delivery
2. add monitoring for troubleshooting
3. use a CI/CD workflow for continuous delivery w/testing for a stable application

so, for me: A, C and E should be the answers

upvoted 51 times

? ?  segkhachat 3ámonths, 2áweeks ago

the person who asking you recommendation is operation manager, it can be related to infrastructure

upvoted 1 times

? ?  ChewSena  Most Recent ?  1ámonth, 2áweeks ago

CDE seems to be correct

upvoted 3 times

? ?  cloud0907 1ámonth, 3áweeks ago

This link provides content to support ACE. https://cloud.google.com/appengine/docs/legacy/standard/java/migrating-to-java8

upvoted 1 times

? ?  FigVam 1ámonth, 3áweeks ago

Selected Answer: CDE

CDE, they're talking about practices, not services

upvoted 1 times

? ?  ChewSena 2ámonths, 2áweeks ago

CDE seems to be right

upvoted 1 times

? ?  kratosmat 2ámonths, 2áweeks ago

Selected Answer: CDE

A is wrong, is it possibile to deploy an ejb application on app engine?

upvoted 2 times

? ?  taer 3ámonths ago

Selected Answer: CDE

C. Instrument the application with a monitoring tool like Stackdriver Debugger: Monitoring and debugging tools are essential to ensure
the proper functioning of your application in the cloud environment. Stackdriver Debugger can provide insights into your application's
performance, helping you identify and resolve any issues that may arise during the migration process.

upvoted 1 times

? ?  JC0926 3ámonths, 3áweeks ago

Selected Answer: CDE

C. Instrument the application with a monitoring tool like Stackdriver Debugger: A monitoring tool like Stackdriver Debugger can help in
identifying and debugging issues that arise during the migration process. It can also provide insights into the performance and availability
of the application after it has been migrated to the cloud.

D. Select an automation framework to reliably provision the cloud infrastructure: Automating the provisioning of the cloud infrastructure
can help ensure that the process is reliable and repeatable. It can also help reduce the risk of errors and increase the speed of the
migration process.

E. Deploy a continuous integration tool with automated testing in a staging environment: Deploying a continuous integration tool with
automated testing in a staging environment can help ensure that the application is thoroughly tested before it is deployed to production.
This can help reduce the risk of issues arising in production and provide greater confidence in the stability and reliability of the
application.

upvoted 2 times

? ?  JC0926 3ámonths, 3áweeks ago

F. Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable: Migrating from MySQL to a managed
NoSQL database like Google Cloud Datastore or Bigtable can be beneficial in terms of scalability, performance, and cost, but it may not
be necessary during the migration process. It may be more important to focus on ensuring that the application code and infrastructure
are correctly configured for the cloud environment before considering database migration.

upvoted 2 times

? ?  JC0926 3ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

8/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B. Integrate Cloud Dataflow into the application to capture real-time metrics: Integrating Cloud Dataflow into the application can help
capture real-time metrics, but it may not be critical during the migration process. The focus of the migration process should be on
ensuring that the application is deployed and functioning correctly in the cloud. Real-time metrics can be added later once the
application is stable and running smoothly in the cloud.

upvoted 2 times

? ?  JC0926 3ámonths, 3áweeks ago

A. Port the application code to run on Google App Engine: While porting the application code to run on Google App Engine is a valid
option, it may not be the best option for every application. Moreover, it may require significant changes to the application code, which
can be time-consuming and expensive. Therefore, it may not be the most critical practice to consider when migrating a J2EE application
to the cloud.

upvoted 2 times

? ?  BeCalm 3ámonths, 3áweeks ago

A does not make sense. A Ops Manager is not going to make the call or do the work on rewriting an app to work with GAE. This question is
asking for the viewpoint of an Ops Mgr so the responses need to be DevSecOps oriented.

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: CDE

A does not make sense because there's no mention of any improve and move or rip and replace migration strategy.

upvoted 2 times

? ?  Harry_HCL 4ámonths ago

Selected Answer: CDE

cde is correct
upvoted 1 times

? ?  Harry_HCL 4ámonths ago

Basedáonábestápractices,áIáwouldárecommendátheáfollowingáthreeápracticesáforámigratingáaáJ2EEáapplicationátoátheácloud:
C.áInstrumentátheáapplicationáwitháaámonitoringátoolálikeáStackdriveráDebugger
D.áSelectáanáautomationáframeworkátoáreliablyáprovisionátheácloudáinfrastructure
E.áDeployáaácontinuousáintegrationátooláwitháautomatedátestingáináaástagingáenvironment

upvoted 3 times

? ?  telp 4ámonths, 1áweek ago

Selected Answer: ADE

By elimination and if the J2EE is a basic application ADE:
B. Integrate Cloud Dataflow => no needed for this application.
C. Stackdriver Debugger => it's not the first monitoring tool to put in place, it's only to debug code execution.
F. Migrate from MySQL to a managed NoSQL database => no need for that, gcp has SQL supported with cloud SQL.

upvoted 3 times

? ?  Fundu80 3ámonths ago

Believe C is one of the correct choices. I came across this while browsing for details:
<quote>
Stackdriver was upgraded in 2020 with new features and rebranded as part of the Google Cloud operations suite of tools. Google Cloud
operations enables organizations to monitor, troubleshoot and operate cloud deployments.
<unquote>
So assuming it would be helpful for operating and troubleshooting cloud deployments during the migration.

upvoted 1 times

? ?  Bashar_Khouri 4ámonths, 1áweek ago

Selected Answer: CDE

For me CDE look like the correct one

upvoted 2 times

? ?  gcppandit 4ámonths, 3áweeks ago

Selected Answer: CDE

A = App Engine only implements a subset of J2EE components so lift and shift is not possible for a J2EE application. If you have EJB, RMI etc
then the application will not deploy. So should not be a recommendation.
B = Dataflow has nothing to do with this solution. So should not be a recommendation.
F = There is no point migrating from MySQL to NoSQL as MySQL is already available from Cloud SQL service.
So the final recomendation should be C, D and E

upvoted 7 times

? ?  simonab23 5ámonths ago

ABE - The question states that the operations manager asks you for a list of recommended practices that she should consider when
migrating a J2EE application to the cloud.
Porting the application code to run on Google App Engine will allow the application to be deployed and scaled automatically. Integrating
Cloud Dataflow into the application will allow the application to capture real-time metrics. Deploying a continuous integration tool with
automated testing in a staging environment will allow the application to be tested automatically and deployed to production when the
tests are successful.

upvoted 1 times

? ?  tdotcat 5ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

9/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: CDE

exisinting vm onprem apps should consider compute engine first, for migration

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

10/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Topic 1

A news feed web service has the following code running on Google App Engine. During peak load, users report that they can see news articles

they already viewed.

What is the most likely cause of this problem?

A. The session variable is local to just a single instance

B. The session variable is being overwritten in Cloud Datastore

C. The URL of the API needs to be modi ed to prevent caching

D. The HTTP Expires header needs to be set to -1 stop caching

Correct Answer: A

Community vote distribution

A (100%)

? ?  jackdbd  Highly Voted ?  1áyear, 1ámonth ago

It's A. AppEngine spins up new containers automatically according to the load. During peak traffic, HTTP requests originated by the same
user could be served by different containers. Given that the variable sessions is recreated for each container, it might store different
data.
The problem here is that this Flask app is stateful. The sessions variable is the state of this app. And stateful variables in AppEngine /
Cloud Run / Cloud Functions are problematic.
A solution would be to store the session in some database (e.g. Firestore, Memorystore) and retrieve it from there. This way the app would
fetch the session from a single place and would be stateless.

upvoted 76 times

? ?  omermahgoub 6ámonths, 2áweeks ago

Very well stated, jack. I just wanted to point, GAE is a webserver platform anyway, so making application stateless or stateful is up to
the developer and has nothing to do with GAE. The issue is about session consistency. GAE spin new container if there's a need, and
based on the code, the session is stored locally, this means, there's no consistency between container, and there's no grantee that the
same container might serve the same user. Thank you Jack, very good explanation

upvoted 8 times

? ?  AzureDP900 8ámonths, 2áweeks ago
I agree with detailed explanation

upvoted 2 times

? ?  H_S 1áyear ago

thank you man that is a great explanation

upvoted 2 times

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

11/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is correct

upvoted 29 times

? ?  Badri9898  Most Recent ?  3ámonths ago

The most likely cause of the reported issue is that the session variable is local to just a single instance.

In the code provided, the sessions variable is a dictionary that stores the viewed news articles for each user. However, this variable is only
stored in memory on the instance that handles the request, and it is not shared between instances. Therefore, when a new request is
handled by a different instance, it will not have access to the same session data, and the user may see previously viewed news articles.

To solve this problem, a shared session management system should be used that can be accessed by all instances. Google App Engine
provides a few options for session management, such as using Memcache or Cloud Datastore to store the session data. By using a shared
session management system, all instances can access the same session data, and users will not see previously viewed news articles.

upvoted 3 times

? ?  simonab23 5ámonths ago
A is the right answer

upvoted 1 times

? ?  jay9114 6ámonths ago

Where in the code does it show that the session variable is local to just a single instance?

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

A
The most likely cause of the issue described in the code is that the session variable is local to just a single instance. In this code, the
session variable is defined as a local dictionary within the Flask application. This means that it is not shared across different instances of
the application and will not be persisted between requests. As a result, when the application is running on multiple instances, each
instance will have its own local copy of the session variable, and users may see news articles that they have already viewed on other
instances.

upvoted 4 times

? ?  omermahgoub 6ámonths, 1áweek ago

To fix this issue, you could consider using a persistent storage solution, such as Cloud Datastore or Cloud SQL, to store the session data
in a way that is shared across all instances of the application. This would allow you to maintain a consistent view of the session data for
each user across all instances of the application.

Other potential causes for this issue, such as modifying the URL of the API to prevent caching or setting the HTTP Expires header to -1
to stop caching, are not related to the issue described in the code and would not likely address the problem.

upvoted 1 times

? ?  Amrit123_ 6ámonths, 1áweek ago

A is correct

upvoted 1 times

? ?  angelumesh 7ámonths ago

Selected Answer: A

stateful variable should be in firestore (redis).

upvoted 1 times

? ?  Racinely 7ámonths, 2áweeks ago

I agree with ackdbd

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is the correct answer app becoming stateful and it should not be sin case of app engine, cloud run and functions

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. The session variable is local to just a single instance
The others are not relevant

upvoted 1 times

? ?  sgo cial 11ámonths ago

Thank you that was nice explanation

upvoted 2 times

? ?  jay9114 11ámonths, 1áweek ago

Where was this presented in the GCP Architecture training & labs?

upvoted 2 times

? ?  backhand 11ámonths, 1áweek ago

vote A
- rule out C,D not thing to do with problem

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

12/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

- rule out B, Q is not mention datastore

upvoted 2 times

? ?  nicoueron 1áyear ago

Selected Answer: A

A of course, it's just a code pb here

upvoted 1 times

? ?  Superr 1áyear, 1ámonth ago

A seems correct
upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

13/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Topic 1

An application development team believes their current logging tool will not meet their needs for their new cloud-based product. They want a

better tool to capture errors and help them analyze their historical log data. You want to help them  nd a solution that meets their needs.

What should you do?

A. Direct them to download and install the Google StackDriver logging agent

B. Send them a list of online resources about logging best practices

C. Help them de ne their requirements and assess viable logging tools

D. Help them upgrade their current tool to take advantage of any new features

Correct Answer: C

Community vote distribution

C (50%)

A (50%)

? ?  dummyemailforexam  Highly Voted ?  3áyears, 2ámonths ago

A. This is GCP exam. They will always promote their services. Not a third party solution.

upvoted 91 times

? ?  Moophoop 3ámonths, 1áweek ago
Requirements before solutions.

upvoted 3 times

? ?  ShadowLord 9ámonths, 2áweeks ago

How do we know the logging required is for GCE .. not for other GKE. or other services

upvoted 4 times

? ?  ShadowLord 9ámonths, 2áweeks ago

Answer should be C

upvoted 4 times

? ?  zr79 8ámonths, 2áweeks ago

I could not disagree with this. 100%

upvoted 3 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Agreed

upvoted 1 times

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

C should be the correct answer here

upvoted 55 times

? ?  aatt1122 6ámonths, 1áweek ago

The Stackderiver agent currently known as Ops agent is the primary agent for collecting telemetry from your Compute Engine
instances. It needs to be installed on GCP services such as GCE instances in order to collect logs from those instances and send them to
cloud logging and monitoring. https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/installation.

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago
I'm surprised, @MeasService.
I guess you had created the question an sugested ans. A.
Then you wrote "C should be the correct answer here".
Do you change your mind ?

upvoted 7 times

? ?  nitinz 2áyears, 3ámonths ago

their current logging tool will not meet their needs for their new cloud-based product.

How do you know its going to be GCP, or AWS. It can be alibaba cloud. So C makes the most sense unless you are assuming stuff. As
architect you are not supposed to assume.

upvoted 5 times

? ?  kkhurana 1áyear, 5ámonths ago

GCP stackdriver is be installed in another cloud products too.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

14/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 10 times

? ?  tartar 2áyears, 10ámonths ago

I would love to choose B, but need to keep my job..

upvoted 11 times

? ?  Vika 2áyears, 3ámonths ago

What all viable logging you would suggest in this scenario! Cloud operations suite has everything.. cloud logging helps with many
thing..In my mind this question is not meant for being a perfectionist but what would mostly work while option C is an approach
that we will take questions ask about tool. Hence selecting A make sense to me. Thoughts!

upvoted 1 times

? ?  621db32  Most Recent ?  1áweek ago

this is a google test - they will push their own product not a process or a 3rd party product

upvoted 1 times

? ?  bucee 2áweeks, 1áday ago

Selected Answer: A

Stackdriver is Google's logging solution. The answer wouldn't be to find another viable 3rd party logging solution.

upvoted 1 times

? ?  JohnWick2020 3áweeks, 6ádays ago
C. Careful, this is a trick question.
There is no mention of the cloud platform, service or logging solution. Most gcp services integrate with cloud logging OOTB so first check
what there technical requirements are before making unguided assumptions and ultimately picking wrong answer.

upvoted 1 times

? ?  JC0926 2ámonths, 1áweek ago

Selected Answer: C

C. Help them define their requirements and assess viable logging tools

To assist the application development team in finding a suitable logging tool for their cloud-based product, it is essential to understand
their specific requirements. Collaborate with the team to define their needs and evaluate various logging tools based on those criteria.
This approach will ensure that the chosen solution aligns with their expectations and enables effective error capturing and log data
analysis.

upvoted 1 times

? ?  main_street 2ámonths, 2áweeks ago

C is correct,
question is "You want to help them find a solution that meets their needs."

upvoted 1 times

? ?  kratosmat 2ámonths, 2áweeks ago

Selected Answer: C

We don't know wehe is the workload, and we don't know if it is on VM, docker, k8s, so we need to asses and the advice.

upvoted 1 times

? ?  Badri9898 3ámonths ago

The best option in this scenario is to help the development team define their requirements and assess viable logging tools. This allows the
team to clearly identify their needs and determine which logging tool would be best suited for their new cloud-based product. Directing
them to download and install the Google StackDriver logging agent (option A) may not necessarily meet their needs or could limit their
options for other logging tools. Sending them a list of online resources about logging best practices (option B) may not provide the
specific guidance and support they need to select an appropriate tool. Helping them upgrade their current tool (option D) may also not
meet their needs or may not be feasible if their current tool is not extensible. Therefore, the best approach is to work with the team to
identify their requirements and assess various logging tools that can meet those requirements.

upvoted 1 times

? ?  taer 3ámonths ago

Selected Answer: C

install the Google StackDriver logging agent without assessing their specific needs and requirements could potentially lead them to adopt
a solution that is not well-suited to their unique situation

upvoted 2 times

? ?  mifrah 3ámonths ago

I go with C.
Question is "You want to help them find a solution that meets their needs."
Maybe "Direct them to install the Stackdriver Logging Agent" is more like Take it or leave it.

upvoted 1 times

? ?  hakunamatataa 3ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

15/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  JC0926 3ámonths, 3áweeks ago

Selected Answer: C

Directing the team to download and install the Google StackDriver logging agent (option A) may be a valid solution if the team's
requirements align with the features provided by StackDriver, but it is important to first assess their needs before recommending a
specific tool.

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: A

It is a GC exam after all!

upvoted 1 times

? ?  romandrigo 3ámonths, 4áweeks ago

Selected Answer: C

Option C, although option A is true, it provides a solution path for the client, since Stackdriver Logging allows you to store, search, analyze,
monitor, and alert on log data and events from Google Cloud Platform; The first thing I learned when selling something was that if the
client asks for help, you shouldn't overwhelm them with more things to deal with at that moment. To do this, first listen, then guide and
finally propose the options for your choice. Option A for me is very invasive for a client asking for help. This answer is more from
experience than from a technical issue, since option A is valid, but I think that, before reaping, you must sow.

upvoted 1 times

? ?  denytirtonadi 4ámonths ago

Correct answer is C, I got the explanation from Whizlabs.

upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: C

It should be C because we need to know their requirements first and then provide solutions (that can be a Google product)

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

16/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Topic 1

You need to reduce the number of unplanned rollbacks of erroneous production deployments in your company's web hosting platform.

Improvement to the QA/

Test processes accomplished an 80% reduction.

Which additional two approaches can you take to further reduce the rollbacks? (Choose two.)

A. Introduce a green-blue deployment model

B. Replace the QA environment with canary releases

C. Fragment the monolithic platform into microservices

D. Reduce the platform's dependency on relational database systems

E. Replace the platform's relational database systems with a NoSQL database

Correct Answer: AC

Community vote distribution

AC (92%)

8%

? ?  JustJack21  Highly Voted ?  1áyear, 9ámonths ago

D) and E) are pointless in this context.
C) is certainly a good practice.
Now between A) and B)
A) Blue green deployment is an application release model that gradually transfers user traffic from a previous version of an app or
microservice to a nearly identical new releaseùboth of which are running in production.
c) In software, a canary process is usually the first instance that receives live production traffic about a new configuration update, either a
binary or configuration rollout. The new release only goes to the canary at first. The fact that the canary handles real user traffic is key: if it
breaks, real users get affected, so canarying should be the first step in your deployment process, as opposed to the last step in testing in
production. "
While both green-blue and canary releases are useful, B) suggests "replacing QA" with canary releases - which is not good. QA got the
issue down by 80%. Hence A) and C)

upvoted 45 times

? ?  jdpinto  Highly Voted ?  2áyears ago

A & C for me

upvoted 33 times

? ?  thiago286  Most Recent ?  1ámonth, 1áweek ago

I am preparing for Google Professional Cloud Architect exam. I was able to access only limited questions here, if anyone has the entire
questions please share them to my email address: thiagogcp12@gmail.com . Thank you in advance!

upvoted 1 times

? ?  jayeshL 3ámonths, 1áweek ago

A & C , because canery is usefull while doing the testing and once satisfied then only roll out otherwise roll back.

upvoted 1 times

? ?  kuosheng 5ámonths, 1áweek ago

A and C; I don't know what canary release is. Canary is also kind of deployment method, and what do you mean canary release. This
methodology will help you accerlerate your deploying many many small new features. Blue/Green would drop old deployment and create
new ones, and it costs much more than other deployments. I think all the stackholder would take care of it much seriously. Microservice is
for seperating a whole function to many small ones. Every team just takes care of the small ones, I think it will help make much less
deployment rollback.

upvoted 1 times

? ?  AShrujit 6ámonths ago

A C for me

upvoted 1 times

? ?  Jaldhi24 6ámonths ago

Selected Answer: AC

A & C are the best answers

upvoted 1 times

? ?  Nickiii 6ámonths, 1áweek ago

I am preparing for Google Professional Cloud Architect exam. I was able to access only limited questions here, if anyone has the entire
questions please share them to my email address: theupperpec@gmail.com . Thank you in advance!

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

17/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  wences 6ámonths ago

These scammers use the questions to create their corpus and charge people more than ET does.

upvoted 3 times

? ?  roaming_panda 5ámonths, 1áweek ago

couldnot agree more

upvoted 1 times

? ?  roaming_panda 5ámonths, 3áweeks ago

agree wences
upvoted 1 times

? ?  Sukon_Desknot 8ámonths ago

Selected Answer: A

C is not the answer, microservices are good but if its an essential and it goes down, it will still lead to downtime in production. A is the only
answer

upvoted 1 times

? ?  Sukon_Desknot 8ámonths ago

Selected Answer: A

The main purpose of Blue-Green deployments is to eliminate down-time. A is the only answer

upvoted 1 times

? ?  Sukon_Desknot 8ámonths ago

The main purpose of Blue-Green deployments is to eliminate down-time. A is the only answer

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: AC

A & C are the best answers

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A & C for me, Canary release doesn't help here

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: AC

A and C
A - Blue Green is a very good deployment strategy
B - Canary but not on the QA environments. With Canary you usually test in production
C - Correct if you assume they have a monolithic application
D - Not relevant
E - Not relevant
upvoted 3 times

? ?  gee1979 9ámonths, 3áweeks ago

Selected Answer: AC

AC...canary (B) cannot replace QA / Test!!!

- A canary release is the same as a regular release as software is being pushed to production, even if it's to a small subset of users.
- A canary release doesn't guarantee detection of all issues with a small percentage of users using it as users won't be testing the app the
way testers do, or may not be using all the features at a time.

- In certain domains, there is a risk of reputational damage, regulatory violations and lawsuits if a canary release were to impact the user.

- Given the risk associated, having a canary release capability doesnÆt rule out the requirement of an exploratory tester.

upvoted 4 times

? ?  abirroy 10ámonths ago

Selected Answer: AC

A & C for me

upvoted 1 times

? ?  nicoueron 1áyear ago

Selected Answer: AC

Without any suggestion about cost, A is correct and C of course

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

18/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #7

Topic 1

To reduce costs, the Director of Engineering has required all developers to move their development infrastructure resources from on-premises

virtual machines

(VMs) to Google Cloud Platform. These resources go through multiple start/stop events during the day and require state to persist. You have been

asked to design the process of running a development environment in Google Cloud while providing cost visibility to the  nance department.

Which two steps should you take? (Choose two.)

A. Use the - -no-auto-delete  ag on all persistent disks and stop the VM

B. Use the - -auto-delete  ag on all persistent disks and terminate the VM

C. Apply VM CPU utilization label and include it in the BigQuery billing export

D. Use Google BigQuery billing export and labels to associate cost to groups

E. Store all state into local SSD, snapshot the persistent disks, and terminate the VM

F. Store all state in Google Cloud Storage, snapshot the persistent disks, and terminate the VM

Correct Answer: AD

Community vote distribution

AD (56%)

DF (44%)

? ?  [Removed]  Highly Voted ?  1áyear, 9ámonths ago

I spent all morning researching this question. I just popped over and took the GCP Practice exam on Google's website and guess what...
this question was on it word for word, but it had slightly different answers, but not by much here is what I learned. The correct answer is
100% A / D and here is why. On the sample question, the "F" option is gone. "A" is there but slightly reworked, it now says: "Use persistent
disks to store the state. Start and stop the VM as needed" which makes much more sense. The practice exam says A and D are correct.
Given the wording of this question, if A and B, where there then both would be correct because of the word "persistent" and not because
of the flag. The "no-auto-delete" makes A slightly safer than B, but it is the "persistent disk" that makes them right, not the flag. Hope that
helps! F is not right because that is a complex way of solving the issue that by choosing Persistent Disk solves it up front. HTH

upvoted 58 times

? ?  [Removed] 10ámonths, 3áweeks ago

(A) is not sense because the flag is to preserve disk when the istances was deleted, when the istances was stopped the data on
persistend disk are not deleted. So good to know that the response was reworked
(B) is wrong because only on AWS you can terminate istances. On GCP the "terminate" action do not exist .

upvoted 2 times

? ?  XAvenger 1áyear, 8ámonths ago
Thank you, it really helps!!

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Very aptly summarized.

upvoted 1 times

? ?  rishab86  Highly Voted ?  2áyears ago

A and D looks correct as per https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete#--auto-delete ;
https://cloud.google.com/billing/docs/how-to/export-data-bigquery

upvoted 21 times

? ?  RKS_2021 1áyear, 11ámonths ago

-no-auto-delete flag does not have effect on the state of the application. I believe D and F are correct ANS,
https://cloud.google.com/compute/docs/instances/stop-start-instance

upvoted 3 times

? ?  JC0926  Most Recent ?  3ámonths ago
same question, official option:
A. Use persistent disks to store the state. Start and stop the VM as needed.
B. Use the "gcloud --auto-delete" flag on all persistent disks before stopping the VM.
C. Apply VM CPU utilization label and include it in the BigQuery billing export.
D. Use BigQuery billing export and labels to relate cost to groups.
E. Store all state in a Local SSD, snapshot the persistent disks, and terminate the VM.

This question will not be tested, no need to read

upvoted 1 times

? ?  telp 3ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

19/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: AD

AD
Use the flag -no-auto-delete with this flag, the disk won't be delete when the VM is terminated.

Billing export to BigQuery enables you to export your daily usage and cost estimates automatically throughout the day to a BigQuery
dataset you specify. You can then access your billing data from BigQuery.

upvoted 1 times

? ?  JC0926 3ámonths, 3áweeks ago

Selected Answer: DF

DF corerct

upvoted 1 times

? ?  curious_aj 4ámonths, 2áweeks ago

I will go with A & D .F option is not chosen because it is mentioned already that state is stored in google cloud storage why then we will be
taking snapshot first of all.Also, if we do,that would mean huge performance loss . It is not recommended to take snapshots in working
hours ,also recreation of disks from snapshots take time so its just not efficient.All other options are easily eliminated as being dicussed by
multiple persons in this forum.

upvoted 1 times

? ?  AShrujit 6ámonths ago

A & D for me

upvoted 1 times

? ?  Jaldhi24 6ámonths ago

Selected Answer: AD

I agree with A,D which is right choice to cost control

upvoted 1 times

? ?  i_am_robot 6ámonths, 2áweeks ago

D. Use Google BigQuery billing export and labels to associate cost to groups
F. Store all state in Google Cloud Storage, snapshot the persistent disks, and terminate the VM

To provide cost visibility to the finance department, you can use Google BigQuery billing export and labels to associate cost to groups. This
will allow you to track the cost of running the development environment in Google Cloud and understand how it is being used by different
teams or projects.

Storing all state in Google Cloud Storage and snapshotting the persistent disks will allow you to persist the state of the development
environment while also being able to stop and start the VM as needed. When you are finished with the VM, you can terminate it to avoid
incurring additional costs. This will also allow you to easily restore the environment to a previous state if needed.

upvoted 2 times

? ?  jasenmornin 7ámonths ago

Selected Answer: DF

if you do the entire sample questions form in cloud.google.com, you will find that one of the questions is exactly this, and Google himself
marks the correct answers as:
- Use persistent disks to store the state. Start and stop the VM as needed (because persistent disks will not be deleted when an instance is
stopped).
- Use BigQuery billing export and labels to relate cost to groups (is correct because exporting daily usage and cost estimates automatically
throughout the day to a BigQuery dataset is a good way of providing visibility to the finance department. Labels can then be used to
group the costs based on team or cost center).

you can do the test here: https://docs.google.com/forms/d/e/1FAIpQLSdvf8Xq6m0kvyIoysdr8WZYCG32WHENStftiHTSdtW4ad2-
0w/viewform
upvoted 2 times

? ?  jasenmornin 7ámonths ago

if you do the entire sample questions form in cloud.google.com, you will find that one of the questions is exactly this, and Google himself
marks the correct answers as:
- Use persistent disks to store the state. Start and stop the VM as needed (because persistent disks will not be deleted when an instance is
stopped).
- Use BigQuery billing export and labels to relate cost to groups (is correct because exporting daily usage and cost estimates automatically
throughout the day to a BigQuery dataset is a good way of providing visibility to the finance department. Labels can then be used to
group the costs based on team or cost center).

you can do the test here: https://docs.google.com/forms/d/e/1FAIpQLSdvf8Xq6m0kvyIoysdr8WZYCG32WHENStftiHTSdtW4ad2-
0w/viewform
upvoted 2 times

? ?  BobLoblawsLawBlog 8ámonths, 1áweek ago

Selected Answer: DF

A - No, because instances are being stopped, not deleted: "When auto-delete is on, the persistent disk is deleted when the instance it is
attached to is deleted." https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete
B - No, because instances are being stopped, not deleted: "When auto-delete is on, the persistent disk is deleted when the instance it is
attached to is deleted."
https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete
C - No, not only concerned with CPU utilization

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

20/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D - Yes, https://cloud.google.com/billing/docs/how-to/bq-examples
E - No. Storing state to SSD then terminating instance, as E mentions, nukes the SSD https://cloud.google.com/compute/docs/disks/local-
ssd#data_persistence
F - Allows state to persist

upvoted 4 times

? ?  romandrigo 4ámonths, 2áweeks ago

I agree with you on option D:, the other option is A: if you read correctly it says "Use the - -no-auto-delete flag" which means that delete
is disabled.

upvoted 2 times

? ?  Rothmansua 2ádays, 23áhours ago

The question doesn't ask about persiting on deletion. This exam is looking for answers that exactly address requirements without
bringing in redundant complexity they don't ask for.
DF sounds right following the explanation of BobLoblawsLawBlog

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

stopping a VM does not delete the persistent disk, the disk is attached to the VM. The only way to remove the disk is by deleting the VM
where you do not specify to preserve the disk

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I agree with A,D which is right choice to cost control

upvoted 1 times

? ?  Rothmansua 2ádays, 23áhours ago

The question doesn't ask about VM deletion. How A is correct?

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: AD

A is a better answer as the no-auto-delete flag on the persistent disk will add an extra layer of security in case the VM gets deleted. B is not
correct in many different ways.
C is not relevant
D is correct for the costs
E and F are not relevant

upvoted 2 times

? ?  diego_alejandro 11ámonths ago

this is an old question ...no exists no-auto-delete flag on gcp today
https://cloud.google.com/sdk/gcloud/reference/compute/instances/set-disk-auto-delete

upvoted 2 times

? ?  jaxclain 6ámonths, 3áweeks ago

probably you didn't read that article lol, at the top of the article you will find the --no-auto-delete...... lol

upvoted 1 times

? ?  enzonil70 11ámonths, 2áweeks ago

Selected Answer: AD

A is correct because persistent disks will not be deleted when an instance is stopped.
D is correct because exporting daily usage and cost estimates automatically throughout the day to a BigQuery dataset is a good way of
providing visibility to the finance department. Labels can then be used to group the costs based on team or cost center.

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

21/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #8

Topic 1

Your company wants to track whether someone is present in a meeting room reserved for a scheduled meeting. There are 1000 meeting rooms

across 5 o ces on 3 continents. Each room is equipped with a motion sensor that reports its status every second. The data from the motion

detector includes only a sensor ID and several different discrete items of information. Analysts will use this data, together with information about

account owners and o ce locations.

Which database type should you use?

A. Flat  le

B. NoSQL

C. Relational

D. Blobstore

Correct Answer: B

Relational databases were not designed to cope with the scale and agility challenges that face modern applications, nor were they built to take

advantage of the commodity storage and processing power available today.

NoSQL  ts well for:
? Developers are working with applications that create massive volumes of new, rapidly changing data types ?Ç" structured, semi-structured,
unstructured and polymorphic data.

Incorrect Answers:

D: The Blobstore API allows your application to serve data objects, called blobs, that are much larger than the size allowed for objects in the

Datastore service.

Blobs are useful for serving large  les, such as video or image  les, and for allowing users to upload large data  les.

Reference:

https://www.mongodb.com/nosql-explained

Community vote distribution

B (100%)

? ?  clouddude  Highly Voted ?  3áyears, 1ámonth ago

I'll go with B.

This is time series data. We also have no idea what kinds of data are being captured so it doesn't appear structurd.

A does not seem reasonable because a flat file is not easy to query and analyze.
B seems reasonable because this accommodates unstructured data.
C seems unreasonable because we have no idea on the structure of the data.
D seems unreasonable beacause there is no such Google database type.

upvoted 28 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 1áday ago

The requirement to join the data to other data sets implies RDBMS.
BigQuery can handle 1GB/s when streaming inserts, I doubt these 1000 sensors will send that much data.

Bigtable seems over the top and not able to fulfil all the requirements.

upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: B

This will be time-series data. The best DB would be a Big Table (also sensorID can be used in the row key for faster retrieval of data)

upvoted 2 times

? ?  AShrujit 6ámonths ago

B for me

upvoted 1 times

? ?  Jaldhi24 6ámonths ago

Selected Answer: B

B is right

upvoted 1 times

? ?  angelumesh 7ámonths ago

Selected Answer: B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

22/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B (No SQL should be the right answer)

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

surprised by the options given, this is a great use case of Bigtable so NoSQL

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B. NoSQL - unstructured data

upvoted 1 times

? ?  Dhiraj03 1áyear ago

Option B - NO SQL (Unstructured)

upvoted 1 times

? ?  Nirca 1áyear, 2ámonths ago

Selected Answer: B

the data is not structed. or at least in a too simple way, (1 table). So No SQL is a good option.

upvoted 1 times

? ?  sasithra 1áyear, 4ámonths ago

B is correct

upvoted 3 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: B

I got this question on my exam.

upvoted 3 times

? ?  AWS56 1áyear, 5ámonths ago

Selected Answer: B

B is the right answer.

upvoted 1 times

? ?  OrangeTiger 1áyear, 6ámonths ago

B is correct.It good for this solution.
C RDB doesnt suppourt 'several different discrete items'.
A&D is not good for analysis.

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

B is the right answer.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

23/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #9

Topic 1

You set up an autoscaling instance group to serve web tra c for an upcoming launch. After con guring the instance group as a backend service

to an HTTP(S) load balancer, you notice that virtual machine (VM) instances are being terminated and re-launched every minute. The instances do

not have a public IP address.

You have veri ed the appropriate web response is coming from each instance using the curl command. You want to ensure the backend is

con gured correctly.

What should you do?

A. Ensure that a  rewall rules exists to allow source tra c on HTTP/HTTPS to reach the load balancer.

B. Assign a public IP to each instance and con gure a  rewall rule to allow the load balancer to reach the instance public IP.

C. Ensure that a  rewall rule exists to allow load balancer health checks to reach the instances in the instance group.

D. Create a tag on each instance with the name of the load balancer. Con gure a  rewall rule with the name of the load balancer as the source

and the instance tag as the destination.

Correct Answer: C

The best practice when con guration a health check is to check health and serve tra c on the same port. However, it is possible to perform

health checks on one port, but serve tra c on another. If you do use two different ports, ensure that  rewall rules and services running on

instances are con gured appropriately. If you run health checks and serve tra c on the same port, but decide to switch ports at some point, be

sure to update both the backend service and the health check.

Backend services that do not have a valid global forwarding rule referencing it will not be health checked and will have no health status.

Reference:

https://cloud.google.com/compute/docs/load-balancing/http/backend-service

Community vote distribution

C (100%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

"A" and "B" wouldn't turn the VMs on or off, it would jsut prevent traffic. "C" would turn them off if the health check is configured to
terminate the VM is it fails. "D" is the start of a pseudo health check without any logic, so it also isn't an answer because it is like "A" and
"B". Correct Answer: "C"

upvoted 29 times

? ?  AzureDP900 8ámonths, 2áweeks ago

agreed and C is right

upvoted 1 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 14 times

? ?  nitinz 2áyears, 3ámonths ago

C because terminated and relaunch.... something wrong with HC.

upvoted 5 times

? ?  [Removed]  Highly Voted ?  1áyear, 4ámonths ago

Selected Answer: C

I got this question on my exam.

upvoted 6 times

? ?  Kings22903  Most Recent ?  7ámonths ago

Please if anyone has full access to the restricted questions here, you can send them to me at this email address: chikingcj@gmail.com, I
will be writing my exams next week and will need your help, please, I've got no $ to subscribe for full access for now. Thank you.

upvoted 3 times

? ?  angelumesh 7ámonths ago

Selected Answer: C

C (LB Health check should be taken care of)

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is the correct answer

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

24/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: C

C. Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.

upvoted 1 times

? ?  FAD04 10ámonths ago

I got this question on my exam 01/09/2022

upvoted 3 times

? ?  zr79 8ámonths, 2áweeks ago

congrats

upvoted 1 times

? ?  YAS007 1áyear, 2ámonths ago

answer C:
https://cloud.google.com/load-balancing/docs/health-check-concepts#ip-ranges

upvoted 2 times

? ?  AWS56 1áyear, 5ámonths ago

Selected Answer: C

C is corect.

upvoted 2 times

? ?  OrangeTiger 1áyear, 6ámonths ago

C is corect.

upvoted 1 times

? ?  OrangeTiger 1áyear, 6ámonths ago

' (VM) instances are being terminated and re-launched every minute. '
Isn't it because the health check is failing.

A & D Maybe aleady there.curl command passed.
B What are you doing. Absolutely no.

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C.
This questions is in sample quesitons of Google
https://docs.google.com/forms/d/e/1FAIpQLSdvf8Xq6m0kvyIoysdr8WZYCG32WHENStftiHTSdtW4ad2-0w/viewform

upvoted 4 times

? ?  TheCloudBoy77 1áyear, 7ámonths ago

A. Ensure that a firewall rules exists to allow source traffic on HTTP/HTTPS to reach the load balancer. >> not correct, load balancer is not
the issue here.
B. Assign a public IP to each instance and configure a firewall rule to allow the load balancer to reach the instance public IP. >> defeats the
purpose of getting load balancers , not correct
C. Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.>> Correct. if using
different port then appropriate FW rule need to be setup to ensure LB can reach backend instances for healthcheck. if healthcheck traffic
is blcked, instances will be marked unhealthy and will be restarted.
D. Create a tag on each instance with the name of the load balancer. Configure a firewall rule with the name of the load balancer as the
source and the instance tag as the destination.>> tagging is not useful here as the instance is not the source of traffic, just the port need
to be opened on FW.

upvoted 4 times

? ?  vincy2202 1áyear, 7ámonths ago

C is the correct answer.

upvoted 2 times

? ?  unnikrisb 1áyear, 8ámonths ago

Option C
If curl command is working then traffic exists.. So we need to check why health checks are failing.. so firewall issues for health check done
by Google probers

upvoted 2 times

? ?  amxexam 1áyear, 10ámonths ago
Let's go with option elimination
A. Ensure that firewall rules exist to allow source traffic on HTTP/HTTPS to reach the load balancer.
>> We don't need a firewall rule to reach LB but VM in the VPN - eliminate the option
B. Assign a public IP to each instance and configure a firewall rule to allow the load balancer to reach the instance public IP.
>> LB don't need a public IP to reach to VM.
C. Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.
>> Correct
D. Create a tag on each instance with the name of the load balancer. Configure a firewall rule with the name of the load balancer as the
source and the instance tag as the destination.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

25/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

>> N/w tagging not needed just port opening needed to reach to VM from LB. This is when you want to separate some traffic to reach to
particular VM than other https://cloud.google.com/vpc/docs/add-remove-network-tags

upvoted 3 times

? ?  rm_2495 1áyear, 11ámonths ago

C is the answer, as a health check determines if a VM is healthy (thereby stopping).

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

C. Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance group.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

26/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #10

Topic 1

You write a Python script to connect to Google BigQuery from a Google Compute Engine virtual machine. The script is printing errors that it cannot

connect to

BigQuery.

What should you do to  x the script?

A. Install the latest BigQuery API client library for Python

B. Run your script on a new virtual machine with the BigQuery access scope enabled

C. Create a new service account with BigQuery access and execute your script with that user

D. Install the bq component for gcloud with the command gcloud components install bq.

Correct Answer: B

Community vote distribution

C (79%)

B (19%)

? ?  kalschi  Highly Voted ?  3áyears, 7ámonths ago

A - If client library was not installed, the python scripts won't run - since the question states the script reports "cannot connect" - the client
library must have been installed. so it's B or C.

B - https://cloud.google.com/bigquery/docs/authorization an access scope is how your client application retrieve access_token with access
permission in OAuth when you want to access services via API call - in this case, it is possible that the python script use an API call instead
of library, if this is true, then access scope is required. client library requires no access scope (as it does not go through OAuth)

C - service account is Google Cloud's best practice
So prefer C.

upvoted 89 times

? ?  gauravagrawal 2áyears, 4ámonths ago

Right.. B might be right only if C was not the option. so i will go with C as it's recommended google's best practice.

upvoted 4 times

? ?  Musk 3áyears ago

Might be an old version

upvoted 4 times

? ?  MQQNB 10ámonths, 1áweek ago

agree
access scope is enabled by default
https://cloud.google.com/bigquery/docs/authorization#authenticate_with_oauth_20

If you use the BigQuery client libraries, you do not need this information, as this is done for you automatically.

upvoted 1 times

? ?  Vika 2áyears, 4ámonths ago

agreed to comment here . C seems like a good option

upvoted 4 times

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

Why not B? It looks better for me.

upvoted 13 times

? ?  nitinz 2áyears, 3ámonths ago

C, no brainer. You need SA for using API period. Thats where your start your troubleshooting.

upvoted 5 times

? ?  Ery 6ámonths ago

Create a new service account with BigQuery access and execute your script with that user: If you want to run the script on an
existing virtual machine, you can create a new service account with the necessary permissions to access BigQuery and then execute
the script using that service account. This will allow the script to connect to BigQuery and access the data it needs.

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

I stand corrected, B you need to have scope. It is union of Scope + Service Account. If scope is not there, you are screwed anyways.

upvoted 1 times

? ?  tartar 2áyears, 10ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

27/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C is ok

upvoted 11 times

? ?  tartar 2áyears, 10ámonths ago

Sorry, B is ok. You can create service account, add user to service account, and grant the user role as Service Account User. You still
need to enable BigQuery scope to make the Python script running the instance to access BigQuery.

upvoted 14 times

? ?  cloudguy1 2áyears, 10ámonths ago

Stop confusing people, B) doesn't make any sense. Why would you use or create a whole new VM just because of a permission
issue? If anything, just stop the instance and edit the scope of the default Compute Service Account and grant it the role through
IAM. C) is the most appropriate answer since you can only set scopes of the default Compute Service Account, if you're using any
other, there's no scope option - its access is dictated strictly by IAM in such scenario. So C) is the answer: Stop the VM, change the
Service Account with the appropriate permissions and done. B) would still need to have permission the set through IAM & Admin,
the scope isn't enough with the default Compute Service Account.

upvoted 31 times

? ?  lferna 2áyears, 10ámonths ago

B makes all sense. If you do what you say, this is the error:
"Request had insufficient authentication scopes."
You need to enable Bigquery scopes. Please, try with your account and then, share your results.
Answer is definelty C.

upvoted 1 times

? ?  asheesh0574 2áyears, 9ámonths ago

Not sure what you want to say. You started by saying that B makes all sense and then you concluded by saying that Answer
is definitely C. As per Braincert , answer should be B

upvoted 5 times

? ?  techalik 2áyears, 7ámonths ago

Configure the Python API to use a service account with relevant BigQuery access enabled. is the right answer.

It is likely that this service account this script is running under does not have the permissions to connect to BigQuery and that could be
causing issues. You can prevent these by using a service account that has the necessary roles to access BigQuery.

Ref: https://cloud.google.com/bigquery/docs/reference/libraries#cloud-console

A service account is a special kind of account used by an application or a virtual machine (VM) instance, not a person.

Ref: https://cloud.google.com/iam/docs/service-accounts

upvoted 5 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 1áday ago

You would need to create a new VM if you wanted to use the machines default service account as they can only be changed when the VM
is created. But using a custom account with IAM roles is more inline with best practice so should be C.

upvoted 1 times

? ?  thiago286 1ámonth, 1áweek ago

I am preparing for Google Professional Cloud Architect exam. I was able to access only limited questions here, if anyone has the entire
questions please share them to my email address: thiagogcp12@gmail.com . Thank you in advance!

upvoted 1 times

? ?  FigVam 1ámonth, 3áweeks ago

Selected Answer: C

service account, of course. AI agrees with me.

upvoted 1 times

? ?  FigVam 1ámonth, 3áweeks ago

thanks Examtopics to allow the discussion! Some true answers exist only here.

upvoted 1 times

? ?  mifrah 3ámonths ago

I go with C.
Why not B: I must not creat a new VM to enable access scope. Just stop the existing VM and edit the settings.
I think it is Google's Best Practice to use different service accounts instead of the default Comute Engine Service account.

upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: C

ChatGPT says C
upvoted 2 times

? ?  MestreCholas 3ámonths, 4áweeks ago

C. Create a new service account with BigQuery access and execute your script with that user.

In order to connect to BigQuery from a Compute Engine virtual machine, you need to authenticate using a service account that has the
necessary permissions to access BigQuery. Therefore, you should create a new service account with the appropriate BigQuery access

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

28/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

scope, and use the associated credentials to execute your Python script.

Installing the latest BigQuery API client library for Python (option A) is important, but it will not solve the authentication issue.

Running your script on a new virtual machine with the BigQuery access scope enabled (option B) is not a valid solution because the
access scope is associated with a service account, not a virtual machine.

Installing the bq component for gcloud (option D) is not necessary for connecting to BigQuery using Python

upvoted 2 times

? ?  medi01 2ámonths, 1áweek ago

Now it says B...
upvoted 1 times

? ?  Jaldhi24 6ámonths ago

Selected Answer: C

C, You need SA for using API period. Thats where your start troubleshooting.

upvoted 1 times

? ?  markus_de 6ámonths, 3áweeks ago

Selected Answer: B

C sounds good but I struggle with the wording "execute script". The script itself is not executed with the service account. But what is the
case that if the VM is started without Access Scope for BigQuery it wan't work. And the problem is GCP does not allow to update access
scopes on an already running instance.

That means B ist the better answer as aa new VM is needed to enable Access Scope,

upvoted 3 times

? ?  curious_aj 4ámonths, 2áweeks ago

C would be the answer as access scope was earlier way of giving access and it was ofcourse not fine-grained .whereas Svc Account is
newer and fine grained method of giving required access . @markus_de no need to enable aceess scope ,required permissions willbe
given via svc account itself only

upvoted 1 times

? ?  jaxclain 7ámonths ago

Selected Answer: C

Always always always pick Google Best Practices for GCP Exams lol I have about 7+ years working as a Google Workspace Deployment
specialist and internally we use Best Practices so even if creating a new VM could fix the issue, the option will still be C (Create a new
service account with bq access and execute the script with it) lol not sure why some here still fight against Best Practices during exams..
https://cloud.google.com/iam/docs/best-practices-service-accounts

upvoted 6 times

? ?  angelumesh 7ámonths ago

Selected Answer: A

A (Latest client library should fix the connectivity issue)

upvoted 1 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: C

C is correct; A - API client libraries are used to interact with BigQuery core resources such as datasets, tables, jobs, and routines

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  cp82 7ámonths, 4áweeks ago

Selected Answer: C

I would go for C as the logical step forward.

upvoted 1 times

? ?  ckorbet 8ámonths ago

Selected Answer: C

C is the correct one

upvoted 1 times

? ?  FedelloKirfed 8ámonths, 1áweek ago

Selected Answer: C

C - Service accounts with limited access are a best practice.

The use of Access scopes (Option B) is only recommended when using default service accounts, which is not a good practice
recommendation either.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

29/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

30/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #11

Topic 1

Your customer is moving an existing corporate application to Google Cloud Platform from an on-premises data center. The business owners

require minimal user disruption. There are strict security team requirements for storing passwords.

What authentication strategy should they use?

A. Use G Suite Password Sync to replicate passwords into Google

B. Federate authentication via SAML 2.0 to the existing Identity Provider

C. Provision users in Google using the Google Cloud Directory Sync tool

D. Ask users to set their Google password to match their corporate password

Correct Answer: C

Provision users to Google's directory

The global Directory is available to both Cloud Platform and G Suite resources and can be provisioned by a number of means. Provisioned users

can take advantage of rich authentication features including single sign-on (SSO), OAuth, and two-factor veri cation.

You can provision users automatically using one of the following tools and services:

Google Cloud Directory Sync (GCDS)

Google Admin SDK -

A third-party connector -

GCDS is a connector that can provision users and groups on your behalf for both Cloud Platform and G Suite. Using GCDS, you can automate

the addition, modi cation, and deletion of users, groups, and non-employee contacts. You can synchronize the data from your LDAP directory

server to your Cloud Platform domain by using LDAP queries. This synchronization is one-way: the data in your LDAP directory server is never

modi ed.

Reference:

https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#authentication-and-identity

Community vote distribution

B (74%)

C (26%)

? ?  gcp_aws  Highly Voted ?  3áyears, 1ámonth ago

The correct answer is B.
GCDS tool only copies the usernames, not the passwords. And more over strict security requirements for the passwords. Not allowed to
copy them onto Google, I think.

Federation technique help resolve this issue. Please correct me if I am wrong.

upvoted 61 times

? ?  zr79 8ámonths, 2áweeks ago

C is the answer
upvoted 3 times

? ?  ExamTopicsFan 2áyears ago

GCDS synchronises password as well and that is the reason why B is the correct answer. Only in B the password doesn't get copied to
GCP.

upvoted 11 times

? ?  Neferith 10ámonths ago

Passwords are also synchronized:
https://support.google.com/a/answer/6120130?hl=en&ref_topic=2679497

upvoted 6 times

? ?  ha d 3áyears ago

you mistaken GCDS for GSPS, from google site "GSPS won't sync an Active Directory password with a Google Account until it's
changed." this if from google to for GCDC "Using GCDSûThe recommended way to add users to your Google Account in an Active
Directory environment is with Google Cloud Directory Sync (GCDS). GCDS automatically syncs user accounts in your Google domain
with user accounts in your Active Directory system."

upvoted 8 times

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

"A" will syncronise passwords between on pre-mise and the GCP, this duplicates the existing strategy plus Google's "built-in" encryption of
all the data. "B" does not support the moving to GCP. "C" The directory sync tool copies the filesystem settings between servers, UNIX
filesystems

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

31/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

have permission settings built in and passwords to log into the permission groups, syncing these would set GCP up the same way their
on-premises
is, plus Google's "built-in" encryption. "D" disrupts the users, so this is not correct. The debate should be between "A" and "C", "C" includes
"A" according to (https://cloud.google.com/solutions/migrating-consumer-accounts-to-cloud-identity-or-g-suite-best-practices-federation)
so
choose "C"

upvoted 21 times

? ?  cetanx 3áyears ago

GCDS syncs user accounts and some other LDAP attributes but not the passwords, with hybrid connectivity to GCP, SAML (or
federation) is the preferred method.

Answer should be "B"

https://cloud.google.com/solutions/patterns-for-authenticating-corporate-users-in-a-hybrid-environment
https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-synchronizing-user-
accounts#deciding_what_to_provision

upvoted 13 times

? ?  BiddlyBdoyng 1áweek, 1áday ago

The article implies that ADFS is best but suggests you also need the GCDS. This makes sense, you need the users in Google to
allocate permissions but you don't want to copy the passwords across hence ADFS.

upvoted 1 times

? ?  SamirJ 2áyears, 8ámonths ago

GCDS does sync passwords. Please refer - https://support.google.com/a/answer/6120130. Since the question says client wants to
move to GCP , C should be the answer.

upvoted 5 times

? ?  squishy_ shy 1áyear, 5ámonths ago
This is the best answer so far.

upvoted 1 times

? ?  Gobblegobble 2áyears, 11ámonths ago

B is supported read https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-configuring-single-sign-on

upvoted 4 times

? ?  tsys 2áyears, 3ámonths ago

There is no mention SSO is needed.

upvoted 3 times

? ?  tartar 2áyears, 10ámonths ago

B is ok.

upvoted 5 times

? ?  tartar 2áyears, 10ámonths ago

miss typed.. C is ok

upvoted 11 times

? ?  nitinz 2áyears, 3ámonths ago

B, you dont want to store password as per security guidelines provided in question.

upvoted 3 times

? ?  Rothmansua  Most Recent ?  2ádays, 23áhours ago

Selected Answer: C

Federation would connect to existing Identity Provider that runs who knows where.
Using GCDS corporate accounts will create application user identities in GCP and will let you use those identities in the Cloud (as the
question objective implies)

upvoted 1 times

? ?  621db32 1áweek ago

C is the preferred solution in 2023

upvoted 1 times

? ?  nescafe7 2áweeks, 5ádays ago

Selected Answer: C

C seems more appropriate because it meets the requirements and is simple.

upvoted 1 times

? ?  thiago286 1ámonth, 1áweek ago

I am preparing for Google Professional Cloud Architect exam. I was able to access only limited questions here, if anyone has the entire
questions please share them to my email address: thiagogcp12@gmail.com . Thank you in advance!

upvoted 1 times

? ?  ChewSena 2ámonths, 2áweeks ago

C is the correct answer

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

32/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  kratosmat 2ámonths, 2áweeks ago

Selected Answer: C

The federation could be the best option, but it has a strict requirement, the company must have an Identity Provider SAML 2.

upvoted 1 times

? ?  taer 3ámonths ago

Selected Answer: B

B, 100%

upvoted 1 times

? ?  PankajKapse 3ámonths, 1áweek ago

Selected Answer: B

https://partner.cloudskillsboost.google/course_sessions/2653258/video/195360

google cloud directory sync only syncs objects like users and groups without their passwords once a user is synced to cloud identity, it
needs to also have a password or a means of authentication there are a few options to choose from (a) different password for google
cloud account (b) federated identity with on-prem security (c) external identity provider

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

B definitively cannot be a universally correct response due to limitations with GCDS as per the documentation.

Not all password formats are supported. For details, see Additional user attributes.

If you use Active Directory, you can use Password Sync to sync user passwords from Active Directory to your Google domain. For details,
see Sync passwords with Active Directory.

https://support.google.com/a/answer/6120130?hl=en&ref_topic=2679497

upvoted 2 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: B

GCDS is for provisioning, not authentication

upvoted 1 times

? ?  MestreCholas 3ámonths, 4áweeks ago

Selected Answer: B

B) is Right ->Use federated authenticated via SAML 2.0 to the existing identity provider. User's passwords are stored on-premise,
authentication happens on-premise, there is no user disruption, and on successful authentication, the access token is shared to access
application or gcp services.
C) Is wrong. provision users in google using the google cloud directory sinc tool -> with google cloud directory sync, it only hashes the
password as salted SHA512 and gets synced from the source. Plus this may break the strict password requirement. Your credential details
are now stored in 2 places

upvoted 2 times

? ?  JC0926 3ámonths, 4áweeks ago

Selected Answer: B

For this scenario, the most appropriate authentication strategy would be B. Federate authentication via SAML 2.0 to the existing Identity
Provider. This approach can achieve single sign-on and will not cause too much disruption to existing users. Additionally, since there are
strict security team requirements, SAML federated authentication can ensure the security of password storage and transmission.

Other options may cause issues with security and user experience, such as
option A, which may lead to security issues with password synchronization,
option C, which may require password reassignment,
option D, which may cause user confusion or forgotten passwords.

upvoted 1 times

? ?  lokiinaction 4ámonths ago

the question is asking about authentication strategy, not the user management or provisioning strategy, so once users are provisioned
into GCP through GCDS, then the authentication strategy is federating the authentication request.
https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction
so I still don't get why C is the answer...

upvoted 1 times

? ?  CosminCiuc 5ámonths ago

GCDS works with Active Directory. The question does not specify what is the current on-premises Identity Provider. I would think that B is
the correct answer.

upvoted 3 times

? ?  Laso 5ámonths ago

Selected Answer: B

I think the option is B, C doesn't syncronize pwd, only provisioning

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

33/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

34/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #12

Topic 1

Your company has successfully migrated to the cloud and wants to analyze their data stream to optimize operations. They do not have any

existing code for this analysis, so they are exploring all their options. These options include a mix of batch and stream processing, as they are

running some hourly jobs and live- processing some data as it comes in.

Which technology should they use for this?

A. Google Cloud Dataproc

B. Google Cloud Data ow

C. Google Container Engine with Bigtable

D. Google Compute Engine with Google BigQuery

Correct Answer: B

Cloud Data ow is a fully-managed service for transforming and enriching data in stream (real time) and batch (historical) modes with equal

reliability and expressiveness -- no more complex workarounds or compromises needed.

Reference:

https://cloud.google.com/data ow/

Community vote distribution

B (100%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

All four options can accomplish what the question asks, in regards to batching and streaming processes. "A" is for Apache Spark and
Hadoop, a juggernaut in speed of data processing. "B" is Google's best attempt at TIBCO, Ab Initio, and other processing technology, built
explicity for visualizing batch operations and streams without through various labeled circuit boards. "C" and "D" are used within "A" and
"B" and would require more work and higher risk. I'd guess Google wants you to select "B"

upvoted 30 times

? ?  asure 3áyears ago
Well worded

upvoted 2 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 6 times

? ?  bnlcnd 2áyears, 5ámonths ago

Google wants you to select "B" ---- +10000

upvoted 3 times

? ?  nitinz 2áyears, 3ámonths ago

B, dataflow

upvoted 3 times

? ?  2g  Highly Voted ?  3áyears, 5ámonths ago

answer: B

upvoted 6 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek ago

The word analysis throws me off. Wonder if the question is just written incorrectly here? I'd say Dataflow is a key tool to enable the
processing of the data to be able to do the analysis but feels like the final analysis should be in a database.

upvoted 1 times

? ?  alekonko 3ámonths, 1áweek ago

Selected Answer: B

B is the answer
upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: B

A is a managed Hadoop and Spark service. C and D are mostly for petabyte kinds of data. So remains B (suitable for ETL jobs)

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

To analyze a data stream and optimize operations, your company could consider using Google Cloud Dataflow, which is a fully-managed,
cloud-native data processing service that can handle both batch and stream processing.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

35/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Google Cloud Dataflow is designed to handle large volumes of data and can scale up or down automatically to meet the needs of the
workload. It provides a number of pre-built connectors and integrations that make it easy to ingest data from a variety of sources, and it
offers a range of processing options, including batch processing and stream processing, that can be used to analyze the data in real-time.

Option A: Google Cloud Dataproc, option C: Google Container Engine with Bigtable, and option D: Google Compute Engine with Google
BigQuery, while potentially useful for certain types of data processing, would not necessarily be well-suited to handle both batch and
stream processing in the way that Google Cloud Dataflow can

upvoted 2 times

? ?  thamaster 6ámonths, 2áweeks ago

answer is D for me the question is which tool for analyse data. Dataflow does not analyse data

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

ok for B

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

B is the right answer

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is correct

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B. Google Cloud Dataflow

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago

correct is B use data flow for stream and batch process

upvoted 1 times

? ?  Jay_Krish 9ámonths, 4áweeks ago

Selected Answer: B

Answer seems to be B in most other websites as well.

https://cloud.google.com/solutions/authenticating-corporate-users-in-a-hybrid-environment

upvoted 2 times

? ?  Nirca 1áyear, 2ámonths ago

Selected Answer: B

Data flow is Air flow open source project. Is an ETL tool. Exactly for copying source data to destination; online or batch.

upvoted 1 times

? ?  Nirca 1áyear, 2ámonths ago

Selected Answer: B

Data flow is Air flow open source project. Is an ETL tool. Exactly for copying source data to destination; online or batch.

upvoted 1 times

? ?  AWS56 1áyear, 5ámonths ago

Selected Answer: B

Google wants you to select "B" ---- +10000

upvoted 3 times

? ?  anjuagrawal 1áyear, 5ámonths ago

The questions is asking the solution for analysing and not for processing. DataFlow is to process batch and stream data but analysing for
batch and stream is done with BigQuery. I would go with D

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

36/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #13

Topic 1

Your customer is receiving reports that their recently updated Google App Engine application is taking approximately 30 seconds to load for some

of their users.

This behavior was not reported before the update.

What strategy should you take?

A. Work with your ISP to diagnose the problem

B. Open a support ticket to ask for network capture and  ow data to diagnose the problem, then roll back your application

C. Roll back to an earlier known good release initially, then use Stackdriver Trace and Logging to diagnose the problem in a

development/test/staging environment

D. Roll back to an earlier known good release, then push the release again at a quieter period to investigate. Then use Stackdriver Trace and

Logging to diagnose the problem

Correct Answer: C

Stackdriver Logging allows you to store, search, analyze, monitor, and alert on log data and events from Google Cloud Platform and Amazon

Web Services

(AWS). Our API also allows ingestion of any custom log data from any source. Stackdriver Logging is a fully managed service that performs at

scale and can ingest application and system log data from thousands of VMs. Even better, you can analyze all that log data in real time.

Reference:

https://cloud.google.com/logging/

Community vote distribution

C (93%)

7%

? ?  TosO  Highly Voted ?  3áyears, 7ámonths ago

C is the answer
upvoted 24 times

? ?  MyPractice  Highly Voted ?  3áyears, 6ámonths ago

Key word: This behavior was not reported before the update
A - Not Correct as it was working before with same ISP
B - New code update caused an issue- why to open support ticket
C - I agree with C
D - This requires downtime and live prod affected too

upvoted 16 times

? ?  MyPractice 3áyears, 6ámonths ago

"then use Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment" they are NOT asking us
to setup Dev/Text/Stage.. meaning the environment already exist and we have to use it

upvoted 1 times

? ?  ha d 3áyears ago

"then use Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment" this is not asking for
set environment either, it just says to diagnose problem in other environment so C it is

upvoted 1 times

? ?  FigVam  Most Recent ?  1ámonth, 3áweeks ago

Selected Answer: C

should be C

upvoted 1 times

? ?  alekonko 3ámonths, 1áweek ago

Selected Answer: C

C is the answer
upvoted 1 times

? ?  JC0926 3ámonths, 2áweeks ago

Option C is also a valid strategy in this scenario. Rolling back to an earlier known good release initially and using Stackdriver Trace and
Logging to diagnose the problem in a development/test/staging environment can help diagnose the issue without impacting production
users.

However, the reason why option D may be a better approach is that it allows for investigation during a quieter period, which can reduce
the impact of any issues that may occur during the investigation. Rolling back to a known good release and then pushing the release
again at a quieter period can help to ensure that users are not impacted during the investigation.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

37/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

ok for C

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is perfect to troubleshoot latency issues with app

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: C

C. Roll back to an earlier known good release initially, then use Stackdriver Trace and Logging to diagnose the problem in a
development/test/staging environment

A and B are not relevant
D - no IT manager will ever allow re-deployment of erroneous code in production, even in a quiet period...!

upvoted 3 times

? ?  Kiroo 1ámonth, 4áweeks ago

I agree why not D, but in the past I faced issues only reproducible in prd, at that situation D was a possibility but usually yep C is for
sure

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago

correct answer is C use the standard practise

upvoted 1 times

? ?  Amit_arch 9ámonths, 3áweeks ago

Selected Answer: D

How come everyone is agreeing to C!! In option C after rollback, the investigation will happen only on the earlier good release. Whereas in
option D, all the troubleshooting will happen on current/problematic build. Option D should be the right option as it resolves the issue in
short term and provides room for further investigation without downtime.

upvoted 1 times

? ?  BiddlyBdoyng 9ámonths, 1áweek ago

Option C is investigating the bad build in test. The problem with option D is it is user impacting. Always best to attempt to find the
problem in a test environment first. D could end-up being an option of last resort if all attempts to diagnose in test fail but I doubt any
business person would be happy with D as it impacts service.

upvoted 3 times

? ?  zr79 8ámonths, 2áweeks ago

you want to minimize the business loose, best option is to rollback and use stack-driver to diagnose the issue

upvoted 1 times

? ?  p lourenco 1áyear ago

Selected Answer: C

The correct answer is c.

upvoted 1 times

? ?  Nirca 1áyear, 2ámonths ago

Selected Answer: C

C is the answer.
upvoted 2 times

? ?  HeyBuddy95 1áyear, 3ámonths ago

Selected Answer: C

Answer is C

upvoted 1 times

? ?  Pime13 1áyear, 5ámonths ago

Selected Answer: C

choose C

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

C is the correct answer

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

38/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

39/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #14

Topic 1

A production database virtual machine on Google Compute Engine has an ext4-formatted persistent disk for data  les. The database is about to

run out of storage space.

How can you remediate the problem with the least amount of downtime?

A. In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux.

B. Shut down the virtual machine, use the Cloud Platform Console to increase the persistent disk size, then restart the virtual machine

C. In the Cloud Platform Console, increase the size of the persistent disk and verify the new space is ready to use with the fdisk command in

Linux

D. In the Cloud Platform Console, create a new persistent disk attached to the virtual machine, format and mount it, and con gure the

database service to move the  les to the new disk

E. In the Cloud Platform Console, create a snapshot of the persistent disk restore the snapshot to a new larger disk, unmount the old disk,

mount the new disk and restart the database service

Correct Answer: A

On Linux instances, connect to your instance and manually resize your partitions and  le systems to use the additional disk space that you

added.

Extend the  le system on the disk or the partition to use the added space. If you grew a partition on your disk, specify the partition. If your disk

does not have a partition table, specify only the disk ID. sudo resize2fs /dev/[DISK_ID][PARTITION_NUMBER] where [DISK_ID] is the device name

and [PARTITION_NUMBER] is the partition number for the device where you are resizing the  le system.

Reference:

https://cloud.google.com/compute/docs/disks/add-persistent-disk

Community vote distribution

A (86%)

14%

? ?  TosO  Highly Voted ?  3áyears, 7ámonths ago

A is the correct answer because the question says "with minimum downtime"

upvoted 25 times

? ?  passnow  Highly Voted ?  3áyears, 6ámonths ago

least amount of downtime? is the sugar word. You miss that you miss all. Everything there is correct but I believe its only A that fits that
requirement

upvoted 13 times

? ?  raj117 2áyears, 2ámonths ago

but in option A, nowhere it is mentioned to shut down the VM.

upvoted 1 times

? ?  monkeym 1áyear, 11ámonths ago

No need to reboot.

upvoted 1 times

? ?  alekonko  Most Recent ?  3ámonths, 1áweek ago

Selected Answer: A

A is correct, resize disk don't required reboot or downtime
https://cloud.google.com/compute/docs/disks/resize-persistent-disk

upvoted 1 times

? ?  JC0926 3ámonths, 2áweeks ago

Selected Answer: D

Option C is also a valid strategy in this scenario. Rolling back to an earlier known good release initially and using Stackdriver Trace and
Logging to diagnose the problem in a development/test/staging environment can help diagnose the issue without impacting production
users.

However, the reason why option D may be a better approach is that it allows for investigation during a quieter period, which can reduce
the impact of any issues that may occur during the investigation. Rolling back to a known good release and then pushing the release
again at a quieter period can help to ensure that users are not impacted during the investigation.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

A: Increasing the size of the persistent disk in the Cloud Platform Console and using the resize2fs command in Linux.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

40/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Increasing the size of the persistent disk can be done without requiring the virtual machine to be shut down, and the resize2fs command
can be used to resize the ext4 filesystem on the disk to take advantage of the additional space. This will allow you to add more storage
space to the virtual machine without disrupting the database service.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A https://cloud.google.com/compute/docs/disks/resize-persistent-disk?_ga=2.233866652.-3622898.1631303718

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

yes, A is right
upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago

A resize the disk standard command

upvoted 1 times

? ?  abirroy 9ámonths, 2áweeks ago

Selected Answer: A

In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in Linux.

upvoted 1 times

? ?  Kubernetes 10ámonths, 1áweek ago

A is correct

upvoted 1 times

? ?  sgo cial 11ámonths ago
A is correct answer
https://cloud.google.com/compute/docs/disks/resize-persistent-disk?_ga=2.233866652.-3622898.1631303718

upvoted 1 times

? ?  raaj_p 11ámonths ago

The resize2fs command is used to enlarge or shrink an ext2/3/4 file system on a device. You can enlarge a mounted file system, but you
must unmount the file system before you can shrink it. You can specify the desired size of the file system in order to either enlarge or
shrink it.

upvoted 4 times

? ?  belly265 1áyear, 4ámonths ago

A is the correct answer because you can just resize it without any downtime

upvoted 3 times

? ?  PhuocT 1áyear, 6ámonths ago

Selected Answer: A

Vote A

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

A is the correct answer

upvoted 1 times

? ?  rm_2495 1áyear, 11ámonths ago

A is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

41/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #15

Topic 1

Your application needs to process credit card transactions. You want the smallest scope of Payment Card Industry (PCI) compliance without

compromising the ability to analyze transactional data and trends relating to which payment methods are used.

How should you design your architecture?

A. Create a tokenizer service and store only tokenized data

B. Create separate projects that only process credit card data

C. Create separate subnetworks and isolate the components that process credit card data

D. Streamline the audit discovery phase by labeling all of the virtual machines (VMs) that process PCI data

E. Enable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the auditor

Correct Answer: A

Reference:

https://www.sans.org/reading-room/whitepapers/compliance/ways-reduce-pci-dss-audit-scope-tokenizing-cardholder-data-33194

Community vote distribution

A (100%)

? ?  AD2AD4  Highly Voted ?  3áyears, 1ámonth ago

Final Decision to go with Option A. I have done PCI DSS Audit for my project and thats the best suited case. 100% sure to use tokenised
data instead of actual card number

upvoted 37 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I agree. A is the best option

upvoted 2 times

? ?  Musk 3áyears ago

But with A you cannot extract statistics. That is the second r4equirement.

upvoted 4 times

? ?  RitwickKumar 10ámonths, 2áweeks ago

You can as the generated token for a given credit card would be same(generally but there are approaches which can give you
different token for the same sensitive data input). Only thing that you won't know is the actual card number which is not required
for the trend analysis.

When the trend analysis involves referential integrity then tokenization process becomes challenging but still once data is tokenized
correctly you should be able to perform any kind of the analysis.

upvoted 2 times

? ?  Musk 2áyears, 11ámonths ago

Thinking about that better, I think you can because you are only tokenizing the sensitive data, not the transaction type.

upvoted 2 times

? ?  Arimaverick 2áyears, 5ámonths ago

Analyzing Transaction does not require Credit Card number I guess. Only amount of transaction or balance what is needed. We also
perform something similar with transactional data with tokenized PII information. So CC can be tokenized. So answer should be A.

upvoted 4 times

? ?  omermahgoub  Highly Voted ?  6ámonths, 1áweek ago

To minimize the scope of Payment Card Industry (PCI) compliance while still allowing for the analysis of transactional data and trends
related to payment methods, you should consider using a tokenizer service and storing only tokenized data, as described in option A.

Tokenization is a process of replacing sensitive data, such as credit card numbers, with unique, randomly-generated tokens that cannot be
used for fraudulent purposes. By using a tokenizer service and storing only tokenized data, you can reduce the scope of PCI compliance to
only the tokenization service, rather than the entire application. This can help minimize the amount of sensitive data that needs to be
protected and reduce the overall compliance burden.

upvoted 13 times

? ?  oxfordcommaa 5ámonths, 1áweek ago

man, this is an amazing answer. props

upvoted 2 times

? ?  KjChen  Most Recent ?  7ámonths, 3áweeks ago

Selected Answer: A

https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

42/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  andreavale 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. Create a tokenizer service and store only tokenized data

upvoted 1 times

? ?  BiddlyBdoyng 9ámonths, 1áweek ago

B appears the most thorough but the question asks to comply with the smallest scope, network segmentation is not a must. Tokenization
is simpler. C is similar to B, more than required. D & E do not address the problem.

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago

correct answer is A use tokenize

upvoted 2 times

? ?  abirroy 9ámonths, 2áweeks ago

Selected Answer: A

Correct answer A
upvoted 1 times

? ?  Nirca 1áyear, 2ámonths ago

Selected Answer: A

The mandatory stage in PCI is having a encryption/ description system. Data must not be stored as is with PAN. So A IS A MUST. The rest
are nice to have.
upvoted 1 times

? ?  ryzior 1áyear, 2ámonths ago

Selected Answer: A

I think it should be A and C - the paper states clearly, a proper network segmentation is still required to disparate the vault and token
servers from the rest of the flat network.

upvoted 1 times

? ?  sjmsummer 1áyear, 5ámonths ago

I chose A. But why C is not good?

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

A is the correct answer
https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss#a_service_for_handling_sensitive_information

upvoted 3 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A

upvoted 2 times

? ?  TheCloudBoy77 1áyear, 7ámonths ago

A - PCI DSS compliance can be a pain, tokenisation is one way of dealing with it.

upvoted 1 times

? ?  Bhagesh 1áyear, 9ámonths ago

A
Tokenization: A process that replaces the primary account number (PAN) with a surrogate value called a token. The PAN is then stored in a
secure lookup. De-tokenization is the reverse process of looking up a PAN by its token. A token can either be a hash or an assigned value.
https://cloud.google.com/architecture/pci-dss-compliance-in-gcp

upvoted 2 times

? ?  amxexam 1áyear, 10ámonths ago

A as per GCP document https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss

upvoted 3 times

? ?  DreamerK 1áyear, 11ámonths ago

In this question, the analysis is on the payment method. Thus sensitive information like credit card number, holder name,etc is not
needed for the analysis, but also should not be visible to the analyzer. Therefore, it makes perfect sense to tokenize these sensitive
information while not affecting the purpose to analyze the payment method. In this sense, D is not correct since Big Query ACL can only
control access at the table level, not at column level.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

43/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #16

Topic 1

You have been asked to select the storage system for the click-data of your company's large portfolio of websites. This data is streamed in from a

custom website analytics package at a typical rate of 6,000 clicks per minute. With bursts of up to 8,500 clicks per second. It must have been

stored for future analysis by your data science and user experience teams.

Which storage infrastructure should you choose?

A. Google Cloud SQL

B. Google Cloud Bigtable

C. Google Cloud Storage

D. Google Cloud Datastore

Correct Answer: B

Google Cloud Bigtable is a scalable, fully-managed NoSQL wide-column database that is suitable for both real-time access and analytics

workloads.

Good for:
? Low-latency read/write access
? High-throughput analytics
? Native time series support
Common workloads:
? IoT,  nance, adtech
? Personalization, recommendations
? Monitoring
? Geospatial datasets
? Graphs
Incorrect Answers:

C: Google Cloud Storage is a scalable, fully-managed, highly reliable, and cost-e cient object / blob store.

Is good for:
? Images, pictures, and videos
? Objects and blobs
? Unstructured data
D: Google Cloud Datastore is a scalable, fully-managed NoSQL document database for your web and mobile applications.

Is good for:
? Semi-structured application data
? Hierarchical data
? Durable key-value data
? Common workloads:
? User pro les
? Product catalogs
? Game state
Reference:

https://cloud.google.com/storage-options/

Community vote distribution

B (92%)

8%

? ?  victory108  Highly Voted ?  1áyear, 12ámonths ago

B. Google Cloud Bigtable

upvoted 11 times

? ?  jeff001  Highly Voted ?  2áyears, 2ámonths ago

B, Bigtable due to the IoT like requirements

upvoted 7 times

? ?  hiromi  Most Recent ?  2ámonths, 3áweeks ago

Selected Answer: B

B is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

44/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  alekonko 3ámonths, 1áweek ago

Selected Answer: B

Bigtable is a high-perf NoSQL db service that handle large volumes of structured data with low latency

upvoted 1 times

? ?  zerg0 5ámonths ago

Selected Answer: B

The Google Cloud Bigtable goes together with the BigQuery. The question itself gives away a bit.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

For storing click-data that is streamed in at a rate of 6,000 clicks per minute, with bursts of up to 8,500 clicks per second, and that needs to
be stored for future analysis by your data science and user experience teams, you should consider using a scalable, high-performance,
and low-latency NoSQL database such as Google Cloud Bigtable, option B.

Google Cloud Bigtable is a fully managed, high-performance NoSQL database service that is designed to handle large volumes of
structured data with low latency. It is well-suited for storing high-velocity data streams and can scale to handle millions of reads and writes
per second.

Option A: Google Cloud SQL, option C: Google Cloud Storage, and option D: Google Cloud Datastore, would not be suitable for this use
case, as they are not designed to handle high-velocity data streams at this scale.

upvoted 4 times

? ?  i_am_robot 6ámonths, 2áweeks ago

B. Google Cloud Bigtable

Google Cloud Bigtable is a scalable, high-performance NoSQL database that is well-suited for storing large amounts of data with low
latency. It is designed for high-throughput workloads such as streaming data, and is able to handle bursts of up to millions of reads and
writes per second.

Given the high volume of click data that needs to be stored and the requirement for low latency, Google Cloud Bigtable would be a good
choice for storing the data. It is able to handle the high rate of incoming data and provide fast access to the data for analysis by the data
science and user experience teams.

Google Cloud SQL is a fully-managed relational database service, and may not be the best choice for storing high-volume streaming data.
Google Cloud Storage is an object storage service, and may not provide the necessary performance for storing and querying large
amounts of data in real-time. Google Cloud Datastore is a NoSQL document database, and while it may be suitable for storing large
amounts of data, it may not provide the necessary performance for handling high volumes of streaming data.

upvoted 2 times

? ?  Melampos 6ámonths, 3áweeks ago

Selected Answer: B

https://cloud.google.com/bigtable#section-9

upvoted 1 times

? ?  Bry_040706 7ámonths ago
B. Google Cloud Bigtable

upvoted 1 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: B

Cloud Bigtable has all the features to fulfill the requirements mentioned in the question

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

D is right answer
upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

Bigtable for a high volume of writes. 8500 clicks per sec

upvoted 3 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B. Google Cloud Bigtable

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago
for analystics choose big table

upvoted 2 times

? ?  chickennuggets 10ámonths, 2áweeks ago

They don't call it Bigtable for no reason - high throughput and low latency. GCS not enough performance. SQL & datastore not good fit

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

45/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  AashishAggarwal2000 10ámonths, 3áweeks ago

B. Google Cloud Bigtable because it supports :
a) the low latency + high throughput workloads are required in this use-case to support 8500 clicks events per second.
b) the OLAP use case to integrate and analyse this data with various ML and data-science services.

upvoted 3 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right

upvoted 1 times

? ?  sgo cial 11ámonths ago

I would go with Bigtable, because here question is stressing on huge volume of data writes and for analysis pupose....bigtable supports
heavy reads/writes within seconds

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

46/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #17

Topic 1

You are creating a solution to remove backup  les older than 90 days from your backup Cloud Storage bucket. You want to optimize ongoing

Cloud Storage spend.

What should you do?

A. Write a lifecycle management rule in XML and push it to the bucket with gsutil

B. Write a lifecycle management rule in JSON and push it to the bucket with gsutil

C. Schedule a cron script using gsutil ls ?Ç"lr gs://backups/** to  nd and remove items older than 90 days

D. Schedule a cron script using gsutil ls ?Ç"l gs://backups/** to  nd and remove items older than 90 days and schedule it with cron

Correct Answer: B

Community vote distribution

B (100%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

All four are correct answers. Google has built in cron job schduling with Cloud Schedule, so that would place "D" behind "C" in Google's
perspective. Google also has it's own lifecycle management command line prompt gcloud lifecycle so "A" or "B" could be used. JSON is
slightly faster than XML because of the "{" verse "<c>" distinguisher, with a Trie tree used for alphanumeric parsing. So between "A" and
"B", choose "B". Between "B" and "A", "B" is slightly more efficient from the GCP operator perspective. So choose "B".

upvoted 33 times

? ?  nitinz 2áyears, 3ámonths ago

B is correct. Policy = JSON format. No matter if its AWS or GCP.

upvoted 8 times

? ?  ghitesh 3áyears, 5ámonths ago

gsutil command takes only json as input for lifecycle management. In case of API, both XML and json can be used.
https://cloud.google.com/storage/docs/gsutil/commands/lifecycle
https://cloud.google.com/storage/docs/xml-api/put-bucket-lifecycle
https://cloud.google.com/storage/docs/json_api/v1/buckets/update

upvoted 26 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 7 times

? ?  clouddude  Highly Voted ?  3áyears, 1ámonth ago

I'll go with B.
A is not reasonable because life cycle policies are not written in XML.
B is reasonable and is cloud native.
C requires a cron script which needs something to run the script and is a non-cloud native approach.
D requires a cron script which needs something to run the script and is a non-cloud native approach.

upvoted 14 times

? ?  alekonko  Most Recent ?  3ámonths, 1áweek ago

Selected Answer: B

B, gsutil can set policy using json file
https://cloud.google.com/storage/docs/gsutil/commands/lifecycle#examples

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

To remove backup files older than 90 days from a Cloud Storage bucket and optimize ongoing Cloud Storage spend, you should consider
writing a lifecycle management rule in JSON and pushing it to the bucket with gsutil, as described in option B.

Lifecycle management rules allow you to automatically delete objects from a Cloud Storage bucket based on age or other criteria, such as
the object's storage class. By writing a rule in JSON and pushing it to the bucket with gsutil, you can specify that objects older than 90 days
should be deleted, ensuring that the bucket only contains current backup files and minimizing Cloud Storage spend.

Option A, C and D would not be suitable for this use case, as they do not allow you to specify lifecycle management rules that delete
objects based on age.

upvoted 2 times

? ?  i_am_robot 6ámonths, 2áweeks ago

B. Write a lifecycle management rule in JSON and push it to the bucket with gsutil

To remove backup files older than 90 days from a Cloud Storage bucket, you can use the lifecycle management feature in Cloud Storage.
This feature allows you to specify rules to automatically delete objects based on their age.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

47/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  Melampos 6ámonths, 3áweeks ago

Selected Answer: B

https://cloud.google.com/storage/docs/gsutil/commands/lifecycle

upvoted 1 times

? ?  Bry_040706 7ámonths ago

B. Life cycle management using JSON.

upvoted 1 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: B

B with JSON option is correct

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

B is the right answer

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

life cycle management is the answer written in JSON format. JSON is easier to write and read compared to XML which you can not use in
commands

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Lifecycle management with JSON is right .. I will go with B

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B. Write a lifecycle management rule in JSON and push it to the bucket with gsutil

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago
D looks correct schedule cron

upvoted 1 times

? ?  Angel_99 10ámonths, 2áweeks ago
B is the best practice option.

upvoted 1 times

? ?  Angel_99 10ámonths, 2áweeks ago

Selected Answer: B

B is the best practice option.

upvoted 1 times

? ?  andrelsjunior 1áyear ago

Selected Answer: B

B is fine

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

48/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #18

Topic 1

Your company is forecasting a sharp increase in the number and size of Apache Spark and Hadoop jobs being run on your local datacenter. You

want to utilize the cloud to help you scale this upcoming demand with the least amount of operations work and code change.

Which product should you use?

A. Google Cloud Data ow

B. Google Cloud Dataproc

C. Google Compute Engine

D. Google Kubernetes Engine

Correct Answer: B

Google Cloud Dataproc is a fast, easy-to-use, low-cost and fully managed service that lets you run the Apache Spark and Apache Hadoop

ecosystem on Google

Cloud Platform. Cloud Dataproc provisions big or small clusters rapidly, supports many popular job types, and is integrated with other Google

Cloud Platform services, such as Google Cloud Storage and Stackdriver Logging, thus helping you reduce TCO.

Reference:

https://cloud.google.com/dataproc/docs/resources/faq

Community vote distribution

B (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago
"B. Google Cloud Dataproc" is the answer

upvoted 18 times

? ?  VinayakBudapanahalli  Highly Voted ?  2áyears, 5ámonths ago

Dataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying,
streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning
clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data.
https://cloud.google.com/dataproc/docs/concepts/overview#:~:text=Dataproc%20is%20a%20managed%20Spark,%2C%20streaming%2C%
20and%20machine%20learning.&text=With%20less%20time%20and%20money,your%20jobs%20and%20your%20data.

upvoted 10 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Agreed

upvoted 1 times

? ?  alekonko  Most Recent ?  3ámonths, 1áweek ago

Selected Answer: B

B, Dataproc is Hadoop/Spark managed service in GCP

upvoted 1 times

? ?  examch 6ámonths, 1áweek ago

Selected Answer: B

Dataproc is a fully managed and highly scalable service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+ open
source tools and frameworks. Use Dataproc for data lake modernization, ETL, and secure data science, at scale, integrated with Google
Cloud, at a fraction of the cost.

https://cloud.google.com/dataproc

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

To scale the number and size of Apache Spark and Hadoop jobs being run on a local datacenter with the least amount of operations work
and code change, you should consider using Google Cloud Dataproc, option B. Google Cloud Dataproc is a fully-managed service that
makes it easy to run Apache Spark and Hadoop workloads in the cloud. It is designed to simplify the process of setting up and managing
clusters for data processing, and allows you to scale quickly and easily as demand increases.

With Cloud Dataproc, you can create and delete clusters in just a few minutes, and you can use the familiar Apache Spark and Hadoop
APIs and tools to process data. This means that you can utilize the cloud to scale your workloads with minimal changes to your code and
operations work.

Option A: Google Cloud Dataflow, option C: Google Compute Engine, and option D: Google Kubernetes Engine, would not be suitable for
this use case, as they do not provide the same level of support for running Apache Spark and Hadoop workloads as Cloud Dataproc.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

49/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: B

B. Dataproc is managed Apache Spark and Hadoop in GCP

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

Dataproc for Hadoop and spark ecosystem

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B. Google Cloud Dataproc

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago

B data proc for hadoop and spark

upvoted 1 times

? ?  Dhiraj03 1áyear ago

Keyword - Apache Spark and Hadoop jobs - Go with Dataproc

upvoted 1 times

? ?  Superr 1áyear ago

Selected Answer: B

dataproc

upvoted 1 times

? ?  Nirca 1áyear, 2ámonths ago

Selected Answer: B

Google Cloud Dataproc == managed Spark and Hadoop service

upvoted 2 times

? ?  pakochiu 1áyear, 2ámonths ago

Selected Answer: B

B - Dataproc Lift&Shift of Apache Spark and Hadoop jobs

upvoted 1 times

? ?  llanerox 1áyear, 5ámonths ago

B is ok.

upvoted 1 times

? ?  OrangeTiger 1áyear, 6ámonths ago
B Cloud Data Proc?is correct.
Cloud Data Proc can easly migration form hadoop , spark.

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B.

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

Answer is B

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

50/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #19

Topic 1

The database administration team has asked you to help them improve the performance of their new database server running on Google Compute

Engine. The database is for importing and normalizing their performance statistics and is built with MySQL running on Debian Linux. They have an

n1-standard-8 virtual machine with 80 GB of SSD persistent disk.

What should they change to get better performance from this system?

A. Increase the virtual machine's memory to 64 GB

B. Create a new virtual machine running PostgreSQL

C. Dynamically resize the SSD persistent disk to 500 GB

D. Migrate their performance metrics warehouse to BigQuery

E. Modify all of their batch jobs to use bulk inserts into the database

Correct Answer: C

Community vote distribution

C (78%)

7%

Other

? ?  shandy  Highly Voted ?  3áyears, 7ámonths ago

Answer is C because persistent disk performance is based on the total persistent disk capacity attached to an instance and the number of
vCPUs that the instance has. Incrementing the persistent disk capacity will increment its throughput and IOPS, which in turn improve the
performance of MySQL.

upvoted 56 times

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

Assuming that the database is approaching its hardware limits... both options A and C would improve performance, A would increase
number of CPUs and memory, but C would increase memory by more. If it a software problem, it is likly it is a hashing problem (the search
and sort algorithms are not specific enough to search within the database). This problem would not be fixed just by migrating to
PostgreSQL or BigQuery but modifying the inserts would help the situation because it would entail specifications of data lookups.
However, it wouldn't help with search performance just inserts and it doesn't help in normalization. So B, D, and E are eliminated. Since
statistics is based on sets, the larger the number of sets the better the predictions. This means that the largest amount of memory would
not only increase computer performance but also knowledge enhancements. So C beats A.

upvoted 32 times

? ?  AzureDP900 8ámonths, 2áweeks ago
Nice explanation, I will go with C

upvoted 5 times

? ?  trainor 2áyears, 6ámonths ago

Also, if you increased the memory size, it would not be a n1-standard-8 anymore. You should eventually change machine type, not
simply increase memory.

upvoted 4 times

? ?  Matro71 2áyears, 11ámonths ago

The IOPS on SSD on GCP increase with the size.

upvoted 11 times

? ?  tartar 2áyears, 10ámonths ago

C is ok.

upvoted 8 times

? ?  JC0926  Most Recent ?  2ámonths, 1áweek ago

Selected Answer: C

C. Dynamically resize the SSD persistent disk to 500 GB

By increasing the size of the SSD persistent disk, the database server can achieve better performance. A larger SSD persistent disk
provides higher IOPS (input/output operations per second) and throughput, allowing for faster read and write operations. This can help
improve the performance of the MySQL database server running on the Google Compute Engine instance.

upvoted 1 times

? ?  mifrah 3ámonths ago

On another website I found the question with the hint "you are not allowed to reboot the VM before next maintenance window". That
makes it more clear --> C.

upvoted 1 times

? ?  JC0926 3ámonths, 2áweeks ago

Selected Answer: E

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

51/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

E. Modify all of their batch jobs to use bulk inserts into the database: This can be a very effective solution for improving performance. Bulk
inserts can greatly reduce the number of round-trips to the database, which can help to minimize latency and improve overall throughput.

Therefore, option E is the best choice for improving performance in this scenario.

upvoted 2 times

? ?  Jackalski 6ámonths, 3áweeks ago

Selected Answer: D

in option C - even increasing disc can gain performance - that will take few months to face new limits. mySQL is not desiged for
OLAP/analytics - but OLTP.
so I vote on D
upvoted 2 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: C

Correct answer is C. Increased disk capacity improved I/O and direct impacts the performance

upvoted 1 times

? ?  BobLoblawsLawBlog 8ámonths, 1áweek ago

Selected Answer: C

C, because...
N1 8cpu max IOPS = 15,000 https://cloud.google.com/compute/docs/disks/performance#n1_vms

SSD persistent disks can reach up to 30 IOPS per GB of disk. https://cloud.google.com/compute/docs/disks/performance#example
80 GB X 30 IOPS = 2,400 IOPS
500 GB (answer C) X 30 IOPS = 15,000 IOPS = N1 8 cpu max IOPS

upvoted 9 times

? ?  zr79 8ámonths, 2áweeks ago

adding memory to VM will need to be shut down which means the business will be impacted, not good for any option
remember this for your exam

upvoted 4 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: C

C. Dynamically resize the SSD persistent disk to 500 GB

upvoted 1 times

? ?  Amit_arch 9ámonths, 3áweeks ago

Selected Answer: A

The ask is to improve the performance of this system. MySQL server or any relational DB server for this matter gains in performance from
memory increase, as it provides to have larger number of queries be cached. As far system is not just writes, QC always results in better
performance. In an ideal scenario if possible a memory equal to DB size, will give highest possible performance.

upvoted 1 times

? ?  szanio 1áyear ago

answer: C

upvoted 1 times

? ?  ryzior 1áyear, 1ámonth ago

https://cloud.google.com/compute/docs/disks/performance
there is no relationship between a disk size and IOPS/Thruput , just the disk type and vcpu has any significance. The question gives no
details which type of performance improvement they expect (batch loads or data 'normalization' or report generation) so you can only
guess. I'd go with RAM but I have no idea what is the current RAM utilization, I'd go with separate disks if the batch loads are in scope, but
80GB for an SQL server sounds like a joke as well , since this is the cheapest "let'st try and see" method, Id go first with the disk. :)

upvoted 1 times

? ?  Chute5118 11ámonths ago

There is a table that shows the "maximum sustained IOPS" and how it varies with "Read IOPS per GB" and "Write IOPS per GB".

https://cloud.google.com/compute/docs/disks/performance#zonal-persistent-disks

upvoted 4 times

? ?  elaineshi 1áyear, 1ámonth ago

There is, in the page, it's said "Persistent disk performance scales with the size of the disk and with the number of vCPUs on your VM
instance."

upvoted 3 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

Going with C

upvoted 1 times

? ?  Nirca 1áyear, 2ámonths ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

52/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

The answer is C.
IOPs and VCPU are linked closely together.
In our case we have a server with 8VCPs.
GCP best practice is ~ 1 VCPU for 2000-2500 IOPS.
500G PD SSD can produce 15,000 IOPS with 8 VCPs it will work - best practice mode.
If the server was < 8VCPs the answer (C) might not be correct.

Documentation to support:
https://cloud.google.com/compute/docs/disks/performance#optimize_disk_performance

upvoted 4 times

? ?  nm97 1áyear, 3ámonths ago

This question came up on the official google sample review question forum. The correct answer is C.

upvoted 2 times

? ?  tluu 1áyear, 3ámonths ago

Answer is C
A is not correct because increasing the memory size requires a VM restart.

B is not correct because the DB administration team is requesting help with their MySQL instance. Migration to a different product should
not be the solution when other optimization techniques can still be applied first.

C is correct because persistent disk performance is based on the total persistent disk capacity attached to an instance and the number of
vCPUs that the instance has. Incrementing the persistent disk capacity will increment its throughput and IOPS, which in turn improve the
performance of MySQL.

D is not correct because the DB administration team is requesting help with their MySQL instance. Migration to a different product should
not be the solution when other optimization techniques can still be applied first.

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

53/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #20

Topic 1

You want to optimize the performance of an accurate, real-time, weather-charting application. The data comes from 50,000 sensors sending 10

readings a second, in the format of a timestamp and sensor reading.

Where should you store the data?

A. Google BigQuery

B. Google Cloud SQL

C. Google Cloud Bigtable

D. Google Cloud Storage

Correct Answer: C

Google Cloud Bigtable is a scalable, fully-managed NoSQL wide-column database that is suitable for both real-time access and analytics

workloads.

Good for:
? Low-latency read/write access
? High-throughput analytics
? Native time series support
Common workloads:
? IoT,  nance, adtech
? Personalization, recommendations
? Monitoring
? Geospatial datasets
? Graphs
Reference:

https://cloud.google.com/storage-options/

Community vote distribution

C (100%)

? ?  victory108  Highly Voted ?  1áyear, 12ámonths ago

C. Google Cloud Bigtable

upvoted 10 times

? ?  khadar 9ámonths, 3áweeks ago

I too got this question in 10-09-22 exam with similar option and result is pass

upvoted 3 times

? ?  alekonko  Most Recent ?  3ámonths, 1áweek ago

Selected Answer: C

BigTable is NoSQL for IoT

upvoted 1 times

? ?  sivaamum 3ámonths, 4áweeks ago

C is correct

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

To optimize the performance of an accurate, real-time, weather-charting application that receives data from 50,000 sensors sending 10
readings per second, it would be most appropriate to store the data in a distributed, horizontally scalable, NoSQL database such as
Google Cloud Bigtable
Other options, such as Google BigQuery, Google Cloud SQL, and Google Cloud Storage, may not be as well-suited for handling high
volumes of real-time data and may not provide the same level of performance and scalability as Google Cloud Bigtable.

upvoted 1 times

? ?  Bry_040706 7ámonths ago
C. Bigtable, IoT data.

upvoted 1 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: C

C Bigtable

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

54/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C bigtable right answer

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

real-time, IoT, time series and huge writes are some of the keywords to look after for Bigtable

upvoted 3 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: C

C. Google Cloud Bigtable

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago

C big table for IOT data

upvoted 1 times

? ?  abirroy 10ámonths, 1áweek ago

Selected Answer: C

Google Cloud Bigtable

upvoted 1 times

? ?  Dhiraj03 1áyear ago

Keyword - Timestamp - Big table

upvoted 2 times

? ?  szanio 1áyear ago

Go for c

upvoted 1 times

? ?  Nirca 1áyear, 2ámonths ago

Selected Answer: C

C. Google Cloud Bigtable is the Best Practice option

upvoted 1 times

? ?  belly265 1áyear, 4ámonths ago

Ans c - when ever thier is input from IOT devices across and time series data which is huge go for big table in gcp

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Big Table is right choice, hence C is correct

upvoted 1 times

? ?  Narinder 1áyear, 5ámonths ago

Google Cloud Big Table is best for the use-case to store the time-series data, so C is correct

upvoted 1 times

? ?  OrangeTiger 1áyear, 6ámonths ago

I choose C.
A Big Query Seems good.But keyword 'IOT' is here.
B. Google Cloud SQL does'nt work this case.
D. Google Cloud Storage does'nt suppourt realtime analytics.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

55/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #21

Topic 1

Your company's user-feedback portal comprises a standard LAMP stack replicated across two zones. It is deployed in the us-central1 region and

uses autoscaled managed instance groups on all layers, except the database. Currently, only a small group of select customers have access to the

portal. The portal meets a

99,99% availability SLA under these conditions. However next quarter, your company will be making the portal available to all users, including

unauthenticated users. You need to develop a resiliency testing strategy to ensure the system maintains the SLA once they introduce additional

user load.

What should you do?

A. Capture existing users input, and replay captured user load until autoscale is triggered on all layers. At the same time, terminate all

resources in one of the zones

B. Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce ?Çchaos?Ç to

the system by terminating random resources on both zones

C. Expose the new system to a larger group of users, and increase group size each day until autoscale logic is triggered on all layers. At the

same time, terminate random resources on both zones

D. Capture existing users input, and replay captured user load until resource utilization crosses 80%. Also, derive estimated number of users

based on existing user's usage of the app, and deploy enough resources to handle 200% of expected load

Correct Answer: B

Community vote distribution

B (79%)

A (18%)

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

resilience test is not about load, is about terminate resources and service not affected. Think it's B. The best for resilience in to introduce
chaos in the infraestructure

upvoted 77 times

? ?  rockstar9622 3áyears, 5ámonths ago

I agree with @jcmoranp, B) is correct for more info - https://cloud.google.com/solutions/scalable-and-resilient-
apps#test_your_resilience

upvoted 17 times

? ?  AWSPro24 1áyear, 7ámonths ago

Isn't A superior in one way. It will demonstrate that the app is regionally redundant by demonstrating it can survive the loss of an entire
zone. B only demonstrates the app is zonally redundant and can lose a random instance here and there within individual zones which is
not that resilient. Thoughts?

upvoted 8 times

? ?  0xE8D4A51000 8ámonths, 1áweek ago

No. It is only terminating the service in ONE zone. B caters for terminating the service in both zones randomly. You want to be able
to test resiliency when either zone has an outage.

upvoted 6 times

? ?  OSNG  Highly Voted ?  2áyears, 7ámonths ago

Will go with A. Reason:
1. SLA in question is about the Availability (The portal meets a
99,99% availability SLA under these conditions.) therefore maintaining SLA means Availability.
2. Its a user-feedback portal and type of user input is going to be similar or same (A is capturing the user input and replaying it).

Why not B:
The infrastructure is using MIG (Instances created using templates) most likely to be used with Health Check and killing random VMs
cannot test the availability (neither affect the availability as health check will immediately kill the effected Instances and create the other
one.)
Why not D:
SLA is about Availability not reliability or scaling. (As all of it does work hand to hand but still major focus should be on availability.)

--- IF AGREE PLEASE UP VOTE TO MAKE IT CLEAR FOR THE OTHERS --- Thank you.

upvoted 48 times

? ?  RitwickKumar 10ámonths, 2áweeks ago

Only problem with A is that it says "replay captured user load". We are not testing for the incoming unpredictable load due to the
inclusion of unauthenticated users and something that we haven't captured earlier.

Option B covers breadth and depth for the desired SLA.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

56/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 6 times

? ?  jay9114 10ámonths, 1áweek ago

What does "replay captured user load" mean?

upvoted 3 times

? ?  bolu 2áyears, 5ámonths ago

valuable input in terms of 'availability'. did you select this answer in exam too?

upvoted 1 times

? ?  amxexam 1áyear, 9ámonths ago

We are talking about resilience testing where as SLA is an argument of the system.

upvoted 1 times

? ?  amxexam 1áyear, 9ámonths ago

And resilience means the capacity to recover from failure.

upvoted 1 times

? ?  AWSPro24 1áyear, 7ámonths ago

A ensures the app can withstand the loss of a whole Zone which I think is important as well.

upvoted 1 times

? ?  VaraSrinvas  Most Recent ?  2áweeks, 1áday ago

Selected Answer: D

Option D is the best resiliency testing strategy in this scenario as it ensures that the system is tested with actual user data, takes into
account the expected increase in user load, and ensures that the system is adequately scaled to handle the anticipated load.

upvoted 1 times

? ?  JC0926 2ámonths, 1áweek ago

Selected Answer: B

B. Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce
?Çchaos?Ç to the system by terminating random resources on both zones.

By creating synthetic random user input and replaying the load, you can simulate the expected increased user traffic and trigger the
autoscale logic on different layers of the application. Introducing chaos to the system by terminating random resources in both zones
helps test the resiliency and redundancy of the system under stress. This strategy will help ensure that the system can maintain the
99.99% availability SLA when subjected to additional user load.

upvoted 1 times

? ?  telp 3ámonths, 1áweek ago

Selected Answer: B

chaos == test resilience for google

upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: B

This is chaos engineering used by Netflix. https://netflixtechblog.com/tagged/chaos-engineering

upvoted 1 times

? ?  roaming_panda 5ámonths, 3áweeks ago

Selected Answer: B

chaos == checking resilience

upvoted 2 times

? ?  holerina 6ámonths, 2áweeks ago

right thought ,
upvoted 1 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: B

B is correct; Using synthetic/random input is recommended. Chaos Engineering/Symian Army from Netflix is one of the proven
mechanism to test the resilience of the application.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  0xE8D4A51000 8ámonths, 1áweek ago

Selected Answer: B

B caters for terminating the service in both zones randomly. You want to be able to test resiliency when either zone has an outage.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

57/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  zr79 8ámonths, 2áweeks ago

chaos engineering is the buzzword to look after
Answer is B

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B. Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce
?Çchaos?Ç to the system by terminating random resources on both zones

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right

upvoted 2 times

? ?  [Removed] 10ámonths, 2áweeks ago

Selected Answer: B

Question say : "You need to develop a resiliency testing strategy" so introduce Chaos Engineering is the best option in a testing process. I
choose B

upvoted 4 times

? ?  AMohanty 10ámonths, 3áweeks ago

Target is to maintain 99.99% availability.
LAMP - Linux, Apache, MySQL, and PHP
Option B synthetic load is created unless autoscaling is triggered in atleast one layer. We aren't necessarily testing autoscaling on all
layers. So I would rule out B.
+ Incase of a Zonal failure, Availability is affected. We better test Zonal failure instead of chaos termination.

I would go with Option A instead.

upvoted 1 times

? ?  ryzior 1áyear, 1ámonth ago

Selected Answer: B

Comparing scenarios from A and B it looks like A is a specific scenario which could be covered by chaos strategy : shut down whole zone.
Why not? chaos engineering is not just about shutting down singe instances or interfaces, right?
So I'd go with B.
upvoted 1 times

? ?  Gini 1áyear, 1ámonth ago

Selected Answer: B

I go for B as it "create synthetic random user input" which can reveal potential faults. Option A may not cover all possible input since it has
only existing users input.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

58/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #22

Topic 1

One of the developers on your team deployed their application in Google Container Engine with the Docker le below. They report that their

application deployments are taking too long.

You want to optimize this Docker le for faster deployment times without adversely affecting the app's functionality.

Which two actions should you take? (Choose two.)

A. Remove Python after running pip

B. Remove dependencies from requirements.txt

C. Use a slimmed-down base image like Alpine Linux

D. Use larger machine types for your Google Container Engine node pools

E. Copy the source after he package dependencies (Python and pip) are installed

Correct Answer: CE

The speed of deployment can be changed by limiting the size of the uploaded app, limiting the complexity of the build necessary in the

Docker le, if present, and by ensuring a fast and reliable internet connection.

Note: Alpine Linux is built around musl libc and busybox. This makes it smaller and more resource e cient than traditional GNU/Linux

distributions. A container requires no more than 8 MB and a minimal installation to disk requires around 130 MB of storage. Not only do you get

a fully- edged Linux environment but a large selection of packages from the repository.

Reference:

https://groups.google.com/forum/#!topic/google-appengine/hZMEkmmObDU https://www.alpinelinux.org/about/

Community vote distribution

CE (100%)

? ?  aviratna  Highly Voted ?  2áyears ago

C & E:
C: Smaller the base image with minimum dependency faster the container will start
E: Docker image build uses caching. Docker Instructions sequence matter because
applicationÆs dependencies change less frequently than the Python code which will help to reuse the cached layer of dependency and only
add new layer for code change for Python Source code.

upvoted 42 times

? ?  vincy2202  Highly Voted ?  1áyear, 7ámonths ago

C & E are the correct answers.
Kindly refer - https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/

upvoted 7 times

? ?  Badri9898  Most Recent ?  3ámonths ago

The two actions that should be taken to optimize the Dockerfile for faster deployment times without adversely affecting the app's
functionality are:

B. Remove dependencies from requirements.txt: The requirements.txt file should only contain necessary dependencies to reduce the
number of packages to be installed.

C. Use a slimmed-down base image like Alpine Linux: The Alpine Linux image is smaller than Ubuntu and has a smaller attack surface,
which reduces the container's build time and image size.

Therefore, options A, D, and E are not correct as they do not directly address the issue of slow deployment times caused by a bloated
Dockerfile.

upvoted 1 times

? ?  alekonko 3ámonths, 1áweek ago

Selected Answer: CE

C: use smaller image decrease pull time
E: optimize build time using previous cache layer image. generate new layer only for a different app code and requirements

A: can't remove python
B: the developer choose right deps
D: changing istance type don't directly reduce deploy time

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

59/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

C & E
Using a slimmed-down base image like Alpine Linux can help reduce the size of your Docker image, which can lead to faster deployment
times. Alpine Linux is a lightweight Linux distribution that is often used as a base image for Docker images because of its small size.

Additionally, copying the source code after installing the package dependencies can help reduce the image build time because the
dependencies will only need to be installed once, rather than every time the source code is changed. This can lead to faster deployment
times because the image build process will be faster.

upvoted 4 times

? ?  omermahgoub 6ámonths, 1áweek ago

It is not recommended to remove dependencies from the requirements.txt file or remove Python after running pip, as this could
adversely affect the functionality of the application. Similarly, using larger machine types for your Google Container Engine node pools
may not directly affect the deployment times of your application, as the deployment times are primarily dependent on the size and
complexity of the Docker image being deployed.

upvoted 1 times

? ?  sfsdeniso 8ámonths, 1áweek ago

B & E
base image is already cached - so no improvement in build time
B is about removing unnecessary dependencies and not about all of them
i remember saw this question on google's site - BE are correct

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

CE and is perfect
upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: CE

C. Use a slimmed-down base image like Alpine Linux
E. Copy the source after he package dependencies (Python and pip) are installed

upvoted 1 times

? ?  backhand 11ámonths, 1áweek ago

vote C, E
https://cloud.google.com/architecture/best-practices-for-building-containers

upvoted 1 times

? ?  MathMedrado 12ámonths ago

As far as I know, it is necessary to copy the requirements.txt first in order to run pip, it is not possible to run pip without first copying the
requirements.txt. if they copy requirements.txt first then E would work.

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: CE

By means of elimination A B, dont make sence.D . is optimizing the mackie not script. Hence we are left with C& E

upvoted 3 times

? ?  potorange 1áyear, 1ámonth ago

Selected Answer: CE

C.Alpine is a ligweight Linux distro, with smaller image E. Pushing often changing files down the Dockerfile helps reducing image layers
variations. Both help faster image pull operations

upvoted 1 times

? ?  belly265 1áyear, 4ámonths ago

A -invalid
B-dependencies are required
C-it will help as it is one of the best practices to make use of lighter image if possible
D-Not helpful
E-is the best practice to do the steps that changes more frequently at the end . so copy . should be performed at last as it will be changing
more frequently and we can make use of docker caching
hence Answer E is most then C

upvoted 4 times

? ?  aeme 1áyear, 5ámonths ago

What I don't understand is why alpine linux decreases deployment time? Because the base-image layer will be cached after first
installation and won't change until a new os version is used. Therefore after first deployment it won't be faster. But giving more power to
the machine that starts up the container would. So I clearly would argument for D & E

upvoted 2 times

? ?  OrangeTiger 1áyear, 6ámonths ago
C and E appear to be in conflict.
If you install the dependencies in advance, will the size increase?

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

60/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  abhinavbihade 1áyear, 6ámonths ago

C and E

upvoted 1 times

? ?  IKGx1iGetOWGSjAQDD2x3 1áyear, 9ámonths ago

pip installs on alpine will take forever as you'll end up having to compile a lot of stuff; best get rid of that copy . operation though... E
makes sense if you're building locally in an environment with caching, but in a CI/CD system there will be no impact. Very strange
question...

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

61/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #23

Topic 1

Your solution is producing performance bugs in production that you did not see in staging and test environments. You want to adjust your test and

deployment procedures to avoid this problem in the future.

What should you do?

A. Deploy fewer changes to production

B. Deploy smaller changes to production

C. Increase the load on your test and staging environments

D. Deploy changes to a small subset of users before rolling out to production

Correct Answer: D

Community vote distribution

C (67%)

D (31%)

? ?  ghitesh  Highly Voted ?  3áyears, 5ámonths ago

Question Statement: You want to adjust your test and deployment procedures to avoid this problem in the future

So based on this, I think the option "C" is correct, since it is the only one talking about doing changes in the test environment.

upvoted 61 times

? ?  RegisFTM 1áyear, 6ámonths ago

"Your solution is producing performance bugs in production..." - I don't see how "D" would help to detect performance bugs.
- "C" looks more adequate.

upvoted 12 times

? ?  VedaSW 2áyears, 9ámonths ago

C. Increase the load on your test and staging environments.

As you have pointed out in "Question Statement", I do not see C covering "deployment procedures". Test and Staging environment is
more on testing, but not about deployment procedure to production.

So, the only option that cover test and deployment is D. (Yes, kind of unacceptable to have the users to do "testing", but we make it "ok"
by calling it "canary deployment")

upvoted 18 times

? ?  Urban_Life 1áyear, 6ámonths ago

The answer is D
upvoted 8 times

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

A wouldn't prevent the bugs, it would just avoid them. B would help with root-cause analysis because it'd be a smaller change to review. C
would test the performance of the system at its peak processing rates, so this assumes the bugs in production only occur because of
usage. D would allow you to test the new code against smaller user sets to see if it occurs then, and if it still does you know it is not
because of more user responses. So it's a tossup between C and D, D would be the cheaper/quicker answer so I'd choose D first then C if
it's because of usage.

upvoted 33 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is the best

upvoted 1 times

? ?  RitwickKumar 10ámonths, 2áweeks ago

@eroc Looks like you missed the point here it is about the "performance" bugs which are related to the load. Apart from Option C there
is no other way to test out the load prior to production role out. I would chose Option "C": Increase the load on your test and staging
environments
upvoted 3 times

? ?  Sreekey 2áyears, 10ámonths ago

The question is about the performance of the existing Code that they did not detect in Test environments . This is not about new API
release . In order to test the performance they should increase the load in test environment and hence answer C.

upvoted 17 times

? ?  michael_m 10ámonths, 2áweeks ago

According to the question, [Your solution is producing "performance" bugs in production], so I think it is about the load. Plus canary
test will not reproduce the bugs related to high load, I vote for C

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

62/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  PKookNN  Most Recent ?  1áweek, 2ádays ago

Selected Answer: C

Since this is a performance bug, more loads in test and stage would help

upvoted 1 times

? ?  oriori123123 3áweeks ago

Selected Answer: C

The correct answer is C

upvoted 1 times

? ?  Atanu 4áweeks ago

The question asks about test and deployment procedures and option D is the only option covering deployment as well. And deploying to
small subset of user will cover the real time testing scenario as well. Seems Option D quite close.

upvoted 1 times

? ?  salim_ 1ámonth, 4áweeks ago

Selected Answer: D

It's canary deployment to reduce the impact on users in case of issues & bugs.

upvoted 1 times

? ?  Zimcruza 2ámonths ago

How can rolling out to a small subset of users in Prod help if that generates a load that does not reproduce the fault? The only way to
determine the root cause is to use environments where your end-users are not impacted - and Staging is where you should mimic
Production workloads and processes

upvoted 1 times

? ?  Nidhal1920 2ámonths, 2áweeks ago

Both C&E are correct , but since we have this requirement "You want to adjust your test and deployment procedures" , if we choose "C" we
don't adjust our deployment procedures.. So by elimination I'll go for "D".

upvoted 1 times

? ?  naless 2ámonths, 2áweeks ago

If the issue is about performances I can't see how D could be the right choice, since deploying changes to a smaller subset means
decreasing the number of users and therefore the issue could never arise.

upvoted 1 times

? ?  h7m 3ámonths ago

Selected Answer: C

You want to avoid this problem, which you can only do if you load test it in a test environment. With a, b and d you might find the problem
and the solution faster (or not since it's a performance problem), but you do not avoid the problem.

upvoted 1 times

? ?  mifrah 3ámonths ago

I go with C: Increate the load.
Question is: You want to adjust your test and deployment procedures to avoid this problem in the future.
To find performance bugs, I need requests to my application.
Deploy changes to a small set of users, might not trigger the performance bugs. So, no benefit.

upvoted 2 times

? ?  wisnu_ink 3ámonths, 1áweek ago

The keyword is "performance bugs in production that you did not see in staging and test environments" so D should be the answer

upvoted 2 times

? ?  JC0926 3ámonths, 2áweeks ago

Selected Answer: B

This issue is likely a result of a difference in the environment between your staging/test environments and your production environment.
Smaller changes in production deployments allow for more granular control of your infrastructure and are generally easier to
troubleshoot in the event of issues. By deploying smaller changes, you can more easily identify where the issue is and reduce the
likelihood of encountering performance bugs in production.

upvoted 1 times

? ?  MestreCholas 3ámonths, 4áweeks ago

Selected Answer: C

C. Increase the load on your test and staging environments.

One possible reason why performance issues may not be caught in staging and testing environments is that they may not be subject to
the same load as the production environment. By increasing the load on the test and staging environments, you can more accurately
simulate the conditions that the system will be operating under in production, and thus be more likely to catch performance bugs before
they make it to production.

Deploying fewer or smaller changes to production may help reduce the risk of introducing bugs, but it may not necessarily catch
performance issues that only occur under high load.

Deploying changes to a small subset of users before rolling out to production (also known as a canary deployment) can help catch bugs
before they impact the entire user base, but it may not necessarily catch performance issues that only occur under high load.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

63/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  giovanicascaes 4ámonths ago

Initially I've chosen C, but actually D is the correct option, as the statement claims to "adjust your test and deployment procedures", not
"deployment environments".

upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: C

A & B are the same so rejected.
D is rejected as prod bugs would still be faced by users (also it's not like A/B testing).
Hence it's C. You need to improve stage testing

upvoted 1 times

? ?  gcpprofessional 4ámonths, 1áweek ago

C. performance improvement

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

64/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #24

Topic 1

A small number of API requests to your microservices-based application take a very long time. You know that each request to the API can traverse

many services.

You want to know which service takes the longest in those cases.

What should you do?

A. Set timeouts on your application so that you can fail requests faster

B. Send custom metrics for each of your requests to Stackdriver Monitoring

C. Use Stackdriver Monitoring to look for insights that show when your API latencies are high

D. Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice

Correct Answer: D

Reference:

https://cloud.google.com/trace/docs/quickstart# nd_a_trace

Community vote distribution

D (91%)

9%

? ?  euclid  Highly Voted ?  3áyears, 6ámonths ago

D is correct !

upvoted 23 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 9 times

? ?  nitinz 2áyears, 3ámonths ago

D, trace is just for latency testing.

upvoted 4 times

? ?  LaxmanTiwari  Most Recent ?  4áweeks ago

D should be correct, the headline in GC trace documentation says it all: "Cloud Trace is a distributed tracing system for Google Cloud that
collects latency data from applications and displays it in near real-time in the Google Cloud Console."

upvoted 2 times

? ?  LaxmanTiwari 4áweeks ago

even got confused with the C ... after reading the conversation and reference doc D make sense to me.

upvoted 1 times

? ?  alekonko 3ámonths, 1áweek ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  MestreCholas 3ámonths, 4áweeks ago

Why not C?

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

D. Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice

Stackdriver Trace is a distributed tracing system that allows you to understand the relationships between requests and the various
microservices that they touch as they pass through your application. By instrumenting your application with Stackdriver Trace, you can get
a detailed breakdown of the latencies at each microservice, which can help you identify which service is taking the longest in those cases
where a small number of API requests take a very long time.

Setting timeouts on your application or sending custom metrics to Stackdriver Monitoring may not provide the level of detail that you
need to identify the specific service that is causing the latency issues. Looking for insights in Stackdriver Monitoring may also not provide
the necessary level of detail, as it may not show the individual latencies at each microservice.

upvoted 4 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: D

D is the correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

65/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: D

Stacdrive Trace would trace the APIs and helps to identify the bottleneck

upvoted 1 times

? ?  kchandank 7ámonths, 1áweek ago

Selected Answer: D

D is correct trace would report to the latency

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

D is correct trace would report to the latency

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: D

D. Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

this is the best option to find more details

upvoted 1 times

? ?  abirroy 10ámonths, 1áweek ago

Selected Answer: D

Instrument your application with Stackdriver Trace in order to break down the request latencies at each microservice

upvoted 1 times

? ?  [Removed] 10ámonths, 2áweeks ago

Selected Answer: B

D is correct !

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: D

Latency issue in your applicant. trace loy is way to go. Hence D

upvoted 1 times

? ?  Nirca 1áyear, 2ámonths ago

Selected Answer: D

D is best.

upvoted 1 times

? ?  SAMBIT 1áyear, 3ámonths ago

What is used to trace functionsà stack trace

upvoted 1 times

? ?  kuszner 1áyear, 3ámonths ago

Selected Answer: D

no Doubt ...

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for D

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

66/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #25

Topic 1

During a high tra c portion of the day, one of your relational databases crashes, but the replica is never promoted to a master. You want to avoid

this in the future.

What should you do?

A. Use a different database

B. Choose larger instances for your database

C. Create snapshots of your database more regularly

D. Implement routinely scheduled failovers of your databases

Correct Answer: D

Community vote distribution

D (60%)

B (40%)

? ?  Narigdo  Highly Voted ?  3áyears, 7ámonths ago

Answer is D

upvoted 88 times

? ?  Jos 3áyears, 6ámonths ago

Yep, +1 for D

upvoted 20 times

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

@chiar, I agree the question i s not clear. In GCP larger instances have larger number of CPUs, Memory and come with their own private
network. So increases the instance size would help prevent the need for failover during high traffic times. However, routinely scheduled
failovers would allow the team to test the failover when it is not requried. This would make sure it is working when it is required.

upvoted 39 times

? ?  Shariq 3áyears, 6ámonths ago

exactly how do you know the optimal size. it will be a guess. answer should be D

upvoted 7 times

? ?  nescafe7  Most Recent ?  2áweeks, 5ádays ago

Selected Answer: D

Answer is D

upvoted 1 times

? ?  Atanu 3áweeks, 2ádays ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  red_panda 1ámonth ago

Selected Answer: D

For me, the correct answer is D.

B is not plausible. Certainly by creating larger instances the db will crash less, but in any case we cannot know whether this will not
happen again for other reasons, even with the largest instance in the world.
In this case, we should have a ready-to-use DB that can be started up quickly on its own.

This is a good architectural design (which the PCA certification aims to give you).

upvoted 1 times

? ?  LaxmanTiwari 4áweeks ago

Agree with you
upvoted 1 times

? ?  VarunGo 1ámonth, 3áweeks ago

Selected Answer: D

Option D, implementing routinely scheduled failovers of your databases, is the best option in this scenario. This ensures that if the
primary database crashes, the replica will automatically be promoted to the master and take over database operations, preventing any
downtime or data loss. This can be achieved by setting up automatic failover mechanisms or by manually promoting the replica to the
master as soon as the primary database goes down.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

67/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  salim_ 1ámonth, 4áweeks ago

Selected Answer: D

this is to ensure that when incident occurs and need to failover, it will work

upvoted 1 times

? ?  JC0926 2ámonths, 1áweek ago

Selected Answer: D

D. Implement routinely scheduled failovers of your databases

By routinely scheduling failovers of your databases, you can ensure that the database failover process is working correctly and that your
replica can be promoted to master if needed. This practice helps you identify and address any issues with the failover process before they
become critical during high-traffic periods. Scheduled failovers also allow your team to become familiar with the process and improve
their ability to handle real failover situations.

upvoted 2 times

? ?  taer 2ámonths, 3áweeks ago

Selected Answer: D

By practicing and automating the failover process, you can increase the reliability of your databases and reduce the likelihood of issues
during high-traffic periods.

upvoted 1 times

? ?  h7m 3ámonths ago

Selected Answer: D

You want to avoid the failover fail. This helps even if the master crashes for other reasons than the ones you can imagine.

upvoted 1 times

? ?  mifrah 3ámonths ago

I go with D, but the question is definitely not 100% clear. What do I want to avoid the database crashes, or the missing promotion of a
replica to master?
In my opinion, also a bigger database is going to crash sometimes.
So, I would look for the replica promotion.

upvoted 1 times

? ?  JC0926 3ámonths, 2áweeks ago

Selected Answer: D

D
To avoid the scenario where a replica is never promoted to a master, it is important to have failover mechanisms in place. Routine
scheduled failovers can ensure that the replica is promoted to master in case of a crash of the original master. This can be achieved by
setting up automated processes that check the health of the primary database and automatically failover to a replica if necessary.

upvoted 1 times

? ?  MestreCholas 3ámonths, 4áweeks ago

Selected Answer: D

Answer: D. Implement routinely scheduled failovers of your databases. This will ensure that in the event of a primary database failure, the
replica will be automatically promoted to a master, minimizing downtime and ensuring continuous availability of the database.
Additionally, it's important to ensure that the replica is kept up-to-date with the primary database through continuous replication, and that
the failover process is thoroughly tested to ensure its reliability.

upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: D

it is essential to implement routinely scheduled failovers of your databases. This involves periodically promoting a replica to a master,
testing the failover process, and then demoting the original master to a replica. By doing this regularly, you ensure that your replicas are
always up-to-date and ready to take over in case of a failure.

upvoted 2 times

? ?  Ashish_Mishra 4ámonths ago

Selected Answer: D

Option B may help improve performance, but it won't necessarily prevent a crash or ensure that a replica is promoted to a master in the
event of a failure. Option D looks good.

upvoted 2 times

? ?  ckw_1206 4ámonths ago

Selected Answer: D

Choose D. Implement routinely scheduled failovers of your databases.

Implementing routinely scheduled failovers of your databases ensures that the replica is promoted to the master in case the primary
database crashes. This helps in avoiding downtime due to a crashed database. Using a different database may not necessarily solve the
problem, and choosing larger instances for your database may not address the root cause of the problem. Creating snapshots of your
database more regularly is a good practice, but it may not help in avoiding the issue of the primary database crashing.

upvoted 1 times

? ?  ckw_1206 4ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

68/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

choose D. Implement routinely scheduled failovers of your databases.

Implementing routinely scheduled failovers of your databases ensures that the replica is promoted to the master in case the primary
database crashes. This helps in avoiding downtime due to a crashed database. Using a different database may not necessarily solve the
problem, and choosing larger instances for your database may not address the root cause of the problem. Creating snapshots of your
database more regularly is a good practice, but it may not help in avoiding the issue of the primary database crashing.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

69/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #26

Topic 1

Your organization requires that metrics from all applications be retained for 5 years for future analysis in possible legal proceedings.

Which approach should you use?

A. Grant the security team access to the logs in each Project

B. Con gure Stackdriver Monitoring for all Projects, and export to BigQuery

C. Con gure Stackdriver Monitoring for all Projects with the default retention policies

D. Con gure Stackdriver Monitoring for all Projects, and export to Google Cloud Storage

Correct Answer: B

Stackdriver Logging provides you with the ability to  lter, search, and view logs from your cloud and open source application services. Allows

you to de ne metrics based on log contents that are incorporated into dashboards and alerts. Enables you to export logs to BigQuery, Google

Cloud Storage, and Pub/Sub.

Reference:

https://cloud.google.com/stackdriver/

Community vote distribution

D (74%)

B (26%)

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

D is correct and best practice for long term log storage

upvoted 134 times

? ?  AndreUanKenobi 2áyears, 3ámonths ago

+1. For archival purposes, Customer should use Cloud Storage. BigQuery is a datawarehouse, and could eventually import data from
Cloud Storage if necessary.

upvoted 15 times

? ?  anjuagrawal 1áyear, 5ámonths ago

+1 Due to long term storage, cloud storage is better answer than BigQuery

upvoted 10 times

? ?  MeasService  Highly Voted ?  3áyears, 5ámonths ago

A and C can be quickly ruled out because none of them is solution for the requirements "retained for 5 years"

Between B and D, the different is where to store, BigQuery or Cloud Storage. Since the main concern is extended storing period, D
(Correct Answer) is better choice, and the "retained for 5 years for future analysis" further qualifies it, for example, using Coldline storage
class.

With regards of BigQuery, while it is also a low-cost storage, but the main purpose is for analysis. Also, logs stored in Cloud Storage is easy
to transport to BigQuery or do query directly against the files saved in Cloud Storage if and whenever needed.

upvoted 59 times

? ?  Shyeom 3áyears, 5ámonths ago

point : organization requires that metrics from all applications be retained for 5 years

upvoted 2 times

? ?  Shyeom 3áyears, 5ámonths ago

I mean answer : D

upvoted 4 times

? ?  Cloudy_Apple_Juice 2áyears, 8ámonths ago

If you have 2 viable solutions (B&D), then always chose the one that is cost optimised - I chose D

upvoted 5 times

? ?  jvale 1áyear, 3ámonths ago

Bigquery long term storage cost: $0.020 per GB
Cloud Storage archive cost: $0,0012 per GB
Only if metrics need less than 10 GB (free service part on Bigquery) then the correct solution will be B... But all metrics for all
applications during more than 5 years... I think never will be the case :D

upvoted 8 times

? ?  Vika 2áyears, 4ámonths ago

second that! like the way u explained..

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

70/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  trainor 2áyears, 6ámonths ago

The question is about metrics, not logs. I'd go for B.
See https://cloud.google.com/solutions/stackdriver-monitoring-metric-export

upvoted 15 times

? ?  bnlcnd 2áyears, 5ámonths ago

This is a good example. thanks.
But, we can easily change that implementation to dump the metrics to buckets to save lots of money. And, when talking about legal
purpose, 1 hour interval may not be enough. You may have to keep more frequent metrics. So, only cold line or archive work for that
purpose.

upvoted 4 times

? ?  sssss1  Most Recent ?  4ádays, 17áhours ago

Selected Answer: D

Cloud Storage Archive class is the best option for maintaining
archived data such as log data. Also, since the data is not likely to be accessed, Archive
storage would be the most cost-effective option.
Taken as a response to the question from the official study guide

upvoted 1 times

? ?  mrhege 1ámonth, 2áweeks ago

B: We're talking about metrics not logs. Reference solution uses BigQuery: https://cloud.google.com/architecture/stackdriver-monitoring-
metric-export
upvoted 1 times

? ?  GoReplyGCPExam 2ámonths, 1áweek ago

Selected Answer: D

D - save them on storage

upvoted 1 times

? ?  feholen210 3ámonths, 1áweek ago

Selected Answer: D

For Long Term Storage Cloud Storage is preferred.

upvoted 1 times

? ?  Martintranthanh 3ámonths, 1áweek ago

It should be B
The question does not mention cost and important key words are analysis and legal proceedings.

upvoted 3 times

? ?  JC0926 3ámonths, 2áweeks ago

Selected Answer: D

D
By exporting the metrics to Google Cloud Storage, you can also analyze them using a variety of tools such as BigQuery or Data Studio,
which can help you gain insights into the performance and behavior of your applications over time. This can be useful in legal proceedings
where you may need to provide evidence to support your claims.

upvoted 1 times

? ?  telp 3ámonths, 2áweeks ago

Selected Answer: D

D => long term storage is cloud storage archive mode.

Big query is to use the logs to analyze. The question is only about 5 years retention.

upvoted 1 times

? ?  abbottWang 3ámonths, 4áweeks ago

Selected Answer: D

cloud storage is best practice for long term storage

upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: D

Although you would be tempted to select B as the data size looks big and BigQuery supports ML.
But Cloud Storage are best for long term storage, archival can keep data > 365 days.

upvoted 1 times

? ?  r1ck 4ámonths, 1áweek ago

5 years is a long time, to store data in BigQuery
since we can export data from Cloud Storage > BigQuery for analysis
answer should be "Cloud Storage"

upvoted 1 times

? ?  yjring 4ámonths, 2áweeks ago

D. Configure Stackdriver Monitoring for all Projects, and export to Google Cloud Storage."

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

71/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  roaming_panda 5ámonths, 3áweeks ago

Selected Answer: D

always google best practices i.e archival bucket aka D

upvoted 1 times

? ?  vinayvinay3 5ámonths, 3áweeks ago

Selected Answer: D

given the low likelihood of legal proceedings, i'd put it in GCS. we're not doing any analytical queries on this data, then why put it in BQ?

upvoted 2 times

? ?  Yaa 6ámonths, 1áweek ago

Selected Answer: D

D is the answer! saves cost and serves the purpose.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Answer is B: To ensure that metrics from all applications are retained for the required 5-year period for potential legal proceedings, you
should configure Stackdriver Monitoring for all Projects and export the data to BigQuery. BigQuery is a fully managed, cloud-native data
warehouse that allows you to store and analyze large and complex datasets. By exporting the metrics data from Stackdriver Monitoring to
BigQuery, you can take advantage of its scalability and long-term data retention capabilities to store and analyze your metrics over an
extended period of time.

Granting the security team access to the logs in each Project or configuring Stackdriver Monitoring with the default retention policies may
not provide the necessary level of data retention for potential legal proceedings. Exporting the data to Google Cloud Storage may not
offer the same level of analysis and querying capabilities as BigQuery.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

72/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #27

Topic 1

Your company has decided to build a backup replica of their on-premises user authentication PostgreSQL database on Google Cloud Platform.

The database is 4

TB, and large updates are frequent. Replication requires private address space communication.

Which networking approach should you use?

A. Google Cloud Dedicated Interconnect

B. Google Cloud VPN connected to the data center network

C. A NAT and TLS translation gateway installed on-premises

D. A Google Compute Engine instance with a VPN server installed connected to the data center network

Correct Answer: A

Google Cloud Dedicated Interconnect provides direct physical connections and RFC 1918 communication between your on-premises network

and Google's network. Dedicated Interconnect enables you to transfer large amounts of data between networks, which can be more cost

effective than purchasing additional bandwidth over the public Internet or using VPN tunnels.

Bene ts:
? Tra c between your on-premises network and your VPC network doesn't traverse the public Internet. Tra c traverses a dedicated
connection with fewer hops, meaning there are less points of failure where tra c might get dropped or disrupted.
? Your VPC network's internal (RFC 1918) IP addresses are directly accessible from your on-premises network. You don't need to use a NAT
device or VPN tunnel to reach internal IP addresses. Currently, you can only reach internal IP addresses over a dedicated connection. To reach

Google external IP addresses, you must use a separate connection.
? You can scale your connection to Google based on your needs. Connection capacity is delivered over one or more 10 Gbps Ethernet
connections, with a maximum of eight connections (80 Gbps total per interconnect).
? The cost of egress tra c from your VPC network to your on-premises network is reduced. A dedicated connection is generally the least
expensive method if you have a high-volume of tra c to and from Google's network.

Reference:

https://cloud.google.com/interconnect/docs/details/dedicated

Community vote distribution

A (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

A is the one

upvoted 24 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 7 times

? ?  nitinz 2áyears, 3ámonths ago

A, direct connect is private. VPN not enough for 4 TB with huge frequent changes.

upvoted 3 times

? ?  amxexam  Highly Voted ?  1áyear, 10ámonths ago

Let's go with option elimination
A. Google Cloud Dedicated Interconnect
>> Secured, fast connection, hence the choice. This will allow private connection from GCP to the data centre with a fast connection. Cost is
not mentioned in the requirement to eliminate this option.
B. Google Cloud VPN connected to the data centre network
>> We have to think about data flowing on the internet and the requirement talks about private connect. Also not sure how well you
connect VPN with Data Center until you use the hybrid option. https://cloud.google.com/network-
connectivity/docs/vpn/concepts/overview hence eliminate
C. A NAT and TLS translation gateway installed on-premises
>>This is a VM option to reach outside won't for this requirement hence eliminate
D. A Google Compute Engine instance with a VPN server installed connected to the data centre network
>>This is a slow option hence eliminate

Hence A

upvoted 15 times

? ?  mrhege  Most Recent ?  1ámonth, 2áweeks ago

B: Dedicated Interconnect would be a major overkill here and a quite expensive one as well. Requirements mention private _address
space_, not private connection. Data over VPN is just as secure. Also there is no mention that a Google PoP would be available.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

73/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

https://cloud.google.com/network-connectivity/docs/how-to/choose-product

upvoted 1 times

? ?  mohideenks 7ámonths ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is great but expensive for just a database DR but what can we do about that

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

VPN is not private, it is public but encrypted. Also, VPN is not suitable for large updates that happen frequently

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

without any second thought A is right

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. Google Cloud Dedicated Interconnect - large updates and better security, however may not be the most cost effective choice

upvoted 1 times

? ?  GoReplyGCPExam 1áyear, 1ámonth ago

Selected Answer: A

A is the one

upvoted 1 times

? ?  Nirca 1áyear, 2ámonths ago

Selected Answer: A

Direct connect.
upvoted 1 times

? ?  hibi6x 1áyear, 6ámonths ago

Challenge me but this is answer B. I have 4TB DB, frequent update would be what ? 50% daily change means 2TB daily means ~25Mbps.
With VPN I can easily achieved that. It is typical ingress to cloud free ....It would be madness to pay 5k montly only for Directo Connect...

upvoted 5 times

? ?  AmitAr 1áyear, 1ámonth ago
Key points of quesiton -
1) Huge data and
2) on-premises user authentication PostgreSQL - which means security - vpn uses public internet .. so B is not option.
A - should be correct answer

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

A is the correct answer.

upvoted 2 times

? ?  dlpkmr98 1áyear, 8ámonths ago

always go with best practices --Google Cloud Dedicated Interconnect

upvoted 2 times

? ?  aviratna 2áyears ago

A is correct.
- Its a DB replication of 4 TB which will be continuous so Dedicated Interconnect is cost effective when data volume and traffic is high.
- Any other option will be costly because of ingress & egress high volume traffic
- Dedicated Interconnect it can also communicate based on private IP range
- replicating sensitive data like users data over public internet using Cloud VPN is not good option from security perspective.
- Dedicated Interconnect traffic will not go over internet

upvoted 1 times

? ?  gatul28 2áyears, 1ámonth ago

A is expected to be chosen here but this s authentication data and with option A there is no traffic encryption happening. VPN supports
encryption, but has low throughput.

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

74/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A. Google Cloud Dedicated Interconnect

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

75/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #28

Topic 1

Auditors visit your teams every 12 months and ask to review all the Google Cloud Identity and Access Management (Cloud IAM) policy changes in

the previous 12 months. You want to streamline and expedite the analysis and audit process.

What should you do?

A. Create custom Google Stackdriver alerts and send them to the auditor

B. Enable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the auditor

C. Use cloud functions to transfer log entries to Google Cloud SQL and use ACLs and views to limit an auditor's view

D. Enable Google Cloud Storage (GCS) log export to audit logs into a GCS bucket and delegate access to the bucket

Correct Answer: D

Community vote distribution

B (66%)

D (34%)

? ?  ghitesh  Highly Voted ?  3áyears, 5ámonths ago

B. https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors

upvoted 77 times

? ?  aselunar 3áyears, 1ámonth ago

In that scenario the audits were quarterly. For annual audits, Google recommends Cloud Storage.

upvoted 5 times

? ?  AmitAr 1áyear, 1ámonth ago

data is 12 month old, we need not to preserve for 12 months, so cloud storage is not good option. This is required for - Streamline
and expedite the analysis, Answer should be (B)

upvoted 2 times

? ?  Rajuuu 3áyears ago

B IS correct

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is correct

upvoted 1 times

? ?  rockstar9622 3áyears, 5ámonths ago

b) seems correct
upvoted 3 times

? ?  anton_royce 3áyears, 2ámonths ago

I agree. Answer B

upvoted 5 times

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

Think B is better. Export to Bigquery and restrict access to queries with ACLs to auditors

upvoted 34 times

? ?  passnow 3áyears, 6ámonths ago

I thought same as well. I would go with B

upvoted 4 times

? ?  tartar 2áyears, 10ámonths ago

D is ok.

upvoted 7 times

? ?  tartar 2áyears, 10ámonths ago

Sorry, changed my view. B is the recommended practice

upvoted 13 times

? ?  alii 2áyears, 5ámonths ago

don't change your view, D was right :)

upvoted 4 times

? ?  RKS_2021 1áyear, 11ámonths ago

B is correct

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

76/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  trainor 2áyears, 6ámonths ago

I think D is better. B implies too much data manipulation to make it suitable for an audit.

upvoted 3 times

? ?  nitinz 2áyears, 3ámonths ago

D, rest all options are no good.

upvoted 2 times

? ?  AmitAr 1áyear, 1ámonth ago

Please check the keywords in question -- "streamline and expedite" -- Bigquery is suitable not storage bucket. so it should be (B)

upvoted 3 times

? ?  sssss1  Most Recent ?  4ádays, 16áhours ago

Selected Answer: B

https://cloud.google.com/iam/docs/job-functions/auditing#scenario_external_auditors
B complies with requirements <analysis and audit> you can audit from GCS but not analyze the data in it, that is done by BQ

upvoted 1 times

? ?  claorden 2áweeks, 1áday ago

Selected Answer: B

https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors

upvoted 2 times

? ?  jlambdan 3áweeks ago

Selected Answer: B

B with partitioned table expirering after 12 month

upvoted 1 times

? ?  jlambdan 3áweeks ago

Selected Answer: D

D with partitioned table in order to retain only the latest 12 months.

upvoted 1 times

? ?  jlambdan 3áweeks ago

please moderator do not publish the comment above, It is supposed to be B, not D. I mixed them up. Thanks.

upvoted 1 times

? ?  Atanu 4áweeks ago

Selected Answer: D

D is ok and a cheaper solution to the requirement.

upvoted 1 times

? ?  FigVam 1ámonth, 3áweeks ago

Selected Answer: B

should be b, given requirements.

upvoted 1 times

? ?  ggroin 1ámonth, 3áweeks ago

Selected Answer: B

B is correct

upvoted 1 times

? ?  mifrah 3ámonths ago

B: You want to streamline and expedite the analysis and audit process.
For me the expedite the analysis is the key part --> BQ and not GCS.

upvoted 1 times

? ?  JC0926 3ámonths, 2áweeks ago

Selected Answer: D

The auditor can access the GCS bucket directly to review and analyze the logs without any intervention from your team. This approach is
easy to set up, scalable, and ensures that the auditor has access to all the relevant data without any data loss.

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: B

I think the answer is B because B explicitly mentions views and scope -- basically filtering the raw logs to control what the auditor
sees(remove PII) vs. accessing raw logs in GCS.

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: D

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

77/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Lot easier to export to GCS vs. BQ.

upvoted 1 times

? ?  yjring 4ámonths, 2áweeks ago

B.
Enabling Google Cloud Storage (GCS) log export to audit logs into a GCS bucket and delegating access to the bucket may be an option, but
it would require additional configuration and management, and may not provide the same level of queryability as BigQuery.

upvoted 2 times

? ?  Jeena345 4ámonths, 3áweeks ago

Selected Answer: B

It is B

upvoted 1 times

? ?  KirtiMohanta 5ámonths ago

Answer should be B

upvoted 1 times

? ?  preman 5ámonths, 1áweek ago

The question specifically mentions ' to expedite the Analysis and Auditing process' . Normally for storage scenarios option D with GCS
would suffice---but in this scenario since we also need expedited analysis---the any solution involving BQ is preferred. Correct Answer - B.

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

78/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #29

Topic 1

You are designing a large distributed application with 30 microservices. Each of your distributed microservices needs to connect to a database

back-end. You want to store the credentials securely.

Where should you store the credentials?

A. In the source code

B. In an environment variable

C. In a secret management system

D. In a con g  le that has restricted access through ACLs

Correct Answer: C

Reference:

https://cloud.google.com/kms/docs/secret-management

Community vote distribution

C (100%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

Google Secret Management was designed explicitly for this purpose.

upvoted 30 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 7 times

? ?  nitinz 2áyears, 3ámonths ago

C, microservices = GKE = Kubernetes = secrets.

upvoted 5 times

? ?  shandy  Highly Voted ?  3áyears, 7ámonths ago

C is the answer, since key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage permissions
to those keys.

A is incorrect because storing credentials in source code and source control is discoverable, in plain text, by anyone with access to the
source code. This also introduces the requirement to update code and do a deployment each time the credentials are rotated. B is not
correct because consistently populating environment variables would require the credentials to be available, in plain text, when the
session is started. D is incorrect because instead of managing access to the config file and updating manually as keys are rotated, it would
be better to leverage a key management system. Additionally, there is increased risk if the config file contains the credentials in plain text.

upvoted 12 times

? ?  hiromi  Most Recent ?  2ámonths, 3áweeks ago

Selected Answer: C

C is correct

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago
C. In a secret management system

It is important to store the credentials for your database back-end securely in order to protect them from unauthorized access. One way
to do this is by using a secret management system, such as Google Cloud's Secret Manager. Secret Manager is a secure and convenient
storage system for API keys, passwords, and other sensitive data that is designed to protect against unauthorized access. By storing the
credentials in Secret Manager, you can ensure that they are kept secure and can be easily accessed by your microservices as needed.

Storing the credentials in the source code, an environment variable, or a config file that has restricted access through ACLs may not
provide the same level of security as a dedicated secret management system. It is important to ensure that your credentials are stored in a
secure and controlled manner to protect against unauthorized access.

upvoted 2 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: C

C is correct; If credential then always use secret manager.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

C is ok

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

79/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

secret manager is the answer

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is right

upvoted 1 times

? ?  Kubernetes 9ámonths, 2áweeks ago

answer is C

upvoted 1 times

? ?  [Removed] 1áyear, 3ámonths ago

Selected Answer: C

Use Google Secret Manager

upvoted 1 times

? ?  rogerlovato 1áyear, 5ámonths ago

Selected Answer: C

C is correct

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

C is the right answer

upvoted 1 times

? ?  unnikrisb 1áyear, 8ámonths ago

Google Practice exam question with option C : In a key management system
C is correct because key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage permissions to
those keys.
https://cloud.google.com/kms/
For this question, refer to the Mountkirk Games case study.

upvoted 1 times

? ?  unnikrisb 1áyear, 8ámonths ago

Google Practice exam question with option C : In a key management system
Here also C is correct because key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage
permissions to those keys.
https://cloud.google.com/kms/
For this question, refer to the Mountkirk Games case study.

upvoted 1 times

? ?  unnikrisb 1áyear, 8ámonths ago

Again part of practice tests ( option was key management instead of secret management system )
C is correct because key management systems generate, use, rotate, encrypt, and destroy cryptographic keys and manage permissions to
those keys.
https://cloud.google.com/kms/
For this question, refer to the Mountkirk Games case study.

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

C. In a secret management system

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

80/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #30

Topic 1

A lead engineer wrote a custom tool that deploys virtual machines in the legacy data center. He wants to migrate the custom tool to the new cloud

environment.

You want to advocate for the adoption of Google Cloud Deployment Manager.

What are two business risks of migrating to Cloud Deployment Manager? (Choose two.)

A. Cloud Deployment Manager uses Python

B. Cloud Deployment Manager APIs could be deprecated in the future

C. Cloud Deployment Manager is unfamiliar to the company's engineers

D. Cloud Deployment Manager requires a Google APIs service account to run

E. Cloud Deployment Manager can be used to permanently delete cloud resources

F. Cloud Deployment Manager only supports automation of Google Cloud resources

Correct Answer: BF

Community vote distribution

CF (45%)

EF (23%)

CE (21%)

9%

? ?  victory108  Highly Voted ?  1áyear, 12ámonths ago

E. Cloud Deployment Manager can be used to permanently delete cloud resources
F. Cloud Deployment Manager only supports automation of Google Cloud resources

upvoted 68 times

? ?  poseidon24 1áyear, 11ámonths ago

Yup, E + F. In GCP documentation it states as a warning note that deletion made through Deployment Manager scripts cannot be
undone, if devs are not well trained a human errors can impact Business

upvoted 9 times

? ?  AK2020  Highly Voted ?  2áyears ago

C and F- make sense to me

upvoted 39 times

? ?  ssepiro 1áyear, 7ámonths ago

I think this is right. the key of the question is "business risks".

upvoted 3 times

? ?  Fundu80 2ámonths, 4áweeks ago

C - Makes sense, because company engineer may take longer to develop, so more cost and more 'time-to-market'

Reg F:
Can I pls ask how does business care whether you are Google Cloud Resources or legacy data center tools, as long as it servs
business requirement?

So I'm leaning towards E, as the engineers are still in the process of learning CDM and may accidently delete VMs bringing down the
entire application.

upvoted 2 times

? ?  Fundu80 2ámonths, 4áweeks ago

Forgot to mention, once determined as "risks", the mitigation actions below can be followed:
C: Train the existing resources, Hire an experienced personnel
E: Peer Reviews, QA, thorough testing etc.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

yes, C and F right
upvoted 2 times

? ?  Rothmansua  Most Recent ?  2ádays, 22áhours ago

Selected Answer: BF

Cloud Deployment Manager can be deprecated in the future in view of more popular alternative technologies is a risk that hardly be
addressed.
Deployment Manager is only for GCP

upvoted 1 times

? ?  sssss1 4ádays, 16áhours ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

81/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: CF

Option E, is a consequence of option C.
If they don't know how to use it then serious risks will be faced. F, because the tool is created for cloud resources not on-premise
resources.

So C & D are correct

upvoted 1 times

? ?  Bedmed 2ámonths ago

Selected Answer: EF

You want to advocate for the adoption of Google Cloud Deployment Manager.

upvoted 1 times

? ?  Hisayuki 2ámonths, 1áweek ago

Selected Answer: CF

only supports automation of Google Cloud resources - indicates you might use Terraform for Infrastructure as code at multi-cloud

upvoted 1 times

? ?  Chota_Babu 2ámonths, 4áweeks ago

Selected Answer: EF

Read the question again

upvoted 1 times

? ?  taer 3ámonths ago

Selected Answer: CF

C. Cloud Deployment Manager is unfamiliar to the company's engineers: This may lead to a learning curve and potential delays or
mistakes in deployment, as the team becomes familiar with the new system.

F. Cloud Deployment Manager only supports automation of Google Cloud resources: If the company has a multi-cloud or hybrid cloud
environment, the custom tool might need to be adapted or additional tools might be required for managing resources in other cloud
environments or on-premises infrastructure.

upvoted 4 times

? ?  mifrah 3ámonths ago

I go with E&F.
Why not B: Well, it is Googles CI tool, I do not think it is going to be deprecated.
Why not C: Well, if the fact "Cloud Deployment Manager is unfamiliar to the company's engineers" might be a business risk not to use the
service. Great, that would make almost every move into GCP a business risk, because engineers might never know Google Services, like AI
or GKE (when the company is running only VMs onprem)... So C is no option in my opinion. Otherwise Cloud providers can close the
business.

upvoted 1 times

? ?  PRUTHVI12 3ámonths, 1áweek ago

c and e best answer

upvoted 1 times

? ?  telp 3ámonths, 1áweek ago

Selected Answer: EF

E and F
Cloud Deployment Manager only supports automation of Google Cloud resources û The statement is true
Cloud Deployment Manager can be used to permanently delete cloud resources. Yes, the statement is true, and it is a protentional
business risk for human error of the it team

upvoted 2 times

? ?  anshad666 3ámonths, 3áweeks ago

Selected Answer: EF

looks like this correct, consider as a business risk

upvoted 2 times

? ?  PST21 3ámonths, 3áweeks ago

ChatGPT says B &C :-) I feel it is E &F. C - is a risk for any ew technology

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: CF

Process of elimination

upvoted 1 times

? ?  ckw_1206 4ámonths ago

Selected Answer: CF

One business risk of migrating to Cloud Deployment Manager is that it may be unfamiliar to the company's engineers, requiring
additional training and potential delays in adoption. Another risk is that Cloud Deployment Manager only supports automation of Google
Cloud resources, meaning that it may not be suitable for migrating other types of infrastructure, which could lead to a fragmented toolset
for managing different environments. The other options, such as using Python or requiring a Google APIs service account, are not

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

82/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

business risks, and the risk of APIs being deprecated can be mitigated by regular updates and maintenance. Additionally, the risk of
permanently deleting cloud resources is not unique to Cloud Deployment Manager and can be mitigated by proper access controls and
backups.

upvoted 2 times

? ?  MaryMei 4ámonths, 1áweek ago

Selected Answer: CF

I agree with minmin2020's argument

upvoted 1 times

? ?  preman 5ámonths, 1áweek ago

B and E are correct choices. Why because the custom tool has not been migrated yet. If in case of migration to a GCP platform--factors to
consider would be--deprecation and accidental deletion.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

83/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #31

Topic 1

A development manager is building a new application. He asks you to review his requirements and identify what cloud technologies he can use to

meet them. The application must:

1. Be based on open-source technology for cloud portability

2. Dynamically scale compute capacity based on demand

3. Support continuous software delivery

4. Run multiple segregated copies of the same application stack

5. Deploy application bundles using dynamic templates

6. Route network tra c to speci c services based on URL

Which combination of technologies will meet all of his requirements?

A. Google Kubernetes Engine, Jenkins, and Helm

B. Google Kubernetes Engine and Cloud Load Balancing

C. Google Kubernetes Engine and Cloud Deployment Manager

D. Google Kubernetes Engine, Jenkins, and Cloud Load Balancing

Correct Answer: D

Jenkins is an open-source automation server that lets you  exibly orchestrate your build, test, and deployment pipelines. Kubernetes Engine is a

hosted version of

Kubernetes, a powerful cluster manager and orchestration system for containers.

When you need to set up a continuous delivery (CD) pipeline, deploying Jenkins on Kubernetes Engine provides important bene ts over a

standard VM-based deployment

Incorrect Answers:

A: Helm is a tool for managing Kubernetes charts. Charts are packages of pre-con gured Kubernetes resources.

Use Helm to:

Find and use popular software packaged as Kubernetes charts

? Share your own applications as Kubernetes charts
? Create reproducible builds of your Kubernetes applications
? Intelligently manage your Kubernetes manifest  les
? Manage releases of Helm packages
Reference:

https://cloud.google.com/solutions/jenkins-on-kubernetes-engine

Community vote distribution

A (76%)

D (24%)

? ?  rsamant  Highly Voted ?  2áyears ago

it should be A .. helm is needed for "Deploy application bundles using dynamic templates"

Load Balancing should be part of GKE Already

upvoted 47 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A should be fine
upvoted 1 times

? ?  raf2121 1áyear, 10ámonths ago

Kubernetes Engine offers integrated support for two types of Cloud Load Balancing (Ingress and External Network Load Balancing) ,
hence Option A
Reference : https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer

upvoted 3 times

? ?  poseidon24 1áyear, 11ámonths ago

Not for "based on URL", that is the difference.

upvoted 7 times

? ?  ashish_t 1áyear, 8ámonths ago

https://cloud.google.com/kubernetes-engine/docs/tutorials/http-
balancer#optional_serving_multiple_applications_on_a_load_balancer

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

84/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

As per the above document and given example of "fanout-ingress.yaml" in above document and also in GKE sample repository
below
https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/tree/master/load-balancing

it's clear that GKE LB can handle "6. Route network traffic to specific services based on URL"
So NO need for Cloud Load balancing.

Helm satisfy "5. Deploy application bundles using dynamic templates"
and no other option satisfies this point #5.

So correct answer should be:
A

upvoted 11 times

? ?  victory108  Highly Voted ?  1áyear, 12ámonths ago

D. Google Kubernetes Engine, Jenkins, and Cloud Load Balancing

upvoted 33 times

? ?  sssss1  Most Recent ?  4ádays, 16áhours ago

Selected Answer: A

D - GKE already provides LB based on URL https://cloud.google.com/kubernetes-
engine/docs/concepts/ingress#multiple_backend_services

A satisfies all requirements, more specifically: "Deploy application bundles using dynamic templates" - whilst D does not.

Correct Answer: A

upvoted 1 times

? ?  nescafe7 2áweeks, 5ádays ago

Selected Answer: D

The correct answer is D.

upvoted 1 times

? ?  TheCloudGuruu 1ámonth, 2áweeks ago

Selected Answer: A

The correct answer is A

upvoted 1 times

? ?  mifrah 3ámonths ago

In the exam I would have clicked D.
Although you can configure GKE with Ingress for the URL maps, but in my opinion Google provisions a Google Load Balancer behind the
scene, but that makes a Load Balancer necessary --> D.

upvoted 5 times

? ?  Kiroo 1ámonth, 3áweeks ago

Yep, helm is nice but it depends on what you are considering as a dynamic template, when you provision a ingress for dynamic route in
a gke it will create the load balancer and the backends so I would go with D too

upvoted 1 times

? ?  alekonko 3ámonths, 1áweek ago

Selected Answer: A

GKE, Helm (for dynamic templates), Jenkins (for CICD).
With GKE you already have lb, you can use ingress expose app on it in easy way. you can use service loadbalancer if you wanna expose
other dedicated lb.

A is the only right answer because address the deploy with dynamic template

upvoted 1 times

? ?  feholen210 3ámonths, 1áweek ago

Selected Answer: A

It Should be 'A' since GKE already provide a load balancer with it.

upvoted 1 times

? ?  anshad666 3ámonths, 3áweeks ago

Selected Answer: A

Helm needed for Deploy application bundles using dynamic templates

upvoted 2 times

? ?  MestreCholas 3ámonths, 4áweeks ago

Selected Answer: A

The combination of technologies that will meet all of the requirements is option A: Google Kubernetes Engine, Jenkins, and Helm.

Explanation:

1. Google Kubernetes Engine (GKE) is based on Kubernetes, which is an open-source container orchestration system for cloud portability.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

85/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

2. GKE supports auto-scaling of compute resources based on demand.
3. Jenkins is a popular tool for continuous integration and continuous delivery (CI/CD) pipelines.
4. GKE allows running multiple copies of the same application stack in different Kubernetes namespaces or clusters for segregation.
5. Helm is a package manager for Kubernetes that enables the deployment of application bundles using dynamic templates.
6. Cloud Load Balancing can route network traffic to specific services based on URL, and can be used with GKE.

upvoted 2 times

? ?  jay9114 6ámonths, 1áweek ago

Selected Answer: A

Overall great explanation those who selected A as the answer. Thank you for sharing. I agree that the best answer is A.

Load balancing - GKE container-native load balancing through Ingress
Regarding your "other options" section. Since GKE already provides load balancing capabilities, Cloud Load Balancing is not needed since
GKE has container-native load balancing by default for GKE clusters running version 1.17 or later. (1)(2)

Network traffic to specific services based on URL - Gateway traffic management
I read about two traffic management capabilities that provide controls to manage network traffic to "specific" services in GKE. They are
services capacity and traffic splitting. (3)

references -
1. https://cloud.google.com/kubernetes-engine/docs/how-to/container-native-load-balancing
2. https://cloud.google.com/kubernetes-engine/docs/concepts/ingress
3. https://cloud.google.com/kubernetes-engine/docs/concepts/traffic-management

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

A. Google Kubernetes Engine, Jenkins, and Helm

To meet all of the requirements specified, you can use the following combination of technologies:

Google Kubernetes Engine: This is an open-source, container-based platform that can be used to dynamically scale compute capacity
based on demand and run multiple segregated copies of the same application stack. It is also portable across different cloud
environments.

Jenkins: This is an open-source continuous integration and delivery tool that can be used to support continuous software delivery.

Helm: This is an open-source package manager for Kubernetes that can be used to deploy application bundles using dynamic templates.

Other options, such as using Google Kubernetes Engine and Cloud Load Balancing or Google Kubernetes Engine and Cloud Deployment
Manager, may not meet all of the requirements specified. For example, Cloud Load Balancing does not support the ability to route
network traffic to specific services based on URL. Similarly, Cloud Deployment Manager does not support continuous software delivery.

upvoted 3 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  IvanDobrinov 7ámonths ago

The key term here is "what CLOUD technologies you would use".
I'd select D for that simple reason - all components are available with GCP.
Helm is just a package manager for K8s, totally unrelated to "Cloud technology".

upvoted 4 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: D

Correct answer is D; Cloud load balancing supports URL based routing. I will go with D. Helm helps you to manage kubernetes application
which anyways GKE is capable of.

upvoted 4 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A seems to be the answer

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

it's mysterious to Google how they score these exams. Hoping they do not give points based on correct or not correct
like here A and D
upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

86/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #32

Topic 1

You have created several pre-emptible Linux virtual machine instances using Google Compute Engine. You want to properly shut down your

application before the virtual machines are preempted.

What should you do?

A. Create a shutdown script named k99.shutdown in the /etc/rc.6.d/ directory

B. Create a shutdown script registered as a xinetd service in Linux and con gure a Stackdriver endpoint check to call the service

C. Create a shutdown script and use it as the value for a new metadata entry with the key shutdown-script in the Cloud Platform Console when

you create the new virtual machine instance

D. Create a shutdown script, registered as a xinetd service in Linux, and use the gcloud compute instances add-metadata command to specify

the service URL as the value for a new metadata entry with the key shutdown-script-url

Correct Answer: C

A startup script, or a shutdown script, is speci ed through the metadata server, using startup script metadata keys.

Reference:

https://cloud.google.com/compute/docs/startupscript

Community vote distribution

C (62%)

D (38%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

https://cloud.google.com/compute/docs/shutdownscript ... So C

upvoted 34 times

? ?  nitinz 2áyears, 3ámonths ago

C, statup/shutdown script = metadata

upvoted 4 times

? ?  VishalB 1áyear, 11ámonths ago

Since the instance is already created Option C gets eliminated. "gcloud compute instances addmetadataö
command can be used to add or update the metadata of a virtual machine instance"

upvoted 4 times

? ?  Gini  Highly Voted ?  3áyears, 1ámonth ago

I have doubts with the answer C because the question states that "You have created the instances" so C works too but the solution cannot
apply to the already created instances. D seems correct to me...

Reference:
https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances

upvoted 18 times

? ?  [Removed] 1áyear, 3ámonths ago

I agree D.
As the mentioned in Question, "You have created several pre-emptible Linux virtual machine...",
and check the answer C , "Create a shutdown script ..... when you create the new virtual machine instance".
The new script apply for new instance creating.
In the Quesiton , the serveral created VMs are being used.
Supply a Cloud Storage URL to the shutdown script file with this key.
https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances

upvoted 3 times

? ?  pepYash 2áyears, 7ámonths ago

Yes. The correct answer should be D.

To add a shutdown script to a running instance, follow the instructions in the Applying a startup script to running instances
documentation but replace the metadata keys with one of the following keys:

shutdown-script: Supply the shutdown script contents directly with this key. Using the gcloud command-line tool, you can provide the
path to a shutdown script file, using the --metadata-from-file flag and the shutdown-script metadata key.
shutdown-script-url: Supply a Cloud Storage URL to the shutdown script file with this key.

upvoted 3 times

? ?  pepYash 2áyears, 7ámonths ago

changed my mind. preemptible vms can be stopped and started anytime. with that flexibility, C is ok.

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

87/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  dsnaghxhinwtsvvmip 2ámonths, 2áweeks ago

xinetd. Xinet makes the D answer be nonsense

upvoted 1 times

? ?  NG123 1áyear ago

I also feel so because the virtual machines are already created.

upvoted 1 times

? ?  JohnWick2020  Most Recent ?  3áweeks, 6ádays ago

C is the answer.
"shutdown-script" metadata to provide a local script.
"Shutdown-script-url" to provide a cloud storage URL where script is stored.

upvoted 1 times

? ?  irmingard_examtopics 3áweeks, 6ádays ago

Selected Answer: D

C will only be an option for new VMs, thus D is the only option!

upvoted 1 times

? ?  h7m 3ámonths ago

Selected Answer: C

Option C works fine. you might want to save the shutdown-script in a file in cloud storage or another static place and provide it's url so
that you can reuse and version it. But option D is much to complicated and impractical to first set up a service to provide a static file to
shut down the server on which the file is served, where you cannot modify it while the server is down .. try to explain that to the new
developer kid 10 years later.

upvoted 1 times

? ?  Kysmor 3ámonths, 2áweeks ago

Correct answer is C!
This link clearly state it: https://cloud.google.com/compute/docs/shutdownscript#provide_shutdown_script_contents_directly

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: C

Add shutdown script via Console during VM creation

upvoted 1 times

? ?  CosminCiuc 5ámonths ago

I believe C is the right answer.
https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances
Apply a shutdown script to running instances
To add a shutdown script to a running instance, follow the instructions in the Applying a startup script to running instances
documentation but replace the metadata keys with one of the following keys:

shutdown-script: Supply the shutdown script contents directly with this key. Using the Google Cloud CLI, you can provide the path to a
shutdown script file, using the --metadata-from-file flag and the shutdown-script metadata key.
shutdown-script-url: Supply a Cloud Storage URL to the shutdown script file with this key.

upvoted 3 times

? ?  examch 6ámonths ago

Selected Answer: D

D is the correct answer,

You are applying the shutdown script to already running instance

Apply a shutdown script to running instances
To add a shutdown script to a running instance, follow the instructions in the Applying a startup script to running instances
documentation but replace the metadata keys with one of the following keys:

shutdown-script: Supply the shutdown script contents directly with this key. Using the Google Cloud CLI, you can provide the path to a
shutdown script file, using the --metadata-from-file flag and the shutdown-script metadata key.
shutdown-script-url: Supply a Cloud Storage URL to the shutdown script file with this key.

https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

A. Create a shutdown script named k99.shutdown in the /etc/rc.6.d/ directory

To properly shut down your application before the virtual machines are preempted, you can create a shutdown script and place it in the
/etc/rc.6.d/ directory. This directory contains scripts that are run when the system is shut down. By creating a script named k99.shutdown
and placing it in this directory, you can ensure that it is run when the system is shut down, including when the virtual machine is
preempted.

Other options, such as creating a shutdown script registered as a xinetd service in Linux and using Stackdriver or the gcloud compute
instances add-metadata command to specify the service URL, may not be necessary for properly shutting down the application before the

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

88/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

virtual machines are preempted. Using the Cloud Platform Console to specify the shutdown script as a metadata entry when creating the
virtual machine instance may not ensure that the script is run when the system is shut down.

upvoted 1 times

? ?  dkumar15 6ámonths, 3áweeks ago

Option C correct - https://cloud.google.com/compute/docs/shutdownscript

upvoted 1 times

? ?  jasenmornin 6ámonths, 4áweeks ago

Selected Answer: C

I see too many people saying that C is incorrect because an shutdown script cannot be assigned to an already created instance. I have just
created one instance and, while running, I have modified its metadata and assigned the shutdown script. Beyond that, my instance was
not preemptible, and this exercise asks for preemtible machines, wich are more flexible with that. I go with C.

upvoted 4 times

? ?  ashrafh 7ámonths, 2áweeks ago

Handle preemption with a shutdown script
When your VM is preempted, you can use a shutdown script to perform cleanup actions before the VM stops. For example, you can
gracefully stop a running process and copy a checkpoint file to Cloud Storage.

The following is a shutdown script that you can add to a running preemptible VM or add to a new preemptible VM when you create it. This
script runs when the VM starts to shut down, before the operating system's normal kill command stops all remaining processes. After
gracefully stopping the desired program, the script performs a parallel upload of a checkpoint file to a Cloud Storage bucket.
https://cloud.google.com/compute/docs/instances/create-use-preemptible#handle_preemption

upvoted 1 times

? ?  diasporabro 8ámonths ago

Selected Answer: D

https://cloud.google.com/compute/docs/shutdownscript#apply_a_shutdown_script_to_running_instances

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is right
To pass in a local shutdown script file, supply the --metadata-from-file flag, followed by a metadata key pair, shutdown-
script=PATH/TO/FILE, where PATH/TO/FILE is a relative path to the shutdown script. For example:

gcloud compute instances create example-instance \
--metadata-from-file shutdown-script=examples/scripts/install.sh

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago
Good luck with the exams

upvoted 1 times

? ?  kazob 8ámonths, 2áweeks ago

Selected Answer: C

D is wrong because shutdown-script-URL only accepts GCS urls

upvoted 3 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: D

D. Create a shutdown script, registered as a xinetd service in Linux, and use the gcloud compute instances add-metadata command to
specify the service URL as the value for a new metadata entry with the key shutdown-script-url

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

89/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #33

Topic 1

Your organization has a 3-tier web application deployed in the same network on Google Cloud Platform. Each tier (web, API, and database) scales

independently of the others. Network tra c should  ow through the web to the API tier and then on to the database tier. Tra c should not  ow

between the web and the database tier.

How should you con gure the network?

A. Add each tier to a different subnetwork

B. Set up software based  rewalls on individual VMs

C. Add tags to each tier and set up routes to allow the desired tra c  ow

D. Add tags to each tier and set up  rewall rules to allow the desired tra c  ow

Correct Answer: D

Google Cloud Platform(GCP) enforces  rewall rules through rules and tags. GCP rules and tags can be de ned once and used across all

regions.

Reference:

https://cloud.google.com/docs/compare/openstack/

https://aws.amazon.com/it/blogs/aws/building-three-tier-architectures-with-security-groups/

Community vote distribution

D (100%)

? ?  shandy  Highly Voted ?  3áyears, 7ámonths ago

D. refer to target filtering. https://cloud.google.com/solutions/best-practices-vpc-design

upvoted 34 times

? ?  AzureDP900 8ámonths, 2áweeks ago

D is right

upvoted 1 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 8 times

? ?  pepYash 2áyears, 7ámonths ago

Thank you for the link.
Precisely:
https://cloud.google.com/solutions/best-practices-vpc-design#target_filtering

upvoted 7 times

? ?  p_inkfreud 5ámonths, 3áweeks ago

perfect! the example in that section is the exact question statement

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

D, firewalls can be done on ip or network tags or service accounts in GCE.

upvoted 4 times

? ?  amxexam  Highly Voted ?  1áyear, 10ámonths ago

Let's go with option elimination

A. Add each tier to a different subnetwork
>> Adding tiers to different subnets does not prevent or block them from accessing each other. Until specific firewall rules on VM or
subnet allow access traffic on a specific port in the rule.

B. Set up software-based firewalls on individual VMs
>> Not a recommended practice will have to enable firewall anyway.

C. Add tags to each tier and set up routes to allow the desired traffic flow
>> Can be done but.

D. Add tags to each tier and set up firewall rules to allow the desired traffic flow
>> Recommended way

Hence D

upvoted 6 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

90/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  red_panda  Most Recent ?  1ámonth ago

Selected Answer: D

For me most suitable answer is D

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

It's D
To configure the network so that traffic flows through the web to the API tier and then on to the database tier, but does not flow between
the web and the database tier, you can add tags to each tier and set up firewall rules to allow the desired traffic flow. By adding tags to
each tier, you can identify the VMs that belong to each tier and create firewall rules that allow traffic between the tiers as needed. For
example, you can create a firewall rule that allows traffic from the web tier to the API tier, and another rule that allows traffic from the API
tier to the database tier. This will ensure that traffic flows through the desired path and is not allowed between the web and database
tiers.

Other options, such as adding each tier to a different subnetwork or setting up software-based firewalls on individual VMs, may not
provide the necessary level of control over the traffic flow between the tiers. Setting up routes to allow the desired traffic flow may not be
sufficient to prevent traffic between the web and database tiers.

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

D is right answer
upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: D

Having 3-tier web application deployed in the same network is wrong to begin with. However, even in different subnets you will need to
apply firewall rules to prevent traffic between selected subnets. In this case they will probably be better of with D.

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

Did this appear in the exam?

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago

use firewall rules
upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: D

A per my comment below .

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

D is the correct answer

upvoted 3 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for D

upvoted 2 times

? ?  unnikrisb 1áyear, 8ámonths ago

From Google practice exam question :
D is correct because as instances scale, they will all have the same tag to identify the tier. These tags can then be leveraged in firewall rules
to allow and restrict traffic as required, because tags can be used for both the target and source.
https://cloud.google.com/vpc/docs/using-vpc
https://cloud.google.com/vpc/docs/routes
https://cloud.google.com/vpc/docs/add-remove-network-tags

upvoted 2 times

? ?  VishalB 2áyears ago
Correct Answer: D
The web tier can communicate with end users and the app tier, and the app tier can communicate with the database tier, but no other
communication between tiers is allowed. The instances running the web tier have a network tag of web, the instances running the app tier
have a network tag of app, and the instances running the database tier have a network tag of db.
https://cloud.google.com/architecture/best-practices-vpc-design#target_filtering

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

D. Add tags to each tier and set up firewall rules to allow the desired traffic flow

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

91/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  un 2áyears, 1ámonth ago

D is correct

upvoted 1 times

? ?  Ausias18 2áyears, 3ámonths ago

answer is D

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

D is ok

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

92/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #34

Topic 1

Your development team has installed a new Linux kernel module on the batch servers in Google Compute Engine (GCE) virtual machines (VMs) to

speed up the nightly batch process. Two days after the installation, 50% of the batch servers failed the nightly batch run. You want to collect

details on the failure to pass back to the development team.

Which three actions should you take? (Choose three.)

A. Use Stackdriver Logging to search for the module log entries

B. Read the debug GCE Activity log using the API or Cloud Console

C. Use gcloud or Cloud Console to connect to the serial console and observe the logs

D. Identify whether a live migration event of the failed server occurred, using in the activity log

E. Adjust the Google Stackdriver timeline to match the failure time, and observe the batch server metrics

F. Export a debug VM into an image, and run the image on a local server where kernel log messages will be displayed on the native screen

Correct Answer: ACE

Community vote distribution

ACE (100%)

? ?  rishab86  Highly Voted ?  2áyears ago

ACE
A. Use Stackdriver Logging to search for the module log entries = Check logs
C. Use gcloud or Cloud Console to connect to the serial console and observe the logs = Check grub messages, remember new kernel
module was installed.
E. Adjust the Google Stackdriver timeline to match the failure time, and observe the batch server metrics = Zoom into the time window
when problem happened.

upvoted 39 times

? ?  Pokchok 2áyears ago

But the assumption you made is that stack driver was already installed on the vms. What if it was not there? Would there be any scope
to install later and retrieve the logs?

upvoted 2 times

? ?  p_inkfreud 5ámonths, 3áweeks ago

But isn't it the same with B? it is talking about 'reading' the logs.

upvoted 1 times

? ?  AmitAr 1áyear, 1ámonth ago

A, B, E
C - doesn't look correct as it ends with "observe the logs" - question is on sharing the details to development team, not to look for cause

upvoted 1 times

? ?  haroldbenites  Highly Voted ?  1áyear, 6ámonths ago

Go for A,B,E.
C is when the VM is running , but in this case the sentence says ôrecollectö. It means that ôerror everö already happened.

upvoted 8 times

? ?  pddddd 1áyear, 5ámonths ago

and how will activity log help?

upvoted 1 times

? ?  Ozymandiax  Most Recent ?  5ámonths, 3áweeks ago

I'm really not sure here. A and E are just OK, and for me the final point is between B o C. many ppl is saying C, but, the question says that
the VM's already failed and you're investigating what happened in the past.

Anyway, there are 2 ways to interpret this, from my point of view:
1) The failure happened and it's going to happen again. In this case ACE would be maybe the best option
BUt
2) The failure happened and you want to investigate this failure, which happened in the past. Therefore ABE would be the right one, as
you are "splunking" in the logs of the past, not having a review of the logs as they happen.

from my personal interpretation I'd go with ABE

upvoted 6 times

? ?  omermahgoub 6ámonths, 1áweek ago

ACE
To collect details on the failure of the batch servers in GCE VMs, you can take the following actions:

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

93/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A: Stackdriver Logging can help you identify any issues related to the new Linux kernel module by searching for log entries related to the
module.

C: Connecting to the serial console allows you to view the logs in real-time as the batch servers are running. This can help you identify any
issues related to the new kernel module.

E: By adjusting the timeline in Stackdriver to match the failure time, you can view the batch server metrics during the time when the
failures occurred. This can help you identify any issues related to the new kernel module.

Other options, such as reading the debug GCE Activity log using the API or Cloud Console, identifying whether a live migration event of
the failed server occurred, or exporting a debug VM into an image and running the image on a local server, may not provide the necessary
information to understand

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: ACE

ACE by eliminating the incorrect answers

upvoted 2 times

? ?  holerina 9ámonths, 1áweek ago

ADE seems correct

upvoted 1 times

? ?  PaulCatalin 9ámonths, 1áweek ago

Selected Answer: ACE

I vote for.

upvoted 1 times

? ?  ehgm 1áyear, 6ámonths ago

This question is very poorly asked.

There is no place saying if the live migration is enabled. If a VM is not set to live migrate, the VM is terminated during host system events.

There is no place saying if you run into problems accessing your instance through SSH or need to troubleshoot an instance that is not fully
booted, so you can enable interactive access to the serial console.

C: https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-using-serial-console
D: https://cloud.google.com/compute/docs/instances/live-migration

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: ACE

ACE are the correct choices

upvoted 2 times

? ?  duocnh 1áyear, 7ámonths ago

Selected Answer: ACE

vote ACE

upvoted 3 times

? ?  MaxNRG 1áyear, 8ámonths ago
I would say that Q26 = ABE
A - since it investigates logs of Linux kernel module installed recently. For that Log Agent should be installed on VMs and Linux syslog is
streamed by default to Stackdriver Logging via agent. So, this answer is relevant to Q's context, it checks if new Linux kernel runs OK.
B - investigates "app-level" issues on GCE, logs API called from this VM, system events, etc.
C - review of Serial Log is useful only for HW/OS crashes, and only during short period of time (since only last 1MB of logs are stored there,
if more logs needed then they are streamed to Stackdriver logging). So, this option doesn't fit 2 days period and also serves different
failure types.
D - live migration event is irrelevant to this Q (transferring hot/running context of one VM to another transparently, so original VM can be
maintained û BIOS/HW updates). Even if that happens, then GCE activity logs in B should cover this.
E - monitoring of metrics at the time of failure makes sense for troubleshooting.
F - smth long and ridiculous.

upvoted 6 times

? ?  [Removed] 1áyear, 8ámonths ago

If I'm reading "F" correctly, it is to export a VM and move it back to a "local" server which I'm reading as "on-prem", your laptop, or your
local datacenter.(aka NOT GCP) So if I'm reading that correctly, that is a very ineffective idea. Keep it in GCP and use the powerful GCP
tools. If somebody feels that I'm reading that wrong, I would love to see from a different POV. If you are reading that the same as me, then
rule out F.

upvoted 1 times

? ?  ashish_t 1áyear, 8ámonths ago
F is practically impossible.

upvoted 1 times

? ?  [Removed] 1áyear, 8ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

94/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

We can also rule out D, Live Migration Event. Unless it is called out, regular VMs can be live migrated without an notice of the guest os. So
this should not be your focus. Rule out D.

https://cloudplatform.googleblog.com/2015/03/Google-Compute-Engine-uses-Live-Migration-technology-to-service-infrastructure-
without-application-downtime.html

upvoted 1 times

? ?  [Removed] 1áyear, 8ámonths ago

So in an effort to rule things out, B is not applicable. The question says Activity Log, but this is now called Audit Log and is a record of who
touched the server and made changes. So if you think a human/service came in after the fact and modified the system, then this would be
useful, but that is not the case here. So rule it out.

https://cloud.google.com/compute/docs/logging/audit-logging

"Who did what, where, and when?"

upvoted 6 times

? ?  maxlearn 1áyear, 8ámonths ago

Is there any reason why can't 'D' be an answer?

upvoted 2 times

? ?  kalamarka 1áyear, 8ámonths ago

"You want to collect details on the failure" says the question. And D is not related with the failure reason but the after failure action.

upvoted 2 times

? ?  victory108 1áyear, 12ámonths ago

A. Use Stackdriver Logging to search for the module log entries
C. Use gcloud or Cloud Console to connect to the serial console and observe the logs
E. Adjust the Google Stackdriver timeline to match the failure time and observe the batch server metrics

upvoted 2 times

? ?  Yogikant 2áyears ago

Serial port output is accessible through the Cloud Console, the gcloud tool, and the Compute Engine API, but only while the VM instance is
running. https://cloud.google.com/compute/docs/instances/viewing-serial-port-output

Requirement is to collect information about failed batch servers which have already happened. Hence C is not suitable. Live migration
doesn't disrupt running VM.

A, B, E.

upvoted 2 times

? ?  aviratna 2áyears ago
I think ACE is correct.
Only the batch job is failed VM is still running so it will still have serial port output

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

95/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #35

Topic 1

Your company wants to try out the cloud with low risk. They want to archive approximately 100 TB of their log data to the cloud and test the

analytics features available to them there, while also retaining that data as a long-term disaster recovery backup.

Which two steps should you take? (Choose two.)

A. Load logs into Google BigQuery

B. Load logs into Google Cloud SQL

C. Import logs into Google Stackdriver

D. Insert logs into Google Cloud Bigtable

E. Upload log  les into Google Cloud Storage

Correct Answer: AE

Community vote distribution

AE (89%)

11%

? ?  rishab86  Highly Voted ?  2áyears ago

Answer is A as they want to load logs for analytics and E for storing data in buckets for long term.

upvoted 28 times

? ?  ionescuandrei  Most Recent ?  2ámonths, 3áweeks ago

Selected Answer: AE

This looks right.
upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

AE
To archive approximately 100 TB of log data to the cloud and test the analytics features available while also retaining the data as a long-
term disaster recovery backup, you can take the following steps:

E: Upload log files into Google Cloud Storage: Google Cloud Storage is a scalable, durable, and fully-managed cloud storage service that
can be used to store large amounts of data. You can upload your log files to Cloud Storage to archive them in the cloud.

A: Load logs into Google BigQuery: Google BigQuery is a fully-managed, cloud-native data warehouse that can be used to analyze large
amounts of data quickly and efficiently. You can load your log data into BigQuery to perform analytics on it and test the available analytics
features.

Other options, such as loading logs into Google Cloud SQL, importing logs into Google Stackdriver, or inserting logs into Google Cloud
Bigtable, may not provide the necessary functionality for archiving and analyzing the log data.

upvoted 4 times

? ?  CMata 7ámonths, 2áweeks ago

Selected Answer: AE

If you want to analize those logs its recommended Big Query. For storing and backup Cloud Storage is your option, so AE

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago
A and E can do the required task

upvoted 2 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: AE

AE are the only options for analysis and archiving

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago

A load in Big query for analytics and E for cloud storage

upvoted 1 times

? ?  Ramheadhunter 10ámonths, 2áweeks ago

Selected Answer: AE

The key word is 'Analytics' here the main reason for moving logs to GCP is to perform Analytics on the data. BigQuery is the best suite for
it. For long term storage it wiould be GC

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

96/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  raaj_p 10ámonths, 3áweeks ago

Selected Answer: CE

Answer is C and E

Key features
Real-time log management and analysis
Cloud Logging is a fully managed service that performs at scale and can ingest application and platform log data, as well as custom log
data from GKE environments, VMs, and other services inside and outside of Google Cloud. Get advanced performance, troubleshooting,
security, and business insights with Log Analytics, integrating the power of BigQuery into Cloud Logging. -
https://cloud.google.com/products/operations

upvoted 1 times

? ?  AMohanty 10ámonths, 2áweeks ago

you don't do realtime log management on 10 TB data.
You only perform analytics on it.
So A for Analytics
E for storage.
upvoted 5 times

? ?  Ramheadhunter 10ámonths, 2áweeks ago

The key word is 'Analytics' here the main reason for moving logs to GCP is to perform Analytics on the data. BigQuery is the best suite
for it. For long term storage it wiould be GCS

upvoted 1 times

? ?  faagee01 10ámonths, 4áweeks ago

Selected Answer: AE

Big Query for analytics, Cloud Storage for long term archive

upvoted 3 times

? ?  Dhiraj03 1áyear ago

For Storage GCS is the best option and for analyzing the data BIg Query makes sense

upvoted 1 times

? ?  Nirca 1áyear, 2ámonths ago

A E are ok

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago
AE are the correct answers

upvoted 3 times

? ?  andeu 1áyear, 6ámonths ago

Answers: A is correct because BigQuery is the fully managed cloud data warehouse for analytics and supports the analytics requirement.
E is correct because Cloud Storage provides the Coldline storage class to support long-term storage with infrequent access, which would
support the long-term disaster recovery backup requirement.
https://cloud.google.com/bigquery/
https://cloud.google.com/stackdriver/
https://cloud.google.com/storage/docs/storage-classes#coldline
https://cloud.google.com/sql/
https://cloud.google.com/bigtable/

upvoted 4 times

? ?  MQQ 1áyear ago

But BigQuery is for SQL DATA, the logs are nosql ?
Why not choose stackdriver?

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A,E

upvoted 3 times

? ?  Bakili 1áyear, 7ámonths ago

A and E

upvoted 1 times

? ?  nocrush 1áyear, 8ámonths ago

A E is the right answer

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

97/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #36

Topic 1

You created a pipeline that can deploy your source code changes to your infrastructure in instance groups for self-healing. One of the changes

negatively affects your key performance indicator. You are not sure how to  x it, and investigation could take up to a week.

What should you do?

A. Log in to a server, and iterate on the fox locally

B. Revert the source code change, and rerun the deployment pipeline

C. Log into the servers with the bad code change, and swap in the previous code

D. Change the instance group template to the previous one, and delete all instances

Correct Answer: B

Community vote distribution

B (63%)

D (36%)

? ?  amxexam  Highly Voted ?  1áyear, 10ámonths ago

Let's go with option elimination

A. Log in to a server, and iterate on the fix locally
>> Long step, hence eliminate

B. Revert the source code change and rerun the deployment pipeline
>> This revert will be logged in the source repo. Will go with this way although D also is correct.

C. login to the servers with the bad code change, and swap in the previous code
>> C is manually doing what can be automatically done by B and C, hence eliminate.

D. Change the instance group template to the previous one and delete all instances
>> This is similar to B but why manually do something which is automated. Hence eliminate. But is also correct. But B is better from code
lifecycle perspective.

Hence B

upvoted 57 times

? ?  ewredtrfygi  Highly Voted ?  2áyears, 10ámonths ago

Too many responses saying B is the answer - I wonder if GCP pays people to provide the wrong answers on this website. It's clearly D, MIG
templates support versioning, they were created to solve this exact problem. You simply select the previous template version, set that as
the new deployment, and it will roll back the KPI depriving deployment and roll out the previous working deployment. The only part of D I
don't like is the "terminate all instances" since you should engage in a rolling deployment, but if it's not a live website I suppose that
would be fine.
https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups

upvoted 56 times

? ?  kimharsh 1áyear ago

Mate , stop confusing people , self healing = unmanaged instance group, and if you use unmanaged IG (not MIG) you can't use
template , so D is wrong

upvoted 6 times

? ?  kimberjdaw 2áyears, 9ámonths ago

Architect exams are usually real-world exams. In the real world, if you're using CI/CD, you do everything via CI/CD. Period.

upvoted 23 times

? ?  XAvenger 1áyear, 9ámonths ago

First of all you need to revert the source of this issue - bad code. You can revert bad template, but when anyone pushes code change to
the repo -> you will have the build with broken code again that will be deployed to your servers.
When you revert your bad code you are free to investigate in another branch during a week without being afraid that any other code
commit will deploy bad code again.

upvoted 29 times

? ?  MestreCholas 3ámonths, 4áweeks ago

changing the instance group template to the previous one and deleting all instances, is also not recommended as it is a drastic
measure that could cause downtime and potential data loss.

upvoted 2 times

? ?  chrismar  Most Recent ?  1áweek ago

Selected Answer: B

Because the question starting from source code

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

98/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  oriori123123 3áweeks ago

Selected Answer: B

by bard:
The correct answer is B. Revert the source code change, and rerun the deployment pipeline.

upvoted 1 times

? ?  JohnWick2020 3áweeks, 6ádays ago

B.
The keyword here is source code not VM configuration. If it was the later then instance group templates is the answer. But in this case
simply rollback your source code change and rerun to last workable version. Simples!

upvoted 2 times

? ?  red_panda 1ámonth ago

Selected Answer: B

B is ok for me
upvoted 1 times

? ?  Kiroo 1ámonth, 3áweeks ago

I would go with D, but it depends on many details.
if it is an app KPI and there is a real issue to the business I would opt to make the app work perfectly again ASAP and then I would address
the issue in the code base.
But I can understand who would say that they would go with the pipeline approach.

upvoted 1 times

? ?  taer 3ámonths ago

Selected Answer: B

If a change negatively affects your key performance indicator, it's best to revert the source code change to a known good state and rerun
the deployment pipeline. This ensures that your infrastructure is restored to a stable state while you investigate and fix the issue.
Reverting the change and redeploying the code will allow your instance groups to continue functioning with the previous stable version,
minimizing the impact on your application and users.

upvoted 1 times

? ?  feholen210 3ámonths, 1áweek ago

Selected Answer: B

B is correct.

upvoted 1 times

? ?  JC0926 3ámonths, 2áweeks ago

Selected Answer: B

If one of the changes negatively affects your key performance indicator and you are unsure how to fix it, the best course of action is to
revert the change to a previous version that was not causing any issues. Once the source code has been reverted, you can rerun the
deployment pipeline to ensure that the infrastructure is running on the previous version that was not causing any issues. This will ensure
that the performance of the infrastructure is restored to its previous state.

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: B

Changed my mind, agree with B. Deletion of all instances is too drastic with D

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: D

B could re-create the same problem, no way to know till troubleshooting analysis has been completed

upvoted 1 times

? ?  Deb2293 3ámonths, 4áweeks ago

Selected Answer: B

Option D would revert all instances to the previous template and delete all current instances, which could potentially solve the issue but
would also result in downtime and the loss of any data or changes made since the previous template.

So B is best

upvoted 1 times

? ?  mateuszma 4ámonths ago

Selected Answer: D

must be D!

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

B
If one of the source code changes negatively affects your key performance indicator and you are not sure how to fix it, and investigation
could take up to a week, the best course of action would be to revert the source code change and rerun the deployment pipeline. This will
allow you to quickly roll back the code change and restore your infrastructure to its previous state, while also allowing you to continue
investigating the issue and identifying a long-term solution.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

99/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Option A, logging in to a server and iterating on the fix locally, would not be an effective solution because it would not fix the issue in your
deployed infrastructure. Option C, logging into the servers with the bad code change and swapping in the previous code, would be a more
complex and time-consuming approach that may not be feasible if you have many servers or instances. Option D, changing the instance
group template to the previous one and deleting all instances, would also be a more complex and time-consuming approach that could
cause significant downtime for your application.

upvoted 2 times

? ?  i_am_robot 6ámonths, 1áweek ago

The best option in this situation would be to revert the source code change and rerun the deployment pipeline. This will allow you to
quickly roll back the change that is negatively affecting your key performance indicator without having to manually log into servers or
make further changes to your infrastructure.

Once you have rolled back the change, you can then focus on investigating the issue and determining a permanent solution. This may
involve making further code changes and rerunning the deployment pipeline again once the issue has been resolved.

Option C, logging into the servers with the bad code change and swapping in the previous code, would likely be time-consuming and
error-prone, as it would require manual intervention on each server. Option D, changing the instance group template and deleting all
instances, would also be a time-consuming and potentially disruptive solution, as it would require the creation and deployment of new
instances, which could take some time. Option A, logging in to a server and iterating on the fix locally, would not be a good option, as it
would not address the issue on the servers that are already running the bad code.

upvoted 1 times

? ?  neokyle 6ámonths, 1áweek ago
The correct answer is D.
Images > Source Code Redeployment.
I encounter this misconception all the time when doing cloud consulting.

Build pipelines often use latest builders and source code often says rebuild using the latest dependencies, so source code-based rollbacks
are often ineffective rollbacks as they're not a 1:1 rollback to what you had. It's very often that using latest updates breaks things in
unexpected ways, that's why using explicit image tags on docker images is considered a best practice and why dependency pinning is a
thing. Using an image-based rollback solution is the only valid solution to rolling back to the previous version of the application and a MIG
allows you to do that. A CICD pipeline/git commit-based rollback will roll back to something close but not the same.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

100/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #37

Topic 1

Your organization wants to control IAM policies for different departments independently, but centrally.

Which approach should you take?

A. Multiple Organizations with multiple Folders

B. Multiple Organizations, one for each department

C. A single Organization with Folders for each department

D. A single Organization with multiple projects, each with a central owner

Correct Answer: C

Folders are nodes in the Cloud Platform Resource Hierarchy. A folder can contain projects, other folders, or a combination of both. You can use

folders to group projects under an organization in a hierarchy. For example, your organization might contain multiple departments, each with its

own set of GCP resources. Folders allow you to group these resources on a per-department basis. Folders are used to group resources that

share common IAM policies. While a folder can contain multiple folders or resources, a given folder or resource can have exactly one parent.

Reference:

https://cloud.google.com/resource-manager/docs/creating-managing-folders

Community vote distribution

C (100%)

? ?  AWS56  Highly Voted ?  3áyears, 7ámonths ago

https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations

I will stick with C
upvoted 26 times

? ?  AzureDP900 8ámonths, 2áweeks ago
C is recommended approach

upvoted 1 times

? ?  red_panda  Most Recent ?  1ámonth ago

Selected Answer: C

Is obviously C
upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

C. A single Organization with Folders for each department

To control IAM policies for different departments independently but centrally, you should create a single organization and use folders to
organize the policies for each department. This approach allows you to centralize the management of IAM policies for all departments
within a single organization, while also allowing you to set up different policies for each department as needed.

Option A, multiple organizations with multiple folders, would not be an effective solution because it would create unnecessary complexity
and make it more difficult to centralize the management of IAM policies. Option B, multiple organizations, one for each department,
would also not be an effective solution because it would create unnecessary complexity and make it more difficult to centralize the
management of IAM policies. Option D, a single organization with multiple projects, each with a central owner, would not be an effective
solution because it would not allow you to set up different policies for each department as needed.

upvoted 2 times

? ?  MarcoEscanor 8ámonths, 1áweek ago

C. It's a best practice and I've done this with my previous and current company :)

upvoted 3 times

? ?  Prashant2022 9ámonths ago

Selected Answer: C

.............

upvoted 1 times

? ?  BiddlyBdoyng 9ámonths ago

The reason it isn't D is that a Dept modelled as a project puts a massive constraint on the dept that they can only have a single project, it's
likely a department will want many projects.

upvoted 2 times

? ?  holerina 9ámonths, 1áweek ago

C single org and multiple folders

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

101/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  cmamiusa 1áyear, 2ámonths ago

Selected Answer: C

C is the right answer

upvoted 1 times

? ?  mygcpjourney2712 1áyear, 3ámonths ago

Selected Answer: C

Will go for C

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

C is the right answer

upvoted 1 times

? ?  nansi 1áyear, 9ámonths ago

C shall be the correct answer

upvoted 1 times

? ?  rikoko 1áyear, 10ámonths ago

C. Seems to be best practice (cf AWS56). And I believe that D should be excluded because it says "Project owner" - it is not best practice
since it's a basic role + it's not even stated as a requisite

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

C. A single Organization with Folders for each department

upvoted 3 times

? ?  un 2áyears, 1ámonth ago

C is correct

upvoted 1 times

? ?  Ausias18 2áyears, 3ámonths ago

Answer is C

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

C is ok

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

102/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #38

Topic 1

You deploy your custom Java application to Google App Engine. It fails to deploy and gives you the following stack trace.

What should you do?

A. Upload missing JAR  les and redeploy your application.

B. Digitally sign all of your JAR  les and redeploy your application

C. Recompile the CLoakedServlet class using and MD5 hash instead of SHA1

Correct Answer: B

Community vote distribution

B (89%)

11%

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

Signing the JAR files grants it permissions. (https://docs.oracle.com/javase/tutorial/deployment/jar/signindex.html)

upvoted 21 times

? ?  nitinz 2áyears, 3ámonths ago

B, SHA1 Digest error in the first line in the error code. With Java errors, always focus on the first line in the error code, rest of the lines
are garbage **mostly**.

upvoted 14 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 8 times

? ?  Urban_Life 1áyear, 8ámonths ago

Where do you go? when we need you for other questions. Plz ans other q's if you have time

upvoted 1 times

? ?  omermahgoub  Highly Voted ?  6ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

103/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

The most likely cause of the error is that one of the JAR files in your application has been tampered with or is corrupt. The SHA1 digest
error indicates that the JAR file's signature does not match the expected value, which could be due to tampering or corruption.

To fix the issue, you should try uploading missing JAR files and redeploying your application. If the issue persists, you may need to digitally
sign all of your JAR files and redeploy your application to ensure that the signatures are valid. You should not try to recompile the Cloaked

upvoted 7 times

? ?  JC0926  Most Recent ?  3ámonths, 2áweeks ago

Selected Answer: A

A. Upload missing JAR files and redeploy your application.

The error message indicates that there is a problem with the SHA1 digest for the "com/altostrat/cloakedservlet.class" file. This can be
caused by a corrupted or incomplete JAR file. Therefore, the best course of action is to upload any missing JAR files and redeploy the
application.

upvoted 1 times

? ?  Racinely 7ámonths, 2áweeks ago

B is correct

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

Ok B but how is this question related to a GCP exam? I guess a google search will be faster than reading the theory around Java (unless
you are a developer).

upvoted 5 times

? ?  zr79 8ámonths, 2áweeks ago

have you done any Azure exams? you will thank Google

upvoted 4 times

? ?  AzureDP900 8ámonths, 2áweeks ago

nothing to do with GCP however basic troubleshooting skills required as a DevOps or Architect, B is fine

upvoted 2 times

? ?  p_inkfreud 5ámonths, 3áweeks ago

I see your point, but for basic troubleshooting of apps, i will usually have access to google (aka stackoverflow homepage). This could
have been a cloud developer question that they repurposed.

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago

B is the right answer

upvoted 1 times

? ?  avinashvidyarthi 1áyear, 1ámonth ago

Selected Answer: B

B is Correct!

upvoted 1 times

? ?  Munna19 1áyear, 1ámonth ago

B is the right answer

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

B is the correct answer

upvoted 2 times

? ?  duocnh 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  TheCloudBoy77 1áyear, 7ámonths ago

Selected Answer: B

B is correct answer

upvoted 1 times

? ?  bala786 1áyear, 11ámonths ago

Option B. Digitally sign all of your JAR files and redeploy your application

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

104/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  victory108 2áyears, 1ámonth ago

B. Digitally sign all of your JAR files and redeploy your application

upvoted 3 times

? ?  un 2áyears, 1ámonth ago

B is correct

upvoted 1 times

? ?  Ausias18 2áyears, 3ámonths ago

Answer is B

upvoted 2 times

? ?  IamFleur 2áyears, 4ámonths ago

B for sure

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

105/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #39

Topic 1

You are designing a mobile chat application. You want to ensure people cannot spoof chat messages, by providing a message were sent by a

speci c user.

What should you do?

A. Tag messages client side with the originating user identi er and the destination user.

B. Encrypt the message client side using block-based encryption with a shared key.

C. Use public key infrastructure (PKI) to encrypt the message client side using the originating user's private key.

D. Use a trusted certi cate authority to enable SSL connectivity between the client application and the server.

Correct Answer: C

Community vote distribution

C (70%)

D (30%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

I am not sure about this one. D works if SSL client authentication is enabled.
C works as well if client encrypts message with private key and server decrypt with public key.
I prefer C.

upvoted 32 times

? ?  JoeShmoe 3áyears, 7ámonths ago

Agree with C

upvoted 5 times

? ?  asfar 3áyears, 5ámonths ago
I agree with C on this one.

upvoted 5 times

? ?  Tobbe  Highly Voted ?  2áyears, 4ámonths ago

Encrypting each block and tagging each message at the client side is an overhead on the application. Best method which has been
adopted since years is contacting SSL provider and use public certificate to encrypt the traffic between client and server.

D is correct

upvoted 10 times

? ?  Alekshar 2áyears, 4ámonths ago

If you use the server's public certificate to encrypt your data you only ensure the right server is the only one to read you.

But anyone can use the same encryption key as you did and pretend to be you. Hence it does not solve our authentication problematic

upvoted 5 times

? ?  PeppaPig 1áyear, 11ámonths ago

SSL doesn't use server's public key to encrypt data. This is definitely wrong. Please read SSL specs. SSL uses a separate session key
for message encryption. This session key is temporary and will be rotated for every single session.

upvoted 3 times

? ?  Tobbe 2áyears, 3ámonths ago

thanks for your insight! C is correct.

upvoted 2 times

? ?  lynx256 2áyears, 2ámonths ago

I cannot agree with you. Before one be able to pretend to be someone else, he should know his (someone's) password on the Chat
Server...

upvoted 1 times

? ?  Meyucho 1áyear, 5ámonths ago

If you use server public key you aren't meeting the goal. Don't miss the "for specific user" in the statement

upvoted 1 times

? ?  oriori123123  Most Recent ?  3áweeks ago

bard say D.
and ChatGTP say C..

upvoted 1 times

? ?  ionescuandrei 2ámonths, 3áweeks ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

106/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

I noticed this question in other tests and the suggested answer was C.

upvoted 1 times

? ?  JC0926 3ámonths, 2áweeks ago

Selected Answer: C

To prevent message spoofing, it is important to ensure that messages cannot be altered or forged by anyone other than the originating
user. One way to accomplish this is by using public key infrastructure (PKI) to encrypt messages using the originating user's private key.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

To ensure that chat messages cannot be spoofed and that the messages are truly sent by a specific user, the best option would be to use
public key infrastructure (PKI) to encrypt the message client side using the originating user's private key. This would allow the recipient to
verify the authenticity of the message by using the originating user's public key to decrypt the message.

Option A, tagging the message with the originating user identifier and the destination user, would not ensure the authenticity of the
message, as it could potentially be forged by an attacker.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option B, encrypting the message using block-based encryption with a shared key, would also not ensure the authenticity of the
message, as the shared key could potentially be compromised by an attacker.

Option D, using a trusted certificate authority to enable SSL connectivity between the client application and the server, would help to
secure the communication channel between the client and the server, but it would not necessarily ensure the authenticity of the chat
messages themselves.

Overall, using PKI and the originating user's private key to encrypt the message would be the most effective way to ensure the
authenticity of the chat messages in your mobile chat application.

upvoted 1 times

? ?  FateSpring eld 7ámonths ago

C is the answer, The requirement is the integrity of messages sent in CIA security (Confidentiality, Integrity, and Availability). For
Confidentiality, using PublicKey of receiver, for Integrity, using PrivateKey of sender. D works in case of SSL client authentication.

upvoted 3 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  AjayPandit 7ámonths, 3áweeks ago

Selected Answer: C

PKI uses X.509 certificates and Public Keys, where the key is used for end-to-end encrypted communication, so that both parties can trust
each other and test their authenticity. PKI is mostly used in TLS/SSL to secure connections between the user and the server, while the user
tests the serverÆs authenticity to make sure itÆs not spoofed

upvoted 1 times

? ?  BiddlyBdoyng 9ámonths ago

C is the best
A: Can be spoofed by amending the tags
B: Shared key so can be spoofed
C: Protects from start to end
D: Encrypts the data in transit to the server. Attack possible on server

upvoted 5 times

? ?  holerina 9ámonths, 1áweek ago
D is the right answer use SSL

upvoted 2 times

? ?  binisho123 9ámonths, 3áweeks ago

"Use public key infrastructure (PKI) to encrypt the message client side using the originating user's PRIVATE KEY" How on earth can you
encrypt with another party Private key???
Answer is D

upvoted 2 times

? ?  aut0pil0t 10ámonths ago

Selected Answer: C

D will just ensure the traffic's confidentiality and no one interferes in the middle. It cannot verify the actual user's identity at the
application level. If it is spoofed at the origin, D will just pass it along. Only a public/private key pair can act against spoofing an identity. D
can supplement C though. Easy C.

upvoted 4 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is good in this scnario

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

107/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  faagee01 10ámonths, 4áweeks ago

SSL connection using Certificates provides secure communication channel between the client and server, thereby preventing anyone from
tampering with the data being exchanged. D is the correct answer

upvoted 1 times

? ?  Xumbegnows 1áyear ago

Selected Answer: D

As far as I understand, you don't encrypt data with your own private key in PKI. Instead you encrypt with the public key of your peer and
the peer can decrypt with its private key since it knows them both.
I believe D makes more sense.

upvoted 4 times

? ?  binisho123 9ámonths, 3áweeks ago

I am like "DAA"
upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: D

D as explained in my comment before .

upvoted 1 times

? ?  slars2k 1áyear, 3ámonths ago

SSL ensures the integrity on the wire but short of proving the identity of a specific user. Hence will go with C

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

108/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #40

Topic 1

As part of implementing their disaster recovery plan, your company is trying to replicate their production MySQL database from their private data

center to their

GCP project using a Google Cloud VPN connection. They are experiencing latency issues and a small amount of packet loss that is disrupting the

replication.

What should they do?

A. Con gure their replication to use UDP.

B. Con gure a Google Cloud Dedicated Interconnect.

C. Restore their database daily using Google Cloud SQL.

D. Add additional VPN connections and load balance them.

E. Send the replicated transaction to Google Cloud Pub/Sub.

Correct Answer: B

Community vote distribution

B (100%)

? ?  chiar  Highly Voted ?  3áyears, 7ámonths ago

I think B is correct. I think it is more reliable.

upvoted 27 times

? ?  mawsman  Highly Voted ?  3áyears, 4ámonths ago

It's latency issues. That won't be solved by adding another VPN tunnel. If it was just a throughput issue then VPN would do, however to
improve latency you need to go layer 2. Answer is B

upvoted 21 times

? ?  SandipGhosal  Most Recent ?  3ámonths, 3áweeks ago

I think the best option is E, using PubSub. In question the main issue is " a small amount of packet loss". As per google PubSub
documentation, data replication among databases is one of the common use case of PubSub. The asynchronously communication of
PubSub can overcome small latency issues. Setting up dedicated interconnect would be very costly and required many pre-requisites.

upvoted 1 times

? ?  Shawnn 3ámonths, 4áweeks ago

You could technically use your private key to encrypt a message, but it would not be secure because anyone who has your public key could
decrypt the message. The recommended practice is to use your private key only for decryption and to use the recipient's public key for
encryption.

I vote for D

upvoted 1 times

? ?  SirajShan 3ámonths, 4áweeks ago

Configuring a Google Cloud Dedicated Interconnect requirement for company is a proximity to colocation facility and meeting condition
to have dedicated interconnect. Had this was possible why did they used Cloud VPN in the first place ? I think answer should be D.

upvoted 1 times

? ?  n_nana 5ámonths, 2áweeks ago

If i face this issue, I will give a try with additional VPN, decision using VPN, maybe because they need encryption as well. With switching to
dedicated interconnect, you have to implement your own VPN solution or application encryption. so it need more anaylsis to just skip VPN
and use dedicated interconnect solution.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

The company should consider configuring a Google Cloud Dedicated Interconnect. A Google Cloud Dedicated Interconnect provides a
private connection between the company's on-premises data center and GCP, which can help to reduce latency and improve the reliability
of the connection. This can be particularly useful for replicating large amounts of data or for applications that require low-latency
connectivity.

Option A, configuring the replication to use UDP, would not necessarily improve the reliability of the connection, as UDP is a
connectionless protocol that does not guarantee delivery of packets.

Option C, restoring the database daily using Google Cloud SQL, would not address the underlying issues with the replication process.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option D, adding additional VPN connections and load balancing them, may help to improve the reliability of the connection by
providing redundancy, but it may not necessarily address latency issues.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

109/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Option E, sending the replicated transaction to Google Cloud Pub/Sub, could potentially help to improve the reliability of the replication
process by allowing the company to handle failures and retries in a more structured way, but it would not necessarily address latency
issues.

Overall, configuring a Google Cloud Dedicated Interconnect is likely to be the most effective solution for addressing latency issues and
packet loss in the replication process.

upvoted 1 times

? ?  VSMu 4ámonths, 4áweeks ago

Why focus on latency when the solution is for disaster recovery? The main issue is packet loss. While this can be solved with
Dedicated Interconnect or Cloud Pub/Sub, PubSub seems like a cheaper alternative that prevents data loss and achieves reliability. I
wouldn't care about latency as the backup is only for DA.. so how does it matter if it goes slowly?

upvoted 3 times

? ?  ashrafh 7ámonths, 2áweeks ago

so just to solve this issue we are going over a Dedicated Interconnect imagine saying this to a your project head.

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

ok for B

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

This is straight forward question, B is right. Dedicated line solves latency issues

upvoted 2 times

? ?  Dhiraj03 1áyear ago

Latency issue - Dedicated Interconnect solves the issue

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B

upvoted 6 times

? ?  TheCloudBoy77 1áyear, 7ámonths ago

Selected Answer: B

Correct answer is B as its a latency issue.

upvoted 4 times

? ?  vincy2202 1áyear, 7ámonths ago

B is the correct answer.

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û Configure Google Cloud Dedicated Interconnect
Company can buy 10G interconnect link ($1700 monthly) with 99.9% SLA. ItÆs comparatively small budget for reliable migration of DB. Only
requirement for company is a proximity to colocation facility (peering edge network).
A û UDP is non-reliable, doesnÆt guarantee delivery
C û daily replication doesnÆt solve problem at all (could be temp workaround), also no improvement in bandwidth, so replication may fail
also.
D û adding VPN tunnels (1.5Gbps x N) could improve the situation (but not VPN connections, which are somehow ôload balancedö)
E û using Cloud Pub/Sub is awkward solution. Would require extra protocol overhead for this Proxy service.

upvoted 5 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 1 times

? ?  kopper2019 2áyears ago

hey guys check Q3 for new Qs, 49 New Qs

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

110/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #41

Topic 1

Your customer support tool logs all email and chat conversations to Cloud Bigtable for retention and analysis. What is the recommended

approach for sanitizing this data of personally identi able information or payment card information before initial storage?

A. Hash all data using SHA256

B. Encrypt all data using elliptic curve cryptography

C. De-identify the data with the Cloud Data Loss Prevention API

D. Use regular expressions to  nd and redact phone numbers, email addresses, and credit card numbers

Correct Answer: C

Reference:

https://cloud.google.com/solutions/pci-dss-compliance-in-gcp#using_data_loss_prevention_api_to_sanitize_data

Community vote distribution

C (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

C is the answer
upvoted 22 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 8 times

? ?  nitinz 2áyears, 3ámonths ago
C, data sanitization = DLP

upvoted 7 times

? ?  omermahgoub  Highly Voted ?  6ámonths, 1áweek ago

The recommended approach for sanitizing data of personally identifiable information or payment card information before storing it in
Cloud Bigtable is option C: De-identify the data with the Cloud Data Loss Prevention API.

The Cloud Data Loss Prevention (DLP) API is a powerful tool that allows you to automatically discover, classify, and redact sensitive data in
your organization. It uses advanced machine learning techniques to accurately identify and protect a wide range of sensitive data types,
including personal information such as names, addresses, phone numbers, and payment card information.

Using the DLP API to de-identify your data before storing it in Cloud Bigtable is the most effective way to ensure that sensitive information
is protected and not accessible to unauthorized users.

upvoted 6 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: Hashing data using SHA256 is not sufficient for protecting sensitive information, as hashes can be reversed using various
techniques.

Option B: Encrypting data using elliptic curve cryptography is a good option for protecting data, but it requires that you have a secure
way to store and manage the encryption keys. If the keys are lost or compromised, the data will be inaccessible.

Option D: Using regular expressions to find and redact phone numbers, email addresses, and credit card numbers can be effective in
some cases, but it requires that you have a complete and up-to-date list of all the data patterns that you want to protect. It is also
prone to errors and may not be able to detect all instances of sensitive data.

upvoted 3 times

? ?  Kiroo 1ámonth, 3áweeks ago

About D, usually is recommended that you don┤t reinvent the wheel specially when talking about security .

upvoted 1 times

? ?  shutupbot  Most Recent ?  2ámonths, 4áweeks ago

Cloud Data Loss Prevention API provides obfuscation and de-identification methods like masking and tokenization. Especially for credit
card transactions, the card numbers are supposed to be tokenized. Therefore, this API is helpful.

upvoted 1 times

? ?  mbrochard 7ámonths, 1áweek ago

Selected Answer: C

C for sure !

upvoted 1 times

? ?  AniketD 7ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

111/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: C

C is correct, DLP is the solution

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

ok for C

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C

upvoted 3 times

? ?  haroldbenites 1áyear, 6ámonths ago
https://cloud.google.com/dlp

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is C

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

C. De-identify the data with the Cloud Data Loss Prevention API

upvoted 3 times

? ?  un 2áyears, 1ámonth ago

C is correct

upvoted 1 times

? ?  sidhappy 2áyears, 2ámonths ago

Effectively reduce data risk with de-identification methods like masking and tokenization
https://cloud.google.com/dlp

upvoted 3 times

? ?  Ausias18 2áyears, 3ámonths ago

Answer is C

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

C is ok

upvoted 1 times

? ?  bnlcnd 2áyears, 5ámonths ago
https://cloud.google.com/dlp
Effectively reduce data risk with de-identification methods like masking and tokenization.
C is right.

upvoted 1 times

? ?  VedaSW 2áyears, 9ámonths ago

I LOL on answer A.... *clap* *clap* *clap*.... hash all data...

upvoted 1 times

? ?  psuthar0101 2áyears, 8ámonths ago

correct answer is C - https://cloud.google.com/solutions/de-identification-re-identification-pii-using-cloud-dlp

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago
yes. thank you for sharing the link

upvoted 1 times

? ?  AshokC 2áyears, 9ámonths ago

C - Cloud Data Loss Prevention (DLP)

upvoted 2 times

? ?  psuthar0101 2áyears, 8ámonths ago

This is correct - https://cloud.google.com/solutions/de-identification-re-identification-pii-using-cloud-dlp

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

112/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #42

Topic 1

You are using Cloud Shell and need to install a custom utility for use in a few weeks. Where can you store the  le so it is in the default execution

path and persists across sessions?

A. ~/bin

B. Cloud Storage

C. /google/scripts

D. /usr/local/bin

Correct Answer: A

Community vote distribution

A (100%)

? ?  ffk  Highly Voted ?  3áyears, 8ámonths ago

A is correct
https://cloud.google.com/shell/docs/how-cloud-shell-works
Cloud Shell provisions 5 GB of free persistent disk storage mounted as your $HOME directory on the virtual machine instance. This storage
is on a per-user basis and is available across projects. Unlike the instance itself, this storage does not time out on inactivity. All files you
store in your home directory, including installed software, scripts and user configuration files like .bashrc and .vimrc, persist between
sessions. Your $HOME directory is private to you and cannot be accessed by other users.

upvoted 63 times

? ?  Jambalaja 2áyears, 2ámonths ago

Maybe also to mention is that ~/bin is located in the $HOME directory

upvoted 12 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Agree. A is right
upvoted 1 times

? ?  zanfo 1áyear, 9ámonths ago
cd ~/ is egual at cd $HOME
~/bin is egual a cd $HOME/bin
the persistent disk in cloud shell is for $HOME

upvoted 8 times

? ?  Shabje 3áyears, 1ámonth ago

WonÆt the persistent disk be auto-delete enabled by default, whereby the work maybe lost. Would that not be sufficient reason to
consider Cloud storage instead. Thanks

upvoted 2 times

? ?  kaush 3áyears ago

The virtual machine instance that backs your Cloud Shell session is not permanently allocated to a Cloud Shell session and
terminates if the session is inactive for an hour. After the instance is terminated, any modifications that you made to it outside your
$HOME are lost.
upvoted 2 times

? ?  zanfo 1áyear, 9ámonths ago

Cloud Shell provisions 5 GB of free persistent disk storage mounted as your $HOME

upvoted 1 times

? ?  akoti 2áyears, 7ámonths ago

$HOME is not ~/bin. So 'C' is the answer.

upvoted 1 times

? ?  zanfo 1áyear, 9ámonths ago
cd ~/ is egual at cd $HOME
~/bin is egual a cd $HOME/bin
the persistent disk in cloud shell is for $HOME

upvoted 4 times

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

Well, I just double checked and if they were referring to the PATH variable then /usr/local/bin is also a correct
answer........................................

upvoted 20 times

? ?  moota  Most Recent ?  4ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

113/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

I tested this. Although ~/bin is not in the default $PATH, choice D is definitely not persisting across sessions.

upvoted 2 times

? ?  Flight1976 2áweeks, 2ádays ago

1. When logging in to cloud shell for the first time, the ~/bin directory does not exist
2. mkdir ~/bin
3. After re-login to the cloud shell, $PATH will automatically add ~/bin
So A is the correct answer

upvoted 1 times

? ?  FI22 6ámonths ago

At this moment default directory cant be set as Cloud storage bucket, so no C.
A will be correct as zonal PD with preinstalled tools 5gb available that does not timeout!

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

The recommended location for storing a custom utility file that you want to use in Cloud Shell and that should be in the default execution
path and persist across sessions is option A: ~/bin.

The ~/bin directory is a personal directory that is in the default execution path for all users in Cloud Shell. Any executable files that you
place in this directory will be available to you whenever you log in to Cloud Shell, and they will persist across sessions.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option B: Cloud Storage is not a suitable location for storing a custom utility file that you want to use in Cloud Shell, as it is not in the
default execution path and would require additional steps to make it accessible.

Option C: The /google/scripts directory is not a suitable location for storing a custom utility file, as it is not in the default execution path
and is intended for use by Google Cloud system processes.

Option D: The /usr/local/bin directory is a system directory that is in the default execution path for all users, but it is not a suitable
location for storing a custom utility file, as any files that you place in this directory may be deleted or overwritten during system
updates.

upvoted 2 times

? ?  Jailbreaker 7ámonths, 3áweeks ago

Selected Answer: A

For sure correct answer is A

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. ~/bin

upvoted 1 times

? ?  vpatiltech 1áyear, 4ámonths ago

Selected Answer: A

Cloud Shell provisions 5 GB of persistent disk storage mounted as your $HOME directory on the Cloud Shell instance. All files you store in
your home directory, including scripts and user configuration files like .bashrc and .vimrc, persist between sessions.

Reference- https://cloud.google.com/shell/?utm_source=google&utm_medium=cpc&utm_campaign=japac-IN-all-en-dr-bkwsrmkt-all-all-
trial-e-dr-1009882&utm_content=text-ad-none-none-DEV_c-CRE_442449534611-ADGP_Hybrid%20%7C%20BKWS%20-
%20EXA%20%7C%20Txt%20~%20Management%20Tools%20~%20Cloud%20Shell_cloud%20shell-general%20-%20Products-
KWID_43700054972141701-kwd-837034669893&userloc_9302140-network_g&utm_term=KW_gcp%20cloud%20shell&gclsrc=ds&gclsrc=ds

upvoted 3 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I think D is correct.ummm

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

A is the correct answer

upvoted 1 times

? ?  exam_war 1áyear, 7ámonths ago

A is for sure. ~ stands for user's home

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

114/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  kopper2019 2áyears ago

hey guys check Q3 for new Qs, 49 New Qs

upvoted 1 times

? ?  aviratna 2áyears ago

A is correct. Cloud Shell provides 5 GB persistent disk and data in Home directory will persist

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

A. ~/bin

upvoted 1 times

? ?  un 2áyears, 1ámonth ago

A is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

115/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #43

Topic 1

You want to create a private connection between your instances on Compute Engine and your on-premises data center. You require a connection

of at least 20

Gbps. You want to follow Google-recommended practices. How should you set up the connection?

A. Create a VPC and connect it to your on-premises data center using Dedicated Interconnect.

B. Create a VPC and connect it to your on-premises data center using a single Cloud VPN.

C. Create a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises data center using Dedicated Interconnect.

D. Create a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises datacenter using a single Cloud VPN.

Correct Answer: A

Community vote distribution

A (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Cloud VPN supports unto 3 Gbps where as Interconnect can support unto 100 gbps... I'll go with A

upvoted 42 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is required for consistent speed and VPN not supports that speed

upvoted 1 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 7 times

? ?  fraloca 2áyears, 5ámonths ago

https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview#network-bandwidth

upvoted 3 times

? ?  nitinz 2áyears, 3ámonths ago

A, 20Gbps dedicated interconnect is the way.

upvoted 2 times

? ?  omermahgoub  Highly Voted ?  6ámonths, 1áweek ago

Answer is A: Dedicated Interconnect is a service that allows you to create a dedicated, high-bandwidth network connection between your
on-premises data center and Google Cloud. It is the recommended solution for creating a private connection between your on-premises
data center and Google Cloud when you require a connection of at least 20 Gbps.

Option B: Using a single Cloud VPN to connect your VPC to your on-premises data center is not suitable for a connection of at least 20
Gbps, as Cloud VPN has a maximum capacity of 30 Gbps.

Option C: The Cloud Content Delivery Network (Cloud CDN) is a globally distributed network of caching servers that speeds up the delivery
of static and dynamic web content. It is not suitable for creating a private connection between your instances on Compute Engine and
your on-premises data center.

Option D: Connecting your Cloud CDN to your on-premises data center using a single Cloud VPN is not suitable for a connection of at least
20 Gbps, as Cloud VPN has a maximum capacity of 30 Gbps.

upvoted 5 times

? ?  MJCLOUD 4ámonths ago

Very nice answer, I think you meant 3 Gbps for Cloud VPN.

upvoted 2 times

? ?  greyhats13  Most Recent ?  6ámonths, 2áweeks ago

Selected Answer: A

the question mention 20gbs for the least, it should be Dedicated Interconnect. The answer is A

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  Jay_Krish 9ámonths, 4áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

116/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

Any connection between On-Prem and GCP and requires high speed I'd choose dedicated interconnect

upvoted 1 times

? ?  avinashvidyarthi 1áyear, 1ámonth ago

A is correct

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

A is the correct answer

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  duocnh 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 3 times

? ?  unnikrisb 1áyear, 8ámonths ago

A is good option (easily elminate C & D) and B with connection speed.
10Gbps per link for Dedicated Interconnect and Direct Peering
1.5-3Gbps per tunnel for Cloud VPN
50Mbps to 10Gbps per connection - Partner Interconnect
noSLA - Carrier Peering

upvoted 1 times

? ?  amxexam 1áyear, 10ámonths ago
Let's go with option elimination
A. Create a VPC and connect it to your on-premises data centre using Dedicated Interconnect.
>> Is the only remaining best option after elimination. As per the document, its partner interconnects with VPN. Interconnect is between
GCP and on-prem. URL1

B. Create a VPC and connect it to your on-premises data centre using a single Cloud VPN.
>> max 3 gigabits per second (Gbps) eliminate the option.

C. Create a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises data centre using Dedicated Interconnect.
>> CDN is for egress traffic or static content hosting hence eliminate the option URL2

D. Create a Cloud Content Delivery Network (Cloud CDN) and connect it to your on-premises datacenter using a single Cloud VPN.
>> CDN is for egress traffic or static content hosting hence eliminate the option URL2

Hence A

URL1 https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview
URL2 https://cloud.google.com/network-connectivity/docs/cdn-interconnect

upvoted 3 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 1 times

? ?  aviratna 2áyears ago

A is correct. Dedicated Interconnect supports upto 80 GBPS

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

A. Create a VPC and connect it to your on-premises data center using Dedicated Interconnect.

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

A is correct

upvoted 1 times

? ?  Ausias18 2áyears, 3ámonths ago

Answer is A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

117/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #44

Topic 1

You are analyzing and de ning business processes to support your startup's trial usage of GCP, and you don't yet know what consumer demand for

your product will be. Your manager requires you to minimize GCP service costs and adhere to Google best practices. What should you do?

A. Utilize free tier and sustained use discounts. Provision a staff position for service cost management.

B. Utilize free tier and sustained use discounts. Provide training to the team about service cost management.

C. Utilize free tier and committed use discounts. Provision a staff position for service cost management.

D. Utilize free tier and committed use discounts. Provide training to the team about service cost management.

Correct Answer: B

Community vote distribution

B (100%)

? ?  crypt0  Highly Voted ?  3áyears, 8ámonths ago

I would choose "B"

upvoted 37 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 10 times

? ?  nitinz 2áyears, 3ámonths ago

B reason minimize GCP service costs

upvoted 3 times

? ?  ehgm  Highly Voted ?  1áyear, 6ámonths ago

Sustained are automatic discounts for running specific GCE a significant portion of the billing month:
https://cloud.google.com/compute/docs/sustained-use-discounts

Committed is for workloads with predictable resource needs between 1 year or 3 year, discount is up to 57% for most resources:
https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts

upvoted 33 times

? ?  squishy_ shy 1áyear, 5ámonths ago

Best answer!

upvoted 4 times

? ?  omermahgoub  Most Recent ?  6ámonths, 1áweek ago

The recommended approach for minimizing GCP service costs and adhering to Google best practices are:
- Free tier: Google Cloud offers a free tier of services that allows you to try out many of its products for free, up to certain usage limits.
Utilizing the free tier can help you minimize your GCP service costs while you are in the trial usage phase.

- Committed use discounts: Committed use discounts are a type of discount that you can apply to certain GCP products by committing to a
certain level of usage over a one- or three-year period. Committed use discounts can help you save on your GCP service costs, but they
require you to commit to a certain level of usage, which may not be suitable if you are unsure of your future demand.

- Providing training to the team about service cost management: It is important that your team is aware of the different options available
for minimizing GCP service costs and understands how to manage and monitor their usage of GCP services. Providing training on service
cost management can help your team make informed decisions about how to use GCP services in the most cost-effective way.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

The recommended approach for minimizing GCP service costs and adhering to Google best practices while your startup is in the trial
usage phase and you don't yet know what consumer demand for your product will be is option D: Utilize free tier and committed use
discounts. Provide training to the team about service cost management.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Sustained use discounts are based on your usage of GCP services over a certain period, and are not available for all GCP products.

Provisioning a staff position for service cost management may not be necessary if you provide training to the team about service cost
management.
upvoted 1 times

? ?  SureshbabuK 7ámonths ago

Selected Answer: B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

118/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Sustained use discounts - Compute Engine - Google Cloudhttps://cloud.google.com ¢ ... ¢ Documentation
Compute Engine offers sustained use discounts on resources that are used for more than 25% of a billing month - There trial could be
more than 7 or 8 days , at this point commitment of use can not be provided due to trails stage of gcp use

upvoted 1 times

? ?  vranjan 7ámonths, 2áweeks ago

The answer is B, because Sustained use discount can give up to 30% and requires no commitment.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

ok for B

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

committed use discounts are for long-run discounts which in the case of startup they're trying GCP. So options C and D are out
B is the correct answer

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I will choose B, D is only long term commitment

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B. Utilize free tier and sustained use discounts. Provide training to the team about service cost management.

upvoted 2 times

? ?  BiddlyBdoyng 9ámonths ago

Sustained use discount makes sense over committed as not enough info to know what to comit to. Provide staff training on how to keep
things cheap gonna further keep cost down.

upvoted 1 times

? ?  cmamiusa 1áyear, 2ámonths ago

Selected Answer: B

B is the correct option

upvoted 1 times

? ?  gcmrjbr 1áyear, 4ámonths ago

free tier (monthly discounts) does not make sense combined with committed use discounts - anual base, dont't you think so?

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

B is the correct answer

upvoted 1 times

? ?  Israel 1áyear, 9ámonths ago

Answer is B

upvoted 1 times

? ?  VishalB 1áyear, 11ámonths ago

Answer B
Sustained use discounts are applied on incremental use after you reach certain usage thresholds. This means that you pay only for the
number of minutes that you use an instance, and Compute Engine automatically gives you the best price. There's no reason to run an
instance for longer than you need it.
- https://cloud.google.com/compute/docs/sustained-use-discounts
Committed use discounts are ideal for workloads with predictable resource needs. When you purchase a committed use contract, you
purchase compute resource (vCPUs, memory, GPUs, and local SSDs) at a discounted price in return for committing to paying for those
resources for 1 year or 3 years. The discount is up to 57% for most resources like machine types or GPUs. The discount is up to 70% for
memory-optimized machine types. For committed use prices for different machine types, see VM instances pricing.
- https://cloud.google.com/compute/docs/instances/signing-up-committed-use-discounts

upvoted 5 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 1 times

? ?  aviratna 2áyears ago

B is correct as demand is not known

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

119/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #45

Topic 1

You are building a continuous deployment pipeline for a project stored in a Git source repository and want to ensure that code changes can be

veri ed before deploying to production. What should you do?

A. Use Spinnaker to deploy builds to production using the red/black deployment strategy so that changes can easily be rolled back.

B. Use Spinnaker to deploy builds to production and run tests on production deployments.

C. Use Jenkins to build the staging branches and the master branch. Build and deploy changes to production for 10% of users before doing a

complete rollout.

D. Use Jenkins to monitor tags in the repository. Deploy staging tags to a staging environment for testing. After testing, tag the repository for

production and deploy that to the production environment.

Correct Answer: D

Reference:

https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes/blob/master/README.md

Community vote distribution

D (100%)

? ?  Googler2  Highly Voted ?  3áyears, 2ámonths ago

I believe the best answer is D, because the tagging is a best practice that is recommended on Jenkins/Spinnaker to deploy the right code
and prevent accidentally (or intentionally) push of wrong code to production environments. See https://stackify.com/continuous-delivery-
git-jenkins/

upvoted 55 times

? ?  zr79 8ámonths, 2áweeks ago
How can I join Google

upvoted 4 times

? ?  AzureDP900 8ámonths, 2áweeks ago

yes, D is correct
upvoted 1 times

? ?  Ziegler 3áyears ago

Agreed with D as the right answer. The url provided explains it better

upvoted 10 times

? ?  Anish17  Highly Voted ?  2áyears, 10ámonths ago

I got this question in real exam. This question states "before deploying to production" environment. So i picked D . I passed the exam.

upvoted 50 times

? ?  bnlcnd 2áyears, 5ámonths ago
that resolved the puzzle :)

upvoted 4 times

? ?  GunaGCP123 1áyear, 8ámonths ago

congrats for passing the exam. practising all 255 questions is sufficient for passing the exam? how much percentage of questions you
got from here roughly?

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

He won't answer, he already passed the exam

upvoted 6 times

? ?  Rzla 1áyear, 10ámonths ago

Agree, its the only answer that meets the requirement of "before deploying to production"

upvoted 2 times

? ?  winset 2áyears, 4ámonths ago

not only 1 Q passed! C is beeter

upvoted 2 times

? ?  megumin  Most Recent ?  7ámonths, 3áweeks ago

Selected Answer: D

ok for D

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

120/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  BiddlyBdoyng 9ámonths ago

Given the question D is the only answer. Everything else pushes to production immediately.

upvoted 1 times

? ?  [Removed] 1áyear, 3ámonths ago

I don't think it was a good idea when new edtion be created and directly deploy to the production ENV without any testing stage even
using Canary deployment.

upvoted 1 times

? ?  [Removed] 1áyear, 3ámonths ago

D is better.

upvoted 1 times

? ?  Davidik79 1áyear, 3ámonths ago

Selected Answer: D

The question states: "... code changes can be verified BEFORE deploying to production", it eliminates option C.
The approach of tagging is the correct practise that DevOps use

upvoted 2 times

? ?  [Removed] 1áyear, 5ámonths ago

Selected Answer: D

Correct answer is D. Question talks about 'before deploying to production'. C talks about after deploying to production.

upvoted 4 times

? ?  hantanbl 1áyear, 5ámonths ago

C is the closest answer
If question is asking 'what's Jenkin best practise' then D is the answer

upvoted 1 times

? ?  Davidik79 1áyear, 3ámonths ago

C involves deploying into production. the question specifies: "BEFORE deploying to production"

upvoted 1 times

? ?  lxgywil 1áyear, 5ámonths ago

I choose D as we want to ensure that code changes can be verified BEFORE deploying to production. Option C suggests that we build and
deploy changes to production for 10% of users.

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: D

Vote D.
C is canary deploy.
But the sentence in question has no word to mean "tested by a small number of users"

upvoted 3 times

? ?  OrangeTiger 1áyear, 5ámonths ago

The revelal solution on this site is wrong, isn't it? I'm getting anxious.

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

D is the correct answer

upvoted 1 times

? ?  ABO_Doma 1áyear, 6ámonths ago

Selected Answer: D

clearly

upvoted 2 times

? ?  phantomsg 1áyear, 6ámonths ago

Selected Answer: D

Question asks to verify before Production. So tagging and deploying appropriately is the right approach.

upvoted 3 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for D.
ôBEFORE DEPLOYING TO PRODUCTIONö

upvoted 1 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: D

C & D are valid answers here. I choose D because they want to test before deploy to production. If the question was : "test to a subset
users in production (Canary) before full deployment", it would have been C

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

121/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 2 times

? ?  jimcsng 1áyear, 7ámonths ago

Selected Answer: D

D is correct while C is not. The question is about ensuring the traceability of the code changes, not necessarily ensure a successful release.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

122/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #46

Topic 1

You have an outage in your Compute Engine managed instance group: all instances keep restarting after 5 seconds. You have a health check

con gured, but autoscaling is disabled. Your colleague, who is a Linux expert, offered to look into the issue. You need to make sure that he can

access the VMs. What should you do?

A. Grant your colleague the IAM role of project Viewer

B. Perform a rolling restart on the instance group

C. Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys

D. Disable autoscaling for the instance group. Add his SSH key to the project-wide SSH Keys

Correct Answer: C

Community vote distribution

C (80%)

A (20%)

? ?  Narinder  Highly Voted ?  1áyear, 5ámonths ago

C, is the correct answer. As per the requirement linux expert would need access to VM to troubleshoot the issue. With health check
enabled, old VM will be terminated as soon as health-check fails for the VM and new VM will be auto-created. So, this situation will prevent
linux expert to troubleshoot the issue. Had it been the case that stack-drover logging is enabled and the expert just want to view the logs
from the Cloud-logs than role to project-viewer could help. But it is specifically mentioned that expert will login into VM to troubleshoot
the issue and not looking at the cloud Logs. So, Option-C is the correct answer.

upvoted 63 times

? ?  devjuliusoh 10ámonths, 1áweek ago

Good explanation

upvoted 2 times

? ?  twistyfries 1áyear, 4ámonths ago

great answer
upvoted 2 times

? ?  AmitAr 1áyear, 1ámonth ago

This looks best justification between A and C.. So, C should be correct answer.

upvoted 2 times

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

C should be correct answer.

upvoted 39 times

? ?  JC0926  Most Recent ?  3ámonths, 2áweeks ago

Selected Answer: C

To allow your colleague, who is a Linux expert, to access the VMs and troubleshoot the issue, you should disable the health check for the
instance group. This will prevent the instance group from automatically removing and replacing unhealthy instances.

You should also add your colleague's SSH key to the project-wide SSH Keys to allow him to SSH into the instances and perform
troubleshooting. This can be done through the Google Cloud Console or the gcloud command-line tool.

upvoted 1 times

? ?  MestreCholas 3ámonths, 3áweeks ago

Selected Answer: C

C. Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys.

Granting the IAM role of project Viewer (Option A) would allow your colleague to view the project resources but not necessarily give them
access to the specific VM instances. Performing a rolling restart on the instance group (Option B) would not resolve the issue, as the
instances keep restarting after 5 seconds. Disabling autoscaling for the instance group (Option D) is not relevant since autoscaling is
already disabled.

Disabling the health check for the instance group will prevent the managed instance group from automatically recreating the instances.
Adding your colleague's SSH key to the project-wide SSH Keys will allow them to access the VMs and troubleshoot the issue. Once the
issue is resolved, you can re-enable the health check for the instance group.

upvoted 2 times

? ?  urssiva 4ámonths, 3áweeks ago

Selected Answer: C

C is the answer
upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

123/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  roaming_panda 5ámonths, 3áweeks ago

C as the machines will keep restarting if hc is not disabled , and then our expert can look around for RCA

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

To allow your colleague access the instances in MIG you should do option D: Disable autoscaling for the instance group. Add his SSH key
to the project-wide SSH Keys.

Disabling autoscaling for the instance group will prevent new instances from being created or terminated while your colleague is trying to
troubleshoot the issue. This will give him a stable environment to work in and will ensure that he can access the instances that are
currently in the instance group.

Adding his SSH key to the project-wide SSH Keys will allow him to connect to the instances using Secure Shell (SSH) without requiring a
password. This is a convenient way to give him access to the instances and can help him troubleshoot the issue more efficiently.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: Granting your colleague the IAM role of project Viewer will not allow him to access the instances in the managed instance
group. The project Viewer role does not include any permissions to access Compute Engine resources.

Option B: Performing a rolling restart on the instance group will not solve the issue and may even make it worse if the instances are
not able to start up properly.

Option C: Disabling the health check for the instance group will not solve the issue and may even make it worse if the instances are not
able to start up properly. Adding his SSH key to the project-wide SSH Keys will allow him to access the instances, but it is not a sufficient
solution on its own.

upvoted 2 times

? ?  n_nana 5ámonths, 2áweeks ago

autoscaling is already disabled. Answer D is not make sense here. i voted it wrongly instead of reply.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

ok for C

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

C. Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys. This will allow the engineer to logon to
the VM's and check the logs. Disabling health checks will prevent rebooting of the VM's.

upvoted 2 times

? ?  holerina 9ámonths, 1áweek ago

C should be the correct answer

upvoted 1 times

? ?  abirroy 10ámonths ago

Selected Answer: C

Disable the health check for the instance group. Add his SSH key to the project-wide SSH Keys

upvoted 1 times

? ?  backhand 10ámonths, 3áweeks ago

vote C
what can project viewer do without access vm by linux expert?

upvoted 1 times

? ?  tricky_learner 11ámonths, 3áweeks ago

Selected Answer: C

I believe that answer C is correct.

upvoted 1 times

? ?  elaineshi 1áyear ago

About create SSH Keys to VM https://cloud.google.com/compute/docs/connect/create-ssh-keys

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: A

For others answering please concentrate on the requirement. The ask is to allow access to the Linux expert and not to go and solve the
problem

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

124/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Hence A.

upvoted 2 times

? ?  Nidhal1920 2ámonths, 2áweeks ago

The Linux expert don't need to see all the resources under my project (least privilege principle) ! So A is not correct. It's C .

upvoted 1 times

? ?  oxfordcommaa 5ámonths, 1áweek ago

Answer A wouldn't be the resolution.
The instances are being restarted, as long as they are being restarted, the linux expert will not be able to log in and do what he needs
to do.
Disabling the health check is how to make the instances available for access...then you add the keys and you are kosher.

upvoted 1 times

? ?  juancambb 1áyear, 4ámonths ago

Selected Answer: C

is C the correct
upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

125/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #47

Topic 1

Your company is migrating its on-premises data center into the cloud. As part of the migration, you want to integrate Google Kubernetes Engine

(GKE) for workload orchestration. Parts of your architecture must also be PCI DSS-compliant. Which of the following is most accurate?

A. App Engine is the only compute platform on GCP that is certi ed for PCI DSS hosting.

B. GKE cannot be used under PCI DSS because it is considered shared hosting.

C. GKE and GCP provide the tools you need to build a PCI DSS-compliant environment.

D. All Google Cloud services are usable because Google Cloud Platform is certi ed PCI-compliant.

Correct Answer: C

Community vote distribution

C (100%)

? ?  rishab86  Highly Voted ?  2áyears ago

Link : https://cloud.google.com/security/compliance/pci-dss
Clearly mention GKE as PCI DSS-Compliant but not all GCP service are PCI DSS-Compliant so answer is definitely C.

upvoted 38 times

? ?  Mikado211 10ámonths, 4áweeks ago

In 2022, GCP is now fully PCI-DSS compliant, so technically D is perfectly true.
But you still have to check that your application is PCI-DSS compliant.

so C is still the best answer.

upvoted 4 times

? ?  MaxNRG 1áyear, 8ámonths ago

C û Kubernetes Engine provides tools you need to build to PCI-DSS compliant environment.

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

But, The paragraph 3 says that all products of google are certified by PCI.

upvoted 2 times

? ?  aviratna  Highly Voted ?  2áyears ago

C: GKE & Compute Engine is PCI DSS compliant while Cloud Function, App Engine are not PC compliant

upvoted 5 times

? ?  omermahgoub  Most Recent ?  6ámonths, 1áweek ago

The most accurate statement is option C: GKE and GCP provide the tools you need to build a PCI DSS-compliant environment.

Google Kubernetes Engine (GKE) is a fully managed service that allows you to deploy and manage containerized applications on Google
Cloud. It is not specifically certified for PCI DSS hosting, but it can be used as part of a PCI DSS-compliant environment if the necessary
controls and safeguards are in place.

Google Cloud Platform (GCP) provides a range of tools and services that can be used to build a PCI DSS-compliant environment, including
Cloud Identity and Access Management (IAM) for controlling access to resources, Cloud Key Management Service (KMS) for managing
encryption keys, and Cloud Security Command Center for monitoring and detecting security threats.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: App Engine is a fully managed platform for building and deploying web and mobile applications, but it is not the only
compute platform on GCP that is certified for PCI DSS hosting. Other compute platforms such as Compute Engine and Google
Kubernetes Engine can also be used as part of a PCI DSS-compliant environment.

Option B: GKE is not considered shared hosting and can be used as part of a PCI DSS-compliant environment if the necessary controls
and safeguards are in place.

Option D: While Google Cloud Platform is certified PCI-compliant, not all of its services are automatically usable in a PCI DSS-compliant
environment. It is up to the user to ensure that they are using the appropriate controls and safeguards to meet the requirements of the
PCI DSS.

upvoted 2 times

? ?  abirroy 10ámonths ago

Selected Answer: C

C is the right answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

126/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: C

I got similar question on my exam.

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C.

upvoted 1 times

? ?  SHOURYA_SOOD 1áyear, 7ámonths ago

Selected Answer: C

C- All of them: GKE, GCE, and GAE ate PCI-DSS-Compliant but A & B says it's only GAE and GCE respectively so cancel them out.
D says all of GCP is PCI DSS-Compliant but it's not true.
So, C seems to be the right answer.

upvoted 1 times

? ?  imranmani 1áyear, 8ámonths ago

C is the right answer

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is C

upvoted 3 times

? ?  victory108 1áyear, 11ámonths ago

C. GKE and GCP provide the tools you need to build a PCI DSS-compliant environment.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

127/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #48

Topic 1

Your company has multiple on-premises systems that serve as sources for reporting. The data has not been maintained well and has become

degraded over time.

You want to use Google-recommended practices to detect anomalies in your company data. What should you do?

A. Upload your  les into Cloud Storage. Use Cloud Datalab to explore and clean your data.

B. Upload your  les into Cloud Storage. Use Cloud Dataprep to explore and clean your data.

C. Connect Cloud Datalab to your on-premises systems. Use Cloud Datalab to explore and clean your data.

D. Connect Cloud Dataprep to your on-premises systems. Use Cloud Dataprep to explore and clean your data.

Correct Answer: B

Community vote distribution

B (100%)

? ?  JohnWick2020  Highly Voted ?  2áyears, 2ámonths ago

Answer is B:

Keynotes from question:
1- On-premise data sources
2- Unfit data; not well maintained and degraded
3- Google-recommended best practice to "detect anomalies" <<-Very important.

Explanation:
A & C - incorrect; Datalab does not provide anomaly detection OOTB. It is used more for data science scenarios like interactive data
analysis and build ML models.
B - CORRECT; DataPrep OOTB provides for fast exploration and anomaly detection and lists cloud storage as an ingestion medium. Refer
to ELT pipeline architecture here = https://cloud.google.com/dataprep
D - incorrect; At this time DataPrep cannot connect to SaaS or on-premise source. Not to be confused for DataFlow which can!

upvoted 43 times

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

Both B and D work, because the question says "Google's Best Practices" uploading the files first would keep the original copies Google
encrypted and stored.

upvoted 12 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is better choice
upvoted 1 times

? ?  skywalker 3áyears, 1ámonth ago

Both of them works....

upvoted 1 times

? ?  Musk 2áyears, 11ámonths ago

You can't connect DataPrep to your on-prem systems. You simply upload a file, but that is not connecting it to your systems.
Because of that, I'd discard D and stay with B.

upvoted 9 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 9 times

? ?  nitinz 2áyears, 3ámonths ago

B, dataprep = visually explore, clean, and prepare data for analysis

upvoted 7 times

? ?  n_nana  Most Recent ?  4ámonths ago

Today, data ingestion to DataPrep can be Application, file upload, database.
so B is also now valid

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

The recommended approach for detecting anomalies in your company data using Google-recommended practices is option B: Upload
your files into Cloud Storage. Use Cloud Dataprep to explore and clean your data.

Cloud Storage is a highly scalable, durable, and secure object storage service that can be used to store and retrieve data from anywhere
on the web. You can use Cloud Storage to store your company data files and make them available for analysis.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

128/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Cloud Dataprep is a fully managed data preparation service that allows you to quickly and easily explore, clean, and transform your data
for analysis. It can help you detect anomalies in your data by providing features such as data profiling, data cleansing, and data
transformation.
upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: Using Cloud Datalab to explore and clean your data is not a recommended approach, as Cloud Datalab is a collaborative data
exploration and visualization platform that is not specifically designed for data preparation tasks such as cleansing and transformation.

Option C: Connecting Cloud Datalab to your on-premises systems is not a recommended approach, as Cloud Datalab is a collaborative
data exploration and visualization platform and is not designed for data preparation tasks such as cleansing and transformation.

Option D: Connecting Cloud Dataprep to your on-premises systems is not necessary, as you can use Cloud Dataprep to explore and
clean data stored in Cloud Storage.

upvoted 1 times

? ?  allen_y_q_huang 6ámonths, 2áweeks ago
ok for B & D, but B is suitable to gcp

upvoted 1 times

? ?  Smaks 6ámonths, 4áweeks ago

Selected Answer: B

Datalab is deprecated : https://cloud.google.com/datalab/docs
New Cloud Dataprep options will give connectivity to relational databases, business applications and extend our integrations across
Google Cloud with Google Sheets: https://www.trifacta.com/blog/cloud-dataprep-trifacta/

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

ok for B

upvoted 1 times

? ?  Cloudexplorer 11ámonths, 1áweek ago

Could anyone provide a link where it explicitly says that Datprep does not connect to on-premises data sources.

In the ingestion layer on the diagram at https://cloud.google.com/dataprep it shows databases as a source.
I can't see anywhere that there is a limitation connecting to on-premises. Would be great if someone could share that.

upvoted 3 times

? ?  BigSteveO 12ámonths ago

Selected Answer: B

It's gotta be B.
upvoted 1 times

? ?  Dhiraj03 1áyear ago

Keyword : Anamolies Data prep is the only product ... So options A and C is eliminated ... Cost effective is storing the data in GCS Cloud
storage ... So option is B

upvoted 1 times

? ?  nkit 1áyear, 2ámonths ago

Selected Answer: B

Dataprep to detect anomalies in Data is the right choice.

upvoted 1 times

? ?  GMats 1áyear, 5ámonths ago

B...It supports only CloudStorage and Bigquery..."So you can start transforming datasets, you hereby instruct Google to allow Trifacta, who
provides the service Dataprep in collaboration with Google, to view and modify project data in Cloud Storage and BigQuery, run Dataflow
jobs, and use all project service accounts."

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B.

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

The question says ôbest practiceö. In GCP , a best practice for many use cases is load to cloud storage and then processing data.

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

129/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B is the right answer

upvoted 1 times

? ?  exam_war 1áyear, 7ámonths ago

B is correct.
datalab: not used for clean,for virtualize and analysis purpose, so A is not correct

upvoted 1 times

? ?  FERIN_02 1áyear, 7ámonths ago

Input sources for GCP Dataprep are
1) Local computer
2) Cloud storage
3) BigQuery

Hence option B
upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

130/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #49

Topic 1

Google Cloud Platform resources are managed hierarchically using organization, folders, and projects. When Cloud Identity and Access

Management (IAM) policies exist at these different levels, what is the effective policy at a particular node of the hierarchy?

A. The effective policy is determined only by the policy set at the node

B. The effective policy is the policy set at the node and restricted by the policies of its ancestors

C. The effective policy is the union of the policy set at the node and policies inherited from its ancestors

D. The effective policy is the intersection of the policy set at the node and policies inherited from its ancestors

Correct Answer: C

Reference:

https://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy

Community vote distribution

C (100%)

? ?  passnow  Highly Voted ?  3áyears, 6ámonths ago

The effective policy for a resource is the union of the policy set at that resource and the policy inherited from its
parent.https://cloud.google.com/iam/docs/resource-hierarchy-access-control

upvoted 28 times

? ?  ghadxx  Highly Voted ?  1áyear, 4ámonths ago

You can set IAM policies at the level of the node, in addition to policies inherited from its parent. Hence, it is a union.

upvoted 10 times

? ?  omermahgoub  Most Recent ?  6ámonths, 1áweek ago

The effective policy at a particular node in the resource hierarchy in GCP is determined by the intersection of the policy set at the node and
policies inherited from its ancestors, as described in option D

Cloud IAM policies in GCP are hierarchical, meaning that policies set at higher levels of the resource hierarchy can be inherited by lower
levels. When a user or service account attempts to access a resource, the effective policy at that resource is determined by evaluating the
policies set at the resource itself and all of its ancestors in the hierarchy. If any of the policies deny access, the user or service account will
be denied access.

For example, consider the following resource hierarchy:
Organization => Folder => Project => Compute Engine instance
If an IAM policy is set at the organization level that allows read access to all Compute Engine instances, and a policy is set at the project
level that denies read access to a specific Compute Engine instance, the effective policy for that instance will be the intersection of the two
policies, which will be to deny read access to the instance.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: The effective policy is not determined only by the policy set at the node, as policies set at higher levels in the hierarchy can
also have an impact on the effective policy.

Option B: The effective policy is not restricted by the policies of its ancestors, as the policies of its ancestors can also be included in the
effective policy if they allow access.

Option C: The effective policy is not the union of the policy set at the node and policies inherited from its ancestors, as the intersection
of the policies is used to determine the effective policy.

upvoted 1 times

? ?  habros 7ámonths ago

Selected Answer: C

C. Is a skewed wording question. Cannot be comprehended right away.

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

ok for C

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

131/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  zr79 8ámonths, 2áweeks ago

English as a second language will struggle here. Good luck to us

upvoted 4 times

? ?  BiddlyBdoyng 9ámonths ago

A: Would mean polcies set at the project or higher meant nothing, this is obviously wrong
B: would mean you could not grant a permissions to a single VM, it would need to be at project or above (you restrict by not giving the
permission)
C : The permission is the sum of all the permissions you are given through the hierarchy, this is correct, you cannot restrict once it is given
at a higher level.
D: Would mean you would need the permission set at ancestor and the node, this would mean to get access to a single VM you would
need to be given access to all VMs at the project level.

upvoted 3 times

? ?  holerina 9ámonths, 1áweek ago

C is correct answer as it inheritance is the basic model of IAM

upvoted 2 times

? ?  avinashvidyarthi 1áyear, 1ámonth ago

Selected Answer: C

C is correct

upvoted 1 times

? ?  Atnafu 1áyear, 6ámonths ago

C
Google Cloud resources are organized hierarchically, where the organization node is the root node in the hierarchy, the projects are the
children of the organization, and the other resources are descendants of projects. You can set Identity and Access Management (IAM)
policies at different levels of the resource hierarchy. Resources inherit the policies of the parent resource. The effective policy for a
resource is the union of the policy set at that resource and the policy inherited from its parent.

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

C is the correct answer

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C.

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is C

upvoted 3 times

? ?  victory108 2áyears, 1ámonth ago

C. The effective policy is the union of the policy set at the node and policies inherited from its ancestors

upvoted 2 times

? ?  DuncanK53 2áyears, 1ámonth ago

Def answer C. Key word is 'union'.

upvoted 1 times

? ?  Ausias18 2áyears, 3ámonths ago

Answer is C

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

132/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #50

Topic 1

You are migrating your on-premises solution to Google Cloud in several phases. You will use Cloud VPN to maintain a connection between your on-

premises systems and Google Cloud until the migration is completed. You want to make sure all your on-premise systems remain reachable during

this period. How should you organize your networking in Google Cloud?

A. Use the same IP range on Google Cloud as you use on-premises

B. Use the same IP range on Google Cloud as you use on-premises for your primary IP range and use a secondary range that does not overlap

with the range you use on-premises

C. Use an IP range on Google Cloud that does not overlap with the range you use on-premises

D. Use an IP range on Google Cloud that does not overlap with the range you use on-premises for your primary IP range and use a secondary

range with the same IP range as you use on-premises

Correct Answer: C

Community vote distribution

C (85%)

B (15%)

? ?  newbie2020  Highly Voted ?  3áyears, 5ámonths ago

Ans is C,
https://cloud.google.com/vpc/docs/using-vpc

"Primary and secondary ranges can't conflict with on-premises IP ranges if you have connected your VPC network to another network with
Cloud VPN, Dedicated Interconnect, or Partner Interconnect."

upvoted 120 times

? ?  Sundeepk 3áyears ago

from the above link - it clearly states - "Primary and secondary ranges for subnets cannot overlap with any allocated range, any primary
or secondary range of another subnet in the same network, or any IP ranges of subnets in peered networks." once we create a VPN,
they all are part of the same network . Hence option C is correct

upvoted 11 times

? ?  Smart 3áyears, 4ámonths ago

Agreed!

upvoted 2 times

? ?  AD2AD4 3áyears, 1ámonth ago
Perfect.. Exact find in link.

upvoted 2 times

? ?  elaineshi 1áyear ago

agree, any ip range, shall use filewall rule to communicate, instead of setting same IP range, which is a mess to control.

upvoted 2 times

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

I think C is correct.

upvoted 21 times

? ?  JoeShmoe 3áyears, 7ámonths ago

Agree with C. Secondary IP range still can't overlap

upvoted 10 times

? ?  AWS56 3áyears, 5ámonths ago

".... and Google Cloud until the migration is completed." Taking this as the key, the intention is to remove the connection b/w on-
prem and GCP once the migration is done. and then the secondary IPs will act as primary. So I will choose D

upvoted 3 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 10 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û Use the same IP range on Google Could as you use on premises for you primary IP range and use a secondary range that
does not overlap with the range you use on premises.
See how primary and secondary IP ranges are used: https://cloud.google.com/vpc/docs/alias-ip
The migration process of services looks as following:
On-prem VM (primary IP) runs some services (secondary IPs), then we create new VM in GCP (on primary IP range), then one-
by-one migrate services from on-prem VM. These services are running on secondary IP range. So, we test migrated services,

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

133/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

while keeping on-prem services still up (that's a point of secondary ranges to not overlap). Once migration is complete we
switch traffic to VM/services on Cloud and shutdown on-prem VM.

upvoted 5 times

? ?  zanfo 1áyear, 9ámonths ago

how to manage the routing table in VPC if is present a subnet with the same network of vpn remote net? the correct answer is C

upvoted 1 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes C it is

upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

C, no brainer. You have on-prem <--> VPN <---> GCP only way this data flow to work in non-over lapping subnets. You can stretch
subnets at layer 7 but you wont be able to route it via VPN.

upvoted 4 times

? ?  JC0926  Most Recent ?  3ámonths, 2áweeks ago

Selected Answer: B

Using an IP range on Google Cloud that does not overlap with the range used on-premises (option C) is a good choice to avoid IP address
conflicts. However, it is important to use the same IP range as the on-premises applications for the primary IP range to ensure that the on-
premises systems remain accessible. Therefore, using the same IP range on Google Cloud as on-premises for the primary IP range and
using a secondary range that does not overlap with the range used on-premises can avoid IP address duplication and ensure that the on-
premises systems remain accessible. Hence, option B is the better choice.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

The recommended approach for organizing your networking in Google Cloud to ensure that all your on-premises systems remain
reachable during the migration is option C: Use an IP range on Google Cloud that does not overlap with the range you use on-premises.

When using Cloud VPN to establish a connection between your on-premises systems and Google Cloud, it is important to ensure that the
IP ranges used in your on-premises systems and Google Cloud do not overlap. If the IP ranges overlap, it can cause conflicts and make it
difficult to route traffic between your on-premises systems and Google Cloud.

To avoid IP range conflicts, you should use an IP range on Google Cloud that is different from the range you use on-premises. This will
ensure that all your on-premises systems remain reachable during the migration.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: Using the same IP range on Google Cloud as you use on-premises is not a recommended approach, as it can cause IP range
conflicts and make it difficult to route traffic between your on-premises systems and Google Cloud.

Option B: Using the same IP range on Google Cloud as you use on-premises for your primary IP range and a secondary range that does
not overlap with the range you use on-premises is not a recommended approach, as it can still cause IP range conflicts and make it
difficult to route traffic between your on-premises systems and Google Cloud.

Option D: Using an IP range on Google Cloud that does not overlap with the range you use on-premises for your primary

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

ok for C

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

no overlapping
upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C. Use an IP range on Google Cloud that does not overlap with the range you use on-premises

upvoted 1 times

? ?  marksie1988 10ámonths ago

Selected Answer: C

C, IP should never overlap if avoidable. double nat is nasty

upvoted 1 times

? ?  ZLT 1áyear ago

Selected Answer: C

The correct answer is C

upvoted 2 times

? ?  Barry123456 1áyear ago

Selected Answer: C

C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

134/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Why would you ever create an IP overlap?

upvoted 1 times

? ?  jonty4gcp 1áyear, 2ámonths ago

Selected Answer: C

Answer is C

upvoted 1 times

? ?  Davidik79 1áyear, 3ámonths ago

Selected Answer: C

From here: https://cloud.google.com/vpc/docs/create-modify-vpc-networks
"Primary and secondary ranges can't conflict with on-premises IP ranges if you have connected your VPC network to another network with
Cloud VPN, Dedicated Interconnect, or Partner Interconnect."

upvoted 1 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: C

I got similar question on my exam.

upvoted 3 times

? ?  Sreedharveluru 1áyear, 5ámonths ago

ANS - C
Primary and secondary ranges for subnets cannot overlap with any allocated range, any primary or secondary range of another subnet in
the same network, or any IPv4 ranges of subnets in peered networks.

upvoted 2 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Is D corecct?!
Really?

I agree with C is correct.

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

C is the correct answer

upvoted 1 times

? ?  ABO_Doma 1áyear, 6ámonths ago

Selected Answer: C

Within a VPC network, all primary and secondary IPv4 ranges must be unique, but they do not need to be contiguous.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

135/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #51

Topic 1

You have found an error in your App Engine application caused by missing Cloud Datastore indexes. You have created a YAML  le with the

required indexes and want to deploy these new indexes to Cloud Datastore. What should you do?

A. Point gcloud datastore create-indexes to your con guration  le

B. Upload the con guration  le to App Engine's default Cloud Storage bucket, and have App Engine detect the new indexes

C. In the GCP Console, use Datastore Admin to delete the current indexes and upload the new con guration  le

D. Create an HTTP request to the built-in python module to send the index con guration  le to your application

Correct Answer: A

Community vote distribution

A (100%)

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago
Correct A, you have to recreate the indexes

upvoted 28 times

? ?  nitinz 2áyears, 3ámonths ago

A, if index is missing then create it.

upvoted 3 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 11 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes, use this command in cloud sell to create indexes.
gcloud datastore create indexes

upvoted 4 times

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

A is incorrect because the command is actually gcloud datastore indexes create.
(https://cloud.google.com/sdk/gcloud/reference/datastore/indexes/create).

upvoted 14 times

? ?  bogd 2áyears, 4ámonths ago

It might have changed recently - I was able to find documentation mentioning "datastore create-indexes":

https://cloud.google.com/appengine/docs/standard/python/datastore/indexes

upvoted 12 times

? ?  jlambdan  Most Recent ?  3ámonths, 1áweek ago

A)
https://cloud.google.com/datastore/docs/tools/indexconfig#Datastore_Updating_indexes

upvoted 2 times

? ?  MestreCholas 3ámonths, 3áweeks ago

Selected Answer: A

A. Point gcloud datastore create-indexes to your configuration file.

To deploy new indexes to Cloud Datastore, you can use the gcloud datastore create-indexes command and point it to the YAML
configuration file containing the required indexes. This command will create the new indexes in Cloud Datastore for your application.

Option B is not correct because App Engine does not automatically detect and create indexes from uploaded configuration files in Cloud
Storage.

Option C is also not correct because deleting current indexes in Datastore Admin is not necessary to upload new indexes.

Option D is not correct because there is no built-in Python module that can send the index configuration file to your application

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

To deploy new indexes to Cloud Datastore, you should use the gcloud datastore create-indexes command and point it to your
configuration file. The correct option is therefore A: Point gcloud datastore create-indexes to your configuration file.

Here's the general format of the gcloud datastore create-indexes command:

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

136/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Copy code
gcloud datastore create-indexes [FILE]
Where [FILE] is the path to your configuration file. The configuration file should be in YAML format and contain a list of indexes that you
want to create.

For example:

Copy code
gcloud datastore create-indexes index.yaml
This command will create the indexes specified in the index.yaml file in Cloud Datastore.

upvoted 5 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is correct answer

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is correct

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

gcloud app deploy index.yaml

upvoted 2 times

? ?  AMEJack 8ámonths, 3áweeks ago

Answer is A.
Option C: no create index from configuration file in the firestore console.

upvoted 1 times

? ?  Najmuddoja 9ámonths, 1áweek ago

Is A still the valid answer, many other website resources saying C is the right answer

upvoted 1 times

? ?  holerina 9ámonths, 1áweek ago

A looks like more logical answer

upvoted 1 times

? ?  [Removed] 1áyear, 2ámonths ago

A is incorrect.
There is no correct answer, in fact. In 2022, the gcloud parameter "create-index" doesn't exist anymore. However, that was in 2017*. Today,
the right CLI command** should be gcloud datastore "indexes create".

Evidences:
* https://stackoverflow.com/questions/43041126/how-to-create-datastore-composite-indexes-with-just-cloud-functions)
** https://cloud.google.com/sdk/gcloud/reference/datastore/indexes/create

upvoted 7 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  menon_sarath 1áyear, 6ámonths ago

Why not C? Shouldnt the command be datastore indexes create and not create-index as incorrectly stated in option A? Please advice

upvoted 3 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A.
https://cloud.google.com/appengine/docs/standard/python/datastore/indexes

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

New docs :
https://cloud.google.com/sdk/gcloud/reference/datastore/indexes/create

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

137/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

vote A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

138/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #52

Topic 1

You have an application that will run on Compute Engine. You need to design an architecture that takes into account a disaster recovery plan that

requires your application to fail over to another region in case of a regional outage. What should you do?

A. Deploy the application on two Compute Engine instances in the same project but in a different region. Use the  rst instance to serve tra c,

and use the HTTP load balancing service to fail over to the standby instance in case of a disaster.

B. Deploy the application on a Compute Engine instance. Use the instance to serve tra c, and use the HTTP load balancing service to fail over

to an instance on your premises in case of a disaster.

C. Deploy the application on two Compute Engine instance groups, each in the same project but in a different region. Use the  rst instance

group to serve tra c, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster.

D. Deploy the application on two Compute Engine instance groups, each in a separate project and a different region. Use the  rst instance

group to serve tra c, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster.

Correct Answer: C

Community vote distribution

C (100%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

Groups are better for management that non-groups so A and B are eliminated. Keeping the the instances in the same project will help
maintain consistency, so C is better than D.

upvoted 27 times

? ?  LaxmanTiwari 4áweeks ago

make sense.

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

C, because external LB needs **IG** period. It can either be managed or un-managed. You can not do External HTTP LB on instances.
Also External HHTP LB is a Regional resource.

upvoted 11 times

? ?  gigibit  Highly Voted ?  1áyear, 9ámonths ago

Yes, but why not choose a cost-effective solution like A, preferring a not required performance optimization solution like C? The question
it's just asking for a simple fail over

upvoted 9 times

? ?  todos213 1áyear, 3ámonths ago

In real word situation, I'd choose A for a customer. The question doesn't mention performance to be enhanced or scalability.

upvoted 3 times

? ?  BeCalm  Most Recent ?  3ámonths, 2áweeks ago

To set up a load balancer with a Compute Engine backend, your VMs need to be in an instance group. The managed instance group
provides VMs running the backend servers of an external HTTP load balancer

Therefore C

upvoted 1 times

? ?  LaxmanTiwari 4áweeks ago

good catch .

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

Why can't this be A since there is no mention of scaling?

upvoted 1 times

? ?  n_nana 5ámonths, 2áweeks ago

Selected Answer: C

Google recommend using MIG for Zonal outage and multiple MIG for regional outage
https://cloud.google.com/architecture/disaster-recovery#compute-engine
sentence says:
Compute Engine instances are zonal resources, so in the event of a zone outage instances are unavailable by default. Compute Engine
does offer managed instance groups (MIGs) which can automatically scale up additional VMs from pre-configured instance templates,
both within a single zone and across multiple zones within a region. MIGs are ideal for applications that require resilience to zone loss and
are stateless, but require configuration and resource planning. Multiple regional MIGs can be used to achieve region outage resilience for
stateless applications.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

139/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 5 times

? ?  roaming_panda 5ámonths, 3áweeks ago

Selected Answer: C

MIG , in 1 project
upvoted 1 times

? ?  sameer2803 6ámonths ago

the only solution that they want is to address regional outage. scalability or performance is not a concern at this point. MIG is better but
that not what is solving the regional outage.

upvoted 1 times

? ?  n_nana 5ámonths, 2áweeks ago

According to this link, MIG is not only for performance and scalability. It is also for reliability
https://cloud.google.com/architecture/disaster-recovery#compute-engine
sentence says:
Compute Engine instances are zonal resources, so in the event of a zone outage instances are unavailable by default. Compute Engine
does offer managed instance groups (MIGs) which can automatically scale up additional VMs from pre-configured instance templates,
both within a single zone and across multiple zones within a region. MIGs are ideal for applications that require resilience to zone loss
and are stateless, but require configuration and resource planning. Multiple regional MIGs can be used to achieve region outage
resilience for stateless applications.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is D: Deploy the application on two Compute Engine instance groups, each in a separate project and a different region.
Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group in case of a
disaster.

To implement a disaster recovery plan that requires your application to fail over to another region in case of a regional outage, you should
deploy the application on two Compute Engine instance groups, each in a separate project and a different region. This will ensure that the
application is running in at least two regions, so that if one region experiences an outage, the application can still be accessed from the
other region.

You can use the HTTP load balancing service to distribute traffic between the two instance groups and to fail over to the standby instance
group in case of a disaster. This will ensure that your application is always available, even in the event of a regional outage.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: Deploying the application on two Compute Engine instances in the same project but in a different region will not provide
enough redundancy, as the instances are still in the same project and could be affected by the same regional outage.

Option B: Deploying the application on a Compute Engine instance and using the HTTP load balancing service to fail over to an instance
on your premises in case of a disaster is not a valid option, as it does not provide the required disaster recovery capability.

Option C: Deploying the application on two Compute Engine instance groups, each in the same project but in a different region, is not a
valid option, as it does not provide the required disaster recovery capability.

upvoted 1 times

? ?  oms_muc 6ámonths, 2áweeks ago

IMHO A would also work (test env). In production relying on instances from other regions (introducing latency) just for possible expected
zonal outages, would not be my best practice.

upvoted 1 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: C

Correct answer is C. No need to host MIG in separate projects.

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

ok for C

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is right choice
upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: C

C - using instance groups when planning for DR is better than having single vm's (https://cloud.google.com/architecture/disaster-
recovery). Having the resources in the same project is probably good for resource management.

upvoted 1 times

? ?  arjunvijayvargiya 8ámonths, 4áweeks ago

Answer should have been A as autoscaling requirements are not mentioned.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

140/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  BiddlyBdoyng 9ámonths ago

I think you need an group for the load balancer so A no good?

upvoted 2 times

? ?  holerina 9ámonths, 1áweek ago

A look like a solution because load balancer will route the traffic between two VMs

upvoted 1 times

? ?  avinashvidyarthi 1áyear, 1ámonth ago

Selected Answer: C

Correct answer is C

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

141/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #53

Topic 1

You are deploying an application on App Engine that needs to integrate with an on-premises database. For security purposes, your on-premises

database must not be accessible through the public internet. What should you do?

A. Deploy your application on App Engine standard environment and use App Engine  rewall rules to limit access to the open on-premises

database.

B. Deploy your application on App Engine standard environment and use Cloud VPN to limit access to the on-premises database.

C. Deploy your application on App Engine  exible environment and use App Engine  rewall rules to limit access to the on-premises database.

D. Deploy your application on App Engine  exible environment and use Cloud VPN to limit access to the on-premises database.

Correct Answer: D

Community vote distribution

D (58%)

B (31%)

11%

? ?  MyPractice  Highly Voted ?  3áyears, 6ámonths ago

Agree with D - "When to choose the flexible environment" "Accesses the resources or services of your Google Cloud project that reside in
the Compute Engine network."
https://cloud.google.com/appengine/docs/the-appengine-environments

upvoted 43 times

? ?  AWS56 3áyears, 5ámonths ago

Why not B ? https://cloud.google.com/appengine/docs/flexible/python/using-third-party-databases

upvoted 3 times

? ?  haroldbenites 1áyear, 6ámonths ago

In a forum mentions that GCE and CAP flex are designed for connect to VPC . With GAP standard is needed a proxy .
https://stackoverflow.com/questions/47537204/how-to-connect-app-engine-and-on-premise-server-through-vpn

upvoted 4 times

? ?  areza 2áyears ago

because app engine standard cant connect to on-prem db

upvoted 20 times

? ?  VSMu 4ámonths, 4áweeks ago

Where does it say appengine cannot connect to on-prem db? With CloudVPN, it shoudl connect as per this
https://cloud.google.com/appengine/docs/flexible/storage-options#on_premises

Also going with D will require app to be containerized. That is not listed in the requirement.

upvoted 3 times

? ?  BeCalm 3ámonths, 3áweeks ago

Your link points to a capability of Flexible environment, not Standard.

upvoted 2 times

? ?  elaineshi 1áyear ago

Isn't the question said "not public internet access"?

upvoted 1 times

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

Right is D:
https://stackoverflow.com/questions/37137914/is-it-possible-to-use-google-app-engine-with-google-cloud-vpn

upvoted 17 times

? ?  amxexam 1áyear, 1ámonth ago

Question is can we restrict acess with VP N ?

upvoted 4 times

? ?  moiradavis 11ámonths, 1áweek ago

The stackoverflow reference if older that the answer (6 years) I think that has changed.

upvoted 1 times

? ?  red_panda  Most Recent ?  3áweeks, 6ádays ago

Selected Answer: D

The correct answer is D. Those who say B are unclear about one piece of information: standard App Engine does not provide direct access
to local resources.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

142/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  nvragavan 2ámonths, 1áweek ago

Selected Answer: D

The below link confirms the standard environment to use cloud VPN. The question is primarily around "an application", which could be
.Net or any application. A flexible environment with Cloud VPN would be right answer.

https://cloud.google.com/appengine/docs/standard/storage-options#on_premises

upvoted 3 times

? ?  jits1984 2ámonths, 1áweek ago

answer - B

upvoted 1 times

? ?  JohanProtin 2ámonths, 3áweeks ago

Selected Answer: B

It's B => https://cloud.google.com/appengine/docs/standard/storage-options?hl=fr#on_premises

upvoted 1 times

? ?  JohanProtin 2ámonths, 3áweeks ago

It's B => https://cloud.google.com/appengine/docs/standard/storage-options?hl=fr#on_premises

upvoted 1 times

? ?  mifrah 3ámonths ago

D: AppEngine Flexible for application and Cloud VPN for connection to onprem

upvoted 1 times

? ?  jlambdan 3ámonths, 1áweek ago

Selected Answer: B

Standard can connect to on premise DB through vpn so B is possible.
https://cloud.google.com/appengine/docs/standard/storage-options#on_premises

Standard > Flexible due to cost per doc:
https://cloud.google.com/appengine/docs/the-appengine-environments
Intended to run for free or at very low cost, where you pay only for what you need and when you need it. For example, your application
can scale to 0 instances when there is no traffic.

So B seems better than D

upvoted 3 times

? ?  BeCalm 3ámonths, 3áweeks ago

App Engine Standard can connect to on prem databases as per
https://cloud.google.com/appengine/docs/standard/storage-options

upvoted 2 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: B

There is no mention of app requiring containerization

upvoted 2 times

? ?  xval 4ámonths, 1áweek ago

Selected Answer: D

you cant access network resources with app engine standard

upvoted 1 times

? ?  VSMu 4ámonths, 3áweeks ago

Selected Answer: B

You need CloudVPN to access database onprem.https://cloud.google.com/appengine/docs/flexible/storage-options#on_premises The app
is not containerized so you cannot choose AppEngine Flex. Hence the answer is B

upvoted 4 times

? ?  steghe 5ámonths, 2áweeks ago

https://cloud.google.com/appengine/docs/flexible/storage-options

Setting up Cloud VPN allows your App Engine app to access your on-premises network without directly exposing the database server to
the public internet.

upvoted 3 times

? ?  ymedlop 6ámonths, 1áweek ago

Selected Answer: B

You can use Serverless VPC Access to connect your App Engine standard environment app directly to your VPC network.
https://cloud.google.com/appengine/docs/legacy/standard/python/connecting-vpc

You need Cloud VPN to connect VPC to an on-premise network avoiding public internet.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

143/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is D: Deploy your application on App Engine flexible environment and use Cloud VPN to limit access to the on-premises
database.

To integrate with an on-premises database while ensuring that the database is not accessible through the public internet, you should
deploy your application on App Engine flexible environment and use Cloud VPN to establish a secure connection between your application
and the on-premises database.

Cloud VPN allows you to create a secure, encrypted connection between your on-premises network and Google Cloud, using Internet
Protocol security (IPSec) tunnels. This will allow your application to communicate with the on-premises database while keeping the
database secure and inaccessible from the public internet.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: Deploying your application on App Engine standard environment and using App Engine firewall rules to limit access to the
on-premises database is not a valid option, as it does not provide the necessary secure connection to the on-premises database.

Option B: Deploying your application on App Engine standard environment and using Cloud VPN to limit access to the on-premises
database is a valid option, but it is not the best choice, as App Engine standard environment is not suitable for applications that require
more control over the runtime environment or that need to run custom libraries.

Option C: Deploying your application on App Engine flexible environment and using App Engine firewall rules to limit access to the on-
premises database is not a valid option, as it does not provide the necessary secure connection to the on-premises database.

upvoted 3 times

? ?  skj19 7ámonths ago

Selected Answer: D

D is correct
A. still using public internet
B. App engine standard - basically a container - does not let you configure and control networks
C. still using public internet
D. App Engine flexible - basically a VM - best suited when you need more control on HW, network level

upvoted 5 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

144/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #54

Topic 1

You are working in a highly secured environment where public Internet access from the Compute Engine VMs is not allowed. You do not yet have a

VPN connection to access an on-premises  le server. You need to install speci c software on a Compute Engine instance. How should you install

the software?

A. Upload the required installation  les to Cloud Storage. Con gure the VM on a subnet with a Private Google Access subnet. Assign only an

internal IP address to the VM. Download the installation  les to the VM using gsutil.

B. Upload the required installation  les to Cloud Storage and use  rewall rules to block all tra c except the IP address range for Cloud

Storage. Download the  les to the VM using gsutil.

C. Upload the required installation  les to Cloud Source Repositories. Con gure the VM on a subnet with a Private Google Access subnet.

Assign only an internal IP address to the VM. Download the installation  les to the VM using gcloud.

D. Upload the required installation  les to Cloud Source Repositories and use  rewall rules to block all tra c except the IP address range for

Cloud Source Repositories. Download the  les to the VM using gsutil.

Correct Answer: A

Community vote distribution

A (77%)

B (23%)

? ?  zaki_b  Highly Voted ?  3áyears, 8ámonths ago

Internet access is not allowed so it should be A. CMIIW

upvoted 52 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 10 times

? ?  kumarp6 2áyears, 8ámonths ago

A is the answer
upvoted 3 times

? ?  nitinz 2áyears, 3ámonths ago

A is the best answer.

upvoted 3 times

? ?  KNG  Highly Voted ?  3áyears, 4ámonths ago

Should be A
https://cloud.google.com/vpc/docs/configure-private-services-access
Note: Even though the IP addresses for Google APIs and services are public, the traffic path from instances that are using Private Google
Access to the Google APIs remains within Google's network.

upvoted 16 times

? ?  ppandher  Most Recent ?  6ámonths ago

Those who are opting for B, Can please explain without Internet access and without Private Google Access enabled how will they
communicate with Cloud Storage ? :)

upvoted 5 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is A: Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access
subnet. Assign only an internal IP address to the VM. Download the installation files to the VM using gsutil.

To install specific software on a Compute Engine instance in a highly secured environment where public Internet access is not allowed, you
can follow these steps:

Upload the required installation files to Cloud Storage.
Configure the VM on a subnet with a Private Google Access subnet. This will allow the VM to access Google APIs and services, such as
Cloud Storage, without requiring a public IP address or internet access.
Assign only an internal IP address to the VM. This will ensure that the VM is not accessible from the public internet.
Download the installation files to the VM using gsutil, which is a command-line tool that allows you to access Cloud Storage from the VM.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option B: Uploading the required installation files to Cloud Storage and using firewall rules to block all traffic except the IP address
range for Cloud Storage is not a valid option, as it does not allow the VM to access the installation files without public internet access.

Option C: Uploading the required installation files to Cloud Source Repositories and using gcloud to download the files to the VM is not
a valid option, as Cloud Source Repositories does not support storing large binary files such as installation files.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

145/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Option D: Uploading the required installation files to Cloud Source Repositories and using firewall rules to block all traffic except the IP
address range for Cloud Source Repositories is not a valid option, as it does not allow the VM to access the installation files without
public internet access.

upvoted 1 times

? ?  habros 7ámonths ago

Selected Answer: A

Eliminate B&D as it connect via public networks despite it being a Google Cloud service.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  stevehlw 7ámonths, 3áweeks ago

With private Google access subnet, the vm can reach external network. With this setting, it violates ôpublic Internet access from the
Compute Engine VMs is not allowedö. Can someone explain why itÆs not B instead?

upvoted 2 times

? ?  ppandher 7ámonths ago

Private Google access means - refer to https://www.youtube.com/watch?v=yd5FtV8aJkk

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is good

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. Upload the required installation files to Cloud Storage. Configure the VM on a subnet with a Private Google Access subnet. Assign only
an internal IP address to the VM. Download the installation files to the VM using gsutil.

upvoted 1 times

? ?  muneebarshad 9ámonths, 3áweeks ago

Selected Answer: B

Configuring Private Google Access is the best way to access Google Services for VM that does not have access to the internet. In order to
access Google Private APIs egress should be opened to the following IP Address restricted.googleapis.com (199.36.153.4/30). VM will
leverage internal networking to access Cloud Storage

https://cloud.google.com/vpc/docs/configure-private-google-access

upvoted 4 times

? ?  6721sora 10ámonths, 1áweek ago

C because Cloud repositories is a private Git within Google cloud. Hence it is ideal for simple pull, push, clone type "git" operations. As this
is within Google cloud and is a private git, you do not need public internet access

upvoted 1 times

? ?  BiddlyBdoyng 9ámonths ago

I think it's not this because Clouse Source Repositories is for source code. Sounds like we are looking for an executable?

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: A

C&D we are all eliminating becoz of source storage repo
Between A& B B looks more tempting to select because it mentions fire wallrule But the problem with B is the statement is wrong the
access will happen from VM to storage and the statement mentions traffic from storage to Vm.
Hence A

upvoted 3 times

? ?  celina123123 1áyear, 5ámonths ago

Selected Answer: A

You have to set Private Google Access for communicating between VM and Storage

upvoted 3 times

? ?  ehgm 1áyear, 6ámonths ago

Unfortunately the question it's poorly designed.
B is correct: https://cloud.google.com/vpc/docs/configure-private-google-access

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

A is the correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

146/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  gcp_learner 1áyear, 6ámonths ago

It cannot be B because I donÆt think anything like ôrestricted IP range for GCSö exists, at best we can use the private access feature. So
while I agree the answer is A, can someone explain why itÆs not C please?

upvoted 1 times

? ?  vartiklis 1áyear, 6ámonths ago

Cloud Source Repositories = Git repositories (for storing source code).
Cloud Storage is perfectly suitable for storing things like installation files.
So it's A :)
https://cloud.google.com/source-repositories/docs/features

upvoted 3 times

? ?  PhilipKoku 1áyear, 6ámonths ago

Selected Answer: A

Google Cloud Storage + Private Google Access

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

147/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #55

Topic 1

Your company is moving 75 TB of data into Google Cloud. You want to use Cloud Storage and follow Google-recommended practices. What should

you do?

A. Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.

B. Move your data onto a Transfer Appliance. Use Cloud Dataprep to decrypt the data into Cloud Storage.

C. Install gsutil on each server that contains data. Use resumable transfers to upload the data into Cloud Storage.

D. Install gsutil on each server containing data. Use streaming transfers to upload the data into Cloud Storage.

Correct Answer: A

Community vote distribution

A (86%)

14%

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

Why not A?

upvoted 30 times

? ?  fraloca 2áyears, 5ámonths ago

"The gsutil tool is the standard tool for small- to medium-sized transfers (less than a few TB)"
https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#transfer-options

upvoted 3 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 9 times

? ?  kumarp6 2áyears, 8ámonths ago

It is A

upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

A, anything over 10TB goes via appliance.

upvoted 14 times

? ?  Begum 9ámonths ago

I have moved 120 TB using gsutil- cost effectively!

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

takes longer though

upvoted 1 times

? ?  AshishK  Highly Voted ?  3áyears, 6ámonths ago

It should be 'A'
Transfer Appliance lets you quickly and securely transfer large amounts of data to Google Cloud Platform via a high capacity storage
server that you lease from Google and ship to our datacenter. Transfer Appliance is recommended for data that exceeds 20 TB or would
take more than a week to upload.

upvoted 28 times

? ?  MyPractice 3áyears, 6ámonths ago

where did u get that 20 TB number - can help to share link?

upvoted 1 times

? ?  onashwani 2áyears, 6ámonths ago

Here is the link:
https://cloud.google.com/transfer-appliance/docs/2.2/overview

upvoted 3 times

? ?  gcp_learner 1áyear, 6ámonths ago

But that link mentions a few hundred terabytes to 1 petabyte not 20TB or did I read that incorrectly?

upvoted 1 times

? ?  Yahowmy 2áyears, 11ámonths ago

To this date Transfer Appliance supported locations are only
United States
Canada

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

148/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

European Union
Norway
Switzerland.
What if data reside in a location other than this?
C is the most convenience for this scenario.

upvoted 8 times

? ?  Ramheadhunter 10ámonths, 1áweek ago

Why assume a scenario no provided in the question. We need to choose the best case scenario based on available information
instead of making assumptions. So A should be good.

upvoted 1 times

? ?  MaheshKaswan  Most Recent ?  1áweek, 2ádays ago

Selected Answer: C

Opion A is partially correct as you would not use a Transfer Appliance Rehydrator to decrypt the data. The Transfer Appliance itself is used
to encrypt and decrypt the data. Option C is correct.

upvoted 1 times

? ?  RVivek 5ámonths ago

Selected Answer: A

gsutil is recommanded for data size less than a TB. That rules out C and D
B says decrypt data using Dataprep not sure this is possible.
https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#transfer-options

upvoted 2 times

? ?  n_nana 5ámonths, 2áweeks ago

Selected Answer: A

It will be more precise with info about the bandwidth
Google says:
The two main criteria to consider with Transfer Appliance are cost and speed. With reasonable network connectivity (for example, 1 Gbps),
transferring 100 TB of data online takes over 10 days to complete. If this rate is acceptable, an online transfer is likely a good solution for
your needs. If you only have a 100 Mbps connection (or worse from a remote location), the same transfer takes over 100 days. At this
point, it's worth considering an offline-transfer option such as Transfer Appliance.
So even for such 100 TB google choose between transfer appliance or online transfer. not going with gsutil at all. it is clear gsutil is
suitable for small to medium size (less than 1 TB)

so with no more details, google recommendation is A

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is A: Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud
Storage.

To move large amounts of data into Google Cloud, it is recommended to use Transfer Appliance. Transfer Appliance is a physical storage
device that you can use to transfer large amounts of data to Google Cloud quickly and securely. Once you have moved your data onto a
Transfer Appliance, you can use a Transfer Appliance Rehydrator to decrypt the data and load it into Cloud Storage.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option B: Using Cloud Dataprep to decrypt the data into Cloud Storage is not a valid option, as Cloud Dataprep is a data preparation
tool that does not support data transfer or decryption.

Option C: Using resumable transfers to upload the data into Cloud Storage is not a recommended option for moving large amounts of
data, as resumable transfers are designed for smaller data sets and may not be efficient for transferring large amounts of data.

Option D: Using streaming transfers to upload the data into Cloud Storage is not a recommended option for moving large amounts of
data, as streaming transfers are designed for transferring real-time data streams and may not be efficient for transferring large
amounts of data.

Therefore, the correct answer is A: Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data
into Cloud Storage.

upvoted 4 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Option A Use a Transfer Appliance Rehydrator

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. Move your data onto a Transfer Appliance. Use a Transfer Appliance Rehydrator to decrypt the data into Cloud Storage.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

149/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  sgo cial 11ámonths ago

Selected Answer: A

A is correct ...
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options
1. gsutil is for lessthan <1 TB data with enough bandwidth, so C and D can be eliminated
2. option b can be eliminated since dataprep for decription is not correct
3. so only left over is a and its offline transfer, since the question did not give any time line when the transfer to be completed

upvoted 3 times

? ?  GoReplyGCPExam 1áyear, 1ámonth ago

Selected Answer: A

https://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: A

I would eliminate B & C as the question clearly methos google recomendations.About 10 TB or canal to we need to me transfer appliance.
Let's not worry about what regions Ohk for now.
Between A & B B is over kill as transfer applion allows decryption. Hence A

upvoted 1 times

? ?  meokey 1áyear, 2ámonths ago

Selected Answer: A

Is Transfer Appliance suitable for me?
"Your data size is greater than or equal to 10TB."
https://cloud.google.com/transfer-appliance/docs/4.0/overview#suitability

upvoted 3 times

? ?  awsarchitect5 1áyear, 4ámonths ago

Selected Answer: A

https://cloud.google.com/blog/ja/topics/developers-practitioners/how-transfer-your-data-google-cloud

upvoted 3 times

? ?  Narinder 1áyear, 5ámonths ago

A, is the correct option for transferring data of few TB to PB.
gsutil is viable option if the data size is about 1TB or less than that.

Reference: https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets

upvoted 2 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: A

I vote A is correct.
Agree with this thread.
https://cloud.google.com/blog/ja/topics/developers-practitioners/how-transfer-your-data-google-cloud

upvoted 1 times

? ?  Atnafu 1áyear, 5ámonths ago

The gsutil tool is the standard tool for small- to medium-sized transfers (less than 1 TB)
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options

upvoted 2 times

? ?  Atnafu 1áyear, 5ámonths ago

A is the Answer
upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

150/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #56

Topic 1

You have an application deployed on Google Kubernetes Engine using a Deployment named echo-deployment. The deployment is exposed using a

Service called echo-service. You need to perform an update to the application with minimal downtime to the application. What should you do?

A. Use kubectl set image deployment/echo-deployment <new-image>

B. Use the rolling update functionality of the Instance Group behind the Kubernetes cluster

C. Update the deployment yaml  le with the new container image. Use kubectl delete deployment/echo-deployment and kubectl create ?Ç"f

<yaml- le>

D. Update the service yaml  le which the new container image. Use kubectl delete service/echo-service and kubectl create ?Ç"f <yaml- le>

Correct Answer: A

Community vote distribution

A (91%)

9%

? ?  ffk  Highly Voted ?  3áyears, 8ámonths ago

A is correct.

B is funny

upvoted 46 times

? ?  DrLu 3áyears, 6ámonths ago

This question asked "You need to perform an update to the application with minimal downtime to the application."

upvoted 2 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 14 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes A is correct
upvoted 3 times

? ?  nitinz 2áyears, 3ámonths ago
Only logical answer is A.

upvoted 1 times

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

Correct is A

upvoted 13 times

? ?  vamgcp  Most Recent ?  4ámonths, 4áweeks ago

To perform an update to the application with minimal downtime on Google Kubernetes Engine (GKE), you can use a rolling update
strategy, which involves updating the application incrementally, one pod at a time, while ensuring that the updated pods are functioning
properly before updating the next set. Here's the general process:
kubectl set image deployment/echo-deployment echo=<new_image_tag>

upvoted 1 times

? ?  roaming_panda 5ámonths, 3áweeks ago

Selected Answer: A

https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps
says rolling updates and mentions same command .
So 100 % A

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is C: Update the deployment yaml file with the new container image. Use kubectl delete deployment/echo-deployment
and kubectl create ûf <yaml-file>.

To perform an update to an application deployed on Google Kubernetes Engine with minimal downtime, you can follow these steps:

Update the deployment yaml file with the new container image.
Use the kubectl delete deployment/echo-deployment command to delete the existing deployment.
Use the kubectl create ûf <yaml-file> command to create a new deployment using the updated yaml file.
This process, known as a rolling update, allows you to update your application with minimal downtime by replacing the old version of the
application with the new version one pod at a time, while ensuring that there is always at least one pod available to serve traffic.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

151/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: Using kubectl set image deployment/echo-deployment <new-image> will update the image of the containers in the
deployment, but it will not perform a rolling update and may result in downtime for the application.

Option B: Using the rolling update functionality of the Instance Group behind the Kubernetes cluster is not a valid option, as the rolling
update functionality is used to update the instances in the instance group, not the containers in a deployment.

Option D: Updating the service yaml file with the new container image and using kubectl delete service/echo-service and kubectl create
ûf <yaml-file> is not a valid option, as the service is not responsible for running the application containers and updating the service will
not update the application.

upvoted 1 times

? ?  CkWongCk 5ámonths ago

A is correct, update template spec image in deployment yml will trigger rollout deploy

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

using kubectl set image deployment/deployment <new-image> will not allow you to perform an update to the application with minimal
downtime, even if the deployment is exposed using a Service.

This command will update the image of the containers in the deployment, but it will not perform a rolling update. A rolling update
allows you to update your application with minimal downtime by replacing the old version of the application with the new version one
pod at a time, while ensuring that there is always at least one pod available to serve traffic. Without a rolling update, all of the pods in
the deployment will be replaced at the same time, which may result in downtime for the application.

upvoted 1 times

? ?  jasenmornin 6ámonths, 4áweeks ago

Selected Answer: A

I think A is correct:

B. I don't understand the objective of this option.
C and D. These are eliminated because they involve suffering a downtime when the resources are eliminated, so they are not fulfilling one
of the requirements.

upvoted 1 times

? ?  markus_de 7ámonths ago

Selected Answer: A

Example from official Kubernetes docu (for NGINX):
kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1

https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

upvoted 3 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is right -- kubectl set image deployment/echo-deployment

upvoted 1 times

? ?  RitwickKumar 10ámonths, 2áweeks ago

Selected Answer: A

Source: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment

Deployment ensures that only a certain number of Pods are down while they are being updated. By default, it ensures that at least 75% of
the desired number of Pods are up (25% max unavailable).

Deployment also ensures that only a certain number of Pods are created above the desired number of Pods. By default, it ensures that at
most 125% of the desired number of Pods are up (25% max surge).

upvoted 6 times

? ?  Mikado211 10ámonths, 4áweeks ago

Selected Answer: A

Answer is A

It can't be C, if you delete and recreate the deployment you will have a downtime between the deletion and the recreation.

upvoted 1 times

? ?  Ric350 11ámonths, 2áweeks ago

It's definitely A. See here under updating a deployment on the right hand side.
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment

upvoted 2 times

? ?  JohnPi 11ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

152/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

https://cloud.google.com/kubernetes-engine/docs/how-to/updating-apps#updating_an_application

upvoted 2 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

D - we can all eliminate as option is wrong as image is set in deployment ym I and not service yml.
A - I will eliminate. as just updating wont put new image This will happen if only the continen gets redeployed due to load or restart.
B- is also a wrong statement for k 8s.
Hence C . It could be the grableled text is new yaml.

upvoted 2 times

? ?  Nirca 1áyear, 1ámonth ago

Selected Answer: A

Yes A is correct
upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I think B.

upvoted 2 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I really confused by a sentence 'Instance Group behind the Kubernetes cluster' .
It's correct to use the rolling update feature,but in gke, do it with a pod and replica.
A is how to work rolling updatee in GKE.So A is correct.

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

153/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #57

Topic 1

Your company is using BigQuery as its enterprise data warehouse. Data is distributed over several Google Cloud projects. All queries on BigQuery

need to be billed on a single project. You want to make sure that no query costs are incurred on the projects that contain the data. Users should be

able to query the datasets, but not edit them.

How should you con gure users' access roles?

A. Add all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery dataViewer on the projects that

contain the data.

B. Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that

contain the data.

C. Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that

contain the data.

D. Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that

contain the data.

Correct Answer: C

Community vote distribution

C (100%)

? ?  kimharsh  Highly Voted ?  1áyear, 4ámonths ago

Selected Answer: C

C is the correct Answer ,
A is wrong because bq User Permission will allow you to edit the dataset, which is something that we don't want in this scenario.
B and D is wrong because "You want to make sure that no query costs are incurred on the projects that contain the data" so you don't
want users to fire quires on the Project that contains the dataset , hence the "dataViewer" permission

https://cloud.google.com/bigquery/docs/access-control

upvoted 16 times

? ?  kratosmat 3ámonths, 1áweek ago

It seems that User Permission doesn't allow to edit data, isn't it?

upvoted 1 times

? ?  RitwickKumar  Highly Voted ?  10ámonths, 2áweeks ago

Selected Answer: C

Both A & C are correct but using the principle of least privileges C is the most appropriate.

BigQuery User: (roles/bigquery.user)
When applied to a dataset, this role provides the ability to read the dataset's metadata and list tables in the dataset.
When applied to a project, this role also provides the ability to run jobs, including queries, within the project. A principal with this role can
enumerate their own jobs, cancel their own jobs, and enumerate datasets within a project. <b>Additionally, allows the creation of new
datasets within the project; the creator is granted the BigQuery Data Owner role(roles/bigquery.dataOwner) on these new datasets.</b>
Lowest-level resources where you can grant this role: Dataset

BigQuery Job User: (roles/bigquery.jobUser)
Provides permissions to run jobs, including queries, within the project.
Lowest-level resources where you can grant this role: Project

Source: https://cloud.google.com/bigquery/docs/access-control

upvoted 14 times

? ?  SidsA  Most Recent ?  3ámonths ago

Selected Answer: C

The "roles/bigquery.jobUser" role provides the permission to run jobs, including querying, exporting and copying data, and creating views
and materialized views. This role does not provide permissions to create, update, or delete BigQuery resources, such as datasets, tables,
and models. Users with this role can only interact with BigQuery through jobs.

The "roles/bigquery.User" role, on the other hand, provides the permission to create, update, and delete BigQuery resources, as well as
run jobs. This role includes all the permissions of the "roles/bigquery.jobUser" role, and in addition allows users to manage BigQuery
resources, such as creating datasets, tables, and models, and modifying their schema and access controls.

upvoted 1 times

? ?  jlambdan 3ámonths, 1áweek ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

154/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is wrong because https://cloud.google.com/bigquery/docs/access-control#bigquery.user
C is correct because https://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser

upvoted 1 times

? ?  jay9114 6ámonths, 1áweek ago

Selected Answer: C

Important statements from the prompt
1. All queries need to be billed to a single project - one project that queries data stored on other projects. Let's call this our billing project.
a. jobUser is the best role to satisfy this need, because it provides permission to run jobs
and queries within a project.

2. Other projects is where the data resides. These projects don't need much access besides the ability to be viewed (not edited).
a. The dataViewer role provide permission to read all datasets in the project.

upvoted 6 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is A: Add all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery
dataViewer on the projects that contain the data.

To make sure that no query costs are incurred on the projects that contain the data and allow users to query the datasets but not edit
them, you should follow these steps:

Add all users to a group.
Grant the group the role of BigQuery user on the billing project. This will allow the group to run queries on BigQuery and incur costs on
the billing project.
Grant the group the role of BigQuery dataViewer on the projects that contain the data. This will allow the group to view the datasets and
run queries on them, but not edit them.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

The BigQuery Job User role (roles/bigquery.jobUser) and the BigQuery User role (roles/bigquery.user) have similar permissions, but
they differ in the scope of their permissions.

The BigQuery Job User role grants users the ability to create and modify query jobs, but it does not grant them the ability to run
queries or incur costs on the project. This role is intended for users who need to create and manage query jobs, but who should not be
able to run queries or incur costs.

The BigQuery User role grants users the ability to run queries and incur costs on the project, in addition to the ability to create and
modify query jobs. This role is intended for users who need to run queries and incur costs on the project, as well as create and manage
query jobs.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Here is a summary of the differences between the BigQuery Job User role and the BigQuery User role:

BigQuery Job User role (roles/bigquery.jobUser):

Can create and modify query jobs
Cannot run queries or incur costs on the project
BigQuery User role (roles/bigquery.user):

Can create and modify query jobs
Can run queries and incur costs on the project
If you want to grant users the ability to create and modify query jobs, but not run queries or incur costs on the project, you should
use the BigQuery Job User role. If you want to grant users the ability to run queries and incur costs on the project, in addition to the
ability to create and modify query jobs, you should use the BigQuery User role.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option B: Granting the group the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that contain
the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.

Option C: Granting the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain
the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.

Option D: Granting the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that contain
the data will not allow the group to incur costs on the billing project and will not meet the requirements of the scenario.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is right
Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects
that contain the data.

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: C

C. Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects
that contain the data.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

155/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  Vedjha 8ámonths, 3áweeks ago

D is the answer:
Cloud BigQuery Roles
Cloud BigQuery IAM Roles
BigQuery Admin - bigquery.*
BigQuery Data Owner - bigquery.datasets.*, bigquery.models.*, bigquery.routines.*,
bigquery.tables.* (Does NOT have access to Jobs!)
BigQuery Data Editor - bigquery.tables.(create/delete/export/get/getData/getIamPolicy/
list/update/updateData/updateTag), bigquery.models.*, bigquery.routines.*,
bigquery.datasets.(create/get/getIamPolicy/updateTag)
BigQuery Data Viewer - get/list bigquery.(datasets/models/routines/tables)
BigQuery Job User - bigquery.jobs.create
BigQuery User - BigQuery Data Viewer + get/list (jobs, capacityCommitments, reservations
etc)
To see data, you need either BigQuery User or BigQuery Data Viewer roles
You CANNOT see data with BigQuery Job User roles
BigQuery Data Owner or Data Viewer roles do NOT have access to jobs!

upvoted 1 times

? ?  kimharsh 1áyear, 4ámonths ago
C is the correct Answer ,
A is wrong because bq User Permission will allow you to edit the dataset, which is something that we don't want in this scenario.
B and D is wrong because "You want to make sure that no query costs are incurred on the projects that contain the data" so you don't
want users to fire quires on the Project that contains the dataset , hence the "dataViewer" permission

https://cloud.google.com/bigquery/docs/access-control

upvoted 1 times

? ?  victory108 1áyear, 5ámonths ago

C. Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects
that contain the data.

upvoted 4 times

? ?  LoveT 1áyear, 6ámonths ago

C looks to be the correct answer

upvoted 2 times

? ?  HenkH 1áyear, 6ámonths ago

Selected Answer: C

JobUser is the correct terminology for bq. Only read access to data sources is required.

upvoted 1 times

? ?  HenkH 1áyear, 6ámonths ago

bq is using jobs - so "user" isn't specific enough, jobuser is.

upvoted 2 times

? ?  elenamatay 1áyear, 5ámonths ago

Hence C

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

156/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #58

Topic 1

You have developed an application using Cloud ML Engine that recognizes famous paintings from uploaded images. You want to test the

application and allow speci c people to upload images for the next 24 hours. Not all users have a Google Account. How should you have users

upload images?

A. Have users upload the images to Cloud Storage. Protect the bucket with a password that expires after 24 hours.

B. Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours.

C. Create an App Engine web application where users can upload images. Con gure App Engine to disable the application after 24 hours.

Authenticate users via Cloud Identity.

D. Create an App Engine web application where users can upload images for the next 24 hours. Authenticate users via Cloud Identity.

Correct Answer: B

Community vote distribution

B (100%)

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

Correct answer is B

upvoted 43 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 8 times

? ?  kumarp6 2áyears, 8ámonths ago
Signed URL ... B is correct

upvoted 3 times

? ?  nitinz 2áyears, 3ámonths ago

B signed URL
upvoted 3 times

? ?  MyPractice  Highly Voted ?  3áyears, 6ámonths ago

Ans B
"When should you use a signed URL? In some scenarios, you might not want to require your users to have a Google account in order to
access Cloud Storage" "Signed URLs contain authentication information in their query string, allowing users without credentials to
perform specific actions on a resource"
https://cloud.google.com/storage/docs/access-control/signed-urls

upvoted 22 times

? ?  fussili  Most Recent ?  3ámonths ago

The correct answer is B.

A is not a good choice because it is not possible to set an expiration time for a password protected Cloud Storage bucket. This means that
if a user had the password, they would be able to upload images to the bucket even after the 24 hour period has expired.

B is the correct answer because a signed URL can be generated to allow specific users to upload images to Cloud Storage without
requiring them to have a Google Account. The URL can be set to expire after 24 hours, which ensures that users can only upload images
during the allowed time period.

C is not the best choice because it involves creating an App Engine web application, which is more complex than using Cloud Storage with
a signed URL. Additionally, App Engine instances cannot be turned off programmatically, so it would not be possible to disable the
application after 24 hours.

D option is similar to option C, but it involves creating an App Engine web application. This would add unnecessary complexity to the
solution, and it would not provide any additional benefits compared to using Cloud Storage with a signed URL.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is B: Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours.

To allow specific users to upload images to Cloud Storage for testing your Cloud ML Engine application, and to not require all users to
have a Google Account, you should use signed URLs. A signed URL is a URL that allows access to a specific resource in Cloud Storage, and
that is only valid for a specified period of time.

To create a signed URL that expires after 24 hours, you can use the gsutil signurl command. For example:

Copy code

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

157/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

gsutil signurl -d 24h service-account.json gs://bucket-name/object-name
This will generate a signed URL that allows users to upload an object to the specified bucket with the specified name, and that will only be
valid for 24 hours.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: Protecting the bucket with a password that expires after 24 hours would not be a secure or scalable solution, as it would
require you to distribute the password to all users and to update the password every 24 hours.

Option C: Creating an App Engine web application where users can upload images, and configuring App Engine to disable the
application after 24 hours, would not allow users to upload images after the application is disabled.

Option D: Creating an App Engine web application where users can upload images for the next 24 hours and authenticating users via
Cloud Identity would not allow users to upload images if they do not have a Google Account.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right, Signed URL's will help in this scnerio.

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B. Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours.

upvoted 1 times

? ?  mv2000 12ámonths ago
On 06/30/2022 Exam.

upvoted 2 times

? ?  mygcpjourney2712 1áyear, 3ámonths ago

Selected Answer: B

signed url

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

B is the correct answer

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B.

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û Have users upload the images to Cloud Storage via signed URL which expires after 24 hours.
Signed URL is a preferable way to allow something with limited timeframe, doesn't require the account

upvoted 1 times

? ?  [Removed] 1áyear, 8ámonths ago

B is right. Signed URL are best for users for short term access.

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 3 times

? ?  rishab86 2áyears ago

Answer B
A signed URL is a URL that provides limited permission and time to make a request. Signed URLs contain authentication information in
their query string, allowing users without credentials to perform specific actions on a resource. When you generate a signed URL, you
specify a user or service account which must have sufficient permission to make the request that the signed URL will make. After you
generate a signed URL, anyone who possesses it can use the signed URL to perform specified actions, such as reading an object, within a
specified period of time.

When should you use a signed URL?
In some scenarios, you might not want to require your users to have a Google account in order to access Cloud Storage, but you still want
to control access using your application-specific logic.

upvoted 4 times

? ?  victory108 2áyears, 1ámonth ago

B. Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

158/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

B is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

159/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #59

Topic 1

Your web application must comply with the requirements of the European Union's General Data Protection Regulation (GDPR). You are responsible

for the technical architecture of your web application. What should you do?

A. Ensure that your web application only uses native features and services of Google Cloud Platform, because Google already has various

certi cations and provides ?Çpass-on?Ç compliance when you use native features.

B. Enable the relevant GDPR compliance setting within the GCPConsole for each of the services in use within your application.

C. Ensure that Cloud Security Scanner is part of your test planning strategy in order to pick up any compliance gaps.

D. De ne a design for the security of data in your web application that meets GDPR requirements.

Correct Answer: D

Reference:

https://www.mobiloud.com/blog/gdpr-compliant-mobile-app/

Community vote distribution

D (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Agree D

upvoted 17 times

? ?  AshokC  Highly Voted ?  2áyears, 9ámonths ago
D - https://cloud.google.com/security/gdpr
The GDPR lays out specific requirements for businesses and organizations who are established in Europe or who serve users in Europe. It:

Regulates how businesses can collect, use, and store personal data
Builds upon current documentation and reporting requirements to increase accountability
Authorizes fines on businesses who fail to meet its requirements

upvoted 11 times

? ?  omermahgoub  Most Recent ?  6ámonths, 1áweek ago

The correct answer is option D: Define a design for the security of data in your web application that meets GDPR requirements.

The General Data Protection Regulation (GDPR) is a comprehensive data protection law that applies to any company that processes the
personal data of individuals in the European Union (EU). As the technical architect of your web application, it is your responsibility to
ensure that the application is compliant with GDPR requirements.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: While it is true that Google has various certifications and provides pass-on compliance when you use native features, simply
using native features and services of Google Cloud Platform is not sufficient to ensure compliance with GDPR. You still need to
implement appropriate controls and safeguards to protect personal data and meet GDPR requirements.

Option B: Enabling the relevant GDPR compliance setting within the GCP console for each of the services in use within your application
may help ensure compliance with GDPR, but it is not sufficient on its own. You still need to implement appropriate controls and
safeguards to protect personal data and meet GDPR requirements.

Option C: Using Cloud Security Scanner as part of your test planning strategy can help identify potential security vulnerabilities and
compliance gaps in your web application, but it is not sufficient on its own to ensure compliance with GDPR. You still need to
implement appropriate controls and safeguards to protect personal data and meet GDPR requirements.

upvoted 4 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Define a design for the security of data in your web application that meets GDPR requirements. D is right

upvoted 1 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: D

I got similar question on my exam.

upvoted 6 times

? ?  vincy2202 1áyear, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

160/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D is the correct answer

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for D

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

D û Define a design for the security of data in your web app that meets GDPR requirements.

upvoted 1 times

? ?  MikeB19 1áyear, 9ámonths ago

A is wrong D is correct. The q refers is ôMicrosoft sqlö not ôMySQLö. App replication in MSsql is achieved with Availability Groups within
MSsql
https://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?
view=sql-server-ver15

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is D

upvoted 3 times

? ?  victory108 2áyears, 1ámonth ago

D. Define a design for the security of data in your web application that meets GDPR requirements.

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

D is correct

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is D

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

D is ok

upvoted 1 times

? ?  CloudGenious 2áyears, 4ámonths ago

you should design your app such that they meet GDPR req. As a customer google cloud, GDPR should be part of protection strategy .So
the ans is D.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

161/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #60

Topic 1

You need to set up Microsoft SQL Server on GCP. Management requires that there's no downtime in case of a data center outage in any of the

zones within a

GCP region. What should you do?

A. Con gure a Cloud SQL instance with high availability enabled.

B. Con gure a Cloud Spanner instance with a regional instance con guration.

C. Set up SQL Server on Compute Engine, using Always On Availability Groups using Windows Failover Clustering. Place nodes in different

subnets.

D. Set up SQL Server Always On Availability Groups using Windows Failover Clustering. Place nodes in different zones.

Correct Answer: D

Community vote distribution

A (56%)

D (41%)

? ?  learningpv  Highly Voted ?  3áyears, 5ámonths ago

A seems correct.
"... high availability (HA) configuration for Cloud SQL instances... A Cloud SQL instance configured for HA is also called a regional instance
and is located in a primary and secondary zone within the configured region.
In the event of an instance or zone failure, this configuration reduces downtime, and your data continues to be available to client
applications."

upvoted 49 times

? ?  DevOpsi er 2áweeks, 2ádays ago

you said it yourself,"this configuration reduces downtime", it doesn't remove it completely as required, so the correct answer is D.
See @taer answer:
"...Cloud SQL's high availability configuration relies on a primary instance and a standby instance that resides in a different zone within
the same region. In the event of an outage, Cloud SQL will automatically failover to the standby instance, but the failover process takes
some time, usually around 1-2 minutes, which might result in a brief downtime.

On the other hand, using SQL Server Always On Availability Groups with Windows Failover Clustering allows you to have multiple
replicas of your data distributed across different zones. The failover process between these replicas is faster and more seamless,
reducing the risk of downtime even further..."

upvoted 2 times

? ?  diluviouniv 1áyear, 11ámonths ago

but it says: you need to setup SQL Server

upvoted 11 times

? ?  learningpv 3áyears, 5ámonths ago

It applies for MySQL and HA is not available for MS SQL

upvoted 5 times

? ?  Jos 3áyears, 5ámonths ago

Yes it is available, its in beta, but when creating a "SQL Server 2017 Standard" in Cloud SQL menu you can chose single one or HA
(regional).

upvoted 3 times

? ?  cetanx 2áyears, 11ámonths ago
It is available, please see;
https://cloud.google.com/sql/docs/sqlserver/high-availability?_ga=2.30855355.-503483612.1582800507
Also a video from Google;
https://youtu.be/vMUpNoukwnM

upvoted 9 times

? ?  mrealtor 1áyear, 2ámonths ago

You need to set up a Microsoft SQL server. Why are we talking about Cloud SQL

upvoted 4 times

? ?  tycho 10ámonths, 2áweeks ago

and what is Cloud SQL -> a managed service for MySQL, Posrgers, and MS SQL server

upvoted 8 times

? ?  SMS  Highly Voted ?  3áyears, 3ámonths ago

Answer is A. Cloud SQL supports SQL Server and selecting high availability provides automatic failover within a region.

upvoted 25 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

162/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  red_panda  Most Recent ?  3áweeks, 4ádays ago

Selected Answer: D

For me is D

upvoted 1 times

? ?  TheCloudGuruu 1ámonth, 2áweeks ago

Selected Answer: A

A is the right answer

upvoted 1 times

? ?  stfnz 1ámonth, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  kratosmat 2ámonths, 1áweek ago

Selected Answer: A

https://cloud.google.com/sql/docs/sqlserver/high-availability

upvoted 2 times

? ?  gghggg 1ámonth, 4áweeks ago
Question is for Microsoft SQL.

CloudSQL supports MySQL, PostgreSQL, and SQL Server as per : https://cloud.google.com/sql/docs/sqlserver/introduction No MSSQL
support. Therefore answer is D.

upvoted 1 times

? ?  GCPAnji 3ámonths ago

Anser should be A. high availability enabling through Availability group is the way to maintain no downtime within a GCP region.

upvoted 1 times

? ?  taer 3ámonths ago

Selected Answer: D

Cloud SQL's high availability configuration relies on a primary instance and a standby instance that resides in a different zone within the
same region. In the event of an outage, Cloud SQL will automatically failover to the standby instance, but the failover process takes some
time, usually around 1-2 minutes, which might result in a brief downtime.

On the other hand, using SQL Server Always On Availability Groups with Windows Failover Clustering allows you to have multiple replicas
of your data distributed across different zones. The failover process between these replicas is faster and more seamless, reducing the risk
of downtime even further.

upvoted 5 times

? ?  DevOpsi er 2áweeks, 2ádays ago
Excellent explanation! thanks!!!

upvoted 1 times

? ?  mifrah 3ámonths ago

A. Cloud SQL supports MSSQL & HA. Using a managed Service is always better than IaaC.
It should also be cheaper. I would only go with MSSQL on Compute Engine, if you need special Microsoft features, that are not provided by
Cloud SQL (e.g. AD user integration).

upvoted 2 times

? ?  jlambdan 3ámonths, 1áweek ago

Selected Answer: A

A it's the doc
https://cloud.google.com/sql/docs/mysql/high-availability
see also https://cloud.google.com/compute/docs/instances/sql-server/disaster-recovery-for-microsoft-sql-
server#high_availability_and_disaster_recovery

B spanner is not sql server

C is not possible, the doc says all nodes are in one subnets. "all WSFC nodes are deployed in the same zone and use a common subnet" =>
https://cloud.google.com/compute/docs/instances/sql-server/configure-failover-cluster-instance-pd-multi-writer

D is not possible. Nodes needs to share the same persistent disk but it is a zonal resources. So you can't put them in different zones.
"Because persistent disks in multi-writer mode are a zonal resource, all WSFC nodes are deployed in the same zon" =>
https://cloud.google.com/compute/docs/instances/sql-server/configure-failover-cluster-instance-pd-multi-writer

upvoted 4 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: A

D required native SQL Server and Windows functionality which is not an answer in a Google exam!

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

163/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Deb2293 4ámonths ago

Selected Answer: B

If it requires high availability, why not go with Cloud Spanner?
Cloud Spanner delivers industry-leading high availability (99.999%) for multi-regional instances
https://cloud.google.com/spanner#:~:text=Cloud%20Spanner%20delivers%20industry%2Dleading,region%20and%20multi%2Dregion%20
configurations.
upvoted 1 times

? ?  stock28_CA 4ámonths ago

This question is about the HA of the MY SQL server not about the HA of cloud SQL instance. The answer should be D

upvoted 1 times

? ?  Jeena345 4ámonths, 3áweeks ago

Selected Answer: A

A is correct

upvoted 1 times

? ?  SambuSoni 4ámonths, 3áweeks ago

Answer is C

upvoted 1 times

? ?  rocktehk 4ámonths, 4áweeks ago

"Configuring a SQL Server failover cluster instance that uses persistent disks in multi-writer mode"
https://cloud.google.com/compute/docs/instances/sql-server/configure-failover-cluster-instance-pd-multi-writer
Says:
"""
Microsoft SQL Server Always On Failover Cluster Instances (FCI) let you run a single SQL Server instance across multiple Windows Server
Failover Cluster (WSFC) nodes. At any point in time, one of the cluster nodes actively hosts the SQL instance. In the event of a failure, WSFC
automatically transfers ownership of the instance's resources to another node.
SQL Server FCI requires data to be located on shared storage so that it can be accessed across all WSFC nodes. (...)
Because persistent disks in multi-writer mode are a zonal resource, all WSFC nodes are deployed in the same zone and use a common
subnet. Clients communicate with the SQL Server instance over an internal TCP load balancer. This load balancer uses the Windows Server
Failover Clustering agent to determine which WSFC node is currently hosting the SQL instance and routes traffic to that instance.
"""
So "D" seems not possible.

upvoted 1 times

? ?  Il_Tera 4ámonths, 4áweeks ago

Selected Answer: A

Now MSSQL is provided as managed service by GCP so setting HA automatically configure everything for you and use different zones to
ensure HA

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

164/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #61

Topic 1

The development team has provided you with a Kubernetes Deployment  le. You have no infrastructure yet and need to deploy the application.

What should you do?

A. Use gcloud to create a Kubernetes cluster. Use Deployment Manager to create the deployment.

B. Use gcloud to create a Kubernetes cluster. Use kubectl to create the deployment.

C. Use kubectl to create a Kubernetes cluster. Use Deployment Manager to create the deployment.

D. Use kubectl to create a Kubernetes cluster. Use kubectl to create the deployment.

Correct Answer: B

Community vote distribution

B (100%)

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

It has to be B. gcloud for creating cluster and kubectl for creating deployment

upvoted 52 times

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

May I ask why C is correct?
I thought B was correct.

upvoted 26 times

? ?  res3 2áyears, 12ámonths ago

yeap, gcloud command to create K8s cluster https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster

upvoted 3 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 9 times

? ?  kumarp6 2áyears, 8ámonths ago

B is correct, when you create a nodes in GKE you use gcloud rather than kubectl...

upvoted 4 times

? ?  nitinz 2áyears, 3ámonths ago

B, gcloud to manage GKE and to manage pods use kubctl.

upvoted 2 times

? ?  vamgcp  Most Recent ?  4ámonths, 4áweeks ago

Create a Google Kubernetes Engine (GKE) cluster: You can use the Google Cloud Console or the gcloud command-line tool to create a GKE
cluster, which will provide the underlying infrastructure for running your application.

Deploy the application to the cluster: You can use the kubectl command-line tool to apply the Kubernetes Deployment file provided by the
development team to the cluster.kubectl apply -f deployment.yaml

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

is the correct answer https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

B is the correct answer cluster https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right

upvoted 1 times

? ?  SAMBIT 1áyear, 3ámonths ago

Kubctle comes live only when cluster has been created in the cloud console using cloud command

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

165/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  ghadxx 1áyear, 4ámonths ago

Selected Answer: B

Deployment Manager is used to automate the process of provisioning infrastructure. Therefore, gcloud and Deployment Manager do the
same thing. Meanwhile, kubectl is used to run commands against an already created cluster.

upvoted 5 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B.
gcloud for create clusters.
kubectl is used when the cluster already has been created. For example to create deployments.
Kubectl has configured a config file where is specified the default cluster.

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

B is correct

upvoted 1 times

? ?  Zinhle 1áyear, 7ámonths ago

Hi all may someone please share the link for the bank of questions because I cannot seem to locate them.

thank you

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û use gcloud to create cluster, use kubectl to create a deployment.
https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster
In fact, kubectl run creates a deployment.
https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app

upvoted 1 times

? ?  ale183 1áyear, 9ámonths ago

Question for all , do we know if only new questions are part of the bank for new exam? Have any of the old questions appeared on new
exam?

upvoted 3 times

? ?  xaliq 1áyear, 9ámonths ago

B is corrent

upvoted 1 times

? ?  Raja101 1áyear, 9ámonths ago

Why not A ?

upvoted 3 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

166/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #62

Topic 1

You need to evaluate your team readiness for a new GCP project. You must perform the evaluation and create a skills gap plan which incorporates

the business goal of cost optimization. Your team has deployed two GCP projects successfully to date. What should you do?

A. Allocate budget for team training. Set a deadline for the new GCP project.

B. Allocate budget for team training. Create a roadmap for your team to achieve Google Cloud certi cation based on job role.

C. Allocate budget to hire skilled external consultants. Set a deadline for the new GCP project.

D. Allocate budget to hire skilled external consultants. Create a roadmap for your team to achieve Google Cloud certi cation based on job

role.

Correct Answer: A

Community vote distribution

B (78%)

A (16%)

6%

? ?  KouShikyou  Highly Voted ?  3áyears, 7ámonths ago

B is correct.

upvoted 42 times

? ?  nitinz 2áyears, 3ámonths ago

B, looks like cooked up question. Not gonna show up on actual test. Even if it does show up, its not market.

upvoted 6 times

? ?  ACE_ASPIRE 1áyear, 9ámonths ago

exactly

upvoted 1 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes it is

upvoted 1 times

? ?  passnow 3áyears, 6ámonths ago

I would agree with you because the question says create a skills gap plan

upvoted 3 times

? ?  mawsman  Highly Voted ?  3áyears, 4ámonths ago

I think it's B. "You must perform the evaluation and create a skills gap plan incorporates the business goal of cost optimization." The goal
is to cost optimize - they might have deployed 2 projects but are they cost optimized? I think the only way to evaluate the skills gap in cost
optimization is to make them get certified and use the results to determine cost optimization skills gap. Quickly pushing another project
deadline would not help with cost optimization.

upvoted 18 times

? ?  Smart 3áyears, 4ámonths ago

Agreed. How is setting up a GCP project deadline helping towards skill gap and cost optimization.

upvoted 3 times

? ?  PKookNN  Most Recent ?  1áweek, 2ádays ago

Selected Answer: B

B is a better answer

upvoted 1 times

? ?  kaaiden 1ámonth, 1áweek ago

Selected Answer: A

GCP partnership need 3 project

upvoted 1 times

? ?  TheCloudGuruu 1ámonth, 2áweeks ago

Selected Answer: B

B is the right answer

upvoted 1 times

? ?  grejao 2ámonths, 4áweeks ago

It shows more like a trick than a really question.
Furthermore are this question relevant for certification?

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

167/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  telp 3ámonths, 2áweeks ago

Selected Answer: B

You are in a google exam. Always choose certification for your teams.

upvoted 2 times

? ?  Deb2293 4ámonths ago

Selected Answer: A

The answer should be A.
If your team has deployed two GCP projects successfully to date, then they should be knowing good amount of GCP. It's unnecessary for
each to take GCP EXAM. So it shouldn't be B

upvoted 1 times

? ?  Deb2293 3ámonths, 4áweeks ago

I change my answer, should be B.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is option B: Allocate budget for team training. Create a roadmap for your team to achieve Google Cloud certification
based on job role.

To evaluate your team's readiness for a new GCP project and create a skills gap plan, you should consider the business goal of cost
optimization. One way to optimize costs is to invest in training for your team to increase their skills and knowledge of GCP. This can help
your team become more efficient and effective in using GCP, potentially resulting in cost savings over time. You should allocate budget for
team training and create a roadmap for your team to achieve Google Cloud certification based on their job roles. This will help ensure that
your team has the necessary skills and knowledge to successfully deploy the new GCP project.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A: Allocating budget for team training and setting a deadline for the new GCP project is a good start, but it is not sufficient on
its own to ensure that your team is ready for the project. You also need to create a roadmap for your team to achieve Google Cloud
certification based on their job roles.

Option C: Allocating budget to hire skilled external consultants and setting a deadline for the new GCP project may help ensure that the
project is completed on time, but it does not address the skills gap within your team. To ensure that your team is ready for future
projects, you should allocate budget for team training and create a roadmap for your team to achieve Google Cloud certification based
on their job roles.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option D: Allocating budget to hire skilled external consultants and creating a roadmap for your team to achieve Google Cloud
certification based on their job roles may help ensure that your team has the necessary skills and knowledge for the new GCP
project, but it does not address the long-term needs of your team. Investing in training for your team can help increase their skills
and knowledge of GCP, potentially resulting in cost savings over time.

upvoted 1 times

? ?  ukar 6ámonths, 1áweek ago

Selected Answer: B

it is B

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 3áweeks ago

Selected Answer: B

B is correct

upvoted 1 times

? ?  jgnogueira 7ámonths, 1áweek ago

Selected Answer: B

B is correct

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

B is the answer, why not Google advertises itself with Google certs?

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right

upvoted 1 times

? ?  Rajeev26 8ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

168/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

b is correct

upvoted 1 times

? ?  KinjalAkhani 9ámonths, 2áweeks ago

Questions itself states that need to do evaluation and create a skills gap plan to achieve business goal of cost optimization. Based on
questions keys B should be the correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

169/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #63

Topic 1

You are designing an application for use only during business hours. For the minimum viable product release, you'd like to use a managed product

that automatically scales to zero so you don't incur costs when there is no activity.

Which primary compute resource should you choose?

A. Cloud Functions

B. Compute Engine

C. Google Kubernetes Engine

D. AppEngine  exible environment

Correct Answer: A

Community vote distribution

A (89%)

11%

? ?  abirroy  Highly Voted ?  9ámonths, 2áweeks ago

Selected Answer: A

A. Cloud Functions - managed service scales down to 0
B. Compute Engine - not a managed service
C. Google Kubernetes Engine - not a managed service and wont scale down to 0
D. AppEngine flexible environment - managed service but wont scale down to 0

upvoted 10 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Agree with A

upvoted 2 times

? ?  victory108  Highly Voted ?  1áyear, 5ámonths ago

A. Cloud Functions

upvoted 10 times

? ?  vpatiltech 1áyear, 4ámonths ago

Cloud function is more for event driven computing. We surely need k8s or app engine. Flex always have 1 instance running. So GKE
should be the option

upvoted 4 times

? ?  YAS007 1áyear, 1ámonth ago

from the doc :https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler

Note: If you specify a minimum of zero nodes, an idle node pool can scale down completely. However, at least one node must always
be available in the cluster to run system Pods.

upvoted 1 times

? ?  6721sora 10ámonths, 1áweek ago

But no cost for System/Control nodes

upvoted 1 times

? ?  LaxmanTiwari  Most Recent ?  1ámonth, 2áweeks ago

agree A

upvoted 1 times

? ?  grejao 2ámonths, 4áweeks ago

Selected answer: D
I choosed D, it appears that we do not. have a right answer here.
A. Cloud Functions - its more for event driven computing, not for full application
B. Compute Engine - not a managed service
C. Google Kubernetes Engine - not a managed service and wont scale down to 0
D. AppEngine flexible environment - Only Standard App Engine can scale to 0.

upvoted 1 times

? ?  Bedmed 2ámonths ago

yes, but only Standard environment, not flexible environment

upvoted 1 times

? ?  GCPAnji 3ámonths ago

For an application that is only used during business hours and needs to scale to zero during periods of inactivity to minimize costs, a good
choice would be a Function-as-a-Service (FaaS) product like AWS Lambda or Google Cloud Functions.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

170/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  jlambdan 3ámonths, 1áweek ago

Selected Answer: B

A a cloud function is not an application

B compute engine via MIG you can use an autoscaler with a schedule.
https://cloud.google.com/compute/docs/autoscaler/scaling-schedules
You then can go from 0 to more instance when required

C K8s is two complex for this.
You can have an autoscaler for the cluster in order to get the node number to 0, but it require the node to have no pods running. So you
have to configure your deployments and all your workload to scale to 0 too.
Other interference will be pod affinity, anti-affinity, disruption budget or unmanaged pod preventing pod eviction from node. But if the
pod is not evicted, the node cannot be deleted.
"autoscaler respects scheduling and eviction rules set on Pods. These restrictions can prevent a node from being deleted by the
autoscaler. " => https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#scheduling-and-disruption

D flexible cannot scale to 0

upvoted 1 times

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: A

A. Cloud Functions

upvoted 1 times

? ?  WFCheong 5ámonths, 3áweeks ago

Why not D? App Engines also can scale down to zero when there is no activity. https://cloud.google.com/appengine/docs/the-appengine-
environments#:~:text=Intended%20to%20run%20for%20free,when%20there%20is%20no%20traffic. Intended to run for free or at very
low cost, where you pay only for what you need and when you need it. For example, your application can scale to 0 instances when there
is no traffic.

upvoted 3 times

? ?  CkWongCk 5ámonths ago

There are 2 mode for App engines, standard and flexible.

The standard environment can scale from zero instances up to thousands very quickly. In contrast, the flexible environment must have
at least one instance running for each active version and can take longer to scale out in response to traffic.

upvoted 4 times

? ?  thamaster 6ámonths, 1áweek ago

Selected Answer: A

the only to scale to 0 is A

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is A. Cloud Functions.

Cloud Functions is a serverless compute service that lets you run code without provisioning or managing infrastructure. One of the key
benefits of using Cloud Functions is that it automatically scales to meet the demands of your workload and automatically scales down to
zero when there is no activity. This means that you only pay for the compute resources that you consume, which can help to reduce costs
when your application is not in use. Additionally, Cloud Functions is easy to use and allows you to deploy your code with minimal effort,
making it a good choice for a minimum viable product release.

upvoted 1 times

? ?  backhand 10ámonths, 3áweeks ago

vote A
this is easy one. key word: managed product, scales to zero
scale to zero: app engine standard, cloud function

upvoted 2 times

? ?  vijbabu 11ámonths, 3áweeks ago

Selected Answer: B

Answer is B

upvoted 1 times

? ?  Dhiraj03 1áyear ago

Selected Answer: A

Cloud Functions can scale to zero when not in use

upvoted 1 times

? ?  Sskhan 1áyear, 5ámonths ago

Selected Answer: A

Answer is A, As Option D App engine flexible can have minimum 1 instance active.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

171/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Sskhan 1áyear, 5ámonths ago

Answer is A, As Option D App engine flexible can have minimum 1 instance active.

upvoted 1 times

? ?  elenamatay 1áyear, 5ámonths ago

Isn't it that with Kubernetes version 1.7 you can have a minimum of 0 in your node pool, being able to scale to 0? See
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#minimum_and_maximum_node_pool_size blue note

upvoted 3 times

? ?  jvale 1áyear, 3ámonths ago

"However, at least one node must always be available in the cluster to run system Pods"

upvoted 3 times

? ?  meokey 1áyear, 2ámonths ago

"You are not charged for system Pods" - so I guess it still fulfills the requirement "you don't incur costs when there is no activity"
https://cloud.google.com/kubernetes-engine/pricing

upvoted 1 times

? ?  HenkH 1áyear, 6ámonths ago

Functions is the only service billed against actual usage

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

172/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #64

Topic 1

You are creating an App Engine application that uses Cloud Datastore as its persistence layer. You need to retrieve several root entities for which

you have the identi ers. You want to minimize the overhead in operations performed by Cloud Datastore. What should you do?

A. Create the Key object for each Entity and run a batch get operation

B. Create the Key object for each Entity and run multiple get operations, one operation for each entity

C. Use the identi ers to create a query  lter and run a batch query operation

D. Use the identi ers to create a query  lter and run multiple query operations, one operation for each entity

Correct Answer: A

Community vote distribution

A (100%)

? ?  shashu07  Highly Voted ?  3áyears ago

Correct Answer: A
Create the Key object for each Entity and run a batch get operation
https://cloud.google.com/datastore/docs/best-practices
Use batch operations for your reads, writes, and deletes instead of single operations. Batch operations are more efficient because they
perform multiple operations with the same overhead as a single operation.
Firestore in Datastore mode supports batch versions of the operations which allow it to operate on multiple objects in a single Datastore
mode call.
Such batch calls are faster than making separate calls for each individual entity because they incur the overhead for only one service call.
If multiple entity groups are involved, the work for all the groups is performed in parallel on the server side.

upvoted 41 times

? ?  AzureDP900 8ámonths, 2áweeks ago

works fine .. A is right

upvoted 1 times

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Agree A

upvoted 7 times

? ?  vamgcp  Most Recent ?  4ámonths, 4áweeks ago

By using the "lookup by key" API of Cloud Datastore, you can minimize the overhead in operations performed by Cloud Datastore and
optimize the performance of your App Engine application.
from google.cloud import datastore

client = datastore.Client()
keys = [client.key('EntityKind', id) for id in entity_ids]
entities = client.get_multi(keys)

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

A. Create the Key object for each Entity and run a batch get operation

To minimize the overhead in operations performed by Cloud Datastore, you should use the batch get operation to retrieve multiple
entities in a single API call. To do this, you should create a Key object for each entity that you want to retrieve, then pass the Key objects to
the batch get operation. This will allow you to retrieve multiple entities in a single API call, reducing the number of operations performed
by Cloud Datastore and improving the efficiency of your application.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option B, running multiple get operations, one operation for each entity, would not be an efficient way to retrieve the entities because
it would require multiple API calls to Cloud Datastore, which would increase the overhead and decrease the efficiency of the
application.

Option C, using the identifiers to create a query filter and running a batch query operation, would not be an efficient way to retrieve the
entities because it would require performing a query operation, which is generally more expensive than a get operation.

Option D, using the identifiers to create a query filter and running multiple query operations, one operation for each entity, would not
be an efficient way to retrieve the entities because it would require performing multiple query operations, which are generally more
expensive than get operations.

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

A is ok

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

173/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is correct https://cloud.google.com/datastore/docs/best-practices#api_calls

upvoted 1 times

? ?  RitwickKumar 10ámonths, 2áweeks ago

Selected Answer: A

https://cloud.google.com/datastore/docs/concepts/entities#datastore-datastore-batch-lookup-python

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

go for A.

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

A is the right answer

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

A û create a key object for each entity, and run a batch get operations.
See Batch Operations section here: https://cloud.google.com/datastore/docs/concepts/entities
var keys = new Key[] { _keyFactory.CreateKey(1), _keyFactory.CreateKey(2) };
var tasks = _db.Lookup(keys[0], keys[1]);

1 and 2 are identifiers of the Key. Check Key / Identifier definition on the same link (top of that page)
Such batch calls are faster than making separate calls for each individual entity because they incur the overhead for only one service call.

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

A. Create the Key object for each Entity and run a batch get operation

upvoted 1 times

? ?  un 2áyears, 1ámonth ago

A is correct

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

Agree A.
Look at best practices: Ref: https://cloud.google.com/datastore/docs/best-practices#api_calls
"Use batch operations for your reads, writes, and deletes instead of single operations. Batch operations are more efficient because they
perform multiple operations with the same overhead as a single operation."
C is also good but is no longer recommended: https://cloud.google.com/appengine/docs/standard/java/datastore/queries

upvoted 2 times

? ?  Steve21 2áyears, 3ámonths ago

Ans. A like this in Java Iterator<Entity> tasks = datastore.get(datastore.get(taskKey1), datastore.get(taskKey2));
and taskKey1
upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

174/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #65

Topic 1

You need to upload  les from your on-premises environment to Cloud Storage. You want the  les to be encrypted on Cloud Storage using

customer-supplied encryption keys. What should you do?

A. Supply the encryption key in a .boto con guration  le. Use gsutil to upload the  les.

B. Supply the encryption key using gcloud con g. Use gsutil to upload the  les to that bucket.

C. Use gsutil to upload the  les, and use the  ag --encryption-key to supply the encryption key.

D. Use gsutil to create a bucket, and use the  ag --encryption-key to supply the encryption key. Use gsutil to upload the  les to that bucket.

Correct Answer: A

Community vote distribution

C (50%)

A (50%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

In GCP document, key could be configured in .boto.
I didn't find information show gsutil suppots flag "--encryption-key".

https://cloud.google.com/storage/docs/encryption/customer-supplied-keys

upvoted 36 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 14 times

? ?  nitinz 2áyears, 3ámonths ago

A is correct

upvoted 4 times

? ?  kumarp6 2áyears, 8ámonths ago

.boto file with encryption key, but it will works for individual users, every user should update their own .boto with same key. Also while
retrieving you should use the same key to decryption.

upvoted 3 times

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

I agree, A.(https://cloud.google.com/storage/docs/gsutil/addlhelp/UsingEncryptionKeys#generating-customer-supplied-encryption-keys)

upvoted 18 times

? ?  eTriber  Most Recent ?  1áweek, 1áday ago

Selected Answer: C

https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#command-line

upvoted 1 times

? ?  red_panda 1áweek, 6ádays ago

Selected Answer: C

It's C without doubt.
https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#command-line

upvoted 1 times

? ?  th3stig 2áweeks ago

Selected Answer: C

https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#upload-encrypt

gcloud storage cp SOURCE_DATA gs://BUCKET_NAME/OBJECT_NAME --encryption-key=YOUR_ENCRYPTION_KEY

upvoted 2 times

? ?  MJCLOUD 4ámonths ago

IDK why most here vote for A. But in the docs it clearly states to use the encryption-flag.
https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#upload-encrypt
Even in the boto reference (https://cloud.google.com/storage/docs/boto-gsutil) there is a reference to the above file.
It must be C.

upvoted 5 times

? ?  giovanicascaes 3ámonths, 4áweeks ago

Actually gsutil doesn't support a --encryption-key flag, it uses a .boto file. This is why A is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

175/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Michi1 2áweeks, 1áday ago

gsutil supports customer-supplied encryption keys:
https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#gcloud

upvoted 1 times

? ?  8d31d36 4ámonths, 2áweeks ago

Option A is not a valid solution, as the .boto configuration file is not used to specify the encryption key.

Option B is also not a valid solution, as gcloud config is used to set global flags for the gcloud command-line tool, and does not affect the
use of gsutil.

Option D is not necessary, as you can use an existing bucket and simply specify the encryption key when uploading the files.

Therefore, the correct answer is C: Use gsutil to upload the files, and use the flag --encryption-key to supply the encryption key. This will
encrypt the files on Cloud Storage using the customer-supplied encryption key.

upvoted 5 times

? ?  vamgcp 4ámonths, 4áweeks ago

gsutil cp encrypted_file gs://bucket_name/encrypted_file -h "x-goog-encryption-algorithm:AES256" -h "x-goog-meta-crypto-key:base64-
encoded-encryption-key"
you can use the Cloud Storage transfer service with a customer-supplied encryption key (SSE-C). This allows you to encrypt your data at
the client-side before uploading it to Cloud Storage.

upvoted 2 times

? ?  RVivek 5ámonths ago

Selected Answer: A

boto configuration sets the default encryption key
B will not work
C works. however evry upload you msust specify --encryption-key
D : gsutil mb command option to specify default ke is -key , hence --encryption-key is wrong

upvoted 1 times

? ?  richlee0423 6ámonths, 1áweek ago

C is correct
you can use customer-supplied encryption keys to upload an object with --encryption-key flag
https://cloud.google.com/storage/docs/encryption/using-customer-supplied-keys#upload-encrypt

upvoted 5 times

? ?  TonytheTiger 6ámonths, 3áweeks ago

Answer A - https://cloud.google.com/storage/docs/boto-gsutil
ò Using customer-managed or customer-supplied encryption keys

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is correct https://cloud.google.com/storage/docs/encryption/customer-supplied-keys

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

Do qwiklabs you will understand this CSEK using .boto file

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is right

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

A is the correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

176/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #66

Topic 1

Your customer wants to capture multiple GBs of aggregate real-time key performance indicators (KPIs) from their game servers running on Google

Cloud Platform and monitor the KPIs with low latency. How should they capture the KPIs?

A. Store time-series data from the game servers in Google Bigtable, and view it using Google Data Studio.

B. Output custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to view them.

C. Schedule BigQuery load jobs to ingest analytics  les uploaded to Cloud Storage every ten minutes, and visualize the results in Google Data

Studio.

D. Insert the KPIs into Cloud Datastore entities, and run ad hoc analysis and visualizations of them in Cloud Datalab.

Correct Answer: B

Reference:

https://cloud.google.com/solutions/data-lifecycle-cloud-platform

Community vote distribution

B (97%)

? ?  suryalsp  Highly Voted ?  3áyears, 6ámonths ago

Ans is B. Data studio cannot be used with BigTable
https://datastudio.google.com/datahttps://datastudio.google.com/data

upvoted 32 times

? ?  ErenYeager 7ámonths, 3áweeks ago

As of today you can

upvoted 6 times

? ?  anshumankmr80 6ámonths, 2áweeks ago

Source?

https://lookerstudio.google.com/data?search=big

upvoted 2 times

? ?  HD2023 3ámonths ago

ThatÆs BigQuery, not BigTable, no?

upvoted 1 times

? ?  Raja101 1áyear, 9ámonths ago

A is correct

upvoted 4 times

? ?  kolcsarzs  Highly Voted ?  3áyears, 6ámonths ago

correct is B

upvoted 12 times

? ?  LaxmanTiwari  Most Recent ?  1ámonth, 2áweeks ago

B is correct, you should create custom KPI in Stack Driver

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

B. Output custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to view them.

To capture multiple GBs of aggregate real-time KPIs from game servers running on Google Cloud Platform and monitor them with low
latency, the customer should output custom metrics to Stackdriver from the game servers. Stackdriver allows you to collect and store
custom metrics, as well as view and analyze them in real-time using the Stackdriver Monitoring Console. The customer can create a
Dashboard in the Monitoring Console to view the KPIs and monitor them with low latency.

upvoted 5 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, storing time-series data in Bigtable and viewing it using Data Studio, would not be suitable for capturing and monitoring
real-time KPIs with low latency. Bigtable is a scalable NoSQL database that is optimized for large-scale batch processing, and Data
Studio is a visualization tool that is not designed for real-time data analysis.

Option C, scheduling BigQuery load jobs to ingest analytics files uploaded to Cloud Storage every ten minutes and visualizing the
results in Data Studio, would not be suitable for capturing and monitoring real-time KPIs with low latency. BigQuery is a data
warehouse that is optimized for batch processing, and it is not designed for real-time data analysis.

Option D, inserting the KPIs into Cloud Datastore entities and running ad hoc analysis and visualizations of them in Cloud Datalab,

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

177/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

would not be suitable for capturing and monitoring real-time KPIs with low latency. Cloud Datastore is a NoSQL document database,
and Cloud Datalab is a data analysis and visualization tool that is not designed for real-time data analysis.

upvoted 5 times

? ?  jlambdan 3ámonths, 1áweek ago

big table is not for batch. It is used in IOT...
https://cloud.google.com/bigtable

upvoted 2 times

? ?  Raja101 7ámonths, 2áweeks ago

Selected Answer: A

A is correct

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

B is correct as Data studio does not support bigtable as a source

upvoted 4 times

? ?  zr79 8ámonths, 2áweeks ago

KPI, SLO,SLI all those work with observability which stackdriver

upvoted 2 times

? ?  jay9114 8ámonths, 2áweeks ago

The reference provided seems irrelevant to this question.

upvoted 1 times

? ?  bolington 1áyear ago

A is the correct answer, the key word here, real time and low latency.

upvoted 1 times

? ?  Nirca 1áyear, 1ámonth ago

Selected Answer: B

BigTable has no connection to data studio.
https://datastudio.google.com/data?search=Big

upvoted 6 times

? ?  BeetleJuice 1áyear, 5ámonths ago

Selected Answer: B

B, it is

upvoted 2 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I don't think there is a correct answer, but B looks correct in this.
If use Bigquery, then A is correct .
C is not for realtime.
D Datastore is for small usecase.
Keywords 'real time' ,'analytics'
https://events.withgoogle.com/solution-design-pattern-gaming/analytics-pattern/

upvoted 2 times

? ?  Wonka 1áyear, 6ámonths ago

B is okay

upvoted 1 times

? ?  gcp_learner 1áyear, 6ámonths ago

Selected Answer: B

I will go with B because you can meet the requirement with Cloud Monitoring, formerly Stackdriver monitoring

upvoted 1 times

? ?  gcp_learner 1áyear, 6ámonths ago

Selected Answer: B

You can do this with Cloud Monitoring, formerly Stackdriver Monitoring

upvoted 1 times

? ?  ABO_Doma 1áyear, 6ámonths ago

Selected Answer: B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

178/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

While Google Bigtable may be a good place for this data, there is no direct connector between Bigtable and Google Data Studio. To enable
Google Data Studio to pick up this information, we would have to use something like BigQuery
(https://cloud.google.com/bigquery/external-data-Bigtable) to query data stored in Bigtable, and Google Data Studio can then make use
of this data.

upvoted 7 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

179/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #67

Topic 1

You have a Python web application with many dependencies that requires 0.1 CPU cores and 128 MB of memory to operate in production. You

want to monitor and maximize machine utilization. You also want to reliably deploy new versions of the application. Which set of steps should you

take?

A. Perform the following: 1. Create a managed instance group with f1-micro type machines. 2. Use a startup script to clone the repository,

check out the production branch, install the dependencies, and start the Python app. 3. Restart the instances to automatically deploy new

production releases.

B. Perform the following: 1. Create a managed instance group with n1-standard-1 type machines. 2. Build a Compute Engine image from the

production branch that contains all of the dependencies and automatically starts the Python app. 3. Rebuild the Compute Engine image, and

update the instance template to deploy new production releases.

C. Perform the following: 1. Create a Google Kubernetes Engine (GKE) cluster with n1-standard-1 type machines. 2. Build a Docker image from

the production branch with all of the dependencies, and tag it with the version number. 3. Create a Kubernetes Deployment with the

imagePullPolicy set to 'IfNotPresent' in the staging namespace, and then promote it to the production namespace after testing.

D. Perform the following: 1. Create a GKE cluster with n1-standard-4 type machines. 2. Build a Docker image from the master branch with all

of the dependencies, and tag it with 'latest'. 3. Create a Kubernetes Deployment in the default namespace with the imagePullPolicy set to

'Always'. Restart the pods to automatically deploy new production releases.

Correct Answer: B

Community vote distribution

C (67%)

A (22%)

10%

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

C is correct, need "ifnotpresent"when uploads to container registry

upvoted 38 times

? ?  medi01 2ámonths, 1áweek ago

ifnotpresent won't pull new version.

upvoted 1 times

? ?  TosO  Highly Voted ?  3áyears, 7ámonths ago

C is the best choice. You can create a k8s cluster with just one node and use a different namespaces for staging and production. In
staging, you will test the changes

upvoted 21 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Agreed

upvoted 1 times

? ?  red_panda  Most Recent ?  3áweeks, 1áday ago

Selected Answer: C

C without doubts
upvoted 1 times

? ?  Rothmansua 2ádays, 1áhour ago

How about price for a 0.1 CPU app?

upvoted 1 times

? ?  LaxmanTiwari 1ámonth, 2áweeks ago

C û 1. Create GKE cluster with n1-standard-1 type machine. 2. Build a docker image from production branch with all the dependencies and
tag it with version #. 3. Create a Kubernetes Deployment with the imagePullPolicy set to ôIfNotPresentö in the staging namespace, and
then promote it to production namespace after testing. Pretty interesting questions, where all options physically work, though C
corresponds mostly to all requirements. First of all why GKE, but not GCE? Because, GKE can better utilize resources (pods autoscaling on
the same node), and also it has advanced dashboard for resource utilization. Also, GKE abstracts you from VM/OS û focus just on app.
Then C or D? C - follows Kubernetes best practices (uses normal image version #, instead of marking as "latest"), and also image is
deployed automatically by node agent (kubelet), after test is passed in staging env (Google Best practice). In D version is marked as latest ,
which would sophisticate roll-back process. In Addition with ôAlwaysö policy you need to restart pods manually to deploy new version.
upvoted 7 times
upvoted 1 times

? ?  JC0926 2ámonths, 1áweek ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

180/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Perform the following: 1. Create a Google Kubernetes Engine (GKE) cluster with n1-standard-1 type machines. 2. Build a Docker image
from the production branch with all of the dependencies, and tag it with the version number. 3. Create a Kubernetes Deployment with the
imagePullPolicy set to 'IfNotPresent' in the staging namespace, and then promote it to the production namespace after testing.

This option provides the best solution for monitoring and maximizing machine utilization and reliably deploying new versions of the
application. Using a GKE cluster allows you to take advantage of Kubernetes' features for scaling, rolling updates, and self-healing.
Additionally, building a Docker image with all dependencies ensures a consistent environment, and promoting from the staging
namespace to the production namespace after testing ensures that new releases are reliable.

upvoted 2 times

? ?  amelm 1ámonth, 2áweeks ago

ChatGPT answer ?
upvoted 1 times

? ?  LaxmanTiwari 4áweeks ago

don't rely too much on chatGPT

upvoted 1 times

? ?  Badri9898 2ámonths, 4áweeks ago

-> B This option is the best solution because it satisfies the requirements mentioned in the question. A managed instance group with n1-
standard-1 type machines is a good choice for a Python web application with many dependencies that require 0.1 CPU cores and 128 MB
of memory to operate in production. Building a Compute Engine image with all the dependencies and automatically starting the Python
app simplifies the deployment process, and allows for consistent and reliable deployment of new versions. Rebuilding the Compute
Engine image and updating the instance template to deploy new production releases ensures that the latest version is always deployed.
Additionally, monitoring can be done using Stackdriver Monitoring to maximize machine utilization.

upvoted 5 times

? ?  taer 3ámonths ago

Selected Answer: C

sing Google Kubernetes Engine (GKE) enables better resource management and allows you to monitor and maximize machine utilization
effectively. Creating a Docker image with all the dependencies ensures a consistent environment for your application. By utilizing
Kubernetes Deployments, you can reliably deploy new versions of the application and control the rollout process. Additionally, using a
staging namespace for testing before promoting to the production namespace ensures a safer deployment process.

upvoted 1 times

? ?  jlambdan 3ámonths, 1áweek ago

Selected Answer: B

From the question: You have a Python web application with many dependencies that requires 0.1 CPU cores and 128 MB of memory to
operate in production. You want to monitor and maximize machine utilization. You also want to reliably deploy new versions of the
application. Which set of steps should you take?

The requirement is:
- manage ONE application (monitor + if not healthy restart)
- being possible to reliably deploy new version
- 0.1 CPU
- 128 MB
- maximize machine utilization => consume as close as possible as the required ressources.

A is tempting due to the machine type but it is not following best practice and reliable.

C & D) GKE is overkill if the goal is to manage only one application and goes against consuming as few resources as possible. Also, all this
just to use the same machine type as the one specified in B. Why ?

Answer B is seems the most appropriate.

upvoted 6 times

? ?  jlambdan 3áweeks ago

TLDR: go for C

Hi me again. So, I missed something.
the machine n1-standard-1 has 1 cpu and 3.75 Go memory.
So if you use option B, you use n times this machine for the application replicas whereas if you go with the GKE option, you can put
multiple replicas on the same machine by having only one node in the cluster.
So the correct answer is one with GKE.
Now between C & D you choose C because of the fact that you tag each image with the version of the application therefore reliably
deploy the next version.

So go for C.

upvoted 1 times

? ?  feholen210 3ámonths, 1áweek ago

Selected Answer: C

C is the correct choice here.

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

Suprised at the % of A answers. A script to clone a repo can never be a solution in the era of Git.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

181/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: C

Based on community voting, the answer is C. @Admin can you change the answer to C ?

upvoted 1 times

? ?  8d31d36 4ámonths, 2áweeks ago

The correct answer is C:
In this option, you are using GKE to manage the deployment and scaling of your Python application. By using GKE, you can monitor and
maximize machine utilization, as well as reliably deploy new versions of the application. You can build a Docker image from the production
branch, tag it with a version number, and create a Kubernetes Deployment in a staging namespace for testing. Once the new version has
been tested and approved, you can promote it to the production namespace. This allows you to follow best practices for continuous
integration and continuous deployment (CI/CD) and ensures that the application is always running the latest version.

upvoted 2 times

? ?  medi01 2ámonths, 1áweek ago

But the mentioned pull strategy would NOT pull the image when version would change.

upvoted 1 times

? ?  roaming_panda 5ámonths, 3áweeks ago

Selected Answer: C

c as the k8 needs small capacity machine and also its less hassle to deploy new versions

upvoted 1 times

? ?  Charsoft 6ámonths ago

Selected Answer: A

The other answers are overkill. an F1 Micro has .2 cores and 588 MiB.
https://www.opsdash.com/blog/google-cloud-f1-micro.html#:~:text=Exploring%20the%20Google%20Cloud%20f1-
micro%20Instance%201%20CPU,4%20Disk%20...%205%20The%20VM%20%E2%80%9CFailover%E2%80%9D%20

upvoted 1 times

? ?  thamaster 6ámonths, 1áweek ago

Selected Answer: C

GKE allow reliable deployment, D is not good because tag with latest is not best practice so C is the answer

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Answer is C:

To monitor and maximize machine utilization, reliably deploy new versions of the application, and ensure that the application has the
required 0.1 CPU cores and 128 MB of memory to operate in production, you should perform the following steps:

Create a Google Kubernetes Engine (GKE) cluster with n1-standard-1 type machines. These machines have 1 CPU core and 3.75 GB of
memory, which should be sufficient to run the application and provide some additional resources for other processes.

Build a Docker image from the production branch with all of the dependencies, and tag it with the version number. This will allow you to
easily deploy new versions of the application by building a new image with the updated code and dependencies.

Create a Kubernetes Deployment with the imagePullPolicy set to 'IfNotPresent' in the staging namespace. This will allow you to test the
new version of the application in a staging environment before deploying it to production. After testing, you can promote the Deployment
to the production namespace to deploy the new version to production.

upvoted 7 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, creating a managed instance group with f1-micro type machines and using a startup script to clone the repository, check out
the production branch, install the dependencies, and start the Python app, would not be suitable because f1-micro type machines have
very limited resources (0.2 CPU cores and 0.6 GB of memory) and would not be able to run the application reliably.

Option B, creating a managed instance group with n1-standard-1 type machines and building a Compute Engine image from the
production branch that contains all of the dependencies and automatically starts the Python app, would not be a reliable way to deploy
new versions of the application. Rebuilding the Compute Engine image and updating the instance template would require you to stop
the instances, which would disrupt the availability of the application.

upvoted 6 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option D, creating a GKE cluster with n1-standard-4 type machines, building a Docker image from the master branch with all of the
dependencies, and creating a Kubernetes Deployment in the default namespace with the imagePullPolicy set to 'Always', would not
be a reliable way to deploy new versions of the application. Using the 'Always' imagePullPolicy would cause the Deployment to
always pull the latest version of the image, which could result in unintended deployments if the master branch contains code that is
not ready for production. It is generally recommended to use a more controlled process for deploying code to production, such as
the one outlined in option C.

upvoted 3 times

? ?  IvanDobrinov 7ámonths ago

There's no confirmation if this application is containerized - so how would you pickup GKE?
Ruling out the GKE - the answer that makes sense to me is A).

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

182/776

6/29/23, 1:51 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 6 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

183/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #68

Topic 1

Your company wants to start using Google Cloud resources but wants to retain their on-premises Active Directory domain controller for identity

management.

What should you do?

A. Use the Admin Directory API to authenticate against the Active Directory domain controller.

B. Use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and con gure SAML SSO.

C. Use Cloud Identity-Aware Proxy con gured to use the on-premises Active Directory domain controller as an identity provider.

D. Use Compute Engine to create an Active Directory (AD) domain controller that is a replica of the on-premises AD domain controller using

Google Cloud Directory Sync.

Correct Answer: B

Community vote distribution

B (100%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

According to the reference, my understanding is B is correct.
And in the document(https://cloud.google.com/iap/docs/concepts-overview), it says:
If you need to create Google Accounts for your existing users, you can use Google Cloud Directory Sync to synchronize with your Active
Directory or LDAP server.

Is it possible to explain why correct answer is C?

upvoted 43 times

? ?  AzureDP900 8ámonths, 2áweeks ago

agreed

upvoted 1 times

? ?  MikeB19 1áyear, 9ámonths ago

ItÆs simple. Domain controllers are not meant authenticate saas or web applications. This includes iam. Domain controllers speak ntlm
and Kerberos.
This why we use federation. Because web apps do not speak Kerberos or ntlm. They speak languages such oauth. Hence the need for
ad federation proxy
B is correct

upvoted 4 times

? ?  Bill831231 1áyear, 8ámonths ago

thanks for the explanation, may I ask if we go with SAML, why need sync the useraccount? seems we just need set up the federation
between cloud and on-premise

upvoted 2 times

? ?  BiddlyBdoyng 8ámonths, 4áweeks ago

"...As a prerequisite for access to GCP resources, employees must have a Google identity set up..."

upvoted 1 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 9 times

? ?  kumarp6 2áyears, 8ámonths ago

B should be correct

upvoted 5 times

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

B is the nearest answer I feel !

upvoted 25 times

? ?  LaxmanTiwari  Most Recent ?  1ámonth, 2áweeks ago

B is correct https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction

upvoted 1 times

? ?  vamgcp 4ámonths, 4áweeks ago

Connect your on-premises Active Directory to Google Cloud: You can use Google Cloud Directory Sync (GCDS) to synchronize your on-
premises Active Directory with Google Cloud. This allows you to use your existing Active Directory users and groups in Google Cloud.

Set up single sign-on (SSO): You can use Google Cloud Identity-Aware Proxy (IAP) to set up SSO for your Google Cloud resources. IAP

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

184/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

integrates with your on-premises Active Directory and allows users to log in to Google Cloud using their existing Active Directory
credentials.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

B. Use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML SSO.

To retain their on-premises Active Directory domain controller for identity management while using Google Cloud resources, the company
can use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML single sign-on
(SSO). This will allow users to use their existing Active Directory credentials to access Google Cloud resources, while still maintaining their
on-premises Active Directory domain controller as the primary source of identity management.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, using the Admin Directory API to authenticate against the Active Directory domain controller, would not be a suitable
solution because it would require implementing custom authentication logic in the application, which would be time-consuming and
error-prone.

Option C, using Cloud Identity-Aware Proxy configured to use the on-premises Active Directory domain controller as an identity
provider, would be a suitable solution, but it would not allow you to synchronize Active Directory usernames with cloud identities.

Option D, using Compute Engine to create an Active Directory (AD) domain controller that is a replica of the on-premises AD domain
controller using Google Cloud Directory Sync, would not be a suitable solution because it would require setting up and maintaining an
additional AD domain controller in Google Cloud, which would be unnecessary if the company wants to retain their on-premises AD
domain controller as the primary source of identity management.

upvoted 2 times

? ?  SureshbabuK 7ámonths ago

Selected Answer: B

GCDS and Cloud Identity is provided exactly for this use case

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

B is correct https://cloud.google.com/architecture/identity/federating-gcp-with-active-directory-introduction

upvoted 2 times

? ?  cbarg 12ámonths ago

Selected Answer: B

Answer is B

upvoted 1 times

? ?  SAMBIT 1áyear, 3ámonths ago

https://support.google.com/a/answer/106368?hl=en

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B.
Cloud Directory Sync
https://cloud.google.com/blog/products/identity-security/using-your-existing-identity-management-system-with-google-cloud-platform

upvoted 4 times

? ?  vincy2202 1áyear, 7ámonths ago

B is the correct answer

upvoted 1 times

? ?  pulkit0627 1áyear, 7ámonths ago

B as AD groups are directly mapped to Cloud Directory Sync

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û use Google Cloud Directory Sync to sync Active Directory user names with cloud identities and configure SAML SSO.
Check the flowchart here illustrating integration of your existing identity management system into GCP:
https://cloud.google.com/blog/products/identity-security/using-your-existing-identity-management-system-with-google-cloud-platform
C û does not work, since Cloud IAP serves different purpose. It s a building block toward BeyondCorp, an enterprise security model that
enables every employee to work from untrusted networks without the use of a VPN.

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

185/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  victory108 2áyears, 1ámonth ago

B. Use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML SSO.

upvoted 3 times

? ?  skvi 2áyears, 2ámonths ago

B or C ?

This link states that IAP supports integration with external identities

https://cloud.google.com/iap/docs/external-identities

So in this case do we need Directory Synch? If not then C is the right answer.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

186/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #69

Topic 1

You are running a cluster on Kubernetes Engine (GKE) to serve a web application. Users are reporting that a speci c part of the application is not

responding anymore. You notice that all pods of your deployment keep restarting after 2 seconds. The application writes logs to standard output.

You want to inspect the logs to  nd the cause of the issue. Which approach can you take?

A. Review the Stackdriver logs for each Compute Engine instance that is serving as a node in the cluster.

B. Review the Stackdriver logs for the speci c GKE container that is serving the unresponsive part of the application.

C. Connect to the cluster using gcloud credentials and connect to a container in one of the pods to read the logs.

D. Review the Serial Port logs for each Compute Engine instance that is serving as a node in the cluster.

Correct Answer: B

Community vote distribution

B (100%)

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

think answer is B. C cannot be, you don't need to connect to the container to view logs, you connect to stackdriver for this

upvoted 33 times

? ?  crypt0 3áyears, 8ámonths ago

Is Stackdriver enabled by default?
Stackdriver Logging is independent and first needs to enable with GKE I guess?

upvoted 1 times

? ?  crypt0 3áyears, 8ámonths ago

Please forget this comment ^
Answer B should be correct.

upvoted 8 times

? ?  Jack_in_Large 3áyears, 1ámonth ago

Yes for GKE

upvoted 1 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 7 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes it is B

upvoted 3 times

? ?  crypt0 3áyears, 8ámonths ago

Stackdriver Logging seems to be enabled by default for GKE.

Looking here:
https://cloud.google.com/monitoring/kubernetes-engine/legacy-stackdriver/logging
For container and system logs, GKE deploys a per-node logging agent that reads container logs, adds helpful metadata, and then
stores them. The logging agent checks for container logs in the following sources:

Standard output and standard error logs from containerized processes

I would also go with B

upvoted 9 times

? ?  AzureDP900 8ámonths, 2áweeks ago

agreed with B
upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

B, google wants you to use stackdriver.

upvoted 6 times

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

B is correct. Serial console doesnt give you StdOut

upvoted 8 times

? ?  MestreCholas  Most Recent ?  3ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

187/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: B

B. Review the Stackdriver logs for the specific GKE container that is serving the unresponsive part of the application.

Since the application writes logs to standard output, the logs should be available in the Stackdriver logs for the container running the
unresponsive part of the application. Kubernetes Engine automatically exports these logs to Stackdriver, so you can use the Stackdriver
Logging console to view the logs. Option A is not the best choice because reviewing the logs for each Compute Engine instance would be
time-consuming and may not provide the necessary information. Option C may work, but it involves extra steps and may not be necessary
if the logs are available in Stackdriver. Option D is not relevant in this case because Serial Port logs are not likely to provide useful
information for troubleshooting an unresponsive web application.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

C. Connect to the cluster using gcloud credentials and connect to a container in one of the pods to read the logs.

To inspect the logs of a Kubernetes Engine (GKE) cluster to find the cause of an issue, you can connect to the cluster using gcloud
credentials and connect to a container in one of the pods to read the logs. This will allow you to access the logs of the application as it is
running in the cluster, which should help you identify the cause of the issue.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, reviewing the Stackdriver logs for each Compute Engine instance that is serving as a node in the cluster, would not be
suitable because the application writes logs to standard output, not to Stackdriver.

Option B, reviewing the Stackdriver logs for the specific GKE container that is serving the unresponsive part of the application, would
not be suitable because the application writes logs to standard output, not to Stackdriver.

Option D, reviewing the Serial Port logs for each Compute Engine instance that is serving as a node in the cluster, would not be suitable
because the application writes logs to standard output, not to the Serial Port.

upvoted 2 times

? ?  jaxclain 7ámonths ago

Selected Answer: B

This should be easy, the answer is B.
Just eliminate the wrong answers (A) is not correct because the question is about GKE and not CE.
C and D are totally lost

upvoted 1 times

? ?  habros 7ámonths ago

Selected Answer: B

A, C, D all sounds unfeasible (credentials, and compute engine)

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

is correct answer
upvoted 1 times

? ?  Superr 1áyear ago

Selected Answer: B

correct

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

B is correct ans.I agree.
https://cloud.google.com/blog/ja/products/management-tools/finding-your-gke-logs

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

B is the correct answer

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û Review Stackdriver logs for specific GKE container that is serving the unresponsive part of the app.
This is a most directly matching answer for this Q, since it reviews GKE container logs, by that advertising this Stackdriver feature.
ôFor container and system logs, GKE deploys a per-node logging agent that reads container logs, adds helpful metadata, and then stores
them. The logging agent checks for container logs in the following sources:

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

188/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

ò Standard output and standard error logs from containerized processes
ò kubelet and container runtime logs
ò Logs for system components, such as VM startup scriptsö
Originally we thought, that D is a right answer, since were confused with 2 seconds restart. But, thatÆs restart for Pod, not for Node (GCE)
D û Review Serial Port logs for each Compute Engince instance, that is serving as the in the cluster.
Serial Port output is standard feature of Compute Engine (which retains 1 MB most recent logs for analysis). But, it is irrelevant for PodÆs
restart, caused by malfunction of some container.

upvoted 3 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 3 times

? ?  lovingsmart2000 1áyear, 11ámonths ago

B is right answer. There is a catch here - Legacy logging of GKE with Stackdriver has deprecated. If this is used, you need to migrate to
Cloud Operations for GKE, a new enhanced offering by Google with same functionality.
Future questions will have the answer choices with new tool "Cloud Operations for GKE" instead of Stackdriver.
https://cloud.google.com/monitoring/kubernetes-engine/legacy-stackdriver/logging

upvoted 2 times

? ?  lovingsmart2000 1áyear, 11ámonths ago

B is right answer. There is a catch here - Legacy logging of GKE with Stackdriver has deprecated. If this used, you need to migrate to Cloud
Operations for GKE, a new enhanced offering by Google with same functionality.
Future questions will have the answer choices with new tool "Cloud Operations for GKE" instead of Stackdriver.

upvoted 2 times

? ?  areza 2áyears ago

B is ok

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

189/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #70

Topic 1

You are using a single Cloud SQL instance to serve your application from a speci c zone. You want to introduce high availability. What should you

do?

A. Create a read replica instance in a different region

B. Create a failover replica instance in a different region

C. Create a read replica instance in the same region, but in a different zone

D. Create a failover replica instance in the same region, but in a different zone

Correct Answer: D

Community vote distribution

D (55%)

C (36%)

10%

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Agree D

upvoted 33 times

? ?  kimharsh 1áyear ago

this Question is very Old and should be deleted from the exam , there is no Failover replica now , to do an HA we just confer it for the
SQL instance that we have .

upvoted 15 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 6 times

? ?  nitinz 2áyears, 3ámonths ago

D, its regional product and failover is required for HA

upvoted 3 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes D is right

upvoted 4 times

? ?  GunjGupta  Highly Voted ?  3áyears, 1ámonth ago

Cloud SQL is regional. For high availability, we need to think fo a failover strategy. So Option D meets the requirement.
create failover replica in the same region but in different Zone

upvoted 14 times

? ?  red_panda  Most Recent ?  3áweeks, 1áday ago

Selected Answer: C

C.
Failover is so old and deprecated

upvoted 2 times

? ?  LaxmanTiwari 1ámonth, 2áweeks ago

this Question is very Old and should be deleted from the exam , there is no Failover replica now , to do an HA we just confer it for the SQL
instance that we have .. agreed tested as well

upvoted 1 times

? ?  JC0926 2ámonths, 1áweek ago

Selected Answer: D

Option C is not the best choice because it suggests creating a read replica instance, which is designed to handle read traffic and provide
better performance in read-heavy workloads, but it is not intended for high availability.

On the other hand, Option D suggests creating a failover replica instance in the same region but in a different zone. Failover replicas are
designed specifically for high availability, as they maintain an up-to-date copy of the primary instance's data. If the primary instance
becomes unresponsive or fails, Cloud SQL automatically switches to the failover replica with minimal downtime.

In summary, to introduce high availability for your Cloud SQL instance, you should create a failover replica instance in the same region but
in a different zone (Option D) rather than creating a read replica instance (Option C), which doesn't provide high availability in case of
primary instance failures.

upvoted 2 times

? ?  DevOpsi er 2áweeks, 2ádays ago

thanks!

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

190/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  JC0926 3ámonths, 2áweeks ago

Selected Answer: D

D
C is also not ideal for high availability because creating a read replica in the same region but in a different zone does not provide
automatic failover. A read replica is used for scaling reads and can improve performance, but it is not a failover mechanism.

upvoted 1 times

? ?  cert2020 3ámonths, 4áweeks ago

Agree D.
https://cloud.google.com/sql/docs/mysql/configure-ha
The legacy configuration for high availability used a failover replica instance. The new configuration does not use a failover replica.
Instead, it uses Google's regional persistent disks, which synchronously replicate data at the block level between two zones in a region.

upvoted 1 times

? ?  black_magic 4ámonths ago

The high availability configuration for Cloud SQL has recently changed. Failover replicas will no longer be included in the new Google
recommended configuration and will be considered legacy. Google is moving towards persistent regional disks. This question as well as
the solutions should be updated.

"The legacy configuration for high availability used a failover replica instance. The new configuration does not use a failover replica.
Instead, it uses Google's regional persistent disks, which synchronously replicate data at the block level between two zones in a region. If
you have an existing MySQL instance that uses the legacy high availability configuration, you can update your configuration to use the
current version."
Source: https://cloud.google.com/sql/docs/mysql/configure-ha

upvoted 2 times

? ?  MaryMei 4ámonths ago

https://cloud.google.com/sql/docs/mysql/replication
only read replicas exists

upvoted 1 times

? ?  8d31d36 4ámonths, 2áweeks ago

Option A is not a valid solution, as the .boto configuration file is not used to specify the encryption key.

Option B is also not a valid solution, as gcloud config is used to set global flags for the gcloud command-line tool, and does not affect the
use of gsutil.

Option D is not necessary, as you can use an existing bucket and simply specify the encryption key when uploading the files.

Therefore, the correct answer is C: Use gsutil to upload the files, and use the flag --encryption-key to supply the encryption key. This will
encrypt the files on Cloud Storage using the customer-supplied encryption key.

upvoted 1 times

? ?  RVivek 5ámonths ago

Selected Answer: D

Cloud SQL is regional hence A & B are wrong
C will work . Read replica has to be promted mannually , so down time will be there
D is the highly available soloution

upvoted 3 times

? ?  FI22 5ámonths, 3áweeks ago

The question need to update! As D depricated.
In a disaster recovery scenario, you can promote a replica to convert it to a primary. High availability can be achieved by serving traffic
from replicas.
D is perfect but deprecated.
C is also ok.

upvoted 1 times

? ?  RVivek 5ámonths ago

Both C and D are OK. However C is a manual process and there may be soem down time before the admin get alrted and promote read
replica

upvoted 1 times

? ?  FI22 5ámonths, 3áweeks ago

The read replica processes queries, read requests, and analytics traffic, thus reducing the load on the primary instance. So C ok!

upvoted 1 times

? ?  jay9114 6ámonths ago

Selected Answer: D

"Read Replicas CAN be promoted to master nodes in the case of DR. However, there is downtime entailed. Failover Replicas are designed
to automatically become master nodes."

reference: https://googlecloudarchitect.us/read-replica-versus-failover-replica-in-cloud-
sql/#:~:text=Read%20Replicas%20CAN%20be%20promoted,SQL%20go%20through%20the%20proxy).

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

191/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  jay9114 6ámonths ago

"The legacy process for adding high availability to MySQL instances uses a failover replica."

reference: https://cloud.google.com/sql/docs/mysql/high-availability

upvoted 1 times

? ?  Fundu80 2ámonths, 3áweeks ago

The link referenced above (https://cloud.google.com/sql/docs/mysql/high-availability) states that
"The legacy process for adding high availability to MySQL instances uses a failover replica."
It also states that:
"Cloud SQL plans to discontinue support for legacy HA instances in the future and will soon be announcing a date to do so.
Currently, legacy HA instances are still covered by the Cloud SQL SLA. We recommend you upgrade your existing legacy HA
instances to regional persistent disk HA instances and create new instances using regional persistent disk HA as soon as possible."
So IMHO the answer should be C

upvoted 1 times

? ?  thamaster 6ámonths, 1áweek ago

Selected Answer: D

Read replicas are used to read only purpose
HA need failover in same region but different zone

upvoted 2 times

? ?  sithin_nair 6ámonths, 1áweek ago

Its D.
https://cloud.google.com/sql/docs/mysql/high-availability

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

D. Create a failover replica instance in the same region, but in a different zone

To introduce high availability for a Cloud SQL instance, you can create a failover replica instance in the same region but in a different zone.
This will allow the failover replica to take over in the event of an outage or other issue with the primary instance, ensuring that your
application continues to function without interruption.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, creating a read replica instance in a different region, would not provide high availability because the read replica would not
be able to take over in the event of an issue with the primary instance.

Option B, creating a failover replica instance in a different region, would not provide high availability because the failover replica would
be too far away from the primary instance to take over in a timely manner in the event of an issue.

Option C, creating a read replica instance in the same region but in a different zone, would not provide high availability because the
read replica would not be able to take over in the event of an issue with the primary instance. A read replica is only used for read-only
queries and cannot be promoted to be the primary instance.

upvoted 2 times

? ?  gonlafer 6ámonths, 2áweeks ago

Selected Answer: C

C.
Failover replica is deprecated.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

192/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #71

Topic 1

Your company is running a stateless application on a Compute Engine instance. The application is used heavily during regular business hours and

lightly outside of business hours. Users are reporting that the application is slow during peak hours. You need to optimize the application's

performance. What should you do?

A. Create a snapshot of the existing disk. Create an instance template from the snapshot. Create an autoscaled managed instance group from

the instance template.

B. Create a snapshot of the existing disk. Create a custom image from the snapshot. Create an autoscaled managed instance group from the

custom image.

C. Create a custom image from the existing disk. Create an instance template from the custom image. Create an autoscaled managed

instance group from the instance template.

D. Create an instance template from the existing disk. Create a custom image from the instance template. Create an autoscaled managed

instance group from the custom image.

Correct Answer: C

Community vote distribution

C (100%)

? ?  sdsdfasdf4  Highly Voted ?  2áyears, 6ámonths ago

The easiest way would be to create template from --source-instance, and then create MIG, but it is not listed here, also you cannot create a
MIG from image directly, you need a template, so answer is C (image -> template -> mig).

upvoted 24 times

? ?  6721sora 10ámonths, 1áweek ago

C is correct.
To sdsdfasd4's point - Not recommended to create template from --source-instance as If the existing instance contains a static external
IP address, that address is copied into the instance template and might limit the use of the template.
Templates are best created from images or other templates. Creating the template from a running instance may require work to clean
it up before it can be used for a MIG

upvoted 6 times

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

C is the right answer

upvoted 12 times

? ?  Deb2293  Most Recent ?  3ámonths, 3áweeks ago

Selected Answer: C

Option C is the correct choice because creating a custom image from the existing disk ensures that the application environment is
consistent and does not change between instances, which can reduce variability in performance. Creating an instance template from the
custom image allows you to easily create new instances that are based on the same image, which can save time and effort. Finally,
creating an autoscaled managed instance group allows you to automatically scale the number of instances based on demand, which can
ensure that there are enough instances to handle peak traffic while minimizing costs during periods of low traffic

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

C is ok

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is right.
Create a custom image from the existing disk. Create an instance template from the custom image. Create an autoscaled managed
instance group from the instance template.

upvoted 1 times

? ?  abirroy 9ámonths, 2áweeks ago

Selected Answer: C

C is correct

upvoted 1 times

? ?  mv2000 11ámonths, 4áweeks ago

06/30/2022 Exam

upvoted 8 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

193/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  SHalgatti 1áyear, 4ámonths ago

I think Snapshot option are not correct in this scenario as to take snapshot you need to stop the VM so C looks best option

upvoted 2 times

? ?  PhuocT 1áyear, 6ámonths ago

Selected Answer: C

C is the answer
upvoted 1 times

? ?  Rajasa 1áyear, 6ámonths ago

Selected Answer: C

C is the answer
upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C.
Instance template can not be created from snapshot. Only from an image.

upvoted 3 times

? ?  vincy2202 1áyear, 7ámonths ago

C is the right answer.
https://cloud.google.com/compute/docs/instance-templates/create-instance-
templates#using_custom_or_public_images_in_your_instance_templates

upvoted 3 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

C û create a custom image from the existing disk. Create an instance template from the custom image. Create an autoscaled MIG from
instance template.
A could work if a snapshot was transformed to a custom image. Instance Template can be created only from image.

upvoted 4 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is C

upvoted 3 times

? ?  victory108 2áyears, 1ámonth ago

C. Create a custom image from the existing disk. Create an instance template from the custom image. Create an autoscaled managed
instance group from the instance template.

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is C

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

194/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #72

Topic 1

Your web application has several VM instances running within a VPC. You want to restrict communications between instances to only the paths

and ports you authorize, but you don't want to rely on static IP addresses or subnets because the app can autoscale. How should you restrict

communications?

A. Use separate VPCs to restrict tra c

B. Use  rewall rules based on network tags attached to the compute instances

C. Use Cloud DNS and only allow connections from authorized hostnames

D. Use service accounts and con gure the web application to authorize particular service accounts to have access

Correct Answer: B

Community vote distribution

B (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Agree B

upvoted 24 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes B it is

upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

B is correct

upvoted 2 times

? ?  omermahgoub  Most Recent ?  6ámonths, 1áweek ago

B. Use firewall rules based on network tags attached to the compute instances

To restrict communications between VM instances within a VPC without relying on static IP addresses or subnets, you can use firewall
rules based on network tags attached to the compute instances. This will allow you to specify which instances are allowed to communicate
with each other and on which paths and ports. You can then attach the relevant network tags to the compute instances when they are
created, allowing you to control communication between the instances without relying on static IP addresses or subnets.

upvoted 4 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, using separate VPCs to restrict traffic, would not be a suitable solution because it would not allow the instances to
communicate with each other, which is likely necessary for the functioning of the web application.

Option C, using Cloud DNS and only allowing connections from authorized hostnames, would not be a suitable solution because it
would not allow you to control communication between the instances based on their IP addresses or other characteristics.

Option D, using service accounts and configuring the web application to authorize particular service accounts to have access, would
not be a suitable solution because it would not allow you to control communication between the instances based on their IP addresses
or other characteristics.

upvoted 2 times

? ?  SureshbabuK 7ámonths ago

Selected Answer: B

Access to specific ports and protocol can be controlled only by firewall rule - Hence B is correct. D is not correct as service account is to
authenticate and Authorized a specific machine to resource or service not ports and protocols

upvoted 3 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is the best option.

upvoted 1 times

? ?  abirroy 9ámonths, 2áweeks ago

Selected Answer: B

Use firewall rules based on network tags attached to the compute instances

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

195/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  alexandercamachop 9ámonths, 3áweeks ago

The secret is "paths and ports".
Which tell us Firewall as our only option.

upvoted 3 times

? ?  medi01 2ámonths, 1áweek ago

And how does firewall restrict "paths" pretty please?

upvoted 1 times

? ?  cbarg 11ámonths, 4áweeks ago

Selected Answer: B

B Firewall rules to restrict traffic

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B.

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

B is the right answer

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û use firewall rules based on network tags attached to the compute instances
This answer avoids using IP, which are replaced by tags.

upvoted 3 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 4 times

? ?  areza 2áyears ago

B is ok

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

B. Use firewall rules based on network tags attached to the compute instances

upvoted 2 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

B is ok

upvoted 1 times

? ?  Vika 2áyears, 4ámonths ago

Agree B

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

196/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #73

Topic 1

You are using Cloud SQL as the database backend for a large CRM deployment. You want to scale as usage increases and ensure that you don't

run out of storage, maintain 75% CPU usage cores, and keep replication lag below 60 seconds. What are the correct steps to meet your

requirements?

A. 1. Enable automatic storage increase for the instance. 2. Create a Stackdriver alert when CPU usage exceeds 75%, and change the instance

type to reduce CPU usage. 3. Create a Stackdriver alert for replication lag, and shard the database to reduce replication time.

B. 1. Enable automatic storage increase for the instance. 2. Change the instance type to a 32-core machine type to keep CPU usage below

75%. 3. Create a Stackdriver alert for replication lag, and deploy memcache to reduce load on the master.

C. 1. Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create more space. 2. Deploy

memcached to reduce CPU load. 3. Change the instance type to a 32-core machine type to reduce replication lag.

D. 1. Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create more space. 2. Deploy

memcached to reduce CPU load. 3. Create a Stackdriver alert for replication lag, and change the instance type to a 32-core machine type to

reduce replication lag.

Correct Answer: A

Community vote distribution

A (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Agree with A

upvoted 23 times

? ?  AzureDP900 8ámonths, 2áweeks ago

1. Enable automatic storage increase for the instance. 2. Create a Stackdriver alert when CPU usage exceeds 75%, and change the
instance type to reduce CPU usage. 3. Create a Stackdriver alert for replication lag, and shard the database to reduce replication time.

upvoted 1 times

? ?  9xnine  Highly Voted ?  1áyear ago

Has anyone who has taken the exam recently seen any lingering questions with the Stackdriver nomenclature or is it all cloud logging,
cloud monitoring, etc.?

upvoted 12 times

? ?  red_panda  Most Recent ?  3áweeks, 1áday ago

Selected Answer: A

For me is A.

upvoted 1 times

? ?  jfricker 4ámonths, 2áweeks ago
The correct answer is D.

1. Create a Stackdriver alert when storage exceeds 75%, and increase the available storage on the instance to create more space.
2. Deploy memcached to reduce CPU load.
3. Create a Stackdriver alert for replication lag, and change the instance type to a 32-core machine type to reduce replication lag.

This approach ensures that you are able to address the three requirements specified in the question:

- Monitoring storage usage and increasing storage when it exceeds 75% to avoid running out of storage.
- Reducing CPU load by deploying memcached, which can be used to cache frequently-used data, offloading some of the load from the
database.
- Monitoring replication lag and increasing the number of cores to reduce lag.

upvoted 2 times

? ?  Charsoft 6ámonths ago

It may be A for the simple fact that all the other answers throw in a tiny detail about 32 cores. This seems like a red herring (unnecessary
details that are meant to distract), so for that reason, A is the answer.

upvoted 4 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

197/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is right

upvoted 1 times

? ?  6721sora 10ámonths, 1áweek ago

A is incorrect. because of the wording "Shard the database". How can you shard the database in Cloud SQL without causing major
disruptions? Sharding is not a core feature of RDBMS.

B should be correct. inspite of the mention of a fixed 32 core

upvoted 3 times

? ?  Deb2293 4ámonths ago

https://cloud.google.com/community/tutorials/horizontally-scale-mysql-database-backend-with-google-cloud-sql-and-
proxysql#:~:text=SQL%20and%20ProxySQL.-,Sharding,logic%20or%20a%20query%20router.

You can shard MySQL.

Answer should be A.

upvoted 3 times

? ?  jay9114 8ámonths, 2áweeks ago

Ii
Kk
Kk
You can shard cloudsql. Review this article - https://cloud.google.com/community/tutorials/horizontally-scale-mysql-database-backend-
with-google-cloud-sql-and-
proxysql#:~:text=Common%20approaches%20for%20horizontally%20scaling,with%20Cloud%20SQL%20and%20ProxySQL.

upvoted 4 times

? ?   ercedog 6ámonths, 3áweeks ago

The article only mentions sharding as a concept, and not a solution for cloudsql.

upvoted 1 times

? ?  mj20201 1áyear, 3ámonths ago

Selected Answer: A

vote for A

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

A is the correct answer

upvoted 2 times

? ?  amxexam 1áyear, 10ámonths ago

We can directly eliminate C and D we are doing some work that is already automated.

Still, I cannot make a point why not B is better than A?
I believe adding memcash will give an additional boost

Can someone help me point out why A is better than B?

upvoted 4 times

? ?  Ishu_awsguy 10ámonths, 4áweeks ago

I would say only because of the below line|
"You want to scale as usage increases" Line 1
Creating a 32 core machine upfront where we do not know what was the source machine cores would not be ideal .
in that situation i would go with A

upvoted 3 times

? ?  [Removed] 1áyear, 8ámonths ago

Just to back up what amxexam said, here is the link on automatically increasing storage based on trend analysis:

https://cloud.google.com/sql/docs/mysql/instance-settings#storage-capacity-2ndgen

upvoted 2 times

? ?  HenkH 1áyear, 6ámonths ago

That is correct - but doc only mentions auto storage increase for this specific product (cloud SQL).

upvoted 1 times

? ?  HenkH 8ámonths, 2áweeks ago

Should read MySQl

upvoted 1 times

? ?  cotam 1áyear, 8ámonths ago

I suppose B is not a better option, since it indicates 'add 32core cpu', with no info of the current usage that seems like a over-kill.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

198/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 5 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 4 times

? ?  areza 2áyears ago

A it is

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

A. 1. Enable automatic storage increase for the instance. 2. Create a Stackdriver alert when CPU usage exceeds 75%, and change the
instance type to reduce CPU usage. 3. Create a Stackdriver alert for replication lag, and shard the database to reduce replication time.

upvoted 1 times

? ?  un 2áyears, 1ámonth ago

Agree with A

upvoted 2 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

199/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #74

Topic 1

You are tasked with building an online analytical processing (OLAP) marketing analytics and reporting tool. This requires a relational database

that can operate on hundreds of terabytes of data. What is the Google-recommended tool for such applications?

A. Cloud Spanner, because it is globally distributed

B. Cloud SQL, because it is a fully managed relational database

C. Cloud Firestore, because it offers real-time synchronization across devices

D. BigQuery, because it is designed for large-scale processing of tabular data

Correct Answer: D

Reference:

https://cloud.google.com/ les/BigQueryTechnicalWP.pdf

Community vote distribution

D (93%)

7%

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Agree D

upvoted 19 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 5 times

? ?  Nastrand 2áyears, 5ámonths ago

What about the relational part? BigQuery uses SQL but it's not relational... I'm not sure its D

upvoted 4 times

? ?  lovingsmart2000 1áyear, 11ámonths ago

Pls do not confuse - Cloud SQL and BigQuery are RDBMS. Cloud Datastore, Bigtable are NoSQL.
Right answer is D - BQ

upvoted 11 times

? ?  ri errick 2áyears ago
BigQuery is relational!

upvoted 4 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes it is D

upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

D, OLAP=BQ

upvoted 3 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Well Said

upvoted 1 times

? ?  JasonL_GCP 1áyear, 8ámonths ago

The question asks "This requires a relational database that can operate on hundreds of terabytes of data", but bq doesn't meet this
condition?

upvoted 2 times

? ?  elaineshi 1áyear ago

BigQuery supports relational and query of join tables.

upvoted 2 times

? ?  gfhbox0083  Highly Voted ?  3áyears ago

D, for sure.
BigQuery for OLAP
Google CloudáSpanner for OLTP.

upvoted 14 times

? ?  Ashish1995  Most Recent ?  3ámonths, 2áweeks ago

Selected Answer: D

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

200/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D is obvious

upvoted 1 times

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: D

D is the right one
upvoted 1 times

? ?  SudhirAhirkar 6ámonths, 2áweeks ago

Cloud SQL/Spanner is OLTP DB but not OLAP. BQ is a well-known OLAP for analytics and also supports RBMS feature too... so I would got
with D

upvoted 1 times

? ?  AniketD 7ámonths, 1áweek ago

Selected Answer: D

D is correct. BigQuery is relational. Cloud SQL is not OLAP; moreover it can not store/process hundreds of TB of data. Max size is 64 TB
only.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  SerGCP 8ámonths ago

Selected Answer: D

https://cloud.google.com/products/databases.

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

The words you need to focus "You are tasked with building an online analytical processing (OLAP) marketing analytics and reporting tool"
which is BigQuery

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Big Query for large analytics , D is right

upvoted 1 times

? ?  Andre777 9ámonths, 1áweek ago

Selected Answer: D

The keyword in this context is OLAP. CloudSQL is Relational SQL for OLTP. Capacity wise, BQ supports for PB+ while CloudSQL only have
max capacity of up to ~10TB. Again the questions specifically mention "hundreds of TB of data". So D is the answer.

upvoted 2 times

? ?  deepdowndave 9ámonths, 2áweeks ago

Why is it not CloudSQL? It supports TB data storage and the question is about a relational database, not a data warehouse such as
BigQuery.

upvoted 1 times

? ?  Andre777 9ámonths, 1áweek ago

The keyword in this context is OLAP. CloudSQL is Relational SQL for OLTP. Capacity wise, BQ supports for PB+ while CloudSQL only have
max capacity of up to ~10TB. Again the questions specifically mention "hundreds of TB of data". So D is the answer.

upvoted 1 times

? ?  alexandercamachop 9ámonths, 3áweeks ago

The answer is Big Query, D
Secret: Analytical, Hundreds of TBTs. Relational.
All of this are strictly meet by Big Query, if it had not said Analytical but rather, other keywords like High Availability then Cloud Spanner.

upvoted 1 times

? ?  Thornadoo 11ámonths, 1áweek ago

Selected Answer: D

Guys, this is easy:
OLTP - Cloud Spanner & Cloud SQL
OLAP - Big Query
NoSQL - Filestore and Big Table

So answer is D.
upvoted 1 times

? ?  SamT1 1áyear, 2ámonths ago

Selected Answer: D

Cloud Spanner is used for OLTP and the question is about OLAP, hence bigquery is best suited.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

201/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Laufente 1áyear, 4ámonths ago

Selected Answer: D

D, OLAP is BigQuery

upvoted 3 times

? ?  ZackW 1áyear, 4ámonths ago

Selected Answer: D

dude that voted is wrong lol.
Ans is D as all others have said.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

202/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #75

Topic 1

You have deployed an application to Google Kubernetes Engine (GKE), and are using the Cloud SQL proxy container to make the Cloud SQL

database available to the services running on Kubernetes. You are noti ed that the application is reporting database connection issues. Your

company policies require a post- mortem. What should you do?

A. Use gcloud sql instances restart.

B. Validate that the Service Account used by the Cloud SQL proxy container still has the Cloud Build Editor role.

C. In the GCP Console, navigate to Stackdriver Logging. Consult logs for (GKE) and Cloud SQL.

D. In the GCP Console, navigate to Cloud SQL. Restore the latest backup. Use kubectl to restart all pods.

Correct Answer: C

Community vote distribution

C (100%)

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

post mortem always includes log analysis, answer is C

upvoted 58 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Thanks for the info

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago
C is right for Root Cause Analysis.

upvoted 1 times

? ?  AWS56 3áyears, 5ámonths ago

AGREE C

upvoted 3 times

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is C

upvoted 5 times

? ?  ale_brd_  Most Recent ?  7ámonths, 2áweeks ago

Stackdriver is deprecated, now you must navigate to Cloud Logging.

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is the right answer

upvoted 1 times

? ?  Jay_Krish 9ámonths, 3áweeks ago

Selected Answer: C

Logical answer is C. But is Stackdriver Logging enabled by default? Appreciate if someone could answer this?

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C

upvoted 1 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: C

post mortem = logs

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

C is the correct answer

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

203/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

C û in GCP Console navigate to Stackdriver Logging. Consult logs for Kubernetes Engine and Cloud SQL.
A/D û is an immediate attempt to fix an issue. No analysis.
B û is irrelevant at all. Cloud SQL proxy should not build anything in production.

upvoted 4 times

? ?  lovingsmart2000 1áyear, 11ámonths ago

Answer is C. I request all here - not to blindly follow the answers published at coursera or udemy as most of them are copy-pasted answer
and are not real. Examtopis provides the more accurate answers and also support with comments

upvoted 4 times

? ?  lovingsmart2000 1áyear, 11ámonths ago

Answer is B. I request all here - not to blindly follow the answers published at coursera or udemy as most of them are copy-pasted answer
and are not real. Examtopis provides the more accurate answers and also support with comments

upvoted 2 times

? ?  ashish_t 1áyear, 8ámonths ago

Why Service Account needs Cloud Build Editor role for accessing Cloud SQL?
The role is misleading/wrong, so B is wrong.

upvoted 4 times

? ?  victory108 2áyears, 1ámonth ago

C. In the GCP Console, navigate to Stackdriver Logging. Consult logs for Kubernetes Engine and Cloud SQL.

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

C is correct

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is C

upvoted 2 times

? ?  lynx256 2áyears, 3ámonths ago

IMO - C is ok.
upvoted 1 times

? ?  getzsagar 2áyears, 2ámonths ago

what is IMO ?
upvoted 1 times

? ?  tzKhalil 2áyears, 2ámonths ago

In My Opinion
upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

204/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #76

Topic 1

Your company pushes batches of sensitive transaction data from its application server VMs to Cloud Pub/Sub for processing and storage. What is

the Google- recommended way for your application to authenticate to the required Google Cloud services?

A. Ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.

B. Ensure that VM service accounts do not have access to Cloud Pub/Sub, and use VM access scopes to grant the appropriate Cloud Pub/Sub

IAM roles.

C. Generate an OAuth2 access token for accessing Cloud Pub/Sub, encrypt it, and store it in Cloud Storage for access from each VM.

D. Create a gateway to Cloud Pub/Sub using a Cloud Function, and grant the Cloud Function service account the appropriate Cloud Pub/Sub

IAM roles.

Correct Answer: A

Community vote distribution

A (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Agree A

upvoted 25 times

? ?  nitinz 2áyears, 3ámonths ago

A is correct

upvoted 2 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes A it is

upvoted 2 times

? ?  JustJack21  Highly Voted ?  1áyear, 9ámonths ago

It's because of questions like these that I do not feel guilty about using question banks :D In what world would you accept value
requirements like this from your user? Wouldn't you ask "Do you want to just authenticate? or the data to be encrypted on its way to
pub/sub?"
I'll ignore the first part of the question and assume all data is sensitive, and focus on "What is the Google- recommended way for your
application to authenticate to the required Google Cloud services?" -- The answer then is A.

Use encryption and defense-in-depth for the first part.

upvoted 8 times

? ?  AMEJack 8ámonths, 3áweeks ago
Service accounts use keys

upvoted 1 times

? ?  red_panda  Most Recent ?  3áweeks, 1áday ago

Selected Answer: A

A is correct for me. It's batch, so no cloud function

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

A. Ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.

The Google-recommended way for your application to authenticate to Cloud Pub/Sub and other Google Cloud services when running on
Compute Engine VMs is to use VM service accounts. VM service accounts are automatically created when you create a Compute Engine
VM, and they are associated with the VM instance. To authenticate to Cloud Pub/Sub and other Google Cloud services, you should ensure
that the VM service accounts are granted the appropriate IAM roles.

upvoted 4 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option B, ensuring that VM service accounts do not have access to Cloud Pub/Sub and using VM access scopes to grant the appropriate
Cloud Pub/Sub IAM roles, would not be a suitable solution because VM service accounts are required for authentication to Google
Cloud services.

Option C, generating an OAuth2 access token for accessing Cloud Pub/Sub, encrypting it, and storing it in Cloud Storage for access
from each VM, would not be a suitable solution because it would require manual management of access tokens, which can be error-
prone and insecure.

Option D, creating a gateway to Cloud Pub/Sub using a Cloud Function and granting the Cloud Function service account the
appropriate Cloud Pub/Sub IAM roles, would not be a suitable solution because it would not allow the application to directly
authenticate to Cloud Pub/Sub.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

205/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Great way of explanation..By removing/elimination approach

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: A

https://cloud.google.com/iam/docs/understanding-service-accounts

upvoted 1 times

? ?  Pazzooo 1áyear, 4ámonths ago

Selected Answer: A

The combination of Roles assigned to Service accounts granted to VMs is the way to go. :)

upvoted 2 times

? ?  elenamatay 1áyear, 5ámonths ago

Service accounts are recommended for almost all cases in Pub/Sub (see https://cloud.google.com/pubsub/docs/authentication#service-
accounts)

upvoted 3 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A.

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

A is the correct answer

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

A û ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.
Check Migrating Data to GCP section of this page:
https://cloud.google.com/iam/docs/understanding-service-accounts
You will create a service account key and use it from an external process to call Cloud Platform APIs.

upvoted 3 times

? ?  Bakili 1áyear, 8ámonths ago

A is very correct
upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

A. Ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.

upvoted 3 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Agreed with A
upvoted 1 times

? ?  un 2áyears, 1ámonth ago

A is correct

upvoted 1 times

? ?  kartikjena31 2áyears, 2ámonths ago

Ans. A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

206/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #77

Topic 1

You want to establish a Compute Engine application in a single VPC across two regions. The application must communicate over VPN to an on-

premises network.

How should you deploy the VPN?

A. Use VPC Network Peering between the VPC and the on-premises network.

B. Expose the VPC to the on-premises network using IAM and VPC Sharing.

C. Create a global Cloud VPN Gateway with VPN tunnels from each region to the on-premises peer gateway.

D. Deploy Cloud VPN Gateway in each region. Ensure that each region has at least one VPN tunnel to the on-premises peer gateway.

Correct Answer: D

Community vote distribution

D (100%)

? ?  Googler2  Highly Voted ?  3áyears, 2ámonths ago

It can't be -A - VPC Network Peering only allows private RFC 1918 connectivity across two Virtual Private Cloud (VPC) networks. In this
example is one VPC with on-premise network
https://cloud.google.com/vpc/docs/vpc-peering

It is not definitely - B - Can't be

It is not C - Because Cloud VPN gateways and tunnels are regional objects, not global

So, it the answer is D -
https://cloud.google.com/vpn/docs/how-to/creating-static-vpns

upvoted 35 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Agreed with D.
upvoted 1 times

? ?  amxexam 1áyear, 9ámonths ago

Why not A?
https://cloud.google.com/vpc/docs/vpc-peering#benefits_of_exchanging_custom_routes
The second use case is exactly what is in the question.

Don't get the argument about RFC 1918.
Will go with A
upvoted 1 times

? ?  ochanz 1áyear, 6ámonths ago

https://cloud.google.com/vpc/docs/vpc-peering allows internal IP address connectivity across two VPC so A is not the answer as the
on premise network need to use public IP. cmiiw

upvoted 2 times

? ?  TaherShaker  Highly Voted ?  2áyears, 7ámonths ago

Just Passed my exam and I answered (D) for this question

upvoted 16 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

IS the Exam Idea questions enough dude, for passing this exam?

upvoted 2 times

? ?  M_Asep 1áyear, 6ámonths ago
sound promising dude

upvoted 1 times

? ?  LaxmanTiwari  Most Recent ?  1ámonth, 2áweeks ago

It can't be -A - VPC Network Peering only allows private RFC 1918 connectivity across two Virtual Private Cloud (VPC) networks. In this
example is one VPC with on-premise network https://cloud.google.com/vpc/docs/vpc-peering It is not definitely - B - Can't be It is not C -
Because Cloud VPN gateways and tunnels are regional objects, not global So, it the answer is D - https://cloud.google.com/vpn/docs/how-
to/creating-static-vpn

upvoted 1 times

? ?  vvkds 5ámonths, 2áweeks ago

Selected Answer: D

D looks fine.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

207/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  oms_muc 6ámonths, 1áweek ago

Selected Answer: D

As HA isn't required, why do we need two VPN gateways?

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

D is the correct answer, in order to do A you will need VPN., or interconnect

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

there is two VPN:
1. classic VPN
2. HA VPN

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: D

Cloud VPN Gateway is a regional service, not global.

upvoted 4 times

? ?  elaineshi 1áyear ago

Why not C? services across regions can communicate to each other, VPN only connects to the closet region, and all the VPC shall be
connected if firewall's set.

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for D.
Cloud VPN Gateway is regional. NOt Global
gcloud compute vpn-gateways create GW_NAME \
--network=NETWORK \
--region=REGION
upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

D is the correct answer

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 3 times

? ?  MaxNRG 1áyear, 8ámonths ago

D û Deploy Cloud VPN Gateway in each region. Ensure that each region has at least one VPN tunnel to on-prem peer gateway.
C û could be an option though there is no such concept at global Cloud VPN gateway. In fact, GCP has HA and Classic VPN topologies:
https://cloud.google.com/network-connectivity/docs/how-to/choose-product
In both cases, Cloud VPN gateway is deployed to single region.

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is D

upvoted 3 times

? ?  victory108 2áyears, 1ámonth ago

D. Deploy Cloud VPN Gateway in each region. Ensure that each region has at least one VPN tunnel to the on-premises peer gateway.

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

D is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

208/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #78

Topic 1

Your applications will be writing their logs to BigQuery for analysis. Each application should have its own table. Any logs older than 45 days

should be removed.

You want to optimize storage and follow Google-recommended practices. What should you do?

A. Con gure the expiration time for your tables at 45 days

B. Make the tables time-partitioned, and con gure the partition expiration at 45 days

C. Rely on BigQuery's default behavior to prune application logs older than 45 days

D. Create a script that uses the BigQuery command line tool (bq) to remove records older than 45 days

Correct Answer: B

Community vote distribution

B (100%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

Could you please help clarify? I think B is correct.
It looks like table will be deleted with option A.
https://cloud.google.com/bigquery/docs/managing-tables#updating_a_tables_expiration_time
When you delete a table, any data in the table is also deleted. To automatically delete tables after a specified period of time, set
theádefault table expirationáfor the dataset or set the expiration time when youácreate the table.

upvoted 36 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Agreed and going with B

upvoted 1 times

? ?  kumarp6 2áyears, 8ámonths ago

it is B, if you use option A, on 46th day there is no table/content in table for application :)

upvoted 10 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 8 times

? ?  nitinz 2áyears, 3ámonths ago

B partition table
upvoted 4 times

? ?  aviv  Highly Voted ?  3áyears, 6ámonths ago

Agreed with B.
upvoted 10 times

? ?  FaizAhmed  Most Recent ?  5ádays, 23áhours ago

Selected Answer: B

B is correct

upvoted 1 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

B seems correct as this will partitioning will create a filter criteria on the basis of which specified actions on logs will be taken

upvoted 1 times

? ?  examch 6ámonths ago

Selected Answer: B

B is the correct answer,

If your tables are partitioned by date, the dataset's default table expiration applies to the individual partitions. You can also control
partition expiration using the time_partitioning_expiration flag in the bq command-line tool or the expirationMs configuration setting in
the API. When a partition expires, data in the partition is deleted but the partitioned table is not dropped even if the table is empty.

https://cloud.google.com/bigquery/docs/best-practices-storage

upvoted 3 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

209/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  MarcoEscanor 8ámonths ago

Selected Answer: B

B - You can control partition expiration using the time_partitioning_expiration flag in the bq command-line
https://cloud.google.com/bigquery/docs/best-practices-storage

upvoted 1 times

? ?  AhmedH7793 9ámonths, 2áweeks ago

Selected Answer: B

B is okay

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

Using Table-Partitions.

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Using Table-Partitions.

upvoted 1 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: B

I got similar question on my exam.

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B.
https://cloud.google.com/bigquery/docs/creating-partitioned-tables#sql
CREATE TABLE
mydataset.newtable (transaction_id INT64, transaction_date DATE)
PARTITION BY
transaction_date
OPTIONS(
partition_expiration_days=3,
require_partition_filter=true
)

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

Selected Answer: B

B is the correct answer

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û Make the tables time-partitioned and configure the partition expiration at 45 days.
A û if you use table expiration time, then it will remove the whole table after 45 days.
D û requires extra work and is not automatic.

upvoted 2 times

? ?  Unfaithful 1áyear, 11ámonths ago

Answer: B
Support: https://cloud.google.com/bigquery/docs/best-practices-storage

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

B. Make the tables time-partitioned, and configure the partition expiration at 45 days

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

210/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #79

Topic 1

You want your Google Kubernetes Engine cluster to automatically add or remove nodes based on CPU load.

What should you do?

A. Con gure a HorizontalPodAutoscaler with a target CPU usage. Enable the Cluster Autoscaler from the GCP Console.

B. Con gure a HorizontalPodAutoscaler with a target CPU usage. Enable autoscaling on the managed instance group for the cluster using the

gcloud command.

C. Create a deployment and set the maxUnavailable and maxSurge properties. Enable the Cluster Autoscaler using the gcloud command.

D. Create a deployment and set the maxUnavailable and maxSurge properties. Enable autoscaling on the cluster managed instance group from

the GCP Console.

Correct Answer: A

Community vote distribution

A (100%)

? ?  Unfaithful  Highly Voted ?  1áyear, 11ámonths ago

Answer: A
Support:
How does Horizontal Pod Autoscaler work with Cluster Autoscaler?

Horizontal Pod Autoscaler changes the deployment's or replicaset's number of replicas based on the current CPU load. If the load
increases, HPA will create new replicas, for which there may or may not be enough space in the cluster. If there are not enough resources,
CA will try to bring up some nodes, so that the HPA-created pods have a place to run. If the load decreases, HPA will stop some of the
replicas. As a result, some nodes may become underutilized or completely empty, and then CA will terminate such unneeded nodes.

upvoted 46 times

? ?  LaxmanTiwari 1ámonth, 2áweeks ago

Nice and detailed explanation. I agree with A.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Nice and detailed explanation. I agree with A.

upvoted 1 times

? ?  Rajasa 1áyear, 6ámonths ago

Good Explaination

upvoted 3 times

? ?  natpilot  Highly Voted ?  3áyears, 5ámonths ago

i'm for A, but the question in ambiguous, because requires the autoscale of nodes (not pod) when the cpu overload, but in answer use k8s
pod autoscaler based on cpu load ( cpu load for pod, not nodes ). strange

upvoted 23 times

? ?  p4 2áyears, 7ámonths ago

Agreed, the question is not about pods, but answers are also talking about pods (not only)
A is correct because B is wrong according to
https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler

"Caution: Do not enable Compute Engine autoscaling for managed instance groups for your cluster nodes. GKE's cluster autoscaler is
separate from Compute Engine autoscaling"

upvoted 16 times

? ?  skywalker 3áyears, 1ámonth ago

Confuse with the question like you mentioned. Autoscale is via nodes not pod.. and can only be configure using gcloud command.

upvoted 6 times

? ?  LaxmanTiwari  Most Recent ?  1ámonth, 2áweeks ago
Nice and detailed explanation. I agree with A.

upvoted 1 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

A seems correct. y to create managed instance groups unnecessarily?

upvoted 1 times

? ?  Deb2293 4ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

211/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

The answer is A.
More nodes mean it's horizontal scaling (increase VMs means vertical scaling of infrastructure). Cluster AutoScalar is used for increasing
number of nodes.

upvoted 1 times

? ?  examch 6ámonths ago

Selected Answer: A

A is the Correct answer, Horizontal Pod Autoscaler and Cluster Autoscaler can be used together to provision new pods and new nodes as
per the CPU utilization.

https://www.youtube.com/watch?v=VNAWA6NkoBs

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  Rajeev26 8ámonths, 3áweeks ago

Selected Answer: A

MIG not for GKE as option B and C, D are not relevant to question

upvoted 1 times

? ?  abirroy 9ámonths, 2áweeks ago

Selected Answer: A

Configure a HorizontalPodAutoscaler with a target CPU usage. Enable the Cluster Autoscaler from the GCP Console.

upvoted 1 times

? ?  gee1979 9ámonths, 3áweeks ago

Selected Answer: A

A...

The HPA and CA complement each other for truly efficient scaling. If the load increases, HPA will create new replicas. If there isnÆt enough
space for these replicas, CA will provision some nodes, so that the HPA-created pods have a place to run.

The Horizontal Pod Autoscaler changes the shape of your Kubernetes workload by automatically increasing or decreasing the number of
Pods in response to the workload's CPU or memory consumption, or in response to custom metrics reported from within Kubernetes or
external metrics from sources outside of your cluster.

upvoted 1 times

? ?  6721sora 10ámonths, 1áweek ago

A is wrong.
Pod scaling only spins up additional pods. Not nodes.
Cluster Autoscaler does adding of nodes automatically.
I am surprised that so many people think that A is the correct answer.

Correct answer per me is C

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: A

Horizontal Pod Autoscaler changes the deployment's or replicaset's number of replicas based on the current CPU load. If the load
increases, HPA will create new replicas, for which there may or may not be enough space in the cluster. If there are not enough resources,
CA will try to bring up some nodes, so that the HPA-created pods have a place to run. If the load decreases, HPA will stop some of the
replicas. As a result, some nodes may become underutilized or completely empty, and then CA will terminate such unneeded nodes.

upvoted 1 times

? ?  [Removed] 1áyear, 4ámonths ago

I got one question on my exam which showed autoscaling configuration and was asked to select correct configuration.

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I agree A is correct.
I found quicklab.
Understanding and Combining GKE Autoscaling Strategies.

upvoted 1 times

? ?  ehgm 1áyear, 6ámonths ago

Selected Answer: A

B and D: You must never change the GKE managed instance group.
C and D: maxUnavailable and maxSurge are used for rolling update
A. It is the correct.

upvoted 5 times

? ?  haroldbenites 1áyear, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

212/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Go for A

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

Create Horizontal Autoscaler (min,max for pods):
kubectl autoscale deployment my-app --max 6 --min 4 --cpu-percent 50
Autoscaling cluster:
gcloud container clusters create example-cluster \
--zone us-central1-a \
--node-locations us-central1-a,us-central1-b,us-central1-f \
--num-nodes 2 --enable-autoscaling --min-nodes 1 --max-nodes 4
Check scaling an application and Horizontal Pod Autoscaler:
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
Manual Cluster Resizing: https://cloud.google.com/kubernetes-engine/docs/how-to/resizing-a-cluster
https://cloud.google.com/kubernetes-engine/docs/how-to/scaling-apps

upvoted 5 times

? ?  MaxNRG 1áyear, 8ámonths ago

Correct answer A.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

213/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #80

Topic 1

You need to develop procedures to verify resilience of disaster recovery for remote recovery using GCP. Your production environment is hosted on-

premises. You need to establish a secure, redundant connection between your on-premises network and the GCP network.

What should you do?

A. Verify that Dedicated Interconnect can replicate  les to GCP. Verify that direct peering can establish a secure connection between your

networks if Dedicated Interconnect fails.

B. Verify that Dedicated Interconnect can replicate  les to GCP. Verify that Cloud VPN can establish a secure connection between your

networks if Dedicated Interconnect fails.

C. Verify that the Transfer Appliance can replicate  les to GCP. Verify that direct peering can establish a secure connection between your

networks if the Transfer Appliance fails.

D. Verify that the Transfer Appliance can replicate  les to GCP. Verify that Cloud VPN can establish a secure connection between your

networks if the Transfer Appliance fails.

Correct Answer: B

Community vote distribution

B (100%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

I think B is correct answer.

upvoted 44 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 8 times

? ?  kumarp6 2áyears, 8ámonths ago

Its quite a fun to use Transfer Appliance for DR, I think answer is B

upvoted 6 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Actually, how ca this be given as a option even?

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

only B works

upvoted 1 times

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

Agree B is correct. Transfer appliance is a physical appliance for transferring huge bulk of data. does not fit into disaster recovery testing.
out of A and B, B seems to be more nearest answer. One would not have direct peering and Dedicated interconnect in a solution

upvoted 26 times

? ?  omermahgoub  Most Recent ?  6ámonths, 1áweek ago

The correct answer is B. Verify that Dedicated Interconnect can replicate files to GCP. Verify that Cloud VPN can establish a secure
connection between your networks if Dedicated Interconnect fails.

Dedicated Interconnect is a connection that provides a private, dedicated connection between your on-premises network and GCP over a
Google-owned network. It is a secure and reliable option for connecting your on-premises network to GCP. You can use it to replicate files
to GCP as a part of your disaster recovery plan.

If Dedicated Interconnect fails for any reason, it is a good idea to have a backup solution in place to establish a secure connection
between your networks. Cloud VPN is a secure and reliable solution for establishing a connection between your on-premises network and
GCP. It uses a virtual private network (VPN) tunnel to securely connect the networks, and it is a good backup option if Dedicated
Interconnect fails.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

The Transfer Appliance is a physical storage device that you can use to transfer large amounts of data from your on-premises storage
to GCP. It is not a connection option and cannot be used to establish a secure connection between your on-premises network and GCP.
Therefore, the options C and D are not correct.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

214/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B is ok

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

For DR with Google Cloud and on-prem use Dedicated Interconnect with HA VPN

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right without any second thought. Question is straight forward.

upvoted 1 times

? ?  Rajeev26 8ámonths, 3áweeks ago

Selected Answer: B

Transfer appliance you need to carry to GCP center like water bottle :)

upvoted 1 times

? ?  abirroy 9ámonths, 2áweeks ago

Selected Answer: B

Verify that Dedicated Interconnect can replicate files to GCP. Verify that direct peering can establish a secure connection between your
networks if Dedicated Interconnect fails.

upvoted 1 times

? ?  alexandercamachop 9ámonths, 3áweeks ago

It is definitely B
1. Interconnect is the first option so that is right.
2. Eliminates A, since Direct Peering is not supported in GCP, the option is Google Cloud VPN connection to onpremises site.

upvoted 3 times

? ?  BeCalm 3ámonths, 3áweeks ago

GCP supports direct peering in 100 locations

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

Transfer appliance is a physical appliance for transferring huge bulk of data. does not fit into disaster recovery testing

upvoted 1 times

? ?  Matalf 11ámonths, 1áweek ago

Selected Answer: B

only B have redundacy

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B.
Only when u need connect con G.Suite applications you must use Peering.

upvoted 4 times

? ?  vincy2202 1áyear, 7ámonths ago

B is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  [Removed] 1áyear, 8ámonths ago

B is correct. Dedicated Interconnect with option of Cloud VPN for redundancy

upvoted 3 times

? ?  MaxNRG 1áyear, 8ámonths ago

B.
Cloud VPN provides secure IPSec connection, though Direct Peering doesnÆt. Also, check selection diagram ôWhat GCP connection is right
for you?ö on Hybrid Connectivity page. https://cloud.google.com/hybrid-connectivity/
It explicitly points that Cloud VPN and Dedicated Interconnect are for extension of you Data Center to Cloud (== of private compute
resources). And Direct Peering for accessing GSuite (full set of GCP resources).
Direct Peering: https://cloud.google.com/network-connectivity/docs/direct-peering
Cloud VPN: https://cloud.google.com/network-connectivity/docs/vpn/concepts/overview
Choose Inteconnect Type: https://cloud.google.com/network-connectivity/docs/how-to/choose-product#cloud-interconnect only suggests
Dedicted/Partner and Cloud VPN.
This Disaster Recovery scenario is described here, in section ôTransferring data to and from GCPö:
https://cloud.google.com/architecture/dr-scenarios-building-blocks#transferring_data_to_and_from

upvoted 5 times

? ?  Unfaithful 1áyear, 11ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

215/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Answer: B
Support: Dedicated Interconnect with VPN is a better solution. If a dedicated connection is possible why anyone will use Direct Peering.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

216/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #81

Topic 1

Your company operates nationally and plans to use GCP for multiple batch workloads, including some that are not time-critical. You also need to

use GCP services that are HIPAA-certi ed and manage service costs.

How should you design to meet Google best practices?

A. Provision preemptible VMs to reduce cost. Discontinue use of all GCP services and APIs that are not HIPAA-compliant.

B. Provision preemptible VMs to reduce cost. Disable and then discontinue use of all GCP services and APIs that are not HIPAA-compliant.

C. Provision standard VMs in the same region to reduce cost. Discontinue use of all GCP services and APIs that are not HIPAA-compliant.

D. Provision standard VMs to the same region to reduce cost. Disable and then discontinue use of all GCP services and APIs that are not

HIPAA-compliant.

Correct Answer: B

Community vote distribution

B (100%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

Disabling and then discontinuing allows you to see the effects of not using the APIs, so you can gauge (check) alternatives. So that leaves
B and D as viable answers. The question says only some are not time-critical which implies others are... this means preemptible VMs are
good because they will secure a spot for scaling when needed. So I'm also going to choose B.

upvoted 36 times

? ?  Sur_Nikki 1ámonth, 2áweeks ago

Correctly explained

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

correct

upvoted 1 times

? ?  Musk 2áyears, 11ámonths ago

If others are time-critical, preemtible does not fit. Answer is D.

upvoted 8 times

? ?  Darahaas 2áyears, 9ámonths ago

And the others are not spoken about. By taking the question just by the context that it sets, preemptible is what I choose. So it's B
according to me.
upvoted 3 times

? ?  army234 2áyears, 2ámonths ago

No mention of others in the question. In an exam it's important to not being in individual assumptions and focus on the information
in question. Key word here is "not time-critical"

upvoted 6 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Ver well said..."In an exam it's important to not being in individual assumptions and focus on the information in question. Key
word here is "not time-critical""

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

agree otherwise answer goes to non-preemtible VM's

upvoted 2 times

? ?  Karna  Highly Voted ?  2áyears, 11ámonths ago

They say that some (not all) of the Batch workloads are not time critical which implies that there are time critical Batch workloads for
which Preemptible VMs are not appropriate, so going with D as the answer

upvoted 16 times

? ?  [Removed] 2áyears, 10ámonths ago

I dont think it means use premtible vms for everything. It says to use preemtible vms to reduce cost

upvoted 8 times

? ?  Sur_Nikki  Most Recent ?  1ámonth, 3áweeks ago

B looks good to me

upvoted 1 times

? ?  8d31d36 4ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

217/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

To design a GCP solution that meets Google's best practices for operating nationally with multiple batch workloads, including some that
are not time-critical, and using HIPAA-compliant services while managing service costs, you should provision standard VMs in the same
region to reduce cost, and use GCP services that are HIPAA-compliant as needed. Therefore, the correct option is C.

Preemptible VMs can provide cost savings, but they are not recommended for workloads that are not time-critical, as they may be
interrupted at any time. Provisioning standard VMs in the same region will provide better performance and stability, and can still be cost-
effective by using features such as sustained-use discounts and committed use discounts.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

Assumption here is that cost is more important that the time critical batches, therefore use preemptible instances. Disable and
discontinue is a better option as it gives you opportunity to see the impact before blasting any APIs or services that are not certified.

upvoted 3 times

? ?  Prashant2022 8ámonths, 4áweeks ago

ans is B - HAHAHA

upvoted 2 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

https://cloud.google.com/security/compliance/hipaa#unique_features

upvoted 2 times

? ?  backhand 11ámonths, 1áweek ago

vote B
it's obviously, keywords "multiple batch workloads" and "not time-critical", preemptible vm first choice.

upvoted 1 times

? ?  amonzo 1áyear ago

I think answer is D cos for using preemtible instances, you need to guarantee batch jobs are able to continue without losing data or
causing an issue when vm restarts. There is no that guarantee in the question.

upvoted 1 times

? ?  kimharsh 1áyear, 2ámonths ago

Selected Answer: B

It took me a while to finally decided the answer should be B, for the following reasons :
- in the question it said "multiple batch workloads" it doesn't matter if it's critical or not it's still patching , then we need to pick
Preemptable VM

- from GCP documentation ( https://cloud.google.com/security/compliance/hipaa#unique_features ) , they explicitly talked about
Preemabtible VM covered by the HIPAA , and this question want t make sure that we know this info.

upvoted 2 times

? ?  JohnPi 1áyear, 2ámonths ago

Selected Answer: B

You can also benefit from multi-regional service redundancy as well as the ability to use Preemptible VMs to reduce costs.
https://cloud.google.com/security/compliance/hipaa/

upvoted 1 times

? ?  Rajasa 1áyear, 6ámonths ago

Selected Answer: B

go with B

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B.
https://cloud.google.com/compute/docs/instances/preemptible
If your apps are fault-tolerant and can withstand possible instance preemptions, then preemptible instances can reduce your Compute
Engine costs significantly. For example, batch processing jobs can run on preemptible instances. If some of those instances stop during
processing, the job slows but does not completely stop. Preemptible instances complete your batch processing tasks without placing
additional workload on your existing instances and without requiring you to pay full price for additional normal instances.

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

Selected Answer: B

B is the correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

218/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û Provisioning preemptible VMs to reduce costs. Disable and discontinue use all GCP services and APIs that are not HIPPA-compliant.
A - has neat differences from B. It says just ôdiscontinueö, though key word here is ôdisableö. See this quote from GCP HIPPA page:

Essential best practices:
ò 1) Execute a Google Cloud BAA. You can request a BAA directly from your account manager.
ò 2) Disable or otherwise ensure that you do not use Google Cloud Products that are not explicitly covered by the BAA (see Covered
Products) when working with PHI.

And this page explains that you need to Enable Cloud APIs for your project. You can disable any APIs for your

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

https://cloud.google.com/apis/docs/getting-started?hl=en&visit_id=636991287264824416-979825397&rd=1

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

219/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #82

Topic 1

Your customer wants to do resilience testing of their authentication layer. This consists of a regional managed instance group serving a public

REST API that reads from and writes to a Cloud SQL instance.

What should you do?

A. Engage with a security company to run web scrapers that look your for users' authentication data om malicious websites and notify you if

any is found.

B. Deploy intrusion detection software to your virtual machines to detect and log unauthorized access.

C. Schedule a disaster simulation exercise during which you can shut off all VMs in a zone to see how your application behaves.

D. Con gure a read replica for your Cloud SQL instance in a different zone than the master, and then manually trigger a failover while

monitoring KPIs for our REST API.

Correct Answer: C

Community vote distribution

C (54%)

D (46%)

? ?  Kri_2525  Highly Voted ?  3áyears, 6ámonths ago

As per google documentation(https://cloud.google.com/solutions/scalable-and-resilient-apps) answer is C.

C: A well-designed application should scale seamlessly as demand increases and decreases, and be resilient enough to withstand the loss
of one or more compute resources.
Resilience: designed to withstand the unexpected
A highly-available, or resilient, application is one that continues to function despite expected or unexpected failures of components in the
system. If a single instance fails or an entire zone experiences a problem, a resilient application remains fault tolerantùcontinuing to
function and repairing itself automatically if necessary. Because stateful information isnÆt stored on any single instance, the loss of an
instanceùor even an entire zoneùshould not impact the applicationÆs performance.

upvoted 51 times

? ?  Jack_in_Large 3áyears ago

Shutting off all VMs in a zone is not good approach for testing of authentication

upvoted 5 times

? ?  vartiklis 1áyear, 6ámonths ago

You're not testing *authentication*, you're testing *the resilience of the authentication layer*. "A resilient app is one that continues
to function despite failures of system components" (https://cloud.google.com/architecture/scalable-and-resilient-
apps#resilience_designing_to_withstand_failures) - such as shutting down all VMs in a zone.

upvoted 10 times

? ?  elaineshi 1áyear ago

Agree, Chaos testing is to shutdown random instances.

upvoted 2 times

? ?  KouShikyou  Highly Voted ?  3áyears, 7ámonths ago

Since the question is asking to do a resilience testing, I prefer C.

upvoted 14 times

? ?  Darahaas 2áyears, 9ámonths ago

Resilience testing of the "Authentication Layer", not the "Application". So the answer is B.

upvoted 4 times

? ?  Sur_Nikki  Most Recent ?  1ámonth, 3áweeks ago

Resilience testing of their authentication layer means the testing of availability of service/application even when many of the instances fail
in a particular location. ThatÆs why. Disaster type of scenario is better where all VM instances becomes unavailable in a particular zone

upvoted 1 times

? ?  salim_ 1ámonth, 3áweeks ago

Selected Answer: C

C as it's definition of resilience testing. D is not correct at all as when disaster occurs you don't have time to "Configure a read replica for
your Cloud SQL instance in a different zone than the master"

upvoted 2 times

? ?  Hisayuki 2ámonths, 1áweek ago

Selected Answer: D

D is right

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

220/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  JC0926 2ámonths, 1áweek ago

Selected Answer: D

D. Configure a read replica for your Cloud SQL instance in a different zone than the master, and then manually trigger a failover while
monitoring KPIs for our REST API.

This option is the most suitable for resilience testing of the authentication layer. By configuring a read replica in a different zone than the
master, you add redundancy to your system. Manually triggering a failover while monitoring KPIs for your REST API helps you observe
how the system behaves during a failure and ensures that the authentication layer remains available and performs as expected during an
outage or disaster.

upvoted 1 times

? ?  taer 3ámonths ago

Selected Answer: C

vote for C

upvoted 1 times

? ?  feholen210 3ámonths, 1áweek ago

Selected Answer: C

C is correct.

upvoted 1 times

? ?  Hawik 3ámonths, 1áweek ago

Selected Answer: C

Should be C

upvoted 1 times

? ?  Deb2293 3ámonths, 1áweek ago

Selected Answer: D

D is right

upvoted 1 times

? ?  zeekerblade 3ámonths, 2áweeks ago

Selected Answer: C

In the question,
//This consists of a regional managed instance group serving a public REST API that reads from and writes to a Cloud SQL instance.
It means it consist of MIG, but does not imply it consists of Cloud SQL as well.

Some argues that MIG is regional and don't need to be tested is definitely incorrect.
there are many configurations may differs the behaviors and even cloud provider stated does not mean that we do not need to test it out.

upvoted 1 times

? ?  JC0926 3ámonths, 2áweeks ago

Selected Answer: D

Option D is a valid method of testing the resilience of the authentication layer. Configuring a read replica in a different zone allows for
redundancy and failover capabilities in the event of a disaster. By manually triggering a failover and monitoring the KPIs for the REST API,
the resilience of the authentication layer can be tested under realistic conditions.

upvoted 1 times

? ?  telp 3ámonths, 2áweeks ago

Selected Answer: D

D => instance group is alreay regional the point of failure is the cloud sql so need to test resilience

upvoted 1 times

? ?  Deb2293 3ámonths, 3áweeks ago

Selected Answer: D

Option D is right
Option C is also incorrect because shutting off all VMs in a zone is not a realistic scenario for testing the resilience of the authentication
layer, as it does not simulate a specific failure mode. Additionally, it may have unintended consequences on other parts of the application
that rely on the same VMs.

upvoted 3 times

? ?  Deb2293 3ámonths, 3áweeks ago

Option C is incorrect as it relies on using the bq command line tool to list jobs and job information, which may not be the most efficient
or reliable way to get the information required. Additionally, it may require more manual effort to compile the data for each user.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is D. Configure a read replica for your Cloud SQL instance in a different zone than the master, and then manually
trigger a failover while monitoring KPIs for our REST API.

Resilience testing is a process of evaluating the ability of a system to recover from failures or disruptions. In this case, the customer has an
authentication layer consisting of a regional managed instance group serving a public REST API that reads from and writes to a Cloud SQL
instance.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

221/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

To perform resilience testing, one option would be to configure a read replica for the Cloud SQL instance in a different zone than the
master. This would allow you to test the system's ability to recover from a failure or disruption in one zone. Then, you could manually
trigger a failover while monitoring key performance indicators (KPIs) for the REST API, such as response times and error rates. This will
allow you to see how the system behaves during a failover and identify any potential issues that need to be addressed.

upvoted 5 times

? ?  Sur_Nikki 1ámonth, 2áweeks ago

Sorry, but I don't agree with your answer. I would like to move with C

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, engaging with a security company to run web scrapers looking for users' authentication data on malicious websites, is not
related to resilience testing.

Option B, deploying intrusion detection software to your virtual machines to detect and log unauthorized access, is a good practice for
improving the security of your system, but it is not directly related to resilience testing.

Option C, scheduling a disaster simulation exercise during which you can shut off all VMs in a zone to see how your application
behaves, is a valid method of resilience testing, but it does not involve testing the system's ability to recover from a failure or disruption
in one zone.

upvoted 4 times

? ?   ercedog 6ámonths, 3áweeks ago

Selected Answer: D

Answer is D.

App layer is covered by regional managed instance group.
Database is the point of failure that needs to be addressed.

upvoted 4 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

222/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #83

Topic 1

Your BigQuery project has several users. For audit purposes, you need to see how many queries each user ran in the last month. What should you

do?

A. Connect Google Data Studio to BigQuery. Create a dimension for the users and a metric for the amount of queries per user.

B. In the BigQuery interface, execute a query on the JOBS table to get the required information.

C. Use 'bq show' to list all jobs. Per job, use 'bq ls' to list job information and get the required information.

D. Use Cloud Audit Logging to view Cloud Audit Logs, and create a  lter on the query operation to get the required information.

Correct Answer: C

Community vote distribution

D (72%)

B (24%)

? ?  Googler2  Highly Voted ?  3áyears, 2ámonths ago

D- reasons:
1.-Cloud Audit Logs maintains audit logs for admin activity, data access and system events. BIGQUERY is automatically send to cloud audit
log functionality.
2.- In the filter you can filter relevant BigQuery Audit messages, you can express filters as part of the export

https://cloud.google.com/logging/docs/audit
https://cloud.google.com/bigquery/docs/reference/auditlogs#ids
https://cloud.google.com/bigquery/docs/reference/auditlogs#auditdata_examples

upvoted 43 times

? ?  GooglecloudArchitect 2áyears, 11ámonths ago

D is the right as you can get the monthly view of the query usage across all the users and projects for auditing purpose. C does need
appropriate permission to see the detail level data. Monthly view is tough to get directly from the bq ls or bq show commands.

upvoted 8 times

? ?  Zarmi  Highly Voted ?  3áyears, 1ámonth ago

Answer is D:
https://cloud.google.com/bigquery/docs/reference/auditlogs#example_query_cost_breakdown_by_identity

upvoted 26 times

? ?  ErenYeager 7ámonths, 3áweeks ago
No mention about exporting to bq

upvoted 1 times

? ?  BobbyFlash 1áyear, 6ámonths ago

Nailed it

upvoted 2 times

? ?  TheCloudGuruu  Most Recent ?  1ámonth, 2áweeks ago

Selected Answer: D

Cloud Logging
upvoted 1 times

? ?  VarunGo 1ámonth, 3áweeks ago

Selected Answer: B

B is correct. here's the link - https://cloud.google.com/bigquery/docs/information-schema-jobs

upvoted 3 times

? ?  medi01 2ámonths, 1áweek ago

Selected Answer: B

JOBS system table does exist and it contains exactly the info we need: one record for each job executed by users (query is one of the type
of the jobs)

upvoted 1 times

? ?  JC0926 2ámonths, 1áweek ago

Selected Answer: D

D. Use Cloud Audit Logging to view Cloud Audit Logs, and create a filter on the query operation to get the required information.

Cloud Audit Logging records activities and API calls in Google Cloud services, including BigQuery. You can use Cloud Audit Logging to view
logs and filter them based on specific operations, such as queries in BigQuery. By filtering on the query operation, you can gather the
required information about how many queries each user ran in the last month, which is essential for audit purposes.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

223/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  gcppandit 5ámonths, 3áweeks ago

Selected Answer: D

A is not possible.
B is possible if VIEW is used instead of TABLE in the description. I use this view to get this information regularly.
C. I have no cloud how this can be right answer.
D. Only possible as per text descriptions.

upvoted 3 times

? ?  medi01 2ámonths, 1áweek ago

JOBS being read-only for the user doesn't make it a view.

upvoted 1 times

? ?  foward 5ámonths, 3áweeks ago

Selected Answer: D

d is correct

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is D. Use Cloud Audit Logging to view Cloud Audit Logs, and create a filter on the query operation to get the required
information.

Google Cloud's Cloud Audit Logging service allows you to view, search, and export audit logs for your Google Cloud projects. These audit
logs contain information about the actions that are performed in your project, including queries that are run in BigQuery.

To see how many queries each user ran in the last month, you can use Cloud Audit Logging to view the Cloud Audit Logs for your
BigQuery project. Then, you can create a filter on the query operation to see only the queries that were run. You can also create a filter on
the user field to see the queries that were run by each user. This will allow you to see the number of queries that were run by each user in
the last month, which can be useful for audit purposes.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, connecting Google Data Studio to BigQuery and creating a dimension for the users and a metric for the amount of queries
per user, is a valid method of visualizing data, but it would not provide the specific information about the number of queries that were
run by each user in the last month.

Option B, executing a query on the JOBS table to get the required information, is not a viable option because the JOBS table does not
contain information about the user who ran the query.

Option C, using the 'bq show' and 'bq ls' commands to list job information, is not a viable option because these commands do not
provide information about the user who ran the query.

upvoted 7 times

? ?  mor  2ámonths ago

Option A: Why would this not provide correct information? You could show the content of the JOBS view. :

SELECT
user_email,
CAST(creation_time as DATE) as date,
count(*) as queries_per_day
FROM
region-eu.INFORMATION_SCHEMA.JOBS
WHERE
creation_time BETWEEN TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 45 DAY) AND CURRENT_TIMESTAMP()
AND job_type = 'QUERY'
group by user_email, CAST(creation_time as DATE)

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  [Removed] 7ámonths ago

Bug it's Cloud Audit Logs enabled default for Big Query?

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  ErenYeager 7ámonths, 3áweeks ago

Selected Answer: A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

224/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D is not the answer there was no mention about exporting to bigquery.

A is correct ?
upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

D likely the answer, if B says view I would have selected B, but it says table while its a view

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: D

D. Use Cloud Audit Logging to view Cloud Audit Logs, and create a filter on the query operation to get the required information.

upvoted 2 times

? ?  abdelilahfa 8ámonths, 3áweeks ago
I think the right answer is B
https://cloud.google.com/bigquery/docs/information-schema-jobs

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

For audit purposes, you need to see how many queries each user ran in the last month. This is an example of Cloud Audit log for admin
activity

upvoted 1 times

? ?  6721sora 10ámonths, 1áweek ago

Selected Answer: B

B
since the JOBS view provides this information and is much simpler

upvoted 2 times

? ?  kiappy81 9ámonths ago

the point is that JOBS is not a table, but rather a view. For this reason you have to select option audit log.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

225/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #84

Topic 1

You want to automate the creation of a managed instance group. The VMs have many OS package dependencies. You want to minimize the

startup time for new

VMs in the instance group.

What should you do?

A. Use Terraform to create the managed instance group and a startup script to install the OS package dependencies.

B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the

VM image.

C. Use Puppet to create the managed instance group and install the OS package dependencies.

D. Use Deployment Manager to create the managed instance group and Ansible to install the OS package dependencies.

Correct Answer: B

Community vote distribution

B (100%)

? ?  crypt0  Highly Voted ?  3áyears, 8ámonths ago

Why is it not answer B?

upvoted 41 times

? ?  kumarp6 2áyears, 8ámonths ago

B is the answer,
upvoted 6 times

? ?  Jos 3áyears, 5ámonths ago

It is.

upvoted 10 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 11 times

? ?  nitinz 2áyears, 3ámonths ago

It is B

upvoted 4 times

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

B- minimal start time means a pre-baked golden image

upvoted 20 times

? ?  omermahgoub  Most Recent ?  6ámonths, 1áweek ago

The correct answer is B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed
instance group with the VM image.

Managed instance groups are a way to manage a group of Compute Engine instances as a single entity. If you want to automate the
creation of a managed instance group, you can use tools such as Terraform, Deployment Manager, or Puppet to automate the process.

To minimize the startup time for new VMs in the instance group, you should create a custom VM image with all of the OS package
dependencies pre-installed. This will allow you to create new VMs from the custom image, which will significantly reduce the startup time
compared to installing the dependencies on each VM individually. You can then use Deployment Manager to create the managed instance
group with the custom VM image.

upvoted 6 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, using Terraform to create the managed instance group and a startup script to install the OS package dependencies, would
not minimize the startup time for new VMs in the instance group.

Option C, using Puppet to create the managed instance group and install the OS package dependencies, would not minimize the
startup time for new VMs in the instance group.

Option D, using Deployment Manager to create the managed instance group and Ansible to install the OS package dependencies,
would not minimize the startup time for new VMs in the instance group.

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

226/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with
the VM image.
upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B will reduce the startup time

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

B- minimal start time means a pre-baked golden image

upvoted 4 times

? ?  BigSteveO 11ámonths, 2áweeks ago

As someone who works on Terraform. It may not be Googles best practice, even though it's built in just need to be initialized. But it is the
easiest way to build and restructure infrastructure with a simple line of code change and a quick shell command to apply terraform. I
mean B would work. But it doesn't include the start-up script for the OS dependencies to be loaded. ?>?>? Any feedback?

upvoted 1 times

? ?  Ric350 11ámonths, 2áweeks ago

Start up scripts aren't need here as you're making a custom OS image with all OS package dependencies. Question is not asking for the
easiest way, it's asking how to minimize VM startup times. Not having to run the startup scripts because it's baked into the image is
how I understand and interpret this, therefore B.

upvoted 2 times

? ?  mv2000 11ámonths, 4áweeks ago

06/30/2022 Exam

upvoted 5 times

? ?  rogerlovato 1áyear, 5ámonths ago

Selected Answer: B

B is correct

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B

upvoted 1 times

? ?  Godlike 1áyear, 6ámonths ago

yes B is right

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

B is the right answer

upvoted 2 times

? ?  exam_war 1áyear, 7ámonths ago

go with B. D: it involves so many other third software to configure/manage which makes build more complicated.

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û create a custom VM instance image with all OS dependencies. Use Deployment Manager to create a MIG with the VM image.
Read more about Public and Custom VM Images: https://cloud.google.com/compute/docs/images
Custom images are available in your project only, they donÆt add cost to your VM instances, incur image storage cost (0.085$ GB/month)
D û could be also an alternative (if to consider requirement to install dependencies in start up script). But, last sentence stresses on
ôminimize VMÆs start up timeö. So, B is fastest solution. Also, what is a point to use Ansible if you can complete same task via startup script
of Deployment Manager. Ansible wonÆt make this faster, but just will add 3rd party dependency.

upvoted 3 times

? ?  victory108 2áyears, 1ámonth ago

B. Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with
the VM image.
upvoted 2 times

? ?  un 2áyears, 1ámonth ago

B is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

227/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #85

Topic 1

Your company captures all web tra c data in Google Analytics 360 and stores it in BigQuery. Each country has its own dataset. Each dataset has

multiple tables.

You want analysts from each country to be able to see and query only the data for their respective countries.

How should you con gure the access rights?

A. Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups

as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate dataset with view access with each

respective analyst country-group.

B. Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups

as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate tables with view access with each

respective analyst country-group.

C. Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups

as members. Grant the 'all_analysts' group the IAM role of BigQuery dataViewer. Share the appropriate dataset with view access with each

respective analyst country- group.

D. Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-groups

as members. Grant the 'all_analysts' group the IAM role of BigQuery dataViewer. Share the appropriate table with view access with each

respective analyst country-group.

Correct Answer: A

Community vote distribution

A (80%)

C (20%)

? ?  Sebatian  Highly Voted ?  3áyears, 7ámonths ago

It should be A. The question requires that user from each country can only view a specific data set, so BQ dataViewer cannot be assigned
at project level. Only A could limit the user to query and view the data that they are supposed to be allowed to.

upvoted 53 times

? ?  jits1984 2ámonths, 1áweek ago

Should be C.

https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer

Data viewer role can be applied to a Table and a View.

JobUser can be applied only at a Project level not at a Dataset level

https://cloud.google.com/bigquery/docs/access-control#bigquery.jobUser

upvoted 6 times

? ?  wk  Highly Voted ?  3áyears, 8ámonths ago

Should be C
https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer

When applied to a dataset, dataViewer provides permissions to:

Read the dataset's metadata and to list tables in the dataset.
Read data and metadata from the dataset's tables.
When applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are
necessary to allow the running of jobs.

upvoted 31 times

? ?  Jack_in_Large 3áyears, 1ámonth ago

Option C grant read permission to all datasets globally, which violated the request "You want analysts from each country
to be able to see and query only the data for their respective countries"

So the correct answer is A.

upvoted 27 times

? ?  BrunoTostes 1áyear, 8ámonths ago

https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer
"When applied to a dataset.." you can apply dataViewer role to a specific dataset.

upvoted 8 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

228/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  SR23222  Most Recent ?  2áweeks ago

It should be C. As per https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer, if applied at Dataset level, once can
read data and metadata from the dataset's tables. This clearly means answer is C. Not sure why everyone think its A

upvoted 1 times

? ?  claorden 2áweeks, 3ádays ago

Selected Answer: C

https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer

upvoted 1 times

? ?  red_panda 3áweeks, 1áday ago

Selected Answer: A

A is the correct answer.
For who is thinking it's C the correct: dataViewer cannot perform queries on BQ, dude.

upvoted 2 times

? ?  JohnWick2020 3áweeks, 5ádays ago

A. This question is similar to an earlier one focused on billing users who query.

upvoted 1 times

? ?  TheCloudGuruu 1ámonth, 2áweeks ago

Selected Answer: A

A is the best choice

upvoted 1 times

? ?  salim_ 1ámonth, 3áweeks ago

Selected Answer: A

BigQuery.DataViewer alone is not sufficient to run job (make queries). So necessary need BigQuery.JobUser role to have permission to run
jobs (including queries within projects)

https://cloud.google.com/iam/docs/understanding-roles

"BigQuery Data Viewer ...
...
When applied at the project or organization level, this role can also enumerate all datasets in the project. Additional roles, however, are
necessary to allow the running of jobs."

upvoted 3 times

? ?  izekc 4ámonths, 3áweeks ago

According to ChatGPT, should be C

upvoted 4 times

? ?  Martintranthanh 3ámonths, 1áweek ago

Not sure how you ask , I have asked, ChatGPPT answer A "The recommended approach to configure access rights for analysts from
each country to be able to see and query only the data for their respective countries is:

A. Create a group per country. Add analysts to their respective country-groups. Create a single group 'all_analysts', and add all country-
groups as members. Grant the 'all_analysts' group the IAM role of BigQuery jobUser. Share the appropriate dataset with view access
with each respective analyst country-group.

This approach ensures that analysts from each country can access only the data for their respective countries by sharing the
appropriate dataset with view access with each respective analyst country-group. The 'all_analysts' group is given the IAM role of
BigQuery jobUser to enable the group members to run BigQuery jobs on the datasets they have access to. Additionally, creating a
group per country and adding analysts to their respective country-groups helps to manage access control more efficiently.
"

upvoted 3 times

? ?  MestreCholas 3ámonths, 3áweeks ago

Don't trust ChatGPT in all cases, he misses sometimes

upvoted 1 times

? ?  Deb2293 3ámonths, 3áweeks ago

Type "I think it should be A" and ChatGPT will change it's answer :P

upvoted 4 times

? ?  examch 6ámonths ago

Selected Answer: A

A is the correct answer
The user should be able to query, only Job User can perform query operations, whereas a data viewer can only read the data.

BigQuery Data Viewer is required to read data from the specified dataset. BigQuery Job User is needed to create query jobs from which
the results can be read.

upvoted 5 times

? ?  Charsoft 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

229/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Should be C.
According to the docs, you cannot apply the bigquery.jobUser role to any level lower than the project level.
https://cloud.google.com/iam/docs/understanding-roles

upvoted 2 times

? ?  medi01 2ámonths, 1áweek ago

But in a given example, it IS applied on the project level.

upvoted 1 times

? ?  jay9114 6ámonths ago

So which is more restrictive: dataviewer or jobuser??

upvoted 1 times

? ?  jay9114 5ámonths, 3áweeks ago

Does BigQuery dataViewer role only allow you to view the dataset, not query it?? The jobUser role allows you to view and query the
dataset?

upvoted 1 times

? ?  thamaster 6ámonths ago

Selected Answer: A

answers with table are eliminated as we need to grant permission to dataset for each country.
It remain A and C. We need right to perform query so answer A is good

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is C.

Google Cloud's Identity and Access Management (IAM) system allows you to control access to resources in your Google Cloud project. In
this case, you want analysts from each country to be able to see and query only the data for their respective countries in BigQuery.

To configure the access rights, you should create a group for each country and add the analysts from that country to their respective
country-group. You should then create a single group called 'all_analysts', and add all of the country-groups as members. This will allow
you to manage the access rights for all of the analysts in a single group.

Next, you should grant the 'all_analysts' group the IAM role of BigQuery dataViewer. This will allow the analysts to view the data in
BigQuery, but not modify it. Finally, you should share the appropriate dataset with view access with each respective analyst country-group.
This will allow the analysts to see and query only the data for their respective countries.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, granting the 'all_analysts' group the IAM role of BigQuery jobUser and sharing the appropriate dataset with view access with
each respective analyst country-group, would allow the analysts to view the data in BigQuery, but not modify it.

Option B, granting the 'all_analysts' group the IAM role of BigQuery jobUser and sharing the appropriate tables with view access with
each respective analyst country-group, would allow the analysts to view the data in BigQuery, but not modify it.

Option D, granting the 'all_analysts' group the IAM role of BigQuery dataViewer and sharing the appropriate table with view access with
each respective analyst country-group, would not allow the analysts to see all of the data for their respective countries.

upvoted 2 times

? ?  CloudUpload 6ámonths, 3áweeks ago

Should be C.
https://cloud.google.com/bigquery/docs/access-control

Lowest level of JobUser's lowest level is Project level so can't apply on Datasets.

upvoted 3 times

? ?  TonytheTiger 6ámonths, 3áweeks ago

Answer A: BigQuery Job User
(roles/bigquery.jobUser) Provides permissions to run jobs, including queries, within the
project. https://cloud.google.com/bigquery/docs/access-control#bigquery.dataViewer
dataViewer: Read data and metadata from the table or view

upvoted 1 times

? ?  NareshPokhriyal 6ámonths, 3áweeks ago

C is the right answer

https://cloud.google.com/bigquery/docs/access-control

BigQuery Data Viewer (roles/bigquery.dataViewer)

When applied to a table or view, this role provides permissions to:
- Read data and metadata from the table or view.
- This role cannot be applied to individual models or routines.

When applied to a dataset, this role provides permissions to:
- Read the dataset's metadata and list tables in the dataset.
- Read data and metadata from the dataset's tables.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

230/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

231/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #86

Topic 1

You have been engaged by your client to lead the migration of their application infrastructure to GCP. One of their current problems is that the on-

premises high performance SAN is requiring frequent and expensive upgrades to keep up with the variety of workloads that are identi ed as

follows: 20 TB of log archives retained for legal reasons; 500 GB of VM boot/data volumes and templates; 500 GB of image thumbnails; 200 GB of

customer session state data that allows customers to restart sessions even if off-line for several days.

Which of the following best re ects your recommendations for a cost-effective storage allocation?

A. Local SSD for customer session state data. Lifecycle-managed Cloud Storage for log archives, thumbnails, and VM boot/data volumes.

B. Memcache backed by Cloud Datastore for the customer session state data. Lifecycle-managed Cloud Storage for log archives, thumbnails,

and VM boot/data volumes.

C. Memcache backed by Cloud SQL for customer session state data. Assorted local SSD-backed instances for VM boot/data volumes. Cloud

Storage for log archives and thumbnails.

D. Memcache backed by Persistent Disk SSD storage for customer session state data. Assorted local SSD-backed instances for VM boot/data

volumes. Cloud Storage for log archives and thumbnails.

Correct Answer: D

Community vote distribution

B (70%)

D (30%)

? ?  OSNG  Highly Voted ?  2áyears, 6ámonths ago

B is correct.
WHY NOT OTHERS.
A: is wrong Local SSD in non-persistent therefore cannot be used for session state (as questions also need to save data for users who are
offline for several days).
C: Again Local SSD cannot be used for boot volume (because its Non-persistent again) and always used for temporary data storage.
D: Same reason as C.
WHY B?
Left with B that's why, but the question is how to store Boot/Data volume on Cloud Storage?
- Storing other type of data is easy but most comments were about boot volume.
- Boot volume can be stored to Cloud Storage by creating an Custom Image.
https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#selecting_image_storage_location
---- Upvote if agree for the clarification of others ----

upvoted 82 times

? ?  salim_ 1ámonth, 3áweeks ago

The phrase "500 GB of VM boot/data volumes and templates " does not necessary mean this is the boot volume used by the VM. I
understand it as a kind of repository of boot volumes & templates

upvoted 1 times

? ?  6721sora 10ámonths, 1áweek ago

You rejected D with same reason as C.
But D is using Persistent disk, not local SSD
And not a good idea to keep VM boot data in Cloud storage. As VM needs fast access to boot images to boot up

upvoted 3 times

? ?  melono 8ámonths, 2áweeks ago

also Datastore (now Firestore) is aimed for storing state

upvoted 1 times

? ?  melono 8ámonths, 2áweeks ago
D states local SSD for VMs.
¿The data that you store on a local SSD persists only until the instance is stopped or deleted¿

upvoted 1 times

? ?  BiddlyBdoyng 8ámonths, 2áweeks ago

Yeah, D looks good except the local SSD. Although would we ever want to stop the VMs and not be happy to rebuild? Not sure
about B, a move to noSQL is a big, Cloud Storage for the VM boot disks rules it out for sure.

upvoted 1 times

? ?  Ishu_awsguy 10ámonths, 2áweeks ago
Guys I think its a english error.
The last line need to be read carefully
Decouple the line after , and Vm boot/data volumes.
I think they mean to use vm persistent disks for boot and data volumes.
B is the answer

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

232/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  Ishu_awsguy 10ámonths, 3áweeks ago

How can u use cloud storage for VM boot/data volumnes ?
B is wrong

upvoted 1 times

? ?  Ishu_awsguy 10ámonths, 3áweeks ago

All the options are debatable and have some flaw.
But closes answer is B
although it has a flaw mentioned above but is still better than other options

upvoted 3 times

? ?  siumk  Highly Voted ?  3áyears, 3ámonths ago

IMHO Answer is B:

Memcache backed by Cloud Datastore
https://cloud.google.com/appengine/docs/standard/python/memcache

Compute Engine image can be stored in Cloud Storage
https://cloud.google.com/solutions/image-management-best-practices
After the complete sequence of bytes from the disk are written to the file, the file is archived using the tar format and then compressed
using the GZIP format. You can then upload the resulting *.tar.gz file to Cloud Storage and register it as an image in Compute Engine.

upvoted 15 times

? ?  Ayzen 3áyears, 2ámonths ago

The problem with B is that they are using SAN for data volumes of working VMs, not just to store templates/images. All answers here
are quite bad. But I would go with D, as they are talking about several days of saving users' stale session data, which is something that
can be accomplished with SSD.

upvoted 9 times

? ?  Bijesh 2áyears, 7ámonths ago

@ayzen yes. IS cloud datastore optimized to handle such a data (200GB)

upvoted 1 times

? ?  medi01  Most Recent ?  2ámonths, 1áweek ago

Selected Answer: B

Local SSD cannot be used for neither boot nor data!!! This rules out B&C. Oh, and A too.

upvoted 1 times

? ?  Kamaly 2ámonths, 2áweeks ago

Selected Answer: B

Cloud Datastore is the right solution to store the session data

upvoted 1 times

? ?  feholen210 3ámonths, 1áweek ago

Selected Answer: B

B Seems correct.
upvoted 1 times

? ?  ile02 3ámonths, 4áweeks ago

D. makes more sense

upvoted 1 times

? ?  WAENANY 3ámonths, 4áweeks ago

Selected Answer: D

d makes more sense

upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: D

ChatGPT says option D.

upvoted 1 times

? ?  Deb2293 3ámonths, 3áweeks ago

chatgpt has knowledge till Sept 2021. Don't rely on it bro

upvoted 3 times

? ?  kaleemahmad75 5ámonths, 2áweeks ago

Selected Answer: D

My answer is D
upvoted 1 times

? ?  Santanu_01 6ámonths ago

I will go with Option D as it is best practice to keep similar data together and seprate OS, Volatile and permanent

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

233/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  thamaster 6ámonths ago

Selected Answer: D

i'll go D because i don't think Cloud storage can be used for booting a VM.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is D
For the customer session state data, which needs to be highly available and fast, it is recommend to use Memcache backed by Persistent
Disk SSD storage. This will provide fast read and write access to the data, as well as high availability.

For the VM boot/data volumes, which also require fast read and write access, it is recommend to use local SSD-backed instances. This will
provide the highest performance for these workloads.

For the log archives and thumbnails, which do not require the same level of performance as the other workloads, it is recommend to use
lifecycle-managed Cloud Storage. This will provide a cost-effective solution for storing this data, as it will automatically move data to lower-
cost storage options as it becomes less frequently accessed.

upvoted 8 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, using Local SSD for customer session state data and lifecycle-managed Cloud Storage for log archives, thumbnails, and VM
boot/data volumes, would not provide the necessary performance or availability for the customer session state data.

Option B, using Memcache backed by Cloud Datastore for customer session state data and lifecycle-managed Cloud Storage for log
archives, thumbnails, and VM boot/data volumes, would not provide the necessary performance or availability for the customer session
state data.

C, using Memcache backed by Cloud SQL for customer session state data and assorted local SSD-backed instances for VM boot/data
volumes and Cloud Storage for log archives and thumbnails, would not provide the necessary performance or availability for the
customer session state data.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

There are two issues with this question:
1. Assuming that you only consider migration and not configuration then the answer is B. This is because the vm images will first be
migrated to cloud storage only. VM images can be migrated to cloud storage first and then imported on to the compute engine and saved
on persistent disks (https://cloud.google.com/compute/docs/import/import-existing-image).
2. D can only be correct if by "local SSD's for VM images" they mean persistent local SSD disks
(https://cloud.google.com/compute/docs/disks/local-ssd) which may not be the case as the terminology is clear (?).
In my opinion B is the better answer.

upvoted 1 times

? ?  Rajeev26 8ámonths, 3áweeks ago

Selected Answer: B

i think should be as local SSD not recommended for GCS VM boot volume

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

https://cloud.google.com/compute/docs/disks/local-ssd

upvoted 2 times

? ?  harutheorochimaru 10ámonths, 4áweeks ago

Selected Answer: B

I think cost-effective is a key part in this case. I am going with B

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

234/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #87

Topic 1

Your web application uses Google Kubernetes Engine to manage several workloads. One workload requires a consistent set of hostnames even

after pod scaling and relaunches.

Which feature of Kubernetes should you use to accomplish this?

A. StatefulSets

B. Role-based access control

C. Container environment variables

D. Persistent Volumes

Correct Answer: A

Community vote distribution

A (100%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

StatefulSets is a feature of Kubernetes, which the question asks about. Yes, Persistent volumes are required by StatefulSets
(https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/). See the Google documentations for mentioning of hostnames
(https://cloud.google.com/kubernetes-engine/docs/concepts/statefulset)... Answer A

upvoted 51 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 6 times

? ?  OrangeTiger 1áyear, 5ámonths ago

thank you!

upvoted 1 times

? ?  kumarp6 2áyears, 8ámonths ago

A is correct, statefulset

upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

It is A

upvoted 2 times

? ?  omermahgoub  Highly Voted ?  6ámonths, 1áweek ago

A. StatefulSets

To ensure that a workload in Kubernetes has a consistent set of hostnames even after pod scaling and relaunches, you should use
StatefulSets. StatefulSets are a type of controller in Kubernetes that is used to manage stateful applications. They provide a number of
features that are specifically designed to support stateful applications, including:

Stable, unique network identifiers for each pod in the set
Persistent storage that is automatically attached to pods
Ordered, graceful deployment and scaling of pods
Ordered, graceful deletion and termination of pods
By using StatefulSets, you can ensure that your workload has a consistent set of hostnames even if pods are scaled or relaunched, which
can be important for applications that rely on stable network identifiers.

upvoted 9 times

? ?  Tamirm 4ámonths, 3áweeks ago

You are the best.. thanks for all the hard work to explain

upvoted 2 times

? ?  Wangyu60 3ámonths ago

obviously from chatGPT, but still good to share.

upvoted 1 times

? ?  kaleemahmad75  Most Recent ?  5ámonths, 2áweeks ago

Selected Answer: A

A is the answer
upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

235/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is ok

upvoted 1 times

? ?  Deepak31 7ámonths, 3áweeks ago

A StatefulSet is the Kubernetes controller used to run the stateful application as containers (Pods) in the Kubernetes cluster. StatefulSets
assign a sticky identityùan ordinal number starting from zeroùto each Pod instead of assigning random IDs for each replica Pod. A new
Pod is created by cloning the previous PodÆs data.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

this is straight forward question if you know kubernetes concepts. A is right

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago
I do not know Kubernetes

upvoted 2 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: A

https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/

upvoted 2 times

? ?  mv2000 11ámonths, 4áweeks ago

06/30/2022 Exam

upvoted 3 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A.
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

Selected Answer: A

A is the correct answer

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

A û StatefulSets
StatefulSets are suitable for deploying Kafka, MySQL, Redis, ZooKeeper, and other applications needing unique, persistent identities and
stable hostnames. Read more about StatefulSets. https://cloud.google.com/kubernetes-engine/docs/concepts/statefulset

C û Container Env Variable, are good if you need to init containers with some static content. E.g. Pod passes to containers its HOSTNAME
(where containers are running), namespace and user defined vars. So, env vars is a way for Pod to init containers at start up. But, stable
hostnames must be preserved at Pod level via StatefulSets.
Defining Env Vars for Container: https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/

upvoted 5 times

? ?  Arjun1983 1áyear, 8ámonths ago

StatefulSets are designed to deploy stateful applications and clustered applications that save data to persistent storage, such as Compute
Engine persistent disks. StatefulSets are suitable for deploying Kafka, MySQL, Redis, ZooKeeper, and other applications needing unique,
persistent identities and "stable hostnames". Answer is A

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

A. StatefulSets
upvoted 3 times

? ?  un 2áyears, 1ámonth ago

A is correct

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  BhupalS 2áyears, 6ámonths ago

A is the Ans
https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/

upvoted 1 times

? ?  Chulbul_Pandey 2áyears, 6ámonths ago

StatefulSets for sequencing..
A is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

236/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #88

Topic 1

You are using Cloud CDN to deliver static HTTP(S) website content hosted on a Compute Engine instance group. You want to improve the cache

hit ratio.

What should you do?

A. Customize the cache keys to omit the protocol from the key.

B. Shorten the expiration time of the cached objects.

C. Make sure the HTTP(S) header ?ÇCache-Region?Ç points to the closest region of your users.

D. Replicate the static content in a Cloud Storage bucket. Point CloudCDN toward a load balancer on that bucket.

Correct Answer: A

Reference:

https://cloud.google.com/cdn/docs/best-practices#using_custom_cache_keys_to_improve_cache_hit_ratio

Community vote distribution

A (71%)

D (29%)

? ?  shandy  Highly Voted ?  3áyears, 7ámonths ago

Option A is Correct.
https://cloud.google.com/cdn/docs/caching#cache-keys

upvoted 20 times

? ?  MestreCholas 3ámonths, 3áweeks ago

https://cloud.google.com/cdn/docs/best-practices#cache-hit-ratio

upvoted 4 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 7 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes, A is correct
upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

A, both http and https will use same key.

upvoted 3 times

? ?  gfhbox0083  Highly Voted ?  3áyears ago

A, for sure.
By default, Cloud CDN uses the complete request URL to build the cache key. For performance and scalability, itÆs important to optimize
cache hit ratio. To help optimize your cache hit ratio, you can use custom cache keys .....

upvoted 8 times

? ?  VarunGo  Most Recent ?  1ámonth, 3áweeks ago

Selected Answer: D

D
This option is the best because Cloud Storage has built-in caching and can serve content faster than Compute Engine instances. It also
allows for better scalability and availability. By pointing Cloud CDN towards a load balancer on the Cloud Storage bucket, the cache hit
ratio can be improved as the content will be served directly from the cache without needing to access the Compute Engine instances.

Option A (Customize the cache keys to omit the protocol from the key) may not be effective in improving the cache hit ratio as it only
removes the protocol from the cache key and does not address the underlying issue of slow content delivery.

upvoted 1 times

? ?  Deb2293 4ámonths ago

Selected Answer: D

Customizing the cache keys by omitting the protocol from the key (option A) can be a valid approach to improve the cache hit ratio for
CDN delivered Compute Engine, but it may not be the most effective solution for all cases.

Customizing the cache keys can improve the cache hit ratio by reducing the number of cache misses caused by variations in the request
URL, headers, and parameters. However, customizing the cache keys requires careful consideration of the caching policies, traffic
patterns, and content types

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

237/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A. Customize the cache keys to omit the protocol from the key.

To improve the cache hit ratio with Cloud CDN, you should customize the cache keys to omit the protocol (e.g. HTTP or HTTPS) from the
key. This will allow Cloud CDN to cache the same content under both HTTP and HTTPS, which can help to improve the hit ratio by allowing
Cloud CDN to serve content from cache more frequently.

To customize the cache keys, you can use the --key-include-protocol flag when enabling Cloud CDN for your Compute Engine instance
group or load balancer. Setting this flag to false will cause Cloud CDN to omit the protocol from the cache key.

Other options, such as shortening the expiration time of cached objects or replicating content in Cloud Storage, may also help to improve
the cache hit ratio, but customizing the cache keys to omit the protocol is likely to have the greatest impact.

upvoted 6 times

? ?  AzureDP900 8ámonths, 2áweeks ago

https://cloud.google.com/cdn/docs/best-practices#cache-hit-ratio. A is right

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Each cache entry in a Cloud CDN cache is identified by a cache key. When a request comes into the cache, the cache converts the URI of
the request into a cache key, and then compares it with keys of cached entries. If it finds a match, the cache returns the object
associated with that key.

upvoted 2 times

? ?  aut0pil0t 10ámonths ago

Selected Answer: A

Use case:
"A logo needs to be cached whether displayed through HTTP or HTTPS. When you customize the cache keys for the backend service that
holds the logo, clear the Protocol checkbox so that requests through HTTP and HTTPS count as matches for the logo's cache entry."

https://cloud.google.com/cdn/docs/best-practices#using_custom_cache_keys_to_improve_cache_hit_ratio

upvoted 2 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: A

https://cloud.google.com/cdn/docs/best-practices#using_custom_cache_keys_to_improve_cache_hit_ratio

upvoted 1 times

? ?  ehgm 1áyear, 6ámonths ago

I agree that 'A' action will increase the cache hit ratio, but it doesn't make sense for me to remove HTTPS from parts of my app. All access
must be over HTTPS and HTTP must be blocked or redirected to HTTPS.

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A.

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

A is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  fwfw 1áyear, 7ámonths ago

why not D?

upvoted 2 times

? ?  potorange 1áyear ago

Yeah was considering D at first. But, as Cloud CDN Is already set up, static content should already be cached and moving it to Cloud
Storage wouldnÆt change hit ratio. This is the best explanation I see supporting answer A over D à

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

A û customize cache keys to omit the protocol from the key.
Check Cache Keys: https://cloud.google.com/cdn/docs/caching#cache-keys
It says that you can omit protocol, host or query string in requesting URL. If you omit protocol, then both requests with HTTPS or HTTP
protocol will hit same cached page. Hence, that increases hit ratio.
C û with ôcache-regionö pointing to closest region û would improve latency (not hit ratio). But, ôcache-regionö field doesnÆt exist in HTTP
header at all. Check more about CDN Caching here: https://cloud.google.com/cdn/docs/caching

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û stackdriver automatically collects admin activity logs for most services. Stackdriver Logging Agenct must be installed on each instance
to collect system logs.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

238/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Read more about Logging Agent. https://cloud.google.com/logging/docs/agent/
Logging agent streams logs from 3rd party apps and systems SW (syslog on Linux) to Logging. It is best practice to run the Logging agent
on all your VM instances. It runs on Linux and Windows.
Cloud Audit Logs says that Admin Activity audit logs are always enabled.

upvoted 1 times

? ?  unnikrisb 1áyear, 8ámonths ago

Agree with Option A.. https://cloud.google.com/cdn/docs/best-practices#cache-hit-ratio

upvoted 2 times

? ?  Unfaithful 1áyear, 11ámonths ago

Answer: A
Support: https://cloud.google.com/cdn/docs/best-practices#cache-hit-ratio

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

239/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #89

Topic 1

Your architecture calls for the centralized collection of all admin activity and VM system logs within your project.

How should you collect these logs from both VMs and services?

A. All admin and VM system logs are automatically collected by Stackdriver.

B. Stackdriver automatically collects admin activity logs for most services. The Stackdriver Logging agent must be installed on each instance

to collect system logs.

C. Launch a custom syslogd compute instance and con gure your GCP project and VMs to forward all logs to it.

D. Install the Stackdriver Logging agent on a single compute instance and let it collect all audit and access logs for your environment.

Correct Answer: B

Community vote distribution

B (100%)

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

Does not agree with D. B is the nearest answer I feel !

upvoted 40 times

? ?  KouShikyou 3áyears, 8ámonths ago

Agree.

upvoted 9 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 12 times

? ?  kumarp6 2áyears, 8ámonths ago

B is correct, D is SPOF ...

upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

It is B, all rest are BS

upvoted 2 times

? ?  shandy  Highly Voted ?  3áyears, 7ámonths ago

Admin and event logs are configured by default. VM System logs require a logging agent to be configured. So A is not valid. Answer is B

upvoted 19 times

? ?  jlambdan  Most Recent ?  3ámonths, 1áweek ago

Selected Answer: B

answer is B as per this tutorial step: https://cloud.google.com/logging/docs/logging-gce-quickstart#install-agent

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Stackdriver does not require the Stackdriver Logging agent to be installed in order to collect system logs.
Stackdriver is a cloud monitoring and logging platform that is integrated with Google Cloud Platform (GCP) and is designed to collect,
monitor, and troubleshoot logs from your GCP resources. By default, Stackdriver automatically collects admin activity logs for most GCP
services, as well as VM system logs. This means that you don't need to install the Stackdriver Logging agent or any other agents in order
to collect these logs - they are automatically collected and centralized by Stackdriver.

However, if you want to collect logs from other sources that are not automatically collected by Stackdriver (e.g. logs from applications
running on your VMs, logs from on-premises systems, etc.), you can use the Stackdriver Logging agent to forward these logs to
Stackdriver. The agent is a lightweight daemon that runs on your VMs or other hosts, and it can be used to collect logs from various
sources and forward them to Stackdriver for centralized storage and analysis.

upvoted 5 times

? ?  omermahgoub 6ámonths, 1áweek ago

Answer is A

In Google Cloud Platform (GCP), you can use Stackdriver to collect and centralize all admin activity and VM system logs within your
project. Stackdriver is a powerful cloud monitoring and logging platform that is integrated with GCP, and it provides a number of
features that are specifically designed to help you collect, monitor, and troubleshoot logs from your GCP resources.

One of the key features of Stackdriver is that it automatically collects admin activity logs for most GCP services, as well as VM system
logs. This means that you don't need to install any agents or configure any additional components to collect these logs - they are
automatically collected and centralized by Stackdriver.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

240/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

To view and analyze your logs in Stackdriver, you can use the Stackdriver Logs Viewer, which provides a powerful interface for
searching, filtering, and aggregating your logs. You can also use the Stackdriver Logs API to programmatically access your logs, or use
the Stackdriver Logging agent to forward your logs to other log management or analysis tools.

upvoted 4 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

https://cloud.google.com/logging/docs/agent/logging/installation#before_you_begin

upvoted 4 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Thank you for sharing the link, B is right

upvoted 1 times

? ?  panqueca 1áyear, 1ámonth ago

Selected Answer: B

it's B

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B.

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

B is the correct answer.

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û stackdriver automatically collects admin activity logs for most services. Stackdriver Logging Agenct must be installed on each instance
to collect system logs.
Read more about Logging Agent. https://cloud.google.com/logging/docs/agent/
Logging agent streams logs from 3rd party apps and systems SW (syslog on Linux) to Logging. It is best practice to run the Logging agent
on all your VM instances. It runs on Linux and Windows.
Cloud Audit Logs says that Admin Activity audit logs are always enabled.

upvoted 3 times

? ?  unnikrisb 1áyear, 8ámonths ago

Agree with B

upvoted 1 times

? ?  Unfaithful 1áyear, 11ámonths ago

Answer: B
Solution: https://cloud.google.com/logging/docs/agent/logging/installation#before_you_begin

upvoted 4 times

? ?  victory108 2áyears, 1ámonth ago

B. Stackdriver automatically collects admin activity logs for most services. The Stackdriver Logging agent must be installed on each
instance to collect system log

upvoted 3 times

? ?  un 2áyears, 1ámonth ago

B is correct

upvoted 1 times

? ?  gosi 2áyears, 2ámonths ago

B.
A is wrong because "VM system logs" is not included by default by GCP. wha tis included by default is - "System Events" which by definition
( refer : https://cloud.google.com/logging/docs/audit#system-event) doesnt include VM level system logs e.g. syslogd etc. You need agent
for it.

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

B is ok.

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

241/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

... because Loginng Agent is NOT installed by default on VMs.

Ref: https://cloud.google.com/logging/docs/agent/installation

The Logging agent streams logs from your VM instances and from selected third-party software packages to Cloud Logging. It is a best
practice to run the Logging agent on all your VM instances.

The VM images for Compute Engine and Amazon Elastic Compute Cloud (EC2) don't include the Logging agent, so you must complete
these steps to install it on those instances. The agent runs under both Linux and Windows.

If your VMs are running in Google Kubernetes Engine or App Engine, the agent is already included in the VM image, so you can skip
this page.

If you are running specialized logging workloads that require higher throughput and/or improved resource-efficiency compared to the
standard Cloud Logging agent, consider using the Ops agent.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

242/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #90

Topic 1

You have an App Engine application that needs to be updated. You want to test the update with production tra c before replacing the current

application version.

What should you do?

A. Deploy the update using the Instance Group Updater to create a partial rollout, which allows for canary testing.

B. Deploy the update as a new version in the App Engine application, and split tra c between the new and current versions.

C. Deploy the update in a new VPC, and use Google's global HTTP load balancing to split tra c between the update and current applications.

D. Deploy the update as a new App Engine application, and use Google's global HTTP load balancing to split tra c between the new and

current applications.

Correct Answer: B

Community vote distribution

B (100%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

I think B is correct. Because GAE supports service version control and A/B test.
Is my understanding correct?

upvoted 56 times

? ?  kumarp6 2áyears, 8ámonths ago

Yes, B is correct
upvoted 5 times

? ?  nitinz 2áyears, 3ámonths ago

Only B works.
upvoted 4 times

? ?  ADVIT  Highly Voted ?  3áyears, 4ámonths ago

Only one App Engine application can be created per Project.
So it's B.

upvoted 14 times

? ?  omermahgoub  Most Recent ?  6ámonths, 1áweek ago

B. Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions.

To test an update to an App Engine application with production traffic before replacing the current version, you can deploy the update as a
new version in the App Engine application and split traffic between the new and current versions. This is known as a "blue-green"
deployment, and it allows you to test the new version with a portion of production traffic while the current version is still serving the
remainder of traffic.

To split traffic between the new and current versions, you can use the App Engine traffic splitting feature. This feature allows you to specify
the percentage of traffic that should be sent to each version, and it can be used to gradually ramp up traffic to the new version over time.
This allows you to test the new version with a small portion of traffic initially, and gradually increase the traffic as you become more
confident in the update.

upvoted 6 times

? ?  omermahgoub 6ámonths, 1áweek ago

Other options, such as deploying the update in a new VPC or as a new App Engine application, are not recommended for testing
updates with production traffic, as they can be more complex and may require additional steps to set up.

upvoted 2 times

? ?  TonytheTiger 6ámonths, 3áweeks ago

Answer B : You can use traffic splitting to specify a percentage distribution of traffic across two or more of the versions within a service.
Splitting traffic allows you to conduct A/B testing between your versions and provides control over the pace when rolling out features.
Traffic splitting is applied to URLs that do not explicitly target a version. For example, the following URLs split traffic because they target all
the available versions within the specified service:
https://cloud.google.com/appengine/docs/standard/splitting-traffic

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

243/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B is right , Option D is just to confuse you.
Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions.

upvoted 2 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

Versioning is supported in App Engine.

upvoted 1 times

? ?  ghadxx 1áyear, 4ámonths ago

Selected Answer: B

Versioning is supported in App Engine.

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for D,
The option B don┤t say with wich service will split the traffic.
The option D gives more datail and makes sense.

upvoted 1 times

? ?  ale_brd_ 7ámonths, 1áweek ago

No mate, only one app engine per project can be deployed, you can have multiple version on the same app tho. D is to confuse you. B
is the only feasible answer in here.

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

B is the correct answer

upvoted 1 times

? ?  robotgeek 1áyear, 7ámonths ago

A is not because "Instance Group Updater " is only for Computer Engine MIG

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û Deploy the update as a new version in AppEngine app, and split traffic between the new and current versions.
Traffic Splitting is feature of AppEngine for A/B testing.
https://cloud.google.com/appengine/docs/standard/python/splitting-traffic

upvoted 6 times

? ?  [Removed] 1áyear, 8ámonths ago

B is correct. App Engine supports versioning.

upvoted 1 times

? ?  unnikrisb 1áyear, 8ámonths ago

B is correct... Canary Testing -> Traffic Splitting

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

B. Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions

upvoted 3 times

? ?  un 2áyears, 1ámonth ago

B is the answer
upvoted 1 times

? ?  getzsagar 2áyears, 2ámonths ago

Answer - B
Configure how much traffic the version that you just deployed should receive.

By default, the initial version that you deploy to your App Engine application is automatically configured to receive 100% of traffic.
However, all subsequent versions that you deploy to that same App Engine application must be manually configured, otherwise they
receive no traffic.

For details about how to configure traffic for your versions, see Migrating and Splitting Traffic.
https://cloud.google.com/appengine/docs/admin-api/migrating-splitting-traffic

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

244/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #91

Topic 1

All Compute Engine instances in your VPC should be able to connect to an Active Directory server on speci c ports. Any other tra c emerging

from your instances is not allowed. You want to enforce this using VPC  rewall rules.

How should you con gure the  rewall rules?

A. Create an egress rule with priority 1000 to deny all tra c for all instances. Create another egress rule with priority 100 to allow the Active

Directory tra c for all instances.

B. Create an egress rule with priority 100 to deny all tra c for all instances. Create another egress rule with priority 1000 to allow the Active

Directory tra c for all instances.

C. Create an egress rule with priority 1000 to allow the Active Directory tra c. Rely on the implied deny egress rule with priority 100 to block

all tra c for all instances.

D. Create an egress rule with priority 100 to allow the Active Directory tra c. Rely on the implied deny egress rule with priority 1000 to block

all tra c for all instances.

Correct Answer: A

Community vote distribution

A (100%)

? ?  wk  Highly Voted ?  3áyears, 8ámonths ago

Should be A, there is no implied deny egress but only implied allow egress

https://cloud.google.com/vpc/docs/firewalls#default_firewall_rules

Every VPC network has two implied firewall rules. These rules exist, but are not shown in the Cloud Console:

The implied allow egress rule: An egress rule whose action is allow, destination is 0.0.0.0/0, and priority is the lowest possible (65535) lets
any instance send traffic to any destination, except for traffic blocked by GCP. Outbound access may be restricted by a higher priority
firewall rule. Internet access is allowed if no other firewall rules deny outbound traffic and if the instance has an external IP address or
uses a NAT instance. Refer to Internet access requirements for more details.

The implied deny ingress rule: An ingress rule whose action is deny, source is 0.0.0.0/0, and priority is the lowest possible (65535) protects
all instances by blocking incoming traffic to them. Incoming access may be allowed by a higher priority rule. Note that the default network
includes some additional rules that override this one, allowing certain types of incoming traffic.

upvoted 82 times

? ?  p4 2áyears, 7ámonths ago

from a book:
"Firewall rules control network traffic by blocking or allowing traffic into (ingress) or out of (egress) a network. Two implied firewall rules
are defined with VPCs: one blocks all incoming traffic, and the other allows all outgoing traffic. You can change this behavior
Virtual Private Clouds 115
116 Chapter 6 ? Designing Networks
by defining firewall rules with higher priority. Firewall rules have a priority specified by an integer from 0 to 65535, with 0 being the
highest priority and 65535 being the lowest."

so this confirms A

upvoted 9 times

? ?  kumarp6 2áyears, 8ámonths ago

B is correct...

upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

It is A, rest all do not make sense. If you think of any other option then go back and read about firewalls. Seriously you are not ready for
this exam.

upvoted 2 times

? ?  zr79 8ámonths, 2áweeks ago

thank you

upvoted 1 times

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

Agree Correct is A. There is no implied deny egress only deny ingress rule

upvoted 10 times

? ?  MyPractice 3áyears, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

245/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Agree with A . only Implied allow egress rule (or) Implied deny ingress rule.
There is No "Implied deny egress rule" which rules out C & D

upvoted 3 times

? ?  Emmarof  Most Recent ?  3ámonths, 2áweeks ago

The answer to this question is A.

Explanation:
To enforce the requirement that all Compute Engine instances in your VPC should be able to connect to an Active Directory server on
specific ports while blocking any other traffic emerging from instances, the following two egress rules should be created:

Create an egress rule with priority 1000 to deny all traffic for all instances.
Create another egress rule with priority 100 to allow the Active Directory traffic for all instances.
In this configuration, the rule that allows the AD traffic has a lower priority number than the rule that denies all other traffic. Therefore,
this rule should be evaluated first.

upvoted 1 times

? ?  Deb2293 3ámonths, 3áweeks ago

Selected Answer: A

It should be A.
It cannot be D as The Implied allow egress rule, with its action of ôallowö, allows all traffic out to the 0.0. 0.0/0 destination, which basically
means everywhere. The priority of the implied allow egress rule is the lowest possible, 65535. The implied deny ingress rule, with an action
of ôdenyö, blocks all incoming connections.

upvoted 1 times

? ?  8d31d36 4ámonths, 1áweek ago

The correct answer is B.

To enforce that all Compute Engine instances in a VPC can connect to an Active Directory server on specific ports while blocking any other
traffic, you should create an egress rule with a high priority (lower numerical value) to deny all traffic from all instances, and another
egress rule with a lower priority (higher numerical value) to allow traffic to the Active Directory server on the specific ports.

Option B creates the necessary egress rules in the correct order: a deny-all rule with a high priority (100), followed by an allow rule for the
Active Directory traffic with a lower priority (1000). This way, traffic to the Active Directory server is allowed, but all other traffic is denied.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

It is pretty straight forward question, It this case priority low should be allow and high priority rules deny all requests. A is right

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: A

https://cloud.google.com/vpc/docs/firewalls#priority_order_for_firewall_rules

upvoted 1 times

? ?  mv2000 11ámonths, 4áweeks ago
06/30/2022 Exam question.

upvoted 6 times

? ?  moiradavis 11ámonths, 2áweeks ago

Oh, really? I got this question on my exam 2 years ago, I did not expect to repeat this kind of questions in the current exam.

upvoted 1 times

? ?  Baumster 1áyear, 4ámonths ago

OT: why is there no way to mark questions for review/repeat later on?

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A.
While the priority is higher, the egress rule is more restricted.
While the priority is higher, the ingress rule is more free.

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

Selected Answer: A

A is correct answer

upvoted 1 times

? ?  vchrist 1áyear, 7ámonths ago

Selected Answer: A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

246/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

to understand rules priority:
https://cloud.google.com/vpc/docs/firewalls#priority_order_for_firewall_rules

upvoted 1 times

? ?  nqthien041292 1áyear, 7ámonths ago

Selected Answer: A

Vote A

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

A û create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to allow the
Active Directory traffic for all instances.
Default Firewall rules (aka implied rules) are following:
1) Egress traffic is allowed to all IP/ports.
2) Ingress traffic is disabled completely.
Both these rules have lowest priority (65535) and cannot be removed.
https://cloud.google.com/vpc/docs/firewalls#default_firewall_rules

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

A. Create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to allow the
Active Directory traffic for all instances.

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

A is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

247/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #92

Topic 1

Your customer runs a web service used by e-commerce sites to offer product recommendations to users. The company has begun experimenting

with a machine learning model on Google Cloud Platform to improve the quality of results.

What should the customer do to improve their model's results over time?

A. Export Cloud Machine Learning Engine performance metrics from Stackdriver to BigQuery, to be used to analyze the e ciency of the model.

B. Build a roadmap to move the machine learning model training from Cloud GPUs to Cloud TPUs, which offer better results.

C. Monitor Compute Engine announcements for availability of newer CPU architectures, and deploy the model to them as soon as they are

available for additional performance.

D. Save a history of recommendations and results of the recommendations in BigQuery, to be used as training data.

Correct Answer: D

Community vote distribution

D (100%)

? ?  ghadxx  Highly Voted ?  1áyear, 4ámonths ago

Selected Answer: D

Model performance is generally based on the volume of its training data input. The more the data, the better the model.

upvoted 15 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Yes, correctly said..This is actually a question for Data Engineer role

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I agree with you, D is right

upvoted 1 times

? ?  sgo cial  Highly Voted ?  11ámonths ago

Selected Answer: D

A,B,C is defining about the performance of ML but not the result....only the training data will give good ML result/predictions

upvoted 6 times

? ?  Deb2293  Most Recent ?  3ámonths, 3áweeks ago

Selected Answer: D

Best answer is D. Other 3 makes no sense

upvoted 1 times

? ?  PST21 4ámonths, 1áweek ago

Need to improve the model results and not performance .. hence D

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

Answer is D

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: D

Model performance is generally based on the volume of its training data input. The more the data, the better the model.

upvoted 3 times

? ?  sivre 1áyear, 3ámonths ago

The following insights and recommendations can be exported (to bigquery):
IAM recommender
VM machine type recommender
Managed instance group machine type recommender
Idle PD recommender
Idle VM recommender
Cloud SQL overprovisioned instance recommender
Cloud SQL idle instance recommender
Unattended project recommender
Cloud Run Service Identity recommender
https://cloud.google.com/recommender/docs/bq-export/export-recommendations-to-bq

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

248/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

None of this is correlated with Machine Learning, how can be D? looks more A the answer

upvoted 2 times

? ?  kimharsh 1áyear, 2ámonths ago

what we will do with metrics , it won't improve our Machine learning model , D is the closest answer , also it didn't say export it said
Save , which could be manually moving the data to BQ

upvoted 2 times

? ?  Pime13 1áyear, 5ámonths ago

Selected Answer: D

i vote D

upvoted 3 times

? ?  victory108 1áyear, 5ámonths ago

D. Save a history of recommendations and results of the recommendations in BigQuery, to be used as training data.

upvoted 2 times

? ?  LoveT 1áyear, 6ámonths ago

"training data" is the key in option "D" and that's the answer

upvoted 4 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: D

D seems to be the correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

249/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #93

Topic 1

A development team at your company has created a dockerized HTTPS web application. You need to deploy the application on Google Kubernetes

Engine (GKE) and make sure that the application scales automatically.

How should you deploy to GKE?

A. Use the Horizontal Pod Autoscaler and enable cluster autoscaling. Use an Ingress resource to load-balance the HTTPS tra c.

B. Use the Horizontal Pod Autoscaler and enable cluster autoscaling on the Kubernetes cluster. Use a Service resource of type LoadBalancer

to load-balance the HTTPS tra c.

C. Enable autoscaling on the Compute Engine instance group. Use an Ingress resource to load-balance the HTTPS tra c.

D. Enable autoscaling on the Compute Engine instance group. Use a Service resource of type LoadBalancer to load-balance the HTTPS tra c.

Correct Answer: B

Reference:

https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler

Community vote distribution

A (71%)

B (29%)

? ?  crypt0  Highly Voted ?  3áyears, 8ámonths ago

Why not using Ingress? (A)

upvoted 26 times

? ?  Smart 3áyears, 4ámonths ago

"Ingress is a Kubernetes resource that encapsulates a collection of rules and configuration for routing external HTTP(S) traffic to
internal services.

On GKE, Ingress is implemented using Cloud Load Balancing. When you create an Ingress in your cluster, GKE creates an HTTP(S) load
balancer and configures it to route traffic to your application."

Are you exposing multiple services through single IP address? Hence, do you need routing your traffic?

Correct answer is B.

upvoted 35 times

? ?  Smart 3áyears, 4ámonths ago

My bad, as stated by other, Service doesn't support L7 load balancing. Hence, need to setup ingress resource. Correct answer is A.

upvoted 36 times

? ?  tartar 2áyears, 10ámonths ago

B is ok.
https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app

upvoted 9 times

? ?  GopiSivanathan 2áyears, 8ámonths ago

service resource does a NLB using IP address, however, Ingress does HTTP(S) Load balancer. A should be an answer.

upvoted 8 times

? ?  techalik 2áyears, 7ámonths ago

I think A is OK:
upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

It is A, K8s best way to LB is Ingress.

upvoted 4 times

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

Name is service resource, it's B:

https://cloud.google.com/kubernetes-engine/docs/concepts/service?hl=es-419

upvoted 12 times

? ?  red_panda  Most Recent ?  1áweek, 5ádays ago

Selected Answer: B

Answer is B.
For a single service is more common and simple to configure a single Service with LoadBalancer HTTP(S).

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

250/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  kubinho 2áweeks ago

Guys, why are you tagging option , if it SEEM to be correct for you ?? Most of you are learning for an exam, are you 100% sure that this
option you are voting for is really correct? if you are not sure, just think, dont vote for nothing please, its really annoying

upvoted 2 times

? ?  Atanu 3áweeks ago

Selected Answer: B

B is perfect

upvoted 1 times

? ?  JohnWick2020 3áweeks, 5ádays ago

Answer is A.
Clue is "HTTP(S) web application". Always remember:
- LoadBalancer service spins up a "network load balancer" while
- Ingress is a native feature that spins up a "HTTPS load balancer" by default.

upvoted 1 times

? ?  Rothmansua 2ádays ago

Yes, but the application is already HTTPS (did you notice?), why additional HTTPS ingress in front?

upvoted 1 times

? ?  Danomine416 1ámonth, 4áweeks ago

There are four types of services that Kubernetes supports: ClusterIP, NodePort, LoadBalancer, and Ingress.

ClusterIP is the default service that enables the communication of multiple pods within the cluster.

A NodePort is the simplest networking type of all. It requires no configuration, and it simply routes traffic on a random port on the host to
a random port on the container.

LoadBalancer is the most commonly used service type for Kubernetes networking. It is a standard load balancer service that runs on each
pod and establishes a connection to the outside world, either to networks like the Internet, or within your datacenter.

An Ingress is a Kubernetes object that sits in front of multiple services and acts as an intelligent router. It defines how external traffic can
reach the cluster services, and it configures a set of rules to allow inbound connections to reach the services on the cluster.

upvoted 6 times

? ?  Rothmansua 2ádays ago

No, Ingress is not Kubernetes Service

upvoted 1 times

? ?  LaxmanTiwari 1ámonth, 2áweeks ago

Nice and detailed explanation. I agree with A.

upvoted 1 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Appreciate the way of explanation

upvoted 1 times

? ?  kratosmat 2ámonths, 4áweeks ago

Selected Answer: A

Is it possible to expose HTTPS with service? From my understanding no.

upvoted 1 times

? ?  feholen210 3ámonths, 1áweek ago

Selected Answer: A

A Seems Correct.
upvoted 1 times

? ?  zeekerblade 3ámonths, 2áweeks ago

Selected Answer: B

As the image is already https application.
Although both A and B work, but B should be easier to configure as A need to configure both exposing certificate and https backend
related configuration

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

Selected Answer: B

Ingress is not needed because there is a single service that needs to be load balanced

upvoted 2 times

? ?  n_nana 3ámonths, 3áweeks ago

Selected Answer: A

service of type load balance configure network LB not supporting HTTPS traffic

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

251/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Deb2293 3ámonths, 3áweeks ago

Selected Answer: A

To load-balance the HTTPS traffic, we should use an Ingress resource, which acts as a reverse proxy to route traffic to different services
based on the HTTP(S) header or the hostname. We can use the GKE Ingress controller to manage the Ingress resource, which will
automatically create and manage a Google Cloud Load Balancer to distribute traffic to the pods running the application

upvoted 2 times

? ?  SoftSami 3ámonths, 4áweeks ago

Selected Answer: B

B is the answer
upvoted 1 times

? ?  zerg0 4ámonths, 2áweeks ago

Selected Answer: B

B - recommended way. There is a lab to do exactly the same.

upvoted 2 times

? ?  RVivek 4ámonths, 3áweeks ago

Selected Answer: B

B is a complete solution
A , you have to still configure a service and a ingress controller , which is not mentioned in the answer

upvoted 1 times

? ?  CosminCiuc 5ámonths ago

I would go with option B. The containerized application already supports SSL communication. So creating a Service with a load balancer
should be enough.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

252/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #94

Topic 1

You need to design a solution for global load balancing based on the URL path being requested. You need to ensure operations reliability and end-

to-end in- transit encryption based on Google best practices.

What should you do?

A. Create a cross-region load balancer with URL Maps.

B. Create an HTTPS load balancer with URL Maps.

C. Create appropriate instance groups and instances. Con gure SSL proxy load balancing.

D. Create a global forwarding rule. Con gure SSL proxy load balancing.

Correct Answer: B

Reference:

https://cloud.google.com/load-balancing/docs/https/url-map

Community vote distribution

B (100%)

? ?  victory108  Highly Voted ?  2áyears, 1ámonth ago

B. Create an HTTPS load balancer with URL maps.

upvoted 12 times

? ?  betiy  Highly Voted ?  3áyears, 6ámonths ago

URL paths supported only in HTTP(S) Load balancing
https://cloud.google.com/load-balancing/docs/ssl/#FAQ

upvoted 5 times

? ?  examch  Most Recent ?  6ámonths ago

Selected Answer: B

B is the correct Answer,

Google Cloud HTTP(S) load balancers and Traffic Director use a Google Cloud configuration resource called a URL map to route HTTP(S)
requests to backend services or backend buckets.

For example, with an external HTTP(S) load balancer, you can use a single URL map to route requests to different destinations based on
the rules configured in the URL map:

Requests for https://example.com/video go to one backend service.
Requests for https://example.com/audio go to a different backend service.
Requests for https://example.com/images go to a Cloud Storage backend bucket.
Requests for any other host and path combination go to a default backend service.
URL maps are used with the following Google Cloud products:

External HTTP(S) Load Balancing (global, regional, and classic modes)
Internal HTTP(S) Load Balancing
Traffic Director

https://cloud.google.com/load-balancing/docs/url-map-concepts

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

B. Create an HTTPS load balancer with URL Maps.

An HTTPS load balancer is a type of load balancer that can distribute incoming HTTPS traffic to one or more back-end services, such as
Compute Engine instances or Google Kubernetes Engine clusters. It can also provide SSL/TLS termination, enabling you to use your own
SSL/TLS certificates and keys.

You can use URL Maps to configure the HTTPS load balancer to route traffic based on the URL path being requested. This allows you to set
up different URL paths to be served by different back-end services, providing a high level of flexibility in your load balancing configuration.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, creating a cross-region load balancer with URL Maps, is also a valid solution, but it is not specifically designed for end-to-end
in-transit encryption.

Option C, creating appropriate instance groups and instances and configuring SSL proxy load balancing, is not a complete solution for
global load balancing. SSL proxy load balancing is a feature that enables you to terminate SSL/TLS connections at the load balancer and
establish a new SSL/TLS connection between the load balancer and the back-end service. It is not a global load balancing solution in
and of itself.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

253/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Option D, creating a global forwarding rule and configuring SSL proxy load balancing, is not a complete solution for global load
balancing based on the URL path being requested. A global forwarding rule is a type of load balancing configuration that directs traffic
to a specific back-end service based on the IP address and port of the incoming request. It does not allow for routing based on the URL
path.

Regenerate

upvoted 1 times

? ?  TonytheTiger 6ámonths, 3áweeks ago

Answer B: URL maps used with global external HTTP(S) load balancers and regional external HTTP(S) load balancer support several
advanced traffic management features such as header-based traffic steering, weight-based traffic splitting, and request mirroring.

https://cloud.google.com/load-balancing/docs/https#url-maps

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Sbgani 9ámonths, 3áweeks ago

Selected Answer: B

UrlMaps are used to route requests to a backend service based on rules that you define for the host and path of an incoming URL.

upvoted 5 times

? ?  Sbgani 9ámonths, 3áweeks ago

https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_url_map ANS B

upvoted 2 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

https://cloud.google.com/load-balancing/docs/https/url-map

upvoted 4 times

? ?  AzureDP900 8ámonths, 2áweeks ago
thank you for pointing the link

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

Selected Answer: B

B is correct answer

upvoted 2 times

? ?  nqthien041292 1áyear, 7ámonths ago

Selected Answer: B

Vote B

upvoted 1 times

? ?  un 2áyears, 1ámonth ago

B is correct

upvoted 1 times

? ?  ccmcwolf 2áyears, 2ámonths ago

there are interl https load balancers they are regional https://cloud.google.com/load-balancing/docs/l7-internal

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B

upvoted 2 times

? ?  bnlcnd 2áyears, 4ámonths ago

confused with A vs B. A has the word "cross region" but finally find out HTTP/S Load Balancing is naturally global.
- B

upvoted 5 times

? ?  doumx 2áyears, 6ámonths ago

B easy

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

254/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

255/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #95

Topic 1

You have an application that makes HTTP requests to Cloud Storage. Occasionally the requests fail with HTTP status codes of 5xx and 429.

How should you handle these types of errors?

A. Use gRPC instead of HTTP for better performance.

B. Implement retry logic using a truncated exponential backoff strategy.

C. Make sure the Cloud Storage bucket is multi-regional for geo-redundancy.

D. Monitor https://status.cloud.google.com/feed.atom and only make requests if Cloud Storage is not reporting an incident.

Correct Answer: B

Reference:

https://cloud.google.com/storage/docs/json_api/v1/status-codes

Community vote distribution

B (100%)

? ?  bigob4ek  Highly Voted ?  3áyears, 7ámonths ago

Answer is B
You should use exponential backoff to retry your requests when receiving errors with 5xx or 429 response codes from Cloud Storage.
https://cloud.google.com/storage/docs/request-rate

upvoted 32 times

? ?  AzureDP900 8ámonths, 2áweeks ago
I agree with you, B should be right

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

It is B

upvoted 1 times

? ?  Sbgani  Highly Voted ?  9ámonths, 3áweeks ago
HTTP 408, 429, and 5xx response codes.

Exponential backoff algorithm
For requests that meet both the response and idempotency criteria, you should generally use truncated exponential backoff.

Truncated exponential backoff is a standard error handling strategy for network applications in which a client periodically retries a failed
request with increasing delays between requests.

An exponential backoff algorithm retries requests exponentially, increasing the waiting time between retries up to a maximum backoff
time. See the following workflow example to learn how exponential backoff works:

You make a request to Cloud Storage.

If the request fails, wait 1 + random_number_milliseconds seconds and retry the request.

If the request fails, wait 2 + random_number_milliseconds seconds and retry the request.

If the request fails, wait 4 + random_number_milliseconds seconds and retry the request.

And so on, up to a maximum_backoff time.

Continue waiting and retrying up to a maximum amount of time (deadline), but do not increase the maximum_backoff wait period
between retries
upvoted 8 times

? ?  omermahgoub  Most Recent ?  6ámonths, 1áweek ago

. Implement retry logic using a truncated exponential backoff strategy.

HTTP status codes of 5xx and 429 typically indicate that there is a temporary issue with the service or that the rate of requests is too high.
To handle these types of errors, it is generally recommended to implement retry logic in your application using a truncated exponential
backoff strategy.

Truncated exponential backoff involves retrying the request after an initial delay, and then increasing the delay exponentially for each
subsequent retry up to a maximum delay. This approach helps to reduce the number of failed requests and can improve the reliability of
your application.
upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

256/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Option A, using gRPC instead of HTTP for better performance, is not directly related to handling HTTP status codes of 5xx and 429.
gRPC is a high-performance RPC framework that can be used in place of HTTP, but it is not a solution for handling errors.

Option C, making sure the Cloud Storage bucket is multi-regional for geo-redundancy, may help improve the reliability of the service,
but it is not a solution for handling errors.

Option D, monitoring https://status.cloud.google.com/feed.atom and only making requests if Cloud Storage is not reporting an
incident, is not a practical solution for handling errors. This approach would require constantly monitoring the status page and could
result in significant delays in processing requests. Instead, it is generally recommended to implement retry logic in your application to
handle errors.
upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Sbgani 9ámonths, 3áweeks ago

Selected Answer: B

https://cloud.google.com/storage/docs/retry-strategy

upvoted 2 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

2xx û successful requests;
4xx, 5xx û failed requests;
3xx û requests that require redirect.
https://cloud.google.com/storage/docs/json_api/v1/status-codes

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B

upvoted 2 times

? ?  vincy2202 1áyear, 7ámonths ago

B is the correct answer

upvoted 2 times

? ?  nqthien041292 1áyear, 7ámonths ago

Selected Answer: B

Vote B

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û Implement retry logic using a truncated exponential backoff strategy.
Per HTTP status and error codes for JSON the status codes are:
2xx û successful requests;
4xx, 5xx û failed requests;
3xx û requests that require redirect.
https://cloud.google.com/storage/docs/json_api/v1/status-codes
429 û Too many requests: your app tries to use more that its limit, additional requests will fail. Decrease your clientÆs requests and/or use
truncated exponential backoff (used for all requests with 5xx and 429 errors).
https://cloud.google.com/storage/docs/retry-strategy

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

B. Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests.

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

This B. Implement retry logic using a truncated exponential backoff strategy.

upvoted 1 times

? ?  un 2áyears, 1ámonth ago

Answer is B.
Link provided by bigob4ek has details

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

257/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  CloudGenious 2áyears, 4ámonths ago

As per google, if you run into any issue as increase latency or erroe rate ,pause your ramp up this give cloudstorage more time to scale
your bucket . Best is backoff when 5xx ,429,408 response code

upvoted 2 times

? ?  bnlcnd 2áyears, 4ámonths ago

https://cloud.google.com/storage/docs/exponential-backoff
- B

upvoted 2 times

? ?  awadheshk 2áyears, 8ámonths ago

B is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

258/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #96

Topic 1

You need to develop procedures to test a disaster plan for a mission-critical application. You want to use Google-recommended practices and

native capabilities within GCP.

What should you do?

A. Use Deployment Manager to automate service provisioning. Use Activity Logs to monitor and debug your tests.

B. Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests.

C. Use gcloud scripts to automate service provisioning. Use Activity Logs to monitor and debug your tests.

D. Use gcloud scripts to automate service provisioning. Use Stackdriver to monitor and debug your tests.

Correct Answer: B

Community vote distribution

B (100%)

? ?  crypt0  Highly Voted ?  3áyears, 8ámonths ago

I think answer B is correct:
https://cloud.google.com/solutions/dr-scenarios-planning-guide

upvoted 49 times

? ?  nitinz 2áyears, 3ámonths ago

It is B, Google Best practice ---> never use scripts. They do not trust anyone else's code it seems.

upvoted 9 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 11 times

? ?  kumarp6 2áyears, 8ámonths ago

B is correct

upvoted 2 times

? ?  fraloca 2áyears, 5ámonths ago

https://cloud.google.com/solutions/dr-scenarios-planning-guide#test_your_plan_regularly

upvoted 2 times

? ?  passnow  Highly Voted ?  3áyears, 6ámonths ago

Boom, everyone studied and did their labs, stackdriver is google's recommended tool for monitoring and debbuging. I agree with u all
that B is the correct answer

upvoted 22 times

? ?  faridomu  Most Recent ?  2áweeks, 2ádays ago

Why not A?

upvoted 1 times

? ?  vmaheshwari 3áweeks, 3ádays ago

If someone has complete exam dump then please share it on yannickqueue@gmail.com

upvoted 1 times

? ?  Jlharidon 6ámonths, 2áweeks ago

Selected Answer: B

Deploy managment + Stackdriver trained ig GCSB

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right

upvoted 1 times

? ?  tycho 10ámonths, 2áweeks ago

in practice, D could work as well..

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

259/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  adacek1 4ámonths, 3áweeks ago

yeah, but only native solutions should be taken into consideration (as stated in requirements), so scripts are basically ruled out

upvoted 1 times

? ?  gaojun 1áyear, 2ámonths ago

Answer B is correct

upvoted 1 times

? ?  Skr6266 1áyear, 3ámonths ago

Selected Answer: B

Deployment Manager + Cloud Monitoring and Logging solution.

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

Selected Answer: B

B is the correct answer

upvoted 1 times

? ?  nqthien041292 1áyear, 7ámonths ago

Selected Answer: B

Vote B

upvoted 1 times

? ?  ganeshrev 1áyear, 7ámonths ago

Selected Answer: B

Google recommended Practice

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

B. Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests.

upvoted 4 times

? ?  un 2áyears, 1ámonth ago

B is correct

upvoted 1 times

? ?  hero_india 2áyears, 2ámonths ago

B in the answer
upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

260/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #97

Topic 1

Your company creates rendering software which users can download from the company website. Your company has customers all over the world.

You want to minimize latency for all your customers. You want to follow Google-recommended practices.

How should you store the  les?

A. Save the  les in a Multi-Regional Cloud Storage bucket.

B. Save the  les in a Regional Cloud Storage bucket, one bucket per zone of the region.

C. Save the  les in multiple Regional Cloud Storage buckets, one bucket per zone per region.

D. Save the  les in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region.

Correct Answer: A

Community vote distribution

D (77%)

A (20%)

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

Its D, create multi region buckets in Americas, Europe and Asia

upvoted 53 times

? ?  MyPractice 3áyears, 5ámonths ago

why " multiple Multi-Regional"? - A should be the right ans & addressing the global users - "More importantly, is that multiregional
heavily leverages Edge caching and CDNs to provide the content to the end user"
https://medium.com/google-cloud/google-cloud-storage-what-bucket-class-for-the-best-performance-5c847ac8f9f2

upvoted 12 times

? ?  xavi1 1áyear, 10ámonths ago

because a multi-regional includes all the locations of ONE region, not the others.

upvoted 12 times

? ?  Urban_Life 1áyear, 6ámonths ago
This can't be D. It should be A.

upvoted 5 times

? ?  AmitAr 1áyear, 1ámonth ago

What is point of Multi-Regional bucket, if this need to saved multiple times. I believe option (D) is for creating confusion only. It should
be (A)..

upvoted 7 times

? ?  turbo8p 7ámonths, 2áweeks ago

Check the current create bucket UI. You cannot select Asia multi-region and US multi-region at the same go. So to support global
customer, you need to create multiple Multi-region buckets.

upvoted 7 times

? ?  AmitAr 1áyear, 1ámonth ago

Read the question again.. I think (d) is correct.. eg. 1 bucket in US-multi-region, 2nd in AS-multi-region, 3rd in EU-multi-region

upvoted 5 times

? ?  giovanicascaes 3ámonths, 1áweek ago

Yes, D seems correct. There are 3 multi-regions: ASIA, EU and US. In order to be global, there must be multi-region buckets in this
3 locations.

Reference: https://cloud.google.com/storage/docs/locations#location-mr

upvoted 2 times

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

I would go with A (https://cloud.google.com/storage/docs/locations)

upvoted 31 times

? ?  Hgwells  Most Recent ?  4ádays, 20áhours ago

Selected Answer: D

Multi-region is a large geographic area such as US

upvoted 1 times

? ?  PKookNN 1áweek, 1áday ago

Selected Answer: D

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

261/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

to reduce latency you need a bucket near your users and you can't setup multi-region with Asia/EU/America selected so A is out and we
are left with D.
upvoted 1 times

? ?  red_panda 1áweek, 5ádays ago

Selected Answer: A

A is enought.
Multi-Regional bucket means that bucket automatically replicate contents in all region.
Really don't understand why the most voted ans is D. Make no sense to create multiple Multi-regional bucket!
Indeed, in my opinion it becomes even more expensive and unnecessarily complex to manage

upvoted 1 times

? ?  red_panda 1áweek, 5ádays ago

ERROR - Correct is D
According to Google documentation a multi-region is a geographic area such US or EU (with multiple zones). From question we have
customers over the world and we have to create one bucket per multi-region

upvoted 1 times

? ?  Atanu 2áweeks, 6ádays ago

Selected Answer: A

A is enough

upvoted 1 times

? ?  AmarReddy 3áweeks, 1áday ago

Answer A
Why not D? Google Cloud Storage does not allow you to create multiple Multi-Regional buckets within a single multi-region. The term
"Multi-Regional" means that data in these buckets is automatically distributed across multiple regions, within a general geographic area.
Thus, there is no need to create multiple Multi-Regional buckets in one multi-region because the purpose of Multi-Regional buckets is to
distribute the data across all regions within the selected multi-region automatically.

Option A is a simpler and more effective solution as it requires managing only one bucket, reducing management overhead, and ensuring
automatic data distribution across multiple regions for high availability and reduced latency.

upvoted 2 times

? ?  JohnWick2020 3áweeks, 5ádays ago

Answer is D period!
You create a multi-region bucket each for us, eu and asia - refer to "Location Type" on GCS console form.

upvoted 1 times

? ?  Beardream 1ámonth, 1áweek ago

Selected Answer: C

Why no one voted C? To optimize latency the Cloud Storage solution should be Regional Bucket
(https://cloud.google.com/storage/docs/locations) so we should choose between B or C, but the question says the customers are all over
the world, so remain only the answer C.

upvoted 1 times

? ?  lokiinaction 2ámonths, 3áweeks ago

Selected Answer: C

why no one goes for C? a multi regional bucket provide higher availability but this question is asking for reducing latency, so I will rather
go for C

upvoted 1 times

? ?  BiddlyBdoyng 2ámonths, 3áweeks ago

The problem with D is its a lot of manual duplication of data. Better upload once and use cdn. If Google thought uploading to every region
a good idea they'd have automated it.

upvoted 3 times

? ?  lokiinaction 3ámonths ago

according to GCP: https://cloud.google.com/storage/docs/locations only regional and dual region buckets are providing optimized latency.
Multi-region buckets are only good for providing highest availability. so Why you all select between A and D?

upvoted 2 times

? ?  telp 3ámonths, 2áweeks ago

Selected Answer: D

Cloud stotage multi-region has 3 regions possible NA, EU or Asia. Need the 3 buckets to be global so answer is D

upvoted 1 times

? ?  sithin_nair 3ámonths, 2áweeks ago

Selected Answer: D

Save the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region - Is the right answer.

upvoted 1 times

? ?  ile02 3ámonths, 4áweeks ago

D. Save the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region, is the recommended practice by Google to
minimize latency for customers worldwide. This ensures that the files are stored in multiple locations across the globe, allowing customers

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

262/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

to access them from the nearest location, reducing latency and improving performance. Additionally, Multi-Regional Cloud Storage is
designed for high durability and availability, ensuring that the files are always accessible and protected from any hardware failures or
disasters.

upvoted 3 times

? ?  telp 4ámonths ago

Selected Answer: D

world = EU, NA, ASIA region so need bucket multi region by big region so answer D
https://cloud.google.com/storage/docs/locations

upvoted 1 times

? ?  i_maddog_i 4ámonths ago

Selected Answer: D

As already pointed out, the answer is D.
The company has clients all over the world and we need to use all "Multi-Regions" such as EU ASIA and US.
https://cloud.google.com/storage/docs/locations#location-mr

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

263/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #98

Topic 1

Your company acquired a healthcare startup and must retain its customers' medical information for up to 4 more years, depending on when it was

created. Your corporate policy is to securely retain this data, and then delete it as soon as regulations allow.

Which approach should you take?

A. Store the data in Google Drive and manually delete records as they expire.

B. Anonymize the data using the Cloud Data Loss Prevention API and store it inde nitely.

C. Store the data in Cloud Storage and use lifecycle management to delete  les when they expire.

D. Store the data in Cloud Storage and run a nightly batch script that deletes all expired data.

Correct Answer: C

Community vote distribution

C (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Agree C

upvoted 20 times

? ?  i_maddog_i  Most Recent ?  4ámonths ago

Selected Answer: C

It's C

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I agree with C
upvoted 1 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam.

upvoted 4 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: C

go for C

upvoted 1 times

? ?  Dhiraj03 1áyear ago

Options C undoubtedly

upvoted 1 times

? ?  gaojun 1áyear, 2ámonths ago

Go for C

upvoted 1 times

? ?  [Removed] 1áyear, 4ámonths ago

I got similar question on my exam which involved life cycle management and bucket lock.

upvoted 3 times

? ?  Rajasa 1áyear, 6ámonths ago

Selected Answer: C

Go for C

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C

upvoted 1 times

? ?  vincy2202 1áyear, 7ámonths ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

264/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C is the correct answer

upvoted 1 times

? ?  gabrielzeven 1áyear, 7ámonths ago

D sounds like i would do it, but C sound like a lab or exam

upvoted 1 times

? ?  nqthien041292 1áyear, 7ámonths ago

Selected Answer: C

Vote C

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

C. Store the data in Cloud Storage and use lifecycle management to delete files when they expire.

upvoted 4 times

? ?  un 2áyears, 1ámonth ago

C is correct

upvoted 1 times

? ?  Pdk123 2áyears, 2ámonths ago

Agree C

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

265/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #99

Topic 1

You are deploying a PHP App Engine Standard service with Cloud SQL as the backend. You want to minimize the number of queries to the

database.

What should you do?

A. Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache before

issuing a query to Cloud SQL.

B. Set the memcache service level to dedicated. Create a cron task that runs every minute to populate the cache with keys containing query

results.

C. Set the memcache service level to shared. Create a cron task that runs every minute to save all expected queries to a key called

?Çcached_queries?Ç.

D. Set the memcache service level to shared. Create a key called ?Çcached_queries?Ç, and return database values from the key before using a

query to Cloud SQL.

Correct Answer: A

Community vote distribution

A (100%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

https://cloud.google.com/appengine/docs/standard/php/memcache/using

upvoted 20 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 11 times

? ?  dlzhang 2áyears ago

https://cloud.google.com/memorystore/docs/redis/redis-overview

upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

A is correct

upvoted 6 times

? ?  hiteshrup  Highly Voted ?  2áyears, 6ámonths ago

A dedicated memset is always better than shared until cost-effectiveness specify in the exam as objective. So Option C and D are ruled out.

From A and B, Option B is sending and updating query every minutes which is over killing. So reasonable option left with A which balance
performance and cost.

My answer will be A

upvoted 17 times

? ?  Sur_Nikki  Most Recent ?  1ámonth, 3áweeks ago

Best is A

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago
A is fine.. dedicated mem cache

upvoted 1 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam.

upvoted 5 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: A

https://cloud.google.com/appengine/docs/standard/php/memcache/using

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

266/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  gaojun 1áyear, 2ámonths ago
Obviously, the answer is A

upvoted 1 times

? ?  ehgm 1áyear, 6ámonths ago

Selected Answer: A

Dedicated and shared will resolve the problem, the key is: store all queries in only one key "cached_queries" is not good, we have limits:
https://cloud.google.com/appengine/docs/standard/python/memcache
Create a key of each query is better.

upvoted 3 times

? ?  vincy2202 1áyear, 7ámonths ago

A is the correct answer

upvoted 2 times

? ?  nqthien041292 1áyear, 7ámonths ago

Selected Answer: A

Vote A

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

A. Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache
before issuing a query to Cloud SQL.

upvoted 3 times

? ?  un 2áyears, 1ámonth ago

A is correct

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  ga 2áyears, 4ámonths ago

A is correct

upvoted 1 times

? ?  BobBui 2áyears, 4ámonths ago

My answer is A
upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

267/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #100

Topic 1

You need to ensure reliability for your application and operations by supporting reliable task scheduling for compute on GCP. Leveraging Google

best practices, what should you do?

A. Using the Cron service provided by App Engine, publish messages directly to a message-processing utility service running on Compute

Engine instances.

B. Using the Cron service provided by App Engine, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic using a message-

processing utility service running on Compute Engine instances.

C. Using the Cron service provided by Google Kubernetes Engine (GKE), publish messages directly to a message-processing utility service

running on Compute Engine instances.

D. Using the Cron service provided by GKE, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic using a message-processing

utility service running on Compute Engine instances.

Correct Answer: B

Community vote distribution

B (86%)

14%

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

Answer is B

upvoted 31 times

? ?  Smart  Highly Voted ?  3áyears, 4ámonths ago

B is correct. More appropriately: https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine

upvoted 21 times

? ?  fraloca 2áyears, 5ámonths ago

https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine#schedule-compute-engine

upvoted 3 times

? ?  salim_  Most Recent ?  1ámonth, 3áweeks ago

Selected Answer: B

https://cloud.google.com/blog/products/gcp/reliable-task-scheduling-on-google-compute-engine

upvoted 1 times

? ?  rr4444 3ámonths, 2áweeks ago

Something feels missing/broken about this question

Even before comments in discussion that correctly mentioned Cloud Scheduler, which is not mentioned in the question

upvoted 4 times

? ?  dataqueen_3110 5ámonths, 1áweek ago

"By using Cloud Scheduler for scheduling and Pub/Sub for distributed messaging, you can build an application to reliably schedule tasks
across a fleet of Compute Engine instances." https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine

Answer is B. (Note: It was down to B or D but containerization was not mentioned)

upvoted 1 times

? ?  beehive 5ámonths, 4áweeks ago

Answer is B.
Cloud Scheduler provides a fully managed, enterprise-grade service that lets you schedule events. After you have scheduled a job, Cloud
Scheduler will call the configured event handlers, which can be App Engine services, HTTP endpoints, or Pub/Sub subscriptions.

To run tasks on your Compute Engine instance in response to Cloud Scheduler events, you need to relay the events to those instances.
One way to do this is by calling an HTTP endpoint that runs on your Compute Engine instances. Another option is to pass messages from
Cloud Scheduler to your Compute Engine instances using Pub/Sub.

upvoted 2 times

? ?  habros 7ámonths ago

Selected Answer: B

A and C are outà messages are to be sent to pub sub and processed using a client. D is overkill for this purpose

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

268/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right

upvoted 1 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: B

Ans is B https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine

upvoted 1 times

? ?  Sbgani 9ámonths, 3áweeks ago

Ans is B https://cloud.google.com/architecture/reliable-task-scheduling-compute-engine

refer the examples with diagram

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

the link points to use Cloud Scheduler, and not Cron service provided by App Engine.

upvoted 3 times

? ?  zr79 8ámonths, 2áweeks ago

This is the new way to run schedule

upvoted 2 times

? ?  FAD04 10ámonths ago

I got this question in exam 01/09/2022

upvoted 5 times

? ?  pp0709 10ámonths ago

Selected Answer: D

This solution can be implemented using both A and D
1) With App Engine - https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml
2) With GKE - https://cloud.google.com/kubernetes-engine/docs/how-to/cronjobs

They ask for best practices and it's well known that GKE (aka containers) is the best practice for building modern infra solution.

Yet another confusing PCA question on the card. Honestly, think the quality of the questions can be mightily improved.

upvoted 1 times

? ?  medi01 2ámonths, 1áweek ago

A is a bad solution as "send message directly to the utility" is not really reliable, you'd want pub/sub in between.

upvoted 1 times

? ?  BiddlyBdoyng 8ámonths, 1áweek ago

GKE is too expensive if all you are after is cron scheduling.

upvoted 1 times

? ?  pp0709 10ámonths ago

Sorry, can be implemented using both B and D

upvoted 2 times

? ?  6721sora 10ámonths, 1áweek ago

B says Appengine.
But Cloud Scheduler is itself a managed service.
To schedule jobs via AppEngine, the cron.yaml has to be used.
It can be done similarly via GKE as well.
This question is confusing

upvoted 2 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam.

upvoted 6 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

So, did u passed? If yes then congratulations and please let us know the answer of this question

upvoted 1 times

? ?  ale_brd_ 7ámonths, 1áweek ago
and? speak your thoughts

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

269/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  AzureDP900 12ámonths ago

B is correct , there is no need of GKE for this scenario.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

270/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #101

Topic 1

Your company is building a new architecture to support its data-centric business focus. You are responsible for setting up the network. Your

company's mobile and web-facing applications will be deployed on-premises, and all data analysis will be conducted in GCP. The plan is to

process and load 7 years of archived .csv  les totaling 900 TB of data and then continue loading 10 TB of data daily. You currently have an

existing 100-MB internet connection.

What actions will meet your company's needs?

A. Compress and upload both archived  les and  les uploaded daily using the gsutil ?Ç"m option.

B. Lease a Transfer Appliance, upload archived  les to it, and send it to Google to transfer archived data to Cloud Storage. Establish a

connection with Google using a Dedicated Interconnect or Direct Peering connection and use it to upload  les daily.

C. Lease a Transfer Appliance, upload archived  les to it, and send it to Google to transfer archived data to Cloud Storage. Establish one Cloud

VPN Tunnel to VPC networks over the public internet, and compress and upload  les daily using the gsutil ?Ç"m option.

D. Lease a Transfer Appliance, upload archived  les to it, and send it to Google to transfer archived data to Cloud Storage. Establish a Cloud

VPN Tunnel to VPC networks over the public internet, and compress and upload  les daily.

Correct Answer: B

Community vote distribution

B (100%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago
With option A, daily data would take 27 hours.
My answer is B.
How do you think?

upvoted 48 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 9 times

? ?  kumarp6 2áyears, 8ámonths ago

B is correct

upvoted 5 times

? ?  Jay_82 2áyears, 6ámonths ago

ok but dedicated connection is available from 10 GBPS right where as in question it says internet connection is 100 MB, to me D is
correct.

upvoted 5 times

? ?  9xnine 1áyear ago

Dedicated Interconnect will be a new connection and will not run over the existing internet connection. With dedicated
interconnect the existing ISP becomes irrelevant. If you were trying to use VPN the existing internet connection would be
relevant. Answer is B.

upvoted 4 times

? ?  Jay_82 2áyears, 6ámonths ago

ok but dedicated connection is available from 10 GBPS right where as in question it says internet connection is 100 MB, to me D is
correct.

upvoted 1 times

? ?  Alekshar 2áyears, 3ámonths ago

The 100MB is the internet connection Dedicated Interconnect means physically connecting the on-prem server and google with a
new network cable, I do hope the internal company network is not limited to only 100MB

upvoted 4 times

? ?  nitinz 2áyears, 3ámonths ago

it is B

upvoted 6 times

? ?  wk  Highly Voted ?  3áyears, 8ámonths ago

Agree B. 100Mbps connections for 10TB data transfer is takes too long

https://cloud.google.com/solutions/transferring-big-data-sets-to-gcp#close

upvoted 20 times

? ?  JJu 3áyears, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

271/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

not 100Mbps. 100MB

upvoted 3 times

? ?  misho 3áyears ago

even with 100MB internet it's slow. It's 800 Mbps and transfer for 10 TB will take 2 days

upvoted 5 times

? ?  bogd 2áyears, 4ámonths ago

There is no such thing as a "100MB" internet connection :) . That must be a speed (per second), and I would guess that the "B" is just
a typo (it is highly atypical to measure bandwidth in Bps).

upvoted 5 times

? ?  telp  Most Recent ?  4ámonths ago

Selected Answer: B

Answer B => Dedicated interconnect will provide a private network with 10gbs. The internet limited to 100 mb is not possible to use cloud
VPN ( it will use public internet so be limited for the daily)

upvoted 1 times

? ?  sunny2421 6ámonths ago

B is correct.

upvoted 1 times

? ?  habros 7ámonths ago

Selected Answer: B

B. Since it is a new network just sign up for a dedicated lineà

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Balaji_Sakthi 8ámonths ago

its option B. i think

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

you can not use gsutil to load 10TB daily >>>and then continue loading 10 TB of data daily<<< it will take longer than 24hrs to upload
using gsutil

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is the best, VPN doesn't scale very well for huge data

upvoted 2 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: B

B is the answer.

To support daily 10TB upload, you need Dedicated Interconnect who has at least 10Gbps.

upvoted 2 times

? ?  pp0709 10ámonths ago

Selected Answer: B

Key is "Daily transfer of 10 TB"- Any data transfer that requires more than 3 Gigs in bandwidth, VPN is out of equation.

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago
3Gps not 3Gigs bandwidth

upvoted 1 times

? ?  desertlotus1211 11ámonths ago

Answer is B. The question asks what action would the company need to take to support the requirement...

upvoted 2 times

? ?  AzureDP900 12ámonths ago

I would go with B , VPN is not viable option to transfer that much date with 100mbps.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

272/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  chelovalpo 1áyear ago

My answer is B.
upvoted 1 times

? ?  cpi_web 1áyear, 1ámonth ago

Hmm. Not easy. Everybody tends to B. But I think that can't be correct for the simple reason...

A is obviously not an option because of the initial amount of data. It could be that through compression the amount can be extremely
reduced (I saw examples down to 5% of the original size). But the assumption at that point is, that the entropy is high => that means the
compression is not necessarily the most important factor.

B: The answer doesn't say anything about the reach-ability of the interconnect / peering (meaning IXP's). So one has to assume that they
CAN'T be reached. That means that only C/D can be correct.

I would tend to C because Google obviously would always recommend their own tools (gsutil). Then the hope would be that the
compression and the bandwidth of the VPN is enough to get accomplish the job successfully.

upvoted 3 times

? ?  9xnine 1áyear ago

Dedicated Interconnect will be a new connection and will not run over the existing internet connection. With dedicated interconnect the
existing ISP becomes irrelevant. If you were trying to use VPN the existing internet connection would be relevant. Answer is B.

upvoted 2 times

? ?  gaojun 1áyear, 2ámonths ago

go for B cause "900 TB of data and then continue loading 10 TB of data daily. You currently have an existing 100-MB internet connection."

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

273/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #102

Topic 1

You are developing a globally scaled frontend for a legacy streaming backend data API. This API expects events in strict chronological order with

no repeat data for proper processing.

Which products should you deploy to ensure guaranteed-once FIFO ( rst-in,  rst-out) delivery of data?

A. Cloud Pub/Sub alone

B. Cloud Pub/Sub to Cloud Data ow

C. Cloud Pub/Sub to Stackdriver

D. Cloud Pub/Sub to Cloud SQL

Correct Answer: D

Reference:

https://cloud.google.com/pubsub/docs/ordering

Community vote distribution

B (62%)

A (38%)

? ?  exampanic  Highly Voted ?  3áyears, 5ámonths ago

I believe the answer is B. "Pub/Sub doesn't provide guarantees about the order of message delivery. Strict message ordering can be
achieved with buffering, often using Dataflow." https://cloud.google.com/solutions/data-lifecycle-cloud-platform

upvoted 56 times

? ?  TiagoM 2áyears, 2ámonths ago

Now Pub/Sub guarantees message order. Until the exam does not change I would pick B.

upvoted 5 times

? ?  jask 1áyear, 9ámonths ago

Answer is B. The question is talking about guaranteed-once FIFO delivery of data. Although Pub/sub provides data in order (FIFO)
but it does 'at-least' once delivery of data. So, we need Dataflow for deduplication of data.

upvoted 7 times

? ?  emirhosseini 8ámonths, 3áweeks ago

I believe Pub/Sub now also supports exactly once delivery (in preview):
https://cloud.google.com/pubsub/docs/exactly-once-delivery

upvoted 8 times

? ?  melono 8ámonths, 2áweeks ago

Pub/Sub supports exactly-once delivery, within a cloud region.
The question states ¿global¿, so needs Dataflow

upvoted 5 times

? ?  CosminCiuc 5ámonths ago

I believe that only the frontend is scaled globally. The backend API is the one that requires ordered delivery of the messages and
guaranteed-once delivery of data. Currently, Pub/Sub supports ordered delivery within the same region
(https://cloud.google.com/pubsub/docs/ordering#receiving_messages_in_order) and exactly-once delivery within the same
region (https://cloud.google.com/pubsub/docs/exactly-once-delivery#exactly-once_delivery_guarantees).
The right answer could be A, Pub/Sub alone.

upvoted 1 times

? ?  melono 8ámonths, 2áweeks ago

https://cloud.google.com/pubsub/docs/exactly-once-delivery
reference

upvoted 1 times

? ?  zanfo 1áyear, 3ámonths ago

the correct is B https://cloud.google.com/pubsub/docs/stream-messages-dataflow

upvoted 1 times

? ?  xhova  Highly Voted ?  3áyears, 1ámonth ago

B is the answer. CloudSQL is only for storage, to get the messages in order you need timestamp processed in dataflow to arrange them
before putting it in any storage volume. The system described is not querying a db it is expecting a stream of messages only dataflow can
correct the order. ACID has no value here because the db is not being queried. You'll not find any documentation on pub/sub order being
corrected with a db. See notes below on pub/sub and dataflow using timestamps and windows to ensure order

https://cloud.google.com/pubsub/docs/pubsub-dataflow

upvoted 28 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

274/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  jlambdan  Most Recent ?  2áweeks, 2ádays ago

Selected Answer: A

https://cloud.google.com/pubsub/docs/exactly-once-delivery

upvoted 1 times

? ?  AmarReddy 3áweeks, 1áday ago

Answer: B

upvoted 1 times

? ?  TheCloudGuruu 1ámonth, 2áweeks ago

Selected Answer: B

Pub/Sub and Dataflow

upvoted 1 times

? ?  Hisayuki 2ámonths, 1áweek ago

Selected Answer: B

Pub/Sub is an at-least-once service. So you need to deduplicate messages with DataFlow as a data pipeline.

upvoted 1 times

? ?  JC0926 2ámonths, 2áweeks ago

Selected Answer: B

B
Cloud Pub/Sub to Cloud Dataflow: Cloud Dataflow is a managed data processing service that can be used to process and transform
streaming data. By using Cloud Dataflow with Cloud Pub/Sub, you can implement custom processing logic to ensure data is delivered in
strict chronological order with no repeats. This combination allows you to achieve guaranteed-once FIFO delivery.

upvoted 1 times

? ?  jlambdan 3ámonths ago

Selected Answer: A

https://cloud.google.com/pubsub/docs/ordering

upvoted 1 times

? ?  enado 1áday, 17áhours ago

The Answer should be B simply because the question specifically stated that the solution will be globally scaled.

this same link states:
"If messages have the same ordering key and are in the same REGION, you can enable message ordering and receive the messages in
the order that the Pub/Sub service receives them."

https://cloud.google.com/pubsub/docs/ordering#receiving_messages_in_order

upvoted 1 times

? ?  8d31d36 4ámonths, 1áweek ago

Selected Answer: B

cloud sql does not gurantee FIFO, but cloud data flow does on real time streams and historical batch data

upvoted 2 times

? ?  zerg0 4ámonths, 2áweeks ago

Selected Answer: A

Pub/Sub covers the order and the exact once delivery.

upvoted 2 times

? ?  jay9114 5ámonths, 3áweeks ago

Selected Answer: A

I choose A. As of today, PubSub can single-handedly deal with ensuring the messages are delivery once and in order.

Only once deliver - https://cloud.google.com/pubsub/docs/exactly-once-delivery
Ordering of messages - https://cloud.google.com/pubsub/docs/ordering#receiving_messages_in_order

upvoted 7 times

? ?  honmet 4ámonths, 1áweek ago

the messages need to be from a single region for pub/sub ordering to work.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

To ensure guaranteed-once FIFO (first-in, first-out) delivery of data from a globally scaled frontend to a legacy streaming backend data
API, you can use Cloud Pub/Sub in combination with Cloud Dataflow.

Cloud Pub/Sub is a fully-managed messaging service that enables you to send and receive messages between independent systems. It
provides a range of features to ensure the delivery of messages, including at-least-once delivery, ordering guarantees, and message
deduplication.

Cloud Dataflow is a fully-managed, cloud-native data processing service that allows you to build and deploy data processing pipelines that
can process unbounded streams of data. It can be used to process data from Cloud Pub/Sub in a streaming fashion, ensuring that

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

275/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

messages are processed in strict chronological order with no repeat data.

Together, Cloud Pub/Sub and Cloud Dataflow can provide a reliable, scalable solution for delivering data from a globally scaled frontend to
a legacy streaming backend data API, ensuring guaranteed-once FIFO delivery of the data.

upvoted 6 times

? ?  [Removed] 7ámonths ago

With Cloud Pub/Sub FIFO all transactions will be sequenced in the same order as transactions are received and Cloud Dataflow will ensure
exactly once processing.

upvoted 1 times

? ?  SureshbabuK 7ámonths ago

Selected Answer: A

Only once deliver - https://cloud.google.com/pubsub/docs/exactly-once-delivery
Ordering of messages - https://cloud.google.com/pubsub/docs/ordering#receiving_messages_in_order

both supported by Cloud Pubsub

upvoted 6 times

? ?  Tutun_Aus 7ámonths, 2áweeks ago

B is correct

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right

upvoted 1 times

? ?  satyagade 9ámonths, 2áweeks ago

Selected Answer: B

Dataflow deduplicates messages with respect to the Pub/Sub message ID

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

276/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #103

Topic 1

Your company is planning to perform a lift and shift migration of their Linux RHEL 6.5+ virtual machines. The virtual machines are running in an

on-premises

VMware environment. You want to migrate them to Compute Engine following Google-recommended practices. What should you do?

A. 1. De ne a migration plan based on the list of the applications and their dependencies. 2. Migrate all virtual machines into Compute Engine

individually with Migrate for Compute Engine.

B. 1. Perform an assessment of virtual machines running in the current VMware environment. 2. Create images of all disks. Import disks on

Compute Engine. 3. Create standard virtual machines where the boot disks are the ones you have imported.

C. 1. Perform an assessment of virtual machines running in the current VMware environment. 2. De ne a migration plan, prepare a Migrate for

Compute Engine migration RunBook, and execute the migration.

D. 1. Perform an assessment of virtual machines running in the current VMware environment. 2. Install a third-party agent on all selected

virtual machines. 3. Migrate all virtual machines into Compute Engine.

Correct Answer: C

The framework illustrated in the preceding diagram has four phases:

?Çó Assess. In this phase, you assess your source environment, assess the workloads that you want to migrate to Google Cloud, and assess

which VMs support each workload.

?Çó Plan. In this phase, you create the basic infrastructure for Migrate for Compute Engine, such as provisioning the resource hierarchy and

setting up network access.

?Çó Deploy. In this phase, you migrate the VMs from the source environment to Compute Engine.

?Çó Optimize. In this phase, you begin to take advantage of the cloud technologies and capabilities.

Reference:

https://cloud.google.com/architecture/migrating-vms-migrate-for-compute-engine-getting-started

Community vote distribution

C (95%)

5%

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

Ans ) C ,
Migrate for Compute Engine organizes groups of VMs into Waves. After understanding the dependencies of your applications, create
runbooks that contain groups of VMs and begin your migration!
https://cloud.google.com/migrate/compute-engine/docs/4.5/how-to/migrate-on-premises-to-gcp/overview

upvoted 23 times

? ?  technodev  Highly Voted ?  1áyear, 5ámonths ago

Selected Answer: C

I got this question in my exam.

upvoted 11 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Did u passed...? If yes, then Congratulations and let me know the correct answer

upvoted 1 times

? ?  salim_  Most Recent ?  1ámonth, 2áweeks ago

Selected Answer: C

https://cloud.google.com/migrate/compute-engine/docs/4.11/how-to/migrate-on-premises-to-gcp/overview

upvoted 1 times

? ?  8d31d36 4ámonths, 1áweek ago

Selected Answer: B

The reason why Option B is preferable over Option C is that it involves creating images of all disks and importing them into Compute
Engine, which can significantly reduce the amount of time required for the migration. Additionally, creating standard virtual machines
from the imported disks is a straightforward process, and it ensures that the migrated virtual machines are identical to the on-premises
virtual machines, which can simplify the migration process and minimize the risk of compatibility issues.

upvoted 1 times

? ?  examch 5ámonths, 4áweeks ago

Selected Answer: C

C is the correct answer,

Runbooks are created from the Migrate for Compute Engine Manager. The system queries VMware or AWS for VMs and generates a CSV

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

277/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

for you to edit.

By editing the CSV, you define:

The VMs in a wave.
The order in which those VMs are migrated.
The type and disk space of VMs that are launched on Google Cloud.
Other characteristics that are defined in the Runbook reference.

https://cloud.google.com/migrate/compute-engine/docs/4.8/how-to/organizing-migrations/creating-and-modifying-
runbooks#generating_runbook_templates

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago
C is most suitable for this use

upvoted 1 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam.

upvoted 5 times

? ?  AzureDP900 12ámonths ago

C is right, It defines all logical steps to migrate on-premise to google cloud.

upvoted 2 times

? ?  meokey 1áyear, 2ámonths ago

Does Ans. C) still valid as of latest GCE 5.0?
in the doc "Migrating VM groups" with version GCE 5.0, I do not see "runbook" anymore which is explained up to version GCE 4.8.
https://cloud.google.com/migrate/compute-engine/docs/5.0/how-to/migrating-vm-groups

upvoted 2 times

? ?  gaojun 1áyear, 2ámonths ago

Go for C

upvoted 1 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: C

I got this question on my exam. Answered C.

upvoted 2 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Thanks

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C.

upvoted 1 times

? ?  nikiwi 1áyear, 6ámonths ago

why not A?
seems pretty obvious if you look at the google doc: https://cloud.google.com/migrate/compute-engine/docs/5.0/concepts/lifecycle

upvoted 1 times

? ?  atlasga 1áyear, 6ámonths ago

When you are doing cloud migrations, you do migrations in "waves" which are groupings of one or more applications/workloads.
Moving machines individually would break things, such as dependencies. This is standard industry practice.

upvoted 3 times

? ?  vincy2202 1áyear, 7ámonths ago

C is the correct answer.

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 2 times

? ?  AnilKr 1áyear, 10ámonths ago

Ans - C, Migrate for compute engine(previously known as Velostrata) organizes group of VMs into Waves. Post understanding the
dependencies of applications create runbook and start migration

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

278/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

279/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #104

Topic 1

You need to deploy an application to Google Cloud. The application receives tra c via TCP and reads and writes data to the  lesystem. The

application does not support horizontal scaling. The application process requires full control over the data on the  le system because concurrent

access causes corruption. The business is willing to accept a downtime when an incident occurs, but the application must be available 24/7 to

support their business operations. You need to design the architecture of this application on Google Cloud. What should you do?

A. Use a managed instance group with instances in multiple zones, use Cloud Filestore, and use an HTTP load balancer in front of the

instances.

B. Use a managed instance group with instances in multiple zones, use Cloud Filestore, and use a network load balancer in front of the

instances.

C. Use an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk, and use an HTTP

load balancer in front of the instances.

D. Use an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk, and use a network

load balancer in front of the instances.

Correct Answer: D

Reference:

https://cloud.google.com/compute/docs/instance-groups

Community vote distribution

D (76%)

B (24%)

? ?  VishalB  Highly Voted ?  1áyear, 11ámonths ago

Correct Ans : D
Since the Traffic is TCP, Ans A & C gets eliminated as HTTPS load balance is not supported.
B - File storage system is Cloud Firestore which do not give full control, hence eliminated.
D - Unmanaged instance group with network load balance with regional persistent disk for storage gives full control which is required for
the migration.
upvoted 47 times

? ?  poseidon24 1áyear, 11ámonths ago

almost all good, except for File Storage, is not Cloud Firestore, it is a new service for sharing filesystems across VMs (like a NAS in a
traditional infrastructure).

upvoted 10 times

? ?  kimharsh 1áyear ago

what about the fact that is the unmanaged instance group is not regional , so you can't create it in more than 1 zone ?

upvoted 5 times

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

Ans ) D , unmanaged instance group as application does not support horizontal scaling and network load balancer as no mention of http
traffic .

upvoted 26 times

? ?  TheCloudGuruu  Most Recent ?  1ámonth, 2áweeks ago

Selected Answer: D

must be unmanaged

upvoted 1 times

? ?  JC0926 3ámonths, 1áweek ago

Selected Answer: D

Since the application does not support horizontal scaling, a managed instance group is not required. Instead, an unmanaged instance
group can be used to ensure that the application runs on multiple instances in different zones for high availability.
The network load balancer is designed to handle TCP and UDP traffic
The HTTP(S) load balancer is designed specifically for HTTP and HTTPS traffic.

upvoted 6 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago
Thanks for the apt explanation

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

An unmanaged instance group allows you to create and manage a group of Compute Engine instances manually, rather than using an
autoscaling solution like a managed instance group. This is appropriate for an application that does not support horizontal scaling, as you
can manually create and manage the number of instances needed to meet the traffic demands.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

280/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

To ensure high availability and minimize downtime, you should deploy the instances in different zones and use a regional persistent disk
to store the application's data. This will ensure that the application is still available even if one of the instances or a zone experiences an
outage.

A network load balancer should be used in front of the instances to distribute traffic to the instances. A network load balancer is a highly
available and scalable load balancing solution that operates at the network layer and can handle high volumes of traffic. It can also
balance traffic across multiple zones to ensure that the application is always available to users.

upvoted 13 times

? ?  omermahgoub 6ámonths, 1áweek ago

Therefore, the correct answer is option D: Use an unmanaged instance group with an active and standby instance in different zones,
use a regional persistent disk, and use a network load balancer in front of the instances.

upvoted 7 times

? ?  oms_muc 6ámonths, 1áweek ago

Selected Answer: D

Regional Persistent Disk, as App requires full control of filesystem data without concurrent access (block storage vs. file storage (NAS).
https://cloud.google.com/compute/docs/instance-groups
https://cloud.google.com/load-balancing/docs/choosing-load-balancer

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: D

Option A & B eliminated because we cannot use managed instance group since the app does not support Horizontal scaling
Option C > HTTP load balancer is Layer 7 & application is receiving traffic via TCP
Option D > is best answer because we are using Network load balancer Layer 4 which meets the condition "application receives traffic via
TCP"

upvoted 4 times

? ?   ercedog 6ámonths, 2áweeks ago

Selected Answer: D

Checking the comparative analysis of storage options, we can see that Filestore is not suitable for the workload, hence A and B are out. C
is out because it restricts to HTTP traffic. Answer is D

upvoted 2 times

? ?   ercedog 6ámonths, 2áweeks ago

https://cloud.google.com/architecture/storage-advisor#comparative_analysis

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  SerGCP 8ámonths ago

Selected Answer: B

D is not possibile you cannot create a regional unmanaged instance group. https://cloud.google.com/compute/docs/instance-
groups/creating-groups-of-unmanaged-instances

upvoted 3 times

? ?  KyubiBlaze 6ámonths, 2áweeks ago

Downtime is acceptable, Disk is regional, in case of issue the unmanaged instance group can be moved to other zone, disk has data.
A&B are not at all an option

upvoted 2 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: D

D. Use an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk, and use a
network load balancer in front of the instances.
D is the only option as the application does not support horizontal scaling (no MIG), it needs full control (no filestore) and has TCP traffic
(no HTTP LB).
upvoted 1 times

? ?  Santanu_01 8ámonths, 3áweeks ago

D is more appropriate solution ---In the ques. it is mentioned done not support horizontal scaling --> hence Unmanaged Instance , and
traffic is TCP --- hence N/W
Load balancer
upvoted 1 times

? ?  Rajeev26 8ámonths, 3áweeks ago

Selected Answer: D

does not support horizontal scaling so MIG not needed. TCP traffic so NW LB is ok

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

281/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: D

concurrent access causes corruption - it can not be a managed group

upvoted 1 times

? ?  alexandercamachop 9ámonths, 3áweeks ago

Selected Answer: D

Network load balancer - it says receives traffic in TCP which is located in Layer 4 (Network)>
This discards A, C
So now between B or D, it is simple, managed instance group supports horizontal scaling, so it have to be D, unmanaged instance group.

upvoted 3 times

? ?  Jay_Krish 9ámonths, 3áweeks ago

Wonder how the answer is D? Doesn't sound logical when reading the documentation about unmanaged instance groups. To me answer
D doesn't sound logical.

https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-unmanaged-instances

An unmanaged instance group is a collection of virtual machines (VMs) that reside in a single zone, VPC network, and subnet. An
unmanaged instance group is useful for grouping together VMs that require individual configuration settings or tuning.

upvoted 2 times

? ?  Jay_Krish 9ámonths, 3áweeks ago

To be precise - the explanation says resides in the same zone. The option D says create in Primary and Standby in different zones.
Option D doesn't seem right..

upvoted 1 times

? ?  FAD04 10ámonths ago

I got this question in exam 01/09/22

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

282/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #105

Topic 1

Your company has an application running on multiple Compute Engine instances. You need to ensure that the application can communicate with

an on-premises service that requires high throughput via internal IPs, while minimizing latency. What should you do?

A. Use OpenVPN to con gure a VPN tunnel between the on-premises environment and Google Cloud.

B. Con gure a direct peering connection between the on-premises environment and Google Cloud.

C. Use Cloud VPN to con gure a VPN tunnel between the on-premises environment and Google Cloud.

D. Con gure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud.

Correct Answer: C

Reference -

https://cloud.google.com/architecture/setting-up-private-access-to-cloud-apis-through-vpn-tunnels

Community vote distribution

D (100%)

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

Ans ) D , Reason : high throughput via internal IPs

upvoted 57 times

? ?  ShadowLord 10ámonths, 2áweeks ago

This is tricky questions , it can be achieved by C and D ... Multiple Computes and Costs .. they are trying to test knowledge on VPN and
Tunnels ....

upvoted 1 times

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago

IMHO the correct answer is D.
Reason: "requires high throughput via internal IPs, while minimizing latency" - both are aspects you cannot guarantee with using VPN
traversing the internet.

upvoted 16 times

? ?  Danomine416  Most Recent ?  2ámonths ago

Initially thought 'D' but the question says 'high throughput via internal IPs' so go with VPN answer 'C'

upvoted 1 times

? ?  grejao 2ámonths, 4áweeks ago

It can┤t be VPN, Only interconnect can minimizing latency.

D is the right answer.

upvoted 1 times

? ?  zerg0 4ámonths, 2áweeks ago

Selected Answer: D

Internal IP + high throughput

upvoted 4 times

? ?  WFCheong 5ámonths, 2áweeks ago

Selected Answer: D

Ans ) D , Reason : high throughput via internal IPs

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

D. Configure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud.

A Cloud Dedicated Interconnect is a high-bandwidth, low-latency network connection that allows you to connect your on-premises
environment to Google Cloud Platform (GCP) using a dedicated network connection. It provides a direct physical connection between your
on-premises network and GCP, which can help to reduce latency and increase the throughput of your application.

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, using OpenVPN to configure a VPN tunnel between the on-premises environment and Google Cloud, is not a recommended
approach. OpenVPN is a free and open-source software application that implements virtual private network (VPN) techniques to create
secure point-to-point connections. While OpenVPN can be used to establish a VPN tunnel between an on-premises environment and
GCP, it may not provide the level of performance and reliability required for high-throughput applications.

Option B, configuring a direct peering connection between the on-premises environment and Google Cloud, is not a recommended
approach. Direct Peering is a high-bandwidth, low-latency network connection that allows you to connect your on-premises network

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

283/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

directly to Google's network. While Direct Peering can be used to connect your on-premises environment to GCP, it is typically used for
high-bandwidth workloads such as video streaming and may not be suitable for applications that require low latency.

Option C, using Cloud VPN to configure a VPN tunnel between the on-premises

upvoted 2 times

? ?  medi01 2ámonths, 1áweek ago

Yeah, direct peering is high bandwidth and low latency, but may not be suitable for applications that require low latency.

Good job, ChatGPT...

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Cloud VPN, would also involve routing traffic over the public internet and would not provide the low latency and high throughput
that you need.
upvoted 1 times

? ?  SureshbabuK 7ámonths ago

Selected Answer: D

Highthoughput connection VPN - No Interconnect - Y
via Internal IP addresses. VPN - yes Interconnect - Y
Interconnect yes for both so D

upvoted 3 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  przema86 7ámonths, 3áweeks ago

Selected Answer: D

high throughput => D

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

>>>requires high throughput via internal IPs, while minimizing latency<<< only Dedicated Interconnect can achieve this

upvoted 3 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: D

D. Configure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud.
Interconnect gives very high speed connection and low latency. It gives 10Gbps and 100 Gbps connection, which makes it very fast. In
addition, Interconnect is an enterprise-grade connection to Google Cloud as per documentation.
On the other hand, Cloud VPN is recommended for lower throughput solution, or if you are experimenting with migrating workloads to
Google Cloud.
upvoted 4 times

? ?  hirochan 8ámonths, 2áweeks ago

D is right!

upvoted 2 times

? ?  6721sora 10ámonths, 1áweek ago

To use private internal On-premises IPs to access Google services, I think you need Google Private Google Access. The Private Google
Access can be implemented via VPN or via Dedicated interconnect. But the high throughput means we need a Dedicated interconnect.
Hence the answer is D

Dedicated interconnect by itself cannot allow on-prem internal IPs to connect to Google services without some form of SNATing on the
source.

upvoted 3 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: D

HIGH THROUGHPUT connection is needed--> Dedicated interconnect

upvoted 3 times

? ?  AzureDP900 12ámonths ago

D is right

upvoted 2 times

? ?  AzureDP900 12ámonths ago

D is right!

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

284/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #106

Topic 1

You are managing an application deployed on Cloud Run for Anthos, and you need to de ne a strategy for deploying new versions of the

application. You want to evaluate the new code with a subset of production tra c to decide whether to proceed with the rollout. What should you

do?

A. Deploy a new revision to Cloud Run with the new version. Con gure tra c percentage between revisions.

B. Deploy a new service to Cloud Run with the new version. Add a Cloud Load Balancing instance in front of both services.

C. In the Google Cloud Console page for Cloud Run, set up continuous deployment using Cloud Build for the development branch. As part of

the Cloud Build trigger, con gure the substitution variable TRAFFIC_PERCENTAGE with the percentage of tra c you want directed to a new

version.

D. In the Google Cloud Console, con gure Tra c Director with a new Service that points to the new version of the application on Cloud Run.

Con gure Tra c Director to send a small percentage of tra c to the new version of the application.

Correct Answer: C

Community vote distribution

A (81%)

C (19%)

? ?  VishalB  Highly Voted ?  1áyear, 11ámonths ago

(cid:0) Correct Answer: A
o Each deployment to a service creates a revision. A revision consists of a specific container image, along with environment settings such
as environment variables, memory limits, or concurrency value.
o Once the new revision is deployed to a Service you can manage the traffic using MANAGE TRAFFIC option inside the revision tab
(cid:0) https://cloud.google.com/run/docs/resource-model

upvoted 48 times

? ?  omermahgoub  Highly Voted ?  6ámonths, 1áweek ago

The correct answer is A. Deploy a new revision to Cloud Run with the new version. Configure traffic percentage between revisions.

Cloud Run for Anthos allows you to deploy new revisions of your application with a specific percentage of traffic, which allows you to
perform a gradual rollout of the new version. To do this, you can follow these steps:

Deploy a new revision of your application to Cloud Run with the new version.

In the Cloud Run for Anthos console, navigate to the service that you want to roll out the new version for.

In the "Revisions" tab, you should see the new revision listed alongside the current revision.

Use the traffic percentage slider to specify the percentage of traffic that you want to send to the new revision. You can set the percentage
to a small value initially, such as 5%, and gradually increase it over time as you evaluate the new version.

Once you have set the traffic percentage, Cloud Run for Anthos will start directing a portion of the traffic to the new revision, allowing you
to evaluate the new version with a subset of production traffic.

upvoted 14 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Extremely convinced by your explanations..Have u given this exam?

upvoted 1 times

? ?  baertierchen 1áweek, 6ádays ago

Those Answers are surely generated by ChatgGPT

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option B, deploying a new service and adding a Cloud Load Balancer instance in front of both services, is not recommended because it
would require you to create and manage a separate service for the new version, which would be more complex and less efficient than
deploying a new revision.

Option C, using continuous deployment with Cloud Build and substitution variables, is not relevant to this scenario because it involves
deploying new versions automatically based on changes to a development branch, rather than manually deploying new revisions with a
specific percentage of traffic.

Option D, using Traffic Director, is also not relevant because Traffic Director is used for managing traffic between different services or
clusters, rather than between revisions of the same service.

upvoted 7 times

? ?  TheCloudGuruu  Most Recent ?  1ámonth, 2áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

285/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

Answer is A

upvoted 1 times

? ?  zerg0 4ámonths, 2áweeks ago

Selected Answer: A

https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration

upvoted 4 times

? ?  KM0107 6ámonths, 1áweek ago

Selected Answer: A

Selected A

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  sfsdeniso 8ámonths, 1áweek ago

A is correct
currently there is possibility to use tags to test in production without receiving real traffic:
https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-migration#tags

upvoted 1 times

? ?  sfsdeniso 8ámonths, 1áweek ago

whats wrong with C is configuring deployment from 'development branch'
this is supper ugly

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

you can do a lab on this >>>Deploy Your Website on Cloud Run<<< Manage traffic is there

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. Deploy a new revision to Cloud Run with the new version. Configure traffic percentage between revisions.

upvoted 1 times

? ?  abirroy 9ámonths, 2áweeks ago

Selected Answer: A

Traffic management
Cloud Run for Anthos can now route each request or RPC randomly between multiple revisions of a service with the traffic percentages
you configure. You can use this feature to perform canary deployments of a newer version of your application, sending a small percentage
of the traffic and validating if it is performing correctly, before gradually increasing the traffic.

Similarly, these new traffic management capabilities make it possible to roll back to an older version of your application quickly. You can
manage traffic to your service on the Cloud Console, as well as the gcloud command-line tool.

upvoted 2 times

? ?  abirroy 9ámonths, 2áweeks ago

https://cloud.google.com/blog/products/serverless/new-features-in-cloud-run-for-anthos-ga

upvoted 1 times

? ?  anilcruise 10ámonths ago

Selected Answer: A

A is correct option .. in cloud build there is no such trigger

upvoted 2 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam. I answered it A and it seems to be correct.

upvoted 3 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago
Thanks for answering this time

upvoted 1 times

? ?  AzureDP900 12ámonths ago

A is right

upvoted 1 times

? ?  cdcollector 1áyear ago

Cloud run For *Anthos* - control is on the console https://cloud.google.com/anthos/run/docs/rollouts-rollbacks-traffic-
migration#:~:text=Cloud%20Run%20for%20Anthos%20allows,are%20received%20by%20a%20revision.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

286/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  zr79 8ámonths, 2áweeks ago

But it is not Traffic Director, it is Manage Traffic and the first option A was used before manage traffic

upvoted 1 times

? ?  H_S 1áyear ago

Selected Answer: A

(cid:0) Correct Answer: A

upvoted 2 times

? ?  conmarch2022 1áyear, 2ámonths ago

A https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration#gradual To split traffic between more revisions, click Add
Revision, select the desired revision, and set the percentage to the desired split. Go to Cloud Run

Locate the service in the services list, and click on it.

Click Manage Traffic.

The currently serving new revision is listed. In the form:

Set the currently serving revision percentage to the desired split.
Select one of the previous revisions using the dropdown list and set it to the desired percentage split.
To split traffic between more revisions, click Add Revision, select the desired revision, and set the percentage to the desired split.
Click Save.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

287/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #107

Topic 1

You are monitoring Google Kubernetes Engine (GKE) clusters in a Cloud Monitoring workspace. As a Site Reliability Engineer (SRE), you need to

triage incidents quickly. What should you do?

A. Navigate the prede ned dashboards in the Cloud Monitoring workspace, and then add metrics and create alert policies.

B. Navigate the prede ned dashboards in the Cloud Monitoring workspace, create custom metrics, and install alerting software on a Compute

Engine instance.

C. Write a shell script that gathers metrics from GKE nodes, publish these metrics to a Pub/Sub topic, export the data to BigQuery, and make a

Data Studio dashboard.

D. Create a custom dashboard in the Cloud Monitoring workspace for each incident, and then add metrics and create alert policies.

Correct Answer: D

Reference:

https://cloud.google.com/monitoring/charts/dashboards

Community vote distribution

A (60%)

D (40%)

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

Ans ) A .

upvoted 53 times

? ?  DiegoMDZ  Highly Voted ?  1áyear, 11ámonths ago

It's A for me... Create a dashboard for each incident?? I think D isn't a good choice...

upvoted 24 times

? ?  Nidhal1920  Most Recent ?  2ámonths, 1áweek ago

A & B are not correct : we cannot edit predefined dashboards.
Answer is D

upvoted 2 times

? ?  DRK8109 2ámonths, 2áweeks ago

An incident, also called an alert, is a record of the triggering of an alerting policy. Unless an alerting policy is snoozed or disabled, Cloud
Monitoring opens an incident when a condition of an alerting policy is triggered.

upvoted 3 times

? ?  taer 3ámonths ago

Selected Answer: A

Cloud Monitoring provides predefined dashboards for various Google Cloud services, including GKE.

upvoted 1 times

? ?  Subhajeetpal 3ámonths, 1áweek ago

Ans is A

upvoted 1 times

? ?  JC0926 3ámonths, 1áweek ago

Selected Answer: A

Google Cloud Monitoring provides a set of predefined dashboards that are specific to GKE clusters. These dashboards provide a
comprehensive view of the cluster health, including metrics such as CPU utilization, memory usage, and network traffic. These predefined
dashboards can be used as a starting point to monitor the clusters and triage incidents quickly.

upvoted 3 times

? ?  Deb2293 3ámonths, 3áweeks ago

Selected Answer: A

Cloud Monitoring provides predefined dashboards for GKE that contain relevant metrics, charts, and alerts, which can help SREs quickly
triage incidents. SREs can navigate these dashboards, add metrics as needed, and create alert policies

upvoted 3 times

? ?  AugustoKras011111 3ámonths, 4áweeks ago

Selected Answer: A

between A and B, u don┤t need to create a custom metric. I go with A

upvoted 1 times

? ?  Just_Ninja 3ámonths, 4áweeks ago

D, may you take a look into the dashboard :) and test it out

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

288/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  telp 4ámonths ago

Selected Answer: A

A. So much work to have a dahsboard by accident. The goal of the dashboard is to have all the incidents in the same to compare between
them.

upvoted 1 times

? ?  WFCheong 5ámonths, 2áweeks ago

Selected Answer: A

I vote A, D is too much job and adding complexity to the monitoring dashboard.

upvoted 3 times

? ?  hxhwing 6ámonths ago

Selected Answer: D

https://cloud.google.com/monitoring/charts/dashboards
You can't delete or copy these dashboards, and you can't add or remove charts to these dashboards. However, you can use the dashboard
toolbar to modify the display, and you can copy charts from these dashboards to dashboards that you create.

upvoted 4 times

? ?  thamaster 6ámonths ago

Selected Answer: A

D is not good for me, you can't create a dashboard for each incident how do you compare and find out the most urgent?
so It's A

upvoted 4 times

? ?  RajeshVR 6ámonths ago

Selected Answer: A

Only diff with D is it suggests create dashboard for each incidents and is not a quick solution

upvoted 3 times

? ?  RajeshVR 6ámonths ago

Ans A. Creating dashboard for each incident is not a quick solution, so we can exclud D

upvoted 2 times

? ?  omermahgoub 6ámonths, 1áweek ago

Answer is A:

A. Navigate the predefined dashboards in the Cloud Monitoring workspace, and then add metrics and create alert policies.

Cloud Monitoring provides predefined dashboards that display key metrics and resource groupings for various GCP services, including
GKE. You can use these dashboards to quickly understand the health of your GKE clusters and identify potential incidents. You can also
add additional metrics to the dashboards and create alert policies to be notified when specific conditions are met, allowing you to
proactively triage incidents.

Option D, creating a custom dashboard in the Cloud Monitoring workspace for each incident, and then adding metrics and creating alert
policies, is not a recommended approach. Creating a separate dashboard for each incident may not be necessary or practical, and may
add complexity to your monitoring setup. Additionally, you can add metrics and create alert policies on the predefined dashboards to
proactively triage incidents.

upvoted 11 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option B, navigating the predefined dashboards in the Cloud Monitoring workspace, creating custom metrics, and installing alerting
software on a Compute Engine instance, is not a recommended approach. While you can create custom metrics in Cloud Monitoring,
this is not necessary for triaging incidents and may add complexity to your monitoring setup. Additionally, installing alerting software
on a Compute Engine instance is not required to receive alerts in Cloud Monitoring.

Option C, writing a shell script that gathers metrics from GKE nodes, publishing these metrics to a Pub/Sub topic, exporting the data to
BigQuery, and making a Data Studio dashboard, is not a recommended approach. While this approach may allow you to create custom
dashboards and analyze data, it is not necessary for triaging incidents and may add complexity to your monitoring setup.

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

289/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #108

Topic 1

You are implementing a single Cloud SQL MySQL second-generation database that contains business-critical transaction data. You want to ensure

that the minimum amount of data is lost in case of catastrophic failure. Which two features should you implement? (Choose two.)

A. Sharding

B. Read replicas

C. Binary logging

D. Automated backups

E. Semisynchronous replication

Correct Answer: CD

Backups help you restore lost data to your Cloud SQL instance. Additionally, if an instance is having a problem, you can restore it to a previous

state by using the backup to overwrite it. Enable automated backups for any instance that contains necessary data. Backups protect your data

from loss or damage.

Enabling automated backups, along with binary logging, is also required for some operations, such as clone and replica creation.

Reference:

https://cloud.google.com/sql/docs/mysql/backup-recovery/backups

Community vote distribution

CD (68%)

BD (32%)

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

Ans) C and D
Cloud SQL. If you use Cloud SQL, the fully managed Google Cloud MySQL database, you should enable automated backups and binary
logging for your Cloud SQL instances. This allows you to perform a point-in-time recovery, which restores your database from a backup
and recovers it to a fresh Cloud SQL instance

upvoted 29 times

? ?  HenkH 8ámonths ago

And: a read-replica won't help against "catastrophic failures" like accidental deletions

upvoted 3 times

? ?  RVivek 4ámonths, 3áweeks ago

catastrophic failure means disaster like a zonal datacenter level failure or regional failure

upvoted 2 times

? ?  victory108  Highly Voted ?  1áyear, 11ámonths ago

C. Binary logging
D. Automated backups

upvoted 10 times

? ?  FaizAhmed  Most Recent ?  2ádays, 1áhour ago

C. Binary logging
D. Automated backups

upvoted 1 times

? ?  taer 3ámonths ago

Selected Answer: CD

Binary logging records changes to the data, which can help you recover data and minimize data loss during an unexpected failure.
Automated backups create regular backups of your database, allowing you to restore the database to a specific point in time in case of a
catastrophic failure.

upvoted 1 times

? ?  abbottWang 3ámonths, 4áweeks ago

Selected Answer: CD

backup data automatically

upvoted 1 times

? ?  telp 4ámonths ago

CD => the answer B is for performance issue. The question focus on data loss prevention.

upvoted 1 times

? ?  medi01 2ámonths, 1áweek ago

So you are going with a SINGLE instance of MySQL for a critical business application.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

290/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  okixavi 5ámonths ago

Selected Answer: BD

B and D:
No need to explain D, but B... here is why
When you set up a read replica, automaticaly binary logging is activated. Then, in case of desaster, you can promote manually a read
replica and it will have all data before the desaster occurs.

upvoted 2 times

? ?  r1ck 4ámonths, 1áweek ago

sure, binary logging starts Automatically upon configuring read-replica??
- Don't think so,
https://cloud.google.com/sql/docs/mysql/replication/create-replica

upvoted 2 times

? ?  Jeena345 5ámonths ago

B and D are correct answers as per below reference,
1. Before you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:

Automated backups must be enabled.

2. Binary logging must be enabled which requires point-in-time recovery to be enabled. Learn more about the impact of these logs.

3. At least one backup must have been created after binary logging was enabled.

It means creating read replica already covers binary logging.

Please read the following references for more information

https://cloud.google.com/solutions/cloud-sql-mysql-disaster-recovery-complete-failover-fallback

https://medium.com/google-cloud/cloud-sql-recovering-from-regional-failure-in-10-minutes-or-less-mysql-fc055540a8f0

Replication in Cloud SQL | Cloud SQL for MySQL | Google Cloud

upvoted 1 times

? ?  mmathiou 1ámonth, 3áweeks ago

Yes, you are correct that creating a read replica requires binary logging to be enabled on the primary instance. However, the purpose of
a read replica is to scale read traffic and offload it from the primary instance, not to prevent data loss in case of catastrophic failure.
While enabling binary logging is a requirement for creating a read replica, it is not the primary purpose of a read replica. IMO the two
features that should be implemented to ensure minimum data loss in case of catastrophic failure are Binary logging and Automated
backups.

upvoted 1 times

? ?  examch 5ámonths, 4áweeks ago

Selected Answer: CD

C and D are correct answers,

Backups help you restore lost data to your Cloud SQL instance. Additionally, if an instance is having a problem, you can restore it to a
previous state by using the backup to overwrite it. Enable automated backups for any instance that contains necessary data. Backups
protect your data from loss or damage.

Enabling automated backups, along with binary logging, is also required for some operations, such as clone and replica creation.

https://cloud.google.com/sql/docs/mysql/backup-recovery/backups#what_backups_provide

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

C. Binary logging
D. Automated backups

Binary logging is a feature of MySQL that records all changes made to the database in a binary log file. By enabling binary logging on your
Cloud SQL instance, you can use the log file to recover your database in case of catastrophic failure.

Automated backups are a feature of Cloud SQL that allows you to automatically create and retain backups of your database. By enabling
automated backups, you can restore your database in case of catastrophic failure or other data loss events.

Option A, sharding, is not a recommended approach. Sharding is a technique for distributing data across multiple servers to improve
performance and scalability. While sharding can help to improve the performance of a database, it is not specifically designed to protect
against data loss in case of catastrophic failure.

upvoted 4 times

? ?  omermahgoub 6ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

291/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Option B, read replicas, is not a recommended approach. Read replicas are copies of a database that can be used to offload read traffic
from the primary database. While read replicas can improve the performance of a database, they are not specifically designed to
protect against data loss in case of catastrophic failure.

Option E, semisynchronous replication, is not a recommended approach. Semisynchronous replication is a method of replicating data
between a primary database and one or more secondary databases. While semisynchronous replication can help to ensure that data is
replicated quickly and accurately, it is not specifically designed to protect against data loss in case of catastrophic failure.

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: CD

cd is ok

upvoted 1 times

? ?  diasporabro 7ámonths, 4áweeks ago

Selected Answer: CD

I see Read Replicas as more of a performance thing, than DR thing

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: BD

BD
When you create a new instance in the Google Cloud console, both Automated backups and Enable point-in-time recovery are
automatically enabled. Point-in-time recovery uses binary logs.
Therefore the best protection will be given by Automated Backup and a Read Replica in another zone.
Usually a catastrophic failure assume loss of a datacentre or even a zone. Backup images are distributed on different zones but can take
time to restore. Replicas can be accessed faster.

upvoted 3 times

? ?   ercedog 6ámonths, 2áweeks ago

If you use cloud SDK,you need to specify the command option: https://cloud.google.com/sdk/gcloud/reference/sql/instances/create#--
enable-point-in-time-recovery

upvoted 1 times

? ?  AzureDP900 9ámonths, 2áweeks ago

Binary logging and Automated backups will help .. C, D is right

upvoted 2 times

? ?  hisunilarora 10ámonths, 1áweek ago

C and D are correct
https://cloud.google.com/sql/docs/mysql/replication
Before you can create a read replica of a primary Cloud SQL instance, the instance must meet the following requirements:

Automated backups must be enabled.
Binary logging must be enabled which requires point-in-time recovery to be enabled. Learn more about the impact of these logs.
At least one backup must have been created after binary logging was enabled.

upvoted 2 times

? ?  AzureDP900 12ámonths ago

Choose C, D as right answer without any second thought !

upvoted 2 times

? ?  Nirca 1áyear ago

Selected Answer: CD

Data lose is being managed via datafiles/database backups. Once a day or once every time interval. Between backups the logs(redo/bin
logs/archive logs) must be copied on a continuous manner. Only that way point in time recovery or full recovery without data loss might
be successful. Else you will be able to backup-restore. AND not backup-restore-recovery.

upvoted 6 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

292/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #109

Topic 1

You are working at a sports association whose members range in age from 8 to 30. The association collects a large amount of health data, such

as sustained injuries. You are storing this data in BigQuery. Current legislation requires you to delete such information upon request of the

subject. You want to design a solution that can accommodate such a request. What should you do?

A. Use a unique identi er for each individual. Upon a deletion request, delete all rows from BigQuery with this identi er.

B. When ingesting new data in BigQuery, run the data through the Data Loss Prevention (DLP) API to identify any personal information. As part

of the DLP scan, save the result to Data Catalog. Upon a deletion request, query Data Catalog to  nd the column with personal information.

C. Create a BigQuery view over the table that contains all data. Upon a deletion request, exclude the rows that affect the subject's data from

this view. Use this view instead of the source table for all analysis tasks.

D. Use a unique identi er for each individual. Upon a deletion request, overwrite the column with the unique identi er with a salted SHA256 of

its value.

Correct Answer: B

Community vote distribution

A (58%)

B (42%)

? ?  milan74  Highly Voted ?  1áyear, 11ámonths ago

According to me, the question states "The association collects a large amount of health data, such as sustained injuries." and the nuance
on the word such => " Current legislation requires you to delete "SUCH" information upon request of the subject. " So from that point of
view the question is not to delete the entire user records but specific data related to personal health data. With DLP you can use InfoTypes
and InfoType detectors to specifically scan for those entries and how to act upon them (link https://cloud.google.com/dlp/docs/concepts-
infotypes)
I would say B.

upvoted 68 times

? ?  Arad 1áyear, 7ámonths ago

as PhilipKoku mentioned below:
A) is the correct answer. B) is only masking the data and then when a request is received, it identified the record but it doesnÆt delete it.
D) Is masking the ID.

upvoted 11 times

? ?  mgm7 1áyear, 6ámonths ago

B is not masking the data but identifying where it is to take action on at later date if required

upvoted 5 times

? ?  Ishu_awsguy 10ámonths, 2áweeks ago

There is no need of DLP.
All the data is sensitive but only upon user request it needs deletion.
So A should be the correct answer.

upvoted 10 times

? ?  AmitAr 1áyear, 1ámonth ago

(A) - Primary task is "legislation requires you to delete" .. and B is not deleting.
only A is deleting
upvoted 9 times

? ?  BeCalm 3ámonths, 3áweeks ago

Deletion is implied in "Upon a deletion request, query Data Catalog to find the column with personal information."

upvoted 1 times

? ?  zanfo 1áyear, 3ámonths ago

I want to delete all the informations about the user, not only those individuate by DLP. ALL THE INFORMATIONS of the users...B is not
correct! the correct is A

upvoted 6 times

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago

IMHO a) is the correct answer because it is easier to operate. The question is not how to mask data and so on but just to delete data on
request, so I don't think that we have to use for just the deletion of specific data DLP.

upvoted 32 times

? ?  Hgwells  Most Recent ?  4ádays, 19áhours ago

Selected Answer: B

The question requires deletion of PII data only, i.e PHI columns only. There is no way to keep track of all PHI columns with just option A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

293/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Atanu 2áweeks ago

Selected Answer: B

What's wrong in B... it seems ok as well

upvoted 1 times

? ?  red_panda 3áweeks ago

Selected Answer: B

Answer is B in my opinion.
It ask to delete some informations, not ALL informations about customers

upvoted 1 times

? ?  JohnWick2020 3áweeks, 5ádays ago

B - this is a more comprehensive solution that tackles PII concerns and helps with identifying "such" health information and safely delete.

upvoted 1 times

? ?  JC0926 2ámonths, 2áweeks ago

Selected Answer: A

Using a unique identifier for each individual in BigQuery allows you to efficiently locate and delete specific rows of data related to a
particular individual. When a deletion request comes in, you can easily find and remove all the rows associated with that unique identifier.
This approach ensures that you comply with the legislation requiring the deletion of personal information upon request while maintaining
the integrity and structure of the remaining data.

upvoted 1 times

? ?  JC0926 2ámonths, 2áweeks ago

Other options are either less efficient or do not fully address the requirement to delete personal information:

B. While DLP API and Data Catalog can help identify personal information, they do not provide an efficient way to delete specific data
when a deletion request is made. This method could also be time-consuming and error-prone.

C. Creating a BigQuery view does not actually delete the data; it only hides it from the view. The personal information would still be
stored in the source table, which does not comply with the legislation.

D. Overwriting the unique identifier with a salted SHA256 of its value only obfuscates the data but does not remove it, again not
complying with the legislation's requirement to delete personal information.

upvoted 1 times

? ?  taer 3ámonths ago

Selected Answer: A

Using a unique identifier for each individual allows you to easily find and delete all the rows associated with a specific individual upon their
deletion request. This approach complies with the legislation requiring the deletion of personal information upon request and ensures
that the individual's data is no longer available in BigQuery.

upvoted 1 times

? ?  nvragavan 3ámonths ago

Selected Answer: B

The de-identification process in DLP can delete/reformat sensitive information. Please check Google Cloud Tech channel video below
https://youtu.be/nJKNLl9W-2E?list=PLIivdWyY5sqK9j4_JkC8j1mY4JEGgLdcD&t=179

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: B

The ask is for deletion of the sensitive data, not the entirety of all data associated with a unique identifier.

upvoted 1 times

? ?  romandrigo 3ámonths, 4áweeks ago

Selected Answer: A

option A, option B is valid if the data is new, it says "When ingesting "new data" in....,
but what happens with the previously stored data, it doesn't solve it.
The heading or start of option B, overrides the rest of the paragraph.

upvoted 3 times

? ?  r1ck 4ámonths, 1áweek ago

how about 1000 - 5000 - 10000 unique individuals?
will you be creating unique ID's for each as per #A ?
.. it has to be #B
upvoted 1 times

? ?  jlambdan 3ámonths ago

What is the problem with that ? There are no technical limitation for it. Generated a UUID.

upvoted 1 times

? ?  examch 5ámonths, 4áweeks ago

Selected Answer: B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

294/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B is the correct answer,

Data catalog can be used to delete entry for the table or dataset, data catalog can be used with DLP, for identification of sensitive data in
the tables.

Data Catalog provides three main functions:

Searching for data entries for which you have access
Tagging data entries with metadata
Providing column-level security for BigQuery tables
In addition, Data Catalog can leverage the results of a Cloud Data Loss Prevention (DLP) scan to identify sensitive data directly within Data
Catalog in the form of tag templates.

https://cloud.google.com/data-catalog/docs/samples/data-catalog-delete-entry
https://cloud.google.com/data-catalog/docs/concepts/overview#functions

upvoted 3 times

? ?  Praveen_G 6ámonths ago

Selected Answer: B

B is the correct answer. The request is to delete only 'such information' meaning some specific columns, not the entire rows as mentioned
in option A. So, B is the correct answer where it helps to find specific columns.

upvoted 2 times

? ?  thamaster 6ámonths ago

Selected Answer: A

the only answer that actually delete data is A

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Answer is A. Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.

Using a unique identifier for each individual allows you to easily identify and delete the data of a specific person when a request is made.
You can then simply delete all rows from BigQuery that contain this identifier to fulfill the request.

Option B, running the data through the Data Loss Prevention (DLP) API to identify any personal information and saving the result to Data
Catalog, is not a recommended approach. While the DLP API can be used to identify personal information in data, it is not necessary for
this specific task. Additionally, saving the result to Data Catalog would not enable you to delete the data from BigQuery.

upvoted 6 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option C, creating a BigQuery view over the table that contains all data, is not a recommended approach. While a view can be used to
exclude certain rows from a query, it does not permanently delete the data from the underlying table. Therefore, this approach would
not enable you to fully comply with a request to delete personal data.

Option D, overwriting the column with the unique identifier with a salted SHA256 of its value, is not a recommended approach. While
this approach would obscure the value of the identifier, it would not permanently delete the data from the table. This approach would
also make it more difficult to identify and delete the data of a specific individual when a request is made.

upvoted 3 times

? ?  Praveen_G 6ámonths ago

Option A will delete entire row but the request is to delete only 'such information' meaning specific columns. B is suitable for that

upvoted 2 times

? ?  VSMu 4ámonths, 3áweeks ago

The thing causing confusion to me is that B doesnt mention the last step of actually deleting the info. But if we assume that the
purpose of identifying "such information" is to actually delete (as an implicit thing), B is right. I think the answer in B is not worded
completely.

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

295/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #110

Topic 1

Your company has announced that they will be outsourcing operations functions. You want to allow developers to easily stage new versions of a

cloud-based application in the production environment and allow the outsourced operations team to autonomously promote staged versions to

production. You want to minimize the operational overhead of the solution. Which Google Cloud product should you migrate to?

A. App Engine

B. GKE On-Prem

C. Compute Engine

D. Google Kubernetes Engine

Correct Answer: D

Reference:

https://cloud.google.com/security/compliance/eba-outsourcing-mapping-gcp

Community vote distribution

A (90%)

5%

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

A. App Engine

upvoted 32 times

? ?  arsav  Highly Voted ?  1áyear, 11ámonths ago

Answer should be A as only with App Engine we have a default service account which allows the user to deploy the changes per project.
for GKE we may have to configure additional permission for both DEV and Operations team to deploy the changes.

https://cloud.google.com/appengine/docs/standard/php/service-account

upvoted 21 times

? ?  JohnWick2020  Most Recent ?  3áweeks, 5ádays ago

Answer is A.
By process of elimination you arrive at App Engine or GKE. Now the requirement is to "to minimize the operational overhead of the
solution".
On the IaaS to PaaS spectrum, this can only be App Engine!

IaaS = Compute Engine.
Hybrid = GKE (engineering heavy).
PaaS = App Engine.

upvoted 2 times

? ?  Atanu 1ámonth ago

Selected Answer: A

"You want to minimize the operational overhead of the solution" ..This sentence is they key to go with Option A. GKE carries overhead as
it's not purely PaaS.

upvoted 2 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

It should be D. As GKE is considered to be the master product/service for creating a deployment and managing and keeping all the
environments in SYNC

upvoted 1 times

? ?  sithin_nair 3ámonths, 2áweeks ago

Selected Answer: A

A is right as the requirement is to deploy new changes and manage the application with no operational overhead.

upvoted 1 times

? ?  Deb2293 3ámonths, 3áweeks ago

Selected Answer: A

App Engine could be a valid option for this scenario, as it provides a managed platform for deploying and scaling web applications that
can be updated with new versions. App Engine also provides a default service account that developers can use to deploy changes and
manage the application.

While GKE provides more fine-grained control over the underlying infrastructure and deployment processes, it may not be necessary for
all use cases. If the application is relatively simple and does not require extensive customization or management, App Engine could be a
good choice.

In the end, the choice of platform depends on the specific needs and requirements of the application and organization, and both App

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

296/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Engine and GKE could be valid options. It is important to evaluate each platform's strengths and weaknesses to determine which one is
the best fit for the application and organization.

upvoted 4 times

? ?  abbottWang 3ámonths, 4áweeks ago

Selected Answer: A

App engine already has a default account which allows user to deploy change

upvoted 1 times

? ?  wences 5ámonths, 2áweeks ago

Selected Answer: D

In this one, we are talking about some sort of control degree over the platform, so GKE is the clear option, enabling outsourced and
internal teams to be as dynamic as possible, the devil is in details but thinking in business requirement is my approach.

upvoted 2 times

? ?  windsor_43 6ámonths ago

The Answer is A.

Just had my exam today with a pass, this question was in the exam. Dated 31/12/22

upvoted 8 times

? ?  omermahgoub 6ámonths, 1áweek ago

To allow developers to easily stage new versions of a cloud-based application in the production environment and allow the outsourced
operations team to autonomously promote staged versions to production while minimizing the operational overhead of the solution, you
should migrate to Google Kubernetes Engine (GKE).

GKE is a fully managed service for deploying and managing containerized applications on Google Cloud. It allows you to easily deploy and
scale your applications, and provides features such as rolling updates and canary deployments, which can be used to stage new versions
of your application in the production environment. GKE also has built-in monitoring and logging capabilities, which can help the
outsourced operations team to identify and resolve issues in the production environment.

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, App Engine, is not a recommended approach. While App Engine is a fully managed platform for deploying and scaling web
applications, it does not provide the same level of flexibility and control as GKE when it comes to managing the deployment and scaling
of applications.

Option B, GKE On-Prem, is not a recommended approach. GKE On-Prem is a version of GKE that can be deployed on-premises or in a
virtual private cloud (VPC). It is not designed to be used as a cloud-based application platform.

Option C, Compute Engine, is not a recommended approach. While Compute Engine is a flexible and powerful platform for running
virtual machines, it requires more operational overhead to manage the deployment and scaling of applications compared to GKE. It
does not provide the same level of built-in monitoring and logging capabilities as GKE.

upvoted 1 times

? ?  VSMu 4ámonths, 3áweeks ago

But using GKE requires you to deploy the app as containerized app. Secondly you need to also maintain GKE cluster (additional Ops)
unless you are using GKE Autopilot. Since the question did not mention containerized app, App Engine seems to be the correct
answer (A).

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Balaji_Sakthi 8ámonths ago

app engine

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. App Engine
upvoted 1 times

? ?  abirroy 9ámonths, 1áweek ago

Selected Answer: A

I chose A due to the following statement - You want to minimize the operational overhead of the solution

upvoted 7 times

? ?  AzureDP900 9ámonths, 2áweeks ago

A is correct

upvoted 2 times

? ?  alexandercamachop 9ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

297/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

Answer is A.
Secret is "minimal overhead" = not managing infrastructure, which means App Engine.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

298/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #111

Topic 1

Your company is running its application workloads on Compute Engine. The applications have been deployed in production, acceptance, and

development environments. The production environment is business-critical and is used 24/7, while the acceptance and development

environments are only critical during o ce hours. Your CFO has asked you to optimize these environments to achieve cost savings during idle

times. What should you do?

A. Create a shell script that uses the gcloud command to change the machine type of the development and acceptance instances to a smaller

machine type outside of o ce hours. Schedule the shell script on one of the production instances to automate the task.

B. Use Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments after o ce hours and start

them just before o ce hours.

C. Deploy the development and acceptance applications on a managed instance group and enable autoscaling.

D. Use regular Compute Engine instances for the production environment, and use preemptible VMs for the acceptance and development

environments.

Correct Answer: B

Reference:

https://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs

Community vote distribution

B (89%)

11%

? ?  pamepadero  Highly Voted ?  1áyear, 11ámonths ago

B is the answer.

https://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs
Schedule VMs to auto start and stop: The benefit of a platform like Compute Engine is that you only pay for the compute resources that
you use. Production systems tend to run 24/7; however, VMs in development, test or personal environments tend to only be used during
business hours, and turning them off can save you a lot of money!

https://cloud.google.com/blog/products/storage-data-transfer/save-money-by-stopping-and-starting-compute-engine-instances-on-
schedule

Cloud Scheduler, GCPÆs fully managed cron job scheduler, provides a straightforward solution for automatically stopping and starting VMs.
By employing Cloud Scheduler with Cloud Pub/Sub to trigger Cloud Functions on schedule, you can stop and start groups of VMs
identified with labels of your choice (created in Compute Engine). Here you can see an example schedule that stops all VMs labeled "dev"
at 5pm and restarts them at 9am, while leaving VMs labeled "prod" untouched

upvoted 28 times

? ?  sgo cial 11ámonths ago

Excellent ......even the good CFO is telling leave the office after 5.oo and come next day to work :)

upvoted 8 times

? ?  rzygor 11ámonths ago

Question says that dev/test are "not critical", it doesn't mean that they are not needed at all ...

upvoted 12 times

? ?  Ric350 11ámonths, 2áweeks ago

Great answer and documentation. Def B

upvoted 2 times

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

Ans ) B , assuming VM doesn't need to be up after office hours .

upvoted 24 times

? ?  mifrah  Most Recent ?  3ámonths ago

In my opinion B is over-engineered:
Why not just add an "instance schedule" for start/stop the Compute Engines?
Why creating a scheduler and writing a Cloud Function...

upvoted 1 times

? ?  MaryMei 4ámonths ago

Selected Answer: B

https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations
B seems close to this Google provided service option, the extra step should be using idle VM recommendations to find and stop idle VM
instances to reduce waste of resources

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

299/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  PAUGURU 4ámonths, 1áweek ago

Since the price of preemptibles is 1/4 the price of a standard machine D costs far less than B since office hours are 1/3 of whole day. It
costs less to keep them running 24h as preemptibles.

upvoted 3 times

? ?  DevOpsi er 2áweeks, 2ádays ago

Yes, but preemptibles use GCP excess resources so you will achieve the opposite of the desired effect, during office hours, they will
underperform in the best case (worst case will stop altogether) and, during non-office hours, preemptibles will work well...

upvoted 1 times

? ?  windsor_43 6ámonths ago

The Answer is B.

Just had my exam today with a pass, this question was in the exam. Dated 31/12/22
Thanks to this site it was by far my most valuable

upvoted 3 times

? ?  omermahgoub 6ámonths, 1áweek ago

The correct answer is B. Use Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments
after office hours and start them just before office hours.

One way to optimize the cost of your Compute Engine environments is to stop non-critical instances when they are not in use. In this case,
you could use Cloud Scheduler to trigger a Cloud Function that will stop the acceptance and development environments after office hours
and start them just before office hours. This will allow you to take advantage of the cost savings of not running these environments during
idle times, while still ensuring that they are available during office hours when they are critical.

upvoted 1 times

? ?  omermahgoub 6ámonths, 1áweek ago

Option A, creating a shell script to change the machine type of the development and acceptance instances, is not relevant because it
does not address the issue of cost optimization during idle times.

Option C, using a managed instance group with autoscaling, is not recommended because it would not allow you to take advantage of
the cost savings of stopping the instances during idle times.

Option D, using preemptible VMs for the acceptance and development environments, is also not recommended because preemptible
VMs may be terminated by Google at any time, which would not be suitable for workloads that are critical during office hours.

upvoted 1 times

? ?  oms_muc 6ámonths, 1áweek ago

Selected Answer: B

As B seems the best answer, sadly it's not precise, as you need pub/sub in between.
https://cloud.google.com/scheduler/docs/overview

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

B is ok

upvoted 1 times

? ?  BiddlyBdoyng 7ámonths, 4áweeks ago

Problem with C is it might offer no cost saving if during office hours we see constant use of min might scale. B is best, C
can enhance B
upvoted 1 times

? ?  pp0709 10ámonths ago

If I get this question in the exam, I will choose B and here are the reasons -
The key is 1) Acceptance and Develeopment are CRITICAL during office hours. If you use Preemprible, while it is guranteed to reduce cost,
it can also get preempted during business Critical hours. Hence D is incorrect. A and C are obvious deletes and hence B

upvoted 2 times

? ?  melono 8ámonths, 2áweeks ago
D sayes regular VMs for prod
What are you talking about?
On the other hand, not critical does not mean they are not needed at all.

upvoted 1 times

? ?  RitwickKumar 10ámonths, 2áweeks ago

Selected Answer: B

The debate between option B and option C reminds me of the fact that in IT industry there are no business hours :)

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

300/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Thinking from regulations point of view as well, after business hours means employees are not expected to work if off office timeframe. In
case there is an incident and someone has to work test out the things, then they can spin up the instance.

Hence if is better not to "assume" that employee(s) would be working after the business hours and we should be good to stop the
instances in the relevant projects/environments.

upvoted 3 times

? ?  cloudinit 10ámonths, 2áweeks ago

Selected Answer: B

The answer is B.
See the below from the document: https://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs
Schedule VMs to auto start and stop: The benefit of a platform like Compute Engine is that you only pay for the compute resources that
you use. Production systems tend to run 24/7; however, VMs in development, test or personal environments tend to only be used during
business hours, and turning them off can save you a lot of money! For example, a VM that runs for 10 hours per day, Monday through
Friday costs 75% less to run per month compared to leaving it running.

upvoted 1 times

? ?  patashish 10ámonths, 3áweeks ago

B is right

upvoted 1 times

? ?  diego_alejandro 11ámonths, 3áweeks ago

Ans is C. It states Acceptance and development are "only critical" during business hs, it doesn't mean they are not used after bs hs. Using a
MIG can help reducing the infrastructure while still allowing to use those environments.

upvoted 7 times

? ?  cbarg 11ámonths, 3áweeks ago

Ans is C. It states Acceptance and development are "only critical" during business hs, it doesn't mean they are not used after bs hs. Using a
MIG can help reducing the infrastructure while still allowing to use those environments.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

301/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #112

Topic 1

You are moving an application that uses MySQL from on-premises to Google Cloud. The application will run on Compute Engine and will use Cloud

SQL. You want to cut over to the Compute Engine deployment of the application with minimal downtime and no data loss to your customers. You

want to migrate the application with minimal modi cation. You also need to determine the cutover strategy. What should you do?

A. 1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server. 2.

Stop the on-premises application. 3. Create a mysqldump of the on-premises MySQL server. 4. Upload the dump to a Cloud Storage bucket. 5.

Import the dump into Cloud SQL. 6. Modify the source code of the application to write queries to both databases and read from its local

database. 7. Start the Compute Engine application. 8. Stop the on-premises application.

B. 1. Set up Cloud SQL proxy and MySQL proxy. 2. Create a mysqldump of the on-premises MySQL server. 3. Upload the dump to a Cloud

Storage bucket. 4. Import the dump into Cloud SQL. 5. Stop the on-premises application. 6. Start the Compute Engine application.

C. 1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server. 2.

Stop the on-premises application. 3. Start the Compute Engine application, con gured to read and write to the on-premises MySQL server. 4.

Create the replication con guration in Cloud SQL. 5. Con gure the source database server to accept connections from the Cloud SQL replica.

6. Finalize the Cloud SQL replica con guration. 7. When replication has been completed, stop the Compute Engine application. 8. Promote the

Cloud SQL replica to a standalone instance. 9. Restart the Compute Engine application, con gured to read and write to the Cloud SQL

standalone instance.

D. 1. Stop the on-premises application. 2. Create a mysqldump of the on-premises MySQL server. 3. Upload the dump to a Cloud Storage

bucket. 4. Import the dump into Cloud SQL. 5. Start the application on Compute Engine.

Correct Answer: A

Community vote distribution

C (100%)

? ?  victory108  Highly Voted ?  1áyear, 11ámonths ago

C. 1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL
server. 2. Stop the on-premises application. 3. Start the Compute Engine application, configured to read and write to the on-premises
MySQL server. 4. Create the replication configuration in Cloud SQL. 5. Configure the source database server to accept connections from
the Cloud SQL replica. 6. Finalize the Cloud SQL replica configuration. 7. When replication has been completed, stop the Compute Engine
application. 8. Promote the Cloud SQL replica to a standalone instance. 9. Restart the Compute Engine application, configured to read and
write to the Cloud SQL standalone instance.

upvoted 28 times

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

Ans C, from this guy muhasinem

External replica promotion migration
In the migration strategy of external replica promotion, you create an external database replica and synchronize the existing data to that
replica. This can happen with minimal downtime to the existing database.
When you have a replica database, the two databases have different roles that are referred to in this document as primary and replica.
After the data is synchronized, you promote the replica to be the primary in order to move the management layer with minimal impact to
database uptime.
In Cloud SQL, an easy way to accomplish the external replica promotion is to use the automated migration workflow. This process
automates many of the steps that are needed for this type of migration.

upvoted 18 times

? ?  aliounegdiop  Most Recent ?  1ámonth ago

B is the correct answ

upvoted 1 times

? ?  BiddlyBdoyng 2ámonths, 1áweek ago

Option A, writing to two databases form the app :(
Option C all the way, it also aligns to GCP Data Migration Service.

upvoted 1 times

? ?  DRK8109 2ámonths, 3áweeks ago

mysql dump always causes long downtime.

upvoted 1 times

? ?  musumusu 3ámonths ago

Correct Answer A
C is unnecceory expensive and loss of data at after step 3

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

302/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  BeCalm 3ámonths, 3áweeks ago

C seems to be the best answer but it is still a bit confusing.

So basically there's a bi-directional sync between the 2 databases? Cloud instance is the primary and is writing into the on-prem and on-
prem is being replicated into the Cloud.

upvoted 1 times

? ?  NodummyIQ 5ámonths, 4áweeks ago

Option C is not the correct answer because it involves modifying the application to read and write to both the on-premises MySQL server
and Cloud SQL, which would involve significant modification to the application and could introduce potential complications or errors. It is
generally better to minimize modification to the application when performing a migration. Option D, on the other hand, involves simply
importing a mysqldump of the on-premises MySQL server into Cloud SQL and starting the application on Compute Engine, which is a
simpler and more straightforward approach that involves minimal modification to the application.

upvoted 2 times

? ?  SureshbabuK 6ámonths ago

Selected Answer: C

Examtopic providing A as correct answer is causing confusion,

upvoted 4 times

? ?  Jose56 7ámonths ago

Selected Answer: C

C for minimal downtime

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

Answer is C
we have a new service https://cloud.google.com/database-migration

upvoted 3 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: C

C because it has minimal modification to the application or database. Also it's easier to fail back to the original solution if the cloud
implementation has issues (assuming that there will be a "post-go-live" monitoring period).

upvoted 2 times

? ?  minmin2020 8ámonths, 2áweeks ago

C because it has minimal modification to the application or database. Also it's easier to fail back to the original solution if the cloud
implementation has issues (assuming that there will be a "post-go-live" monitoring period).

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

c is right

upvoted 1 times

? ?  VinayNune 9ámonths ago

Selected Answer: C

Answer is C - Very low downtime. And correct sequence of steps.

upvoted 2 times

? ?  VinayNune 9ámonths ago

In option A, Step 2 and Step 8 mention - "Stop the on-premise application". This is a bit weird. And also the downtime can be long. Option
C looks to be very clear.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

303/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #113

Topic 1

Your organization has decided to restrict the use of external IP addresses on instances to only approved instances. You want to enforce this

requirement across all of your Virtual Private Clouds (VPCs). What should you do?

A. Remove the default route on all VPCs. Move all approved instances into a new subnet that has a default route to an internet gateway.

B. Create a new VPC in custom mode. Create a new subnet for the approved instances, and set a default route to the internet gateway on this

new subnet.

C. Implement a Cloud NAT solution to remove the need for external IP addresses entirely.

D. Set an Organization Policy with a constraint on constraints/compute.vmExternalIpAccess. List the approved instances in the allowedValues

list.

Correct Answer: D

Reference:

https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address

Community vote distribution

D (100%)

? ?  victory108  Highly Voted ?  1áyear, 11ámonths ago

D. Set an Organization Policy with a constraint on constraints/compute.vmExternalIpAccess. List the approved instances in the
allowedValues list.
upvoted 21 times

? ?  AnilKr  Highly Voted ?  1áyear, 10ámonths ago

Ans - D, https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip

you might want to restrict external IP address so that only specific VM instances can use them. This option can help to prevent data
exfiltration or maintain network isolation. Using an Organization Policy, you can restrict external IP addresses to specific VM instances
with constraints to control use of external IP addresses for your VM instances within an organization or a project.

upvoted 16 times

? ?  beehive  Most Recent ?  5ámonths, 3áweeks ago

D is correct one.
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip

upvoted 2 times

? ?  rascalbrick 6ámonths, 2áweeks ago

Show on my Exam,unfortunatekt im failed..:(

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

D is correct

upvoted 1 times

? ?  2M 9ámonths, 3áweeks ago

Selected Answer: D

option D

upvoted 2 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam.

upvoted 2 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Answer pleaase
upvoted 1 times

? ?  AzureDP900 12ámonths ago

D is right. constraints/compute.vmExternalIpAccess

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

304/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  JoeyCASD 1áyear, 1ámonth ago

vote for D
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip

upvoted 2 times

? ?  ss909098 1áyear, 3ámonths ago

Selected Answer: D

D it is

upvoted 1 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: D

I got similar question on my exam. Answered D.

upvoted 3 times

? ?  technodev 1áyear, 5ámonths ago

Got this question in my exam, answered D

upvoted 2 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for D.
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

D is the correct answer
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip

upvoted 1 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: D

D) is correct answer here

upvoted 1 times

? ?  pakilodi 1áyear, 6ámonths ago

D is correct here
upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

305/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #114

Topic 1

Your company uses the Firewall Insights feature in the Google Network Intelligence Center. You have several  rewall rules applied to Compute

Engine instances.

You need to evaluate the e ciency of the applied  rewall ruleset. When you bring up the Firewall Insights page in the Google Cloud Console, you

notice that there are no log rows to display. What should you do to troubleshoot the issue?

A. Enable Virtual Private Cloud (VPC)  ow logging.

B. Enable Firewall Rules Logging for the  rewall rules you want to monitor.

C. Verify that your user account is assigned the compute.networkAdmin Identity and Access Management (IAM) role.

D. Install the Google Cloud SDK, and verify that there are no Firewall logs in the command line output.

Correct Answer: B

Reference:

https://cloud.google.com/network-intelligence-center/docs/ rewall-insights/how-to/using- rewall-insights

Community vote distribution

B (100%)

? ?  nohel  Highly Voted ?  1áyear, 11ámonths ago

Answer is B
when you create a firewall rule there is an option for firewall rule logging on/off. It is set to off by default.
To get firewall insights or view the logs for a specific firewall rule you need to enable logging while creating the rule or you can enable it by
editing that rule.
https://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/using-firewall-insights#enabling-fw-rules-logging

upvoted 28 times

? ?  victory108  Highly Voted ?  1áyear, 11ámonths ago

B. Enable Firewall Rules Logging for the firewall rules you want to monitor.

upvoted 14 times

? ?  RVivek  Most Recent ?  4ámonths, 3áweeks ago

Selected Answer: B

https://cloud.google.com/vpc/docs/firewall-rules-logging

upvoted 1 times

? ?  windsor_43 6ámonths ago

The Answer is B

Just had my exam today with a pass, this question was in the exam. Dated 31/12/22
Thanks to this site it was by far my most valuable

upvoted 3 times

? ?  jay9114 6ámonths, 1áweek ago

Selected Answer: B

You have to enable logging for a firewall rule in order to see the rows.

"When you enable logging for a firewall rule, Google Cloud creates an entry called a connection record each time the rule allows or denies
traffic."

https://cloud.google.com/vpc/docs/firewall-rules-logging

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B. Enable Firewall Rules Logging for the firewall rules you want to monitor.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Enable firewall rules logging , B is right

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

306/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: B

https://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/using-firewall-insights#enabling-fw-rules-logging

upvoted 2 times

? ?  AzureDP900 12ámonths ago

B is most appropriate answer, I will choose B.

upvoted 1 times

? ?  AzureDP900 12ámonths ago

https://cloud.google.com/vpc/docs/firewall-rules-logging

upvoted 1 times

? ?  tannV 1áyear, 1ámonth ago

Answered B. Got this question!

upvoted 2 times

? ?  azureaspirant 1áyear, 4ámonths ago

02/15/21 exam
upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for B

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

B is the correct answer
https://cloud.google.com/network-intelligence-center/docs/firewall-insights/how-to/using-firewall-insights

upvoted 1 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: B

B is the answer here

upvoted 1 times

? ?  nqthien041292 1áyear, 7ámonths ago

Selected Answer: B

Vote B

upvoted 2 times

? ?  mudot 1áyear, 7ámonths ago

Selected Answer: B

firewall rule logging is not enabeld by default

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

307/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #115

Topic 1

Your company has sensitive data in Cloud Storage buckets. Data analysts have Identity Access Management (IAM) permissions to read the

buckets. You want to prevent data analysts from retrieving the data in the buckets from outside the o ce network. What should you do?

A. 1. Create a VPC Service Controls perimeter that includes the projects with the buckets. 2. Create an access level with the CIDR of the o ce

network.

B. 1. Create a  rewall rule for all instances in the Virtual Private Cloud (VPC) network for source range. 2. Use the Classless Inter-domain

Routing (CIDR) of the o ce network.

C. 1. Create a Cloud Function to remove IAM permissions from the buckets, and another Cloud Function to add IAM permissions to the

buckets. 2. Schedule the Cloud Functions with Cloud Scheduler to add permissions at the start of business and remove permissions at the

end of business.

D. 1. Create a Cloud VPN to the o ce network. 2. Con gure Private Google Access for on-premises hosts.

Correct Answer: C

Community vote distribution

A (100%)

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago

Should be A.
For all Google Cloud services secured with VPC Service Controls, you can ensure that:
Resources within a perimeter are accessed only from clients within authorized VPC networks using Private Google Access with either
Google Cloud or on-premises.
https://cloud.google.com/vpc-service-controls/docs/overview

upvoted 61 times

? ?  poseidon24 1áyear, 11ámonths ago

Correct, this is about data exfiltration.
See: https://youtu.be/EXwJFL24QzY

upvoted 11 times

? ?  Sivanaga 9ámonths, 1áweek ago
nice one, thank you man

upvoted 2 times

? ?  mv2000 1áyear ago

Thanks for including the youtube video it was very helpful

upvoted 1 times

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago

IMHO c is wrong - the question is not to restrict access only for business hours but to restrict access to office network.

In my opinion the only realistic approach seems to be a)

https://cloud.google.com/vpc-service-controls/docs/supported-products#table_storage

upvoted 15 times

? ?  RVivek  Most Recent ?  4ámonths, 3áweeks ago

Selected Answer: A

https://cloud.google.com/vpc-service-controls/docs/overview

upvoted 1 times

? ?  vamgcp 5ámonths ago

A is correct because, For all Google Cloud services secured with VPC Service
Controls, you can ensure that resources within a perimeter are accessed only
from clients within authorized VPC networks using Private Google Access with
either Google Cloud or on-premises.

upvoted 1 times

? ?  examch 5ámonths, 3áweeks ago

Selected Answer: A

A is the correct answer,
https://cloud.google.com/vpc-service-controls/docs/overview#isolate
* A VM within a Virtual Private Cloud (VPC) network that is part of a service perimeter can read from or write to a Cloud Storage bucket in
the same perimeter. However, VPC Service Controls doesn't allow VMs within VPC networks that are outside the perimeter to access Cloud
Storage buckets that are inside the perimeter.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

308/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

* A copy operation between two Cloud Storage buckets succeeds if both buckets are in the same service perimeter, but if one of the
buckets is outside the perimeter, the copy operation fails.

* VPC Service Controls doesn't allow a VM within a VPC network that is inside a service perimeter to access Cloud Storage buckets that are
outside the perimeter.

upvoted 2 times

? ?  thamaster 6ámonths ago

Selected Answer: A

answer C will not prevent connection from outside of office network

upvoted 1 times

? ?  cshubham173 6ámonths, 1áweek ago

Selected Answer: A

For all Google Cloud services secured with VPC Service Controls, you can ensure that:
Resources within a perimeter are accessed only from clients within authorized VPC networks using Private Google Access with either
Google Cloud or on-premises.
https://cloud.google.com/vpc-service-controls/docs/overview

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. Best option
B. Not all instances need this restriction
C. You are not restricting remote access. The users can still access remotely using their credentials during the business day. The ask is to
restrict data retrieval from outside the office network (what if they are working from home...?)
D. VPN - too much overhead

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

A. Best option
B. Not all instances need this restriction
C. You are not restricting remote access. The users can still access remotely using their credentials during the business day. The ask is to
restrict data retrieval from outside the office network (what if they are working from home...?)
D. VPN - too much overhead

upvoted 2 times

? ?  kazob 8ámonths, 2áweeks ago

Selected Answer: A

A for obvious reasons

upvoted 1 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam.

upvoted 3 times

? ?  AzureDP900 12ámonths ago

A is right.

VPC Service Controls helps mitigate the following security risks without sacrificing the performance advantages of direct private access to
Google Cloud resources:

Access from unauthorized networks using stolen credentials: By allowing private access only from authorized VPC networks, VPC Service
Controls protects against theft of OAuth credentials or service account credentials.

Data exfiltration by malicious insiders or compromised code: VPC Service Controls complements network egress controls by preventing
clients within those networks from accessing the resources of Google-managed services outside the perimeter.

VPC Service Controls also prevents reading data from or copying data to a resource outside the perimeter. VPC Service Controls prevents
service operations such as a gsutil cp command copying to a public Cloud Storage bucket or a bq mk command copying to a permanent
external BigQuery table.

upvoted 2 times

? ?  megashit 1áyear ago

Selected Answer: A

Should be A

upvoted 1 times

? ?  [Removed] 1áyear, 2ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

309/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Can someone explain why answer A and not D? You need to have VPN to connect to cloud storage, no?
Correct Answer D

upvoted 2 times

? ?  ss909098 1áyear, 3ámonths ago

Selected Answer: A

yes, definitely A
upvoted 1 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: A

I got similar question on my exam. Answered A.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

310/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #116

Topic 1

You have developed a non-critical update to your application that is running in a managed instance group, and have created a new instance

template with the update that you want to release. To prevent any possible impact to the application, you don't want to update any running

instances. You want any new instances that are created by the managed instance group to contain the new update. What should you do?

A. Start a new rolling restart operation.

B. Start a new rolling replace operation.

C. Start a new rolling update. Select the Proactive update mode.

D. Start a new rolling update. Select the Opportunistic update mode.

Correct Answer: C

Community vote distribution

D (100%)

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago

IMHO the correct answer is d) opportunistic mode, not c) proactive mode.

The requirement is not to update any running instances.

see: https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups
For automated rolling updates, you must set the mode to proactive.

Alternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG applies an
opportunistic update only when you manually initiate the update on selected instances or when new instances are created. New instances
can be created when you or another service, such as an autoscaler, resizes the MIG.

upvoted 50 times

? ?  victory108  Highly Voted ?  1áyear, 11ámonths ago

D. Start a new rolling update. Select the Opportunistic update mode.

upvoted 10 times

? ?  jlambdan  Most Recent ?  3ámonths ago

Selected Answer: D

https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#type

upvoted 2 times

? ?  CGS22 3ámonths ago

Selected Answer: D

D. Exam on Mar21 23

upvoted 4 times

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: D

D. Opportunistic updates are applied only when new instances are created by the managed instance group.

Proactive updates are applied automatically to all instances in the group.

Opportunistic updates are a good option if you want to avoid downtime during an update. They are also a good option if you want to test
a new update before rolling it out to all of your instances.

Proactive updates are a good option if you want to ensure that all of your instances are always up to date. They are also a good option if
you want to automate the update process.

upvoted 1 times

? ?  r1ck 4ámonths, 1áweek ago

MIG updates..
Proactive - Automatic update
Opportunistic - Manual update
"D"

upvoted 1 times

? ?  r1ck 4ámonths, 1áweek ago

https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups

upvoted 1 times

? ?  WFCheong 5ámonths, 2áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

311/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: D

MHO the correct answer is d) opportunistic mode, not c) proactive mode.

The requirement is not to update any running instances.

upvoted 1 times

? ?  windsor_43 6ámonths ago

The Answer is D

Just had my exam today with a pass, this question was in the exam. Dated 31/12/22
Thanks to this site it was by far my most valuable

upvoted 4 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

And thanks to you who is ready to help us

upvoted 1 times

? ?  Gimyjony 6ámonths ago

The right answer as per me is D. Below check the below extract from google documentation.
Opportunistic updates
When the update type is set to OPPORTUNISTIC, the MIG applies updates only when you selectively apply the update to specific instances
or when new instances are created by the MIG. A MIG creates new instances when it is resized to add instances, either automatically or
manually. Compute Engine does not actively initiate requests to apply opportunistic updates.

In certain scenarios, an opportunistic update is useful because you don't want to cause instability to the system if it can be avoided. For
example, if you have a non-critical update that can be applied as necessary without any urgency and you have a MIG that is actively being
autoscaled, perform an opportunistic update so that Compute Engine does not actively tear down your existing instances to apply the
update. When resizing down, the autoscaler preferentially terminates instances with the old template as well as instances that are not yet
in a RUNNING state.

upvoted 1 times

? ?  amit06041982 6ámonths ago

SOME ANSWERS ARE WRONG , be aware

upvoted 1 times

? ?  omermahgoub 6ámonths ago

D. Start a new rolling update. Select the Opportunistic update mode.

Rolling updates allow you to deploy updates to your application by gradually replacing instances in a managed instance group. One way
to prevent any impact to the application when deploying a non-critical update is to use the Opportunistic update mode. In this mode, the
managed instance group will only replace instances that are being terminated or are experiencing problems, and will not replace any
healthy instances. This allows you to deploy the update without affecting the availability of the application.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

Option A, starting a new rolling restart operation, would involve replacing all instances in the managed instance group and could result
in an outage.

Option B, starting a new rolling replace operation, would also involve replacing all instances in the managed instance group and could
result in an outage.

Option C, starting a new rolling update with the Proactive update mode, would involve replacing all instances in the managed instance
group and could result in an outage. Therefore, the best option would be to start a new rolling update with the Opportunistic update
mode. This will allow you to deploy the update without affecting the availability of the application.

upvoted 2 times

? ?  rascalbrick 6ámonths, 2áweeks ago

This show on ky exam

upvoted 1 times

? ?  CDL_Learner 7ámonths ago

When the update type is set to OPPORTUNISTIC, the MIG applies updates only when you selectively apply the update to specific instances
or when new instances are created by the MIG.

So option D

upvoted 1 times

? ?  ashrafh 7ámonths, 1áweek ago

Managed instance groups support two types of update mode:

Automatic, or proactive, updates
Selective, or opportunistic, updates
If you want to apply updates automatically, set the mode to proactive.

Alternatively, if an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG applies an
opportunistic update only when you manually initiate the update on selected instances or when new instances are created.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

312/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  Sukon_Desknot 8ámonths ago

If an automated update is potentially too disruptive, you can choose to perform an opportunistic update. The MIG applies an
opportunistic update only when you manually initiate the update on selected instances or when new instances are created. New instances
can be created when you or another service, such as an autoscaler, resizes the MIG. Compute Engine does not actively initiate requests to
apply opportunistic updates on existing instances

https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups#type

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: D

D - By definition the MIG applies an opportunistic update only when you manually initiate the update on selected instances or when new
instances are created.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

313/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #117

Topic 1

Your company is designing its application landscape on Compute Engine. Whenever a zonal outage occurs, the application should be restored in

another zone as quickly as possible with the latest application data. You need to design the solution to meet this requirement. What should you

do?

A. Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to

restore the disk in the same zone.

B. Con gure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application

data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same region. Use the

regional persistent disk for the application data.

C. Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to

restore the disk in another zone within the same region.

D. Con gure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application

data. Whenever a zonal outage occurs, use the instance template to spin up the application in another region. Use the regional persistent disk

for the application data.

Correct Answer: D

Community vote distribution

B (100%)

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago
Answer is B, it only request zonal resiliency.
Regional persistent disk is a storage option that provides synchronous replication of data between two zones in a region. Regional
persistent disks can be a good building block to use when you implement HA services in Compute Engine.

https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk

upvoted 43 times

? ?  AmitRBS 1áyear, 1ámonth ago

B. Agree, clearly itÆs B. Focus on keyword ôzoneö

upvoted 4 times

? ?  Ssoumya  Highly Voted ?  1áyear, 12ámonths ago

Answer is B

upvoted 14 times

? ?  mifrah  Most Recent ?  3ámonths ago

B. In my opinion, I cannot use a regional disk in a different region!!! So, it can only be another zone in the same region. Therefore D must
be wrong!

upvoted 1 times

? ?  SambuSoni 4ámonths, 4áweeks ago

Answer B is Correct - since it talks about spin up application in different zone but same region.

Whereas,D is incoorect , since its talking about spin up application in different region which is not our requirement.

upvoted 1 times

? ?  CosminCiuc 5ámonths ago

If it is a regional persistent disk created in region A for example. If I start the compute engine instance in the region B, how am I going to
use a regional disk from region A (another region)? I do not think it is possible. So answer D should be excluded.
I do believe that the correct answer is B.

upvoted 2 times

? ?  windsor_43 6ámonths ago

The Answer is B

Just had my exam today with a pass, this question was in the exam. Dated 31/12/22
Thanks to this site it was by far my most valuable

upvoted 5 times

? ?  rascalbrick 6ámonths, 2áweeks ago
14/12/22 Exam,but IM failed :(

upvoted 1 times

? ?  Praveen_G 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

314/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Tomorrow 12/27/22 is my exam :)

upvoted 2 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

How was it?

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B. Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the
application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same
region. Use the regional persistent disk for the application data.

upvoted 1 times

? ?  kmreeves 8ámonths, 3áweeks ago

I think this has to be B not D. One the question talks about a zonal failure not a region failure, two the answers both talk about regional
storage not multi-region.

upvoted 1 times

? ?  VinayNune 9ámonths ago

Selected Answer: B

Answer is B

upvoted 2 times

? ?  VinayNune 9ámonths ago

Answer is B. No need to move to another region. This will most probably increase the latency of certain customers.

upvoted 2 times

? ?  cloudinit 10ámonths, 2áweeks ago

Selected Answer: B

Its B, the regional PD is available across the zones.

upvoted 1 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam.

upvoted 2 times

? ?  Bahubali1988 10ámonths, 3áweeks ago

Selected Answer: B

Clearly B - Every region will have different zones, if any issue with one zone , then can replicate/fail over to another zone in the same
region. Instead of creating the instance in new region.

upvoted 1 times

? ?  mv2000 11ámonths, 3áweeks ago

06/30/2022 Exam

upvoted 6 times

? ?  AzureDP900 12ámonths ago

B is right, D is close enough with region so it is not correct.

upvoted 1 times

? ?  AzureDP900 12ámonths ago

I agree with other folks, there are no second thought and is only required for Zone.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

315/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #118

Topic 1

Your company has just acquired another company, and you have been asked to integrate their existing Google Cloud environment into your

company's data center. Upon investigation, you discover that some of the RFC 1918 IP ranges being used in the new company's Virtual Private

Cloud (VPC) overlap with your data center IP space. What should you do to enable connectivity and make sure that there are no routing con icts

when connectivity is established?

A. Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no

overlapping IP space.

B. Create a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT on the overlapping IP

space.

C. Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route advertisement to

block the overlapping IP space.

D. Create a Cloud VPN connection from the new VPC to the data center, and apply a  rewall rule that blocks the overlapping IP space.

Correct Answer: A

Community vote distribution

A (63%)

C (31%)

6%

? ?  VishalB  Highly Voted ?  1áyear, 11ámonths ago

Correct Answer: A
- IP Should not overlap so applying new IP address is the solution

upvoted 34 times

? ?  zanfo 1áyear, 3ámonths ago

A is not correct. "What should you do to enable connectivity and make sure that there are no routing conflicts when connectivity is
established?" if you apply VPN con BGP, the actual IP address will be propagated to on prem environment with overlapping RFC1918 as
result. B is correct with custom route

upvoted 2 times

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago

Answer is C.
https://cloud.google.com/network-connectivity/docs/router/how-to/advertising-custom-ip

upvoted 29 times

? ?  RKS_2021 1áyear, 11ámonths ago

ANS is B
https://cloud.google.com/architecture/best-practices-vpc-design

upvoted 6 times

? ?  elenamatay 1áyear, 5ámonths ago

You can't use Cloud NAT according to this documentation: https://cloud.google.com/nat/docs/troubleshooting#overlapping-ip-
addresses

"Can I use Cloud NAT to connect a VPC network to another network to work around overlapping IP addresses? No, Cloud NAT cannot
apply to any custom route whose next hop is not the default internet gateway. For example, Cloud NAT cannot apply to traffic sent
to a next hop Cloud VPN tunnel, even if the destination is a publicly routable IP address."

upvoted 14 times

? ?  imgcp 1áyear, 11ámonths ago

B is NOT correct. Cloud NAT is specifically used for translating the IP address of the outbound packets destined to the Internet. But
this question is about using VPN communication between two private IP address spaces (RFC1918). Cloud NAT cannot achieve the
purpose here, you can't use Cloud NAT to translate from one private IP to another private ip. I would vote for C.

upvoted 11 times

? ?  Bill831231 1áyear, 8ámonths ago

Thanks for the clarification, just one question, without a solution like NAT or reip, the service on the devices with overlapping IP
subnet will be unavailable for on-premise devices, not sure if the question also about this

upvoted 1 times

? ?  imgcp 1áyear, 11ámonths ago

*you can't use Cloud NAT to translate from one private IP to another private ip to avoid overlapping ip range issue.

upvoted 4 times

? ?  JohnnyBG 1áyear ago

Be careful because CloudNAT for private IPs will be GA soon

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

316/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  meh009 1áyear, 8ámonths ago

The Q states to establish connectivity. This would merely prevent that. Ans is A

upvoted 5 times

? ?  salim_  Most Recent ?  1ámonth, 2áweeks ago

Selected Answer: C

I believe answer is C:
https://cloud.google.com/network-connectivity/docs/router/how-to/advertising-subnets

upvoted 2 times

? ?  JC0926 2ámonths, 2áweeks ago

Selected Answer: A

A. Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no
overlapping IP space.

In a situation where RFC 1918 IP ranges overlap between the new company's VPC and your data center IP space, it is important to
reconfigure the IP addresses to avoid any conflicts. To enable connectivity, first create a Cloud VPN connection between the new VPC and
the data center. Then, set up a Cloud Router to manage routing between the environments. Finally, apply new IP addresses to the new
company's VPC to ensure there is no overlapping IP space with your data center. This will prevent routing conflicts when connectivity is
established.

upvoted 1 times

? ?  Deb2293 3ámonths, 3áweeks ago

Selected Answer: A

Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no
overlapping IP space.When there is overlapping IP space between two networks that need to be connected, it is necessary to re-address
one of the networks to eliminate the conflict.

upvoted 1 times

? ?  i_am_robot 5ámonths ago

Your company has a project in Google Cloud with three Virtual Private Clouds (VPCs). There is a Compute
D283ABFBEDB32CDCE3B3406B9C29DB2F
Engine instance on each VPC. Network subnets do not overlap and must remain separated. The network configuration is shown below.

upvoted 1 times

? ?  thamaster 6ámonths ago

D is out because firewall is for blocking traffic, C is out router does not prevent overlapping IP, B nat does not like that, answer is A

upvoted 1 times

? ?  markus_de 6ámonths, 3áweeks ago

Selected Answer: C

Applying new IP addresses is the second step as it is time intensive, needs a lot of investigation and error-prone. Solution is to establish
connection first (with C) and as a next step re-configure IP addresses.

upvoted 5 times

? ?  jaxclain 7ámonths ago

Selected Answer: A

The correct answer is A, not sure why some here are confused with C... Cloud NAT won't serve this purpose, you can read the
documentation here: https://cloud.google.com/nat/docs/troubleshooting#overlapping-ip-addresses

upvoted 2 times

? ?  moustaoui 7ámonths ago

Answer is A

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

By definition, In cases where you have a VPC subnet and an on-premises route advertisement with overlapping IP ranges, Google Cloud
directs egress traffic depending on their IP ranges.
With custom route advertisements, you choose which routes Cloud Router advertises to your on-premises router through the Border
Gateway Protocol (BGP).
Therefore:
A - is a permanent solution to the problem which will require time, resources, testing and funding should you decide to change the ip
addresses
C - is an interim solution that will help to integrate the new company quickly, with view to change the overlaping range at some point in
the future (if needed)

The question does not mention any information on the time, cost or complexity, therefore I will go for A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

317/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 6 times

? ?  enter_co 8ámonths, 2áweeks ago

Selected Answer: C

The correct answer is A, because:
- we want to establish communication
- we want to solve the overlapping ranges problem.
Therefore,
- The correct answer is C https://cloud.google.com/network-connectivity/docs/router/how-to/advertising-removing-routes#bgp-session
- A is not correct, because applying new addresses only extends the list of advertised addresses
- B won't stop cloud router advertisements
- D firewall rules won't change the list of advertised networks, it only treats the symptom, not the cause

upvoted 1 times

? ?  AzureDP900 9ámonths, 2áweeks ago

A is right

upvoted 1 times

? ?  ShadowLord 10ámonths, 1áweek ago

Selected Answer: C

Options C
"Your company has just acquired another company" . Changing IP is not solution.
Since solution are working independent, the Cloud Router with blocking IP is solution from traffic from each side wrongly traversing to
other side ...

upvoted 3 times

? ?  MQQ 11ámonths, 1áweek ago

it can't be B: https://cloud.google.com/nat/docs/troubleshooting#overlapping-ip-addresses

upvoted 4 times

? ?  imarri876 11ámonths, 2áweeks ago

Selected Answer: B

I'd go with B

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

318/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #119

Topic 1

You need to migrate Hadoop jobs for your company's Data Science team without modifying the underlying infrastructure. You want to minimize

costs and infrastructure management effort. What should you do?

A. Create a Dataproc cluster using standard worker instances.

B. Create a Dataproc cluster using preemptible worker instances.

C. Manually deploy a Hadoop cluster on Compute Engine using standard instances.

D. Manually deploy a Hadoop cluster on Compute Engine using preemptible instances.

Correct Answer: A

Reference:

https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs

Community vote distribution

B (57%)

A (43%)

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago
Should be B, you want to minimize costs.
https://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#preemptible_and_non-preemptible_secondary_workers

upvoted 56 times

? ?  ale_brd_ 7ámonths, 1áweek ago

I think it's A.
The question does not mention anything about minimize the costs, all the questions in GCP exams that require minimize the costs as
requirement literally mention that in the question.
Also in order to minimize the costs you need to build jobs that are fault tolerant, as workers instances are preemptible. This also
requires some kind of Dev investment of work. So if not mentioned in the question fault tolerant and minimize costs then is not
required/needed.

Doc states below:
Only use preemptible nodes for jobs that are fault-tolerant or that are low enough priority that occasional job failure won't disrupt your
business.

upvoted 7 times

? ?  zetalexg 6ámonths, 2áweeks ago

It's dissapointing that you waste your time writting on this topic instead of paying attention at the questions.

upvoted 18 times

? ?  xprtz1 6ámonths, 3áweeks ago

learn to read before apply to the exam

upvoted 6 times

? ?  jasenmornin 6ámonths, 3áweeks ago

The question is very short and literally says "You want to minimize costs" lol

upvoted 9 times

? ?  Yogi42 5ámonths, 1áweek ago

A cost-savings consideration: Using preemptible VMs does not always save costs since preemptions can cause longer job execution
with resulting higher job costs. This is mentioned in above link So I think Ans should be A

upvoted 2 times

? ?  Sukon_Desknot 8ámonths ago

"without modifying the underlying infrastructure" is the watch word. Most likely did not utilize preemptible on-premises

upvoted 4 times

? ?  J19G 1áyear, 8ámonths ago

Agree, the migration guide also recommends to think about preemptible worker nodes:
https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs#using_preemptible_worker_nodes

upvoted 3 times

? ?  ale_brd_ 7ámonths, 1áweek ago

I think it's A.
The question does not mention anything about minimize the costs, all the questions in GCP exams that require minimize the costs
as requirement literally mention that in the question.
Also in order to minimize the costs you need to build jobs that are fault tolerant, as workers instances are preemptible. This also
requires some kind of Dev investment of work. So if not mentioned in the question fault tolerant and minimize costs then is not
required/needed.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

319/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Doc states below:
Only use preemptible nodes for jobs that are fault-tolerant or that are low enough priority that occasional job failure won't disrupt
your business.
upvoted 2 times

? ?  grejao 2ámonths, 4áweeks ago

OMG, you again?

zetalexg says:
It's dissapointing that you waste your time writting on this topic instead of paying attention at the questions.

upvoted 1 times

? ?   recloud  Highly Voted ?  1áyear, 11ámonths ago

It's A, the primary workers can only be standard, where secondary workers can be preemtible.------In addition to using standard Compute
Engine VMs as Dataproc workers (called "primary" workers), Dataproc clusters can use "secondary" workers.
There are two types of secondary workers: preemptible and non-preemptible. All secondary workers in your cluster must be of the same
type, either preemptible or non-preemptible. The default is preemptible.

upvoted 30 times

? ?  Manh 1áyear, 9ámonths ago

agreed

upvoted 2 times

? ?  mateuszma  Most Recent ?  1ámonth, 2áweeks ago

Selected Answer: A

Voting A - preemptible/spot are nice but not in this case. Such setup would require additional changes within ETL pipelines to support
unexpected brakes.

upvoted 1 times

? ?  JC0926 2ámonths, 1áweek ago

Selected Answer: B

B
minimize costs
upvoted 1 times

? ?  taer 3ámonths ago

Selected Answer: A

Preemptible instances may be terminated at any time, which could disrupt your Hadoop jobs.

upvoted 1 times

? ?  musumusu 3ámonths ago

Answer B:
2 reasons: Save Cost and Data Science Team job, which means the job will not run all the time like client side global application or real
time processing.So, job will be used mainly for training models etc.

upvoted 1 times

? ?  mifrah 3ámonths ago

I am voting for A: You want to minimize costs and infrastructure management effort.
Because I do not have any information about the duration of the jobs, I would start with Pre-emptible VMs. Cost reduction of up to 70%, if
I remember correctly.

upvoted 1 times

? ?  segkhachat 3ámonths, 3áweeks ago

Selected Answer: A

Should be A, not all processes are compatible with preemptive instances. There is no mentioning about fault tolerancity.

upvoted 1 times

? ?  Deb2293 3ámonths, 3áweeks ago

Selected Answer: B

to minimize costs and infrastructure management effort, it should be B

upvoted 1 times

? ?  telp 4ámonths ago

Selected Answer: B

Reduce cost = preemptible
No management = no VM
so answer is B
upvoted 2 times

? ?  razabpn 4ámonths, 1áweek ago

Selected Answer: B

B: You want to minimize costs and infrastructure management efforts.
The preemptible worker provides 1. lower price 2ùreduction in infra management, unlike VMs which you need to manage.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

320/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  WFCheong 5ámonths, 2áweeks ago

Selected Answer: B

Should be B, Questions require to minimize costs.

upvoted 1 times

? ?  examch 5ámonths, 3áweeks ago

Selected Answer: B

B is the correct answer, Use ephemeral cluster to reduce cost, use small persistent storage only if necessary.

https://cloud.google.com/architecture/hadoop#moving_to_an_ephemeral_model

upvoted 1 times

? ?  windsor_43 6ámonths ago

The Answer is B

Just had my exam today with a pass, this question was in the exam. Dated 31/12/22
Thanks to this site it was by far my most valuable

upvoted 4 times

? ?  richlee0423 6ámonths ago

Selected Answer: B

Google Dataproc supports different instance types, or preemptible virtual machines, so you can leverage the ideal server sizes that fit your
needs.

upvoted 1 times

? ?  thamaster 6ámonths ago

Selected Answer: B

I'll chose B but I'm not sure because you can't use only pre emptible you need at least one instance.

upvoted 1 times

? ?  gonlafer 6ámonths, 2áweeks ago

Selected Answer: B

Should be B, you want to minimize costs.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

321/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #120

Topic 1

Your company has a project in Google Cloud with three Virtual Private Clouds (VPCs). There is a Compute Engine instance on each VPC. Network

subnets do not overlap and must remain separated. The network con guration is shown below.

Instance #1 is an exception and must communicate directly with both Instance #2 and Instance #3 via internal IPs. How should you accomplish

this?

A. Create a cloud router to advertise subnet #2 and subnet #3 to subnet #1.

B. Add two additional NICs to Instance #1 with the following con guration: ?Çó NIC1 ?ùï VPC: VPC #2 ?ùï SUBNETWORK: subnet #2 ?Çó NIC2

?ùï VPC: VPC #3 ?ùï SUBNETWORK: subnet #3 Update  rewall rules to enable tra c between instances.

C. Create two VPN tunnels via CloudVPN: 1 óÇ? between VPC #1 and VPC #2. 1 óÇ? between VPC #2 and VPC #3. Update  rewall rules to

enable tra c between the instances.

D. Peer all three VPCs: ?Çó Peer VPC #1 with VPC #2. ?Çó Peer VPC #2 with VPC #3. Update  rewall rules to enable tra c between the

instances.

Correct Answer: B

Community vote distribution

B (68%)

C (18%)

14%

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago

According to my understanding the requirement is that only VM1 shall be able to communicate with VM2 and VM3, but not VM2 with VM3.
We can exclude d) as d) would enable VM2 to communicate with VM3 as well - my assumption is, that if the quizzer wanted that d) is the
correct answer, he would make just 2 peerings - 1x between VM1 and VM2 and 1x between VM1 and VM3 repectively the VPCs.
We can exclude c) as well - there is no connection between VPC1 and VPC3.
IMHO a) will not work.
So the only correct answer seems to be b) - what I don't understand is why we have to update the firewall rules as IMHO the default
firewall rules enable such communication (maybe some restrictive rules are implemented - not enough details in the question to clarify
that part). Please correct me if I am wrong.

upvoted 23 times

? ?  Ishu_awsguy 10ámonths, 1áweek ago

The answer is "B". The following link has this - "Use multiple network interfaces when an individual instance needs access to more than
one VPC network, but you don't want to connect both networks directly." https://cloud.google.com/vpc/docs/multiple-interfaces-
concepts

upvoted 10 times

? ?  Pankaj_007 7ámonths, 1áweek ago

B will not work.
VM instances within a VPC network can communicate among themselves using internal IP addresses as long as firewall rules permit.
However, no internal IP address communication is allowed between networks, unless you set up mechanisms such as VPC Network
Peering or Cloud VPN.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

322/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  sameer2803 6ámonths ago

this link says VM can have multiple NICs and attached to different VPCs.
https://cloud.google.com/vpc/docs/create-use-multiple-interfaces
so B is the answer

upvoted 4 times

? ?  lazybeanbag 1áyear, 11ámonths ago

I think it is because the instances are in separate VPCs.

"Google Cloud Virtual Private Cloud (VPC) networks are by default isolated private networking domains. Networks have a global scope
and contain regional subnets. VM instances within a VPC network can communicate among themselves using internal IP addresses as
long as firewall rules permit. However, NO INTERNAL IP ADDRESS COMMUNICATION IS ALLOWED BETWEEN networks, unless you set
up mechanisms such as VPC Network Peering or Cloud VPN."

The instructions for setting up multiple interfaces tells you to check your firewall rules as as the firewall rules of the VPC apply to the
network interface that it is attached to.

https://cloud.google.com/vpc/docs/multiple-interfaces-concepts#firewall_rules_and_multiple_network_interfaces

upvoted 7 times

? ?  Ishu_awsguy 10ámonths, 1áweek ago

The answer is "B". The following link has this - "Use multiple network interfaces when an individual instance needs access to more
than one VPC network, but you don't want to connect both networks directly." https://cloud.google.com/vpc/docs/multiple-
interfaces-concepts

upvoted 3 times

? ?  JeffClarke111 1áyear, 11ámonths ago

Correct, maybe fw on the VM

upvoted 2 times

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is B

upvoted 11 times

? ?  coutcin 1áyear, 1ámonth ago

Instances are exist. You can not add or remove additional NICs to a VM

upvoted 3 times

? ?  natpilot  Most Recent ?  2ámonths, 2áweeks ago

Is D the correct, peering with adeguate forewall rule for only communication of Instance 1 with Instance 2 and 3

upvoted 1 times

? ?  mifrah 3ámonths ago

I vote for B:
VPC peering does not support "cascading". Peer VPC 1 with VPC 2, and VPC 2 with VPC 3 does not allow traffic from VPC 1 to VPC 3.

upvoted 1 times

? ?  razabpn 4ámonths, 1áweek ago

Selected Answer: B

B: NIC usecase when an individual instance needs access to more than one VPC network, but you don't want to connect both networks
directly
https://cloud.google.com/vpc/docs/multiple-interfaces-concepts

upvoted 1 times

? ?  examch 5ámonths, 3áweeks ago

Selected Answer: B

B is the correct answer,

Connect the VPC1 instance to VPC2 instance with NIC1 and Connect VPC1 instance to VPC3 instance with NIC2. And update firewall rules
to enable traffic between them.

https://cloud.google.com/vpc/docs/multiple-interfaces-concepts#firewall_rules_and_multiple_network_interfaces

upvoted 1 times

? ?  thamaster 6ámonths ago

Selected Answer: B

best practice is to add NIC to first instance

upvoted 1 times

? ?  ANKITMANDLA 6ámonths, 3áweeks ago

Only solution is peering. N1 peering to n3 and n3 to n1 makes all network peered. So answer should be D

upvoted 1 times

? ?  Pankaj_007 7ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

323/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B would be incorrect --> As without VPC peering or VPN it will not come into Play.
D --> This is good as once VPN is established from 1 --> 2 and from 2 --> 3 ... data can flow from 1 to 3 via 2 ...

upvoted 1 times

? ?  Pankaj_007 7ámonths, 1áweek ago
I mean C should be correct ..

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: B

B is ok. C&D are wrong because they connect 1 to 2 and 2 to 3 , not 1 to3. 2 and 3 must be unreachable

upvoted 2 times

? ?  vgiuseppe77 7ámonths, 4áweeks ago

Selected Answer: C

- A seems incomplete and not compliant with "network subnets must remain separated" requirement.
- B makes no sense, because if you add NICs with IPs of other subnet, the firewall rules are not needed. Therefore, typically this type of
configuration is considered a safety hole.
- D is incorrect because peering is not transitive.
- So, remains only C.

upvoted 3 times

? ?  Flight1976 2áweeks ago

B will not work.
accroding to https://cloud.google.com/vpc/docs/create-use-multiple-interfaces , You can only configure a network interface when you
create an instance. You cannot add or remove network interfaces from an existing VM.

upvoted 1 times

? ?  n_nana 5ámonths, 3áweeks ago

As stated here, You should check firewalls rules because it will be applied to the NIC.
check this https://cloud.google.com/vpc/docs/multiple-interfaces-concepts#firewall_rules_and_multiple_network_interfaces

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

A. Cloud Router will do half the job, it will allow all traffic
B. Looks like an option - not the best but it answers the question
C. HA VPN gateway in each VPC network (assuming in the same region only), this will do half the job as it needs a lot more to be
configured
D. Only directly peered networks can communicate. Transitive peering is not supported.

upvoted 1 times

? ?  AzureDP900 9ámonths, 2áweeks ago
B seems to be the best choice

upvoted 1 times

? ?  cloudinit 10ámonths, 2áweeks ago

Selected Answer: B

The answer is B, which alone enables communication from Instance 1 to Instance 2 and 3.

upvoted 1 times

? ?  kiranp29587 1áyear ago

Every instance in a VPC network has a default network interface. You can create additional network interfaces attached to your VMs, but
each interface must attach to a different VPC network. Multiple network interfaces let you create configurations in which an instance
connects directly to several VPC networks. Answer B

upvoted 2 times

? ?  ryzior 1áyear ago

Selected Answer: B

D, I think, is wrong, it connects 1 and 2 then 2 and 3 instead 1 and 3.

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: D

vpc peering is way to go

upvoted 1 times

? ?  marleybu 1áyear ago

Peering would be good but transitive peering is not support with GCP (https://cloud.google.com/vpc/docs/vpc-peering#restrictions)
If we peer 1 with 2 and 2 with 3, VPC 1 can't communicate with VPC 3

upvoted 1 times

? ?  parmand 10ámonths ago

Agreed. This is a terrible question. The correct answer is peering 1 with 2 and 1 with 3, but that isn't an option here. I doubt this is a
real question on the exam.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

324/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

325/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #121

Topic 1

You need to deploy an application on Google Cloud that must run on a Debian Linux environment. The application requires extensive con guration

in order to operate correctly. You want to ensure that you can install Debian distribution updates with minimal manual intervention whenever they

become available. What should you do?

A. Create a Compute Engine instance template using the most recent Debian image. Create an instance from this template, and install and

con gure the application as part of the startup script. Repeat this process whenever a new Google-managed Debian image becomes available.

B. Create a Debian-based Compute Engine instance, install and con gure the application, and use OS patch management to install available

updates.

C. Create an instance with the latest available Debian image. Connect to the instance via SSH, and install and con gure the application on the

instance. Repeat this process whenever a new Google-managed Debian image becomes available.

D. Create a Docker container with Debian as the base image. Install and con gure the application as part of the Docker image creation

process. Host the container on Google Kubernetes Engine and restart the container whenever a new update is available.

Correct Answer: B

Reference:

https://cloud.google.com/compute/docs/os-patch-management

Community vote distribution

B (91%)

9%

? ?  victory108  Highly Voted ?  1áyear, 11ámonths ago

B. Create a Debian-based Compute Engine instance, install and configure the application, and use OS patch management to install
available updates.
upvoted 18 times

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is B

upvoted 8 times

? ?  natpilot  Most Recent ?  2ámonths, 2áweeks ago

A is correct, with template and startup script you can create multiple instance with minimal manual intervention; when the new debian
release will be available, you need update only the template with new image of debian distribution.

upvoted 1 times

? ?  examch 5ámonths, 3áweeks ago

Selected Answer: B

B is the correct answer,

Use OS patch management to apply operating system patches across a set of Compute Engine VM instances (VMs). Long running VMs
require periodic system updates to protect against defects and vulnerabilities.

The OS patch management service has two main components:

Patch compliance reporting, which provides insights on the patch status of your VM instances across Windows and Linux distributions.
Along with the insights, you can also view recommendations for your VM instances.
Patch deployment, which automates the operating system and software patch update process. A patch deployment schedules patch jobs.
A patch job runs across VM instances and applies patches.

https://cloud.google.com/compute/docs/os-patch-management

upvoted 3 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 2 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: B

B is the simplest option and with minimal intervention. Other answers may be technically possible but the question does not ask for
anything else (e.g. containers, templates, etc.)

upvoted 2 times

? ?  AzureDP900 9ámonths, 2áweeks ago

I will go with B , there is no need to application configuration each time

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

326/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: B

B is the answer.

https://cloud.google.com/compute/docs/os-patch-management
Use OS patch management to apply operating system patches across a set of Compute Engine VM instances (VMs).

upvoted 2 times

? ?  Jay_Krish 9ámonths, 3áweeks ago

Selected Answer: B

Key words : "with minimal manual intervention whenever they become available"

With OS Patch Management the OS will have all the latest available updates at that point automatically which wouldn't be the case with A.

upvoted 2 times

? ?  backhand 10ámonths, 3áweeks ago

vote B
about A using latest template for update patch, next time you have to make template again.
ans B, make once setting for good basically with no intervention.

upvoted 1 times

? ?  patashish 10ámonths, 3áweeks ago

A is correct answer.
I am not able to understand B option and ppl are simply adding patch management URL as reference. How this will relate with answer ?

A is correct answer.

upvoted 1 times

? ?  enter_co 8ámonths, 2áweeks ago

A only answers to 'minimal management overhead', but not to 'install patches as soon as possible'.

upvoted 1 times

? ?  Ric350 11ámonths, 2áweeks ago

The ask in this question is "you want to ensure that you can install Debian distribution updates (which is OS updates specifically) with
minimal manual intervention whenever they become available." That is accomplished by an OS patch management.
https://cloud.google.com/compute/docs/os-patch-management

upvoted 1 times

? ?  Jhandley67 11ámonths, 3áweeks ago

Selected Answer: A

No. The answer is "A". The key is the least amount of manual intervention. This is identical to the qwiklabs that accompany the course. The
app configuration should be included in the metadata as a script. Patching the OS has nothing to do with the complicated configuration of
the "Application". Hence "A".

upvoted 2 times

? ?  Ric350 11ámonths, 2áweeks ago

@Jhandley67 Yes, patching the OS has nothing to do with the complicated configuration. And yes, that would be ideal when creating a
NEW instance. However, the ask in the question is "you want to ensure that you can install Debian distribution updates (which is OS
updates, not application updates) with minimal manual intervention whenever they become available." That part of the app
configuration being complicated is to trip people up and irrelevant. It is done once when you build a new instance and configure the
app initially. The question is not asking about app updates, it clearly states OS updates. The answer then is B.

upvoted 4 times

? ?  Jhandley67 11ámonths ago

@Ric350 well explained and you are right. The correct answer is "B".

upvoted 2 times

? ?  mv2000 11ámonths, 3áweeks ago

06/30/2022 Exam

upvoted 6 times

? ?  AzureDP900 12ámonths ago

I am not sure why folks are saying B, A is right option.

upvoted 1 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

u only said B in above comments - "I will go with B , there is no need to application configuration each time"

upvoted 1 times

? ?  GPK 1áyear ago

B agree, exam 25/06/2022

upvoted 2 times

? ?  JoeThach 1áyear ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

327/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: B

B. The key words are "minimal intervention", and "the application requires extensive configuration" > OS Patch Management requires the
least manual intervention, and lowest risk too

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

328/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #122

Topic 1

You have an application that runs in Google Kubernetes Engine (GKE). Over the last 2 weeks, customers have reported that a speci c part of the

application returns errors very frequently. You currently have no logging or monitoring solution enabled on your GKE cluster. You want to diagnose

the problem, but you have not been able to replicate the issue. You want to cause minimal disruption to the application. What should you do?

A. 1. Update your GKE cluster to use Cloud Operations for GKE. 2. Use the GKE Monitoring dashboard to investigate logs from affected Pods.

B. 1. Create a new GKE cluster with Cloud Operations for GKE enabled. 2. Migrate the affected Pods to the new cluster, and redirect tra c for

those Pods to the new cluster. 3. Use the GKE Monitoring dashboard to investigate logs from affected Pods.

C. 1. Update your GKE cluster to use Cloud Operations for GKE, and deploy Prometheus. 2. Set an alert to trigger whenever the application

returns an error.

D. 1. Create a new GKE cluster with Cloud Operations for GKE enabled, and deploy Prometheus. 2. Migrate the affected Pods to the new

cluster, and redirect tra c for those Pods to the new cluster. 3. Set an alert to trigger whenever the application returns an error.

Correct Answer: C

Reference:

https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine

Community vote distribution

A (71%)

C (27%)

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago

According to the reference, answer should be A.
https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine

upvoted 39 times

? ?  enter_co 8ámonths, 2áweeks ago

The problem in A) answer is that it is not alert-based. All recent trainings recommend use of alerts for troubleshooting, not dashboards.

upvoted 1 times

? ?  poseidon24 1áyear, 11ámonths ago

correct, from GCP best practices for GKE we should rely on native logging capabilities. No need for additional solutions like
Prometheus. Also it is about reviewing logs, monitoring the service, not receiving alerts each time its happens, that will not provide any
insight on the issue.

upvoted 16 times

? ?  victorlie 1áyear, 10ámonths ago

Also, as long you know there is a problem, i think you should investigate immediately the issue, not wait for new errors

upvoted 6 times

? ?  MF2C 1áyear, 6ámonths ago

But updating cluster requires downtime, isn't it?

upvoted 3 times

? ?  Nick89GR 1áyear, 2ámonths ago

No it actually does not require to shut down the cluster:
https://cloud.google.com/stackdriver/docs/solutions/gke/installing#console_1

upvoted 3 times

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago
IMHO a) is the correct answer, not c)
The point is, that we have a scenario in that often errors in GKE happen - within 2 week a lot of people complained about a lot of errors.
For the past we have no data at all as we have not monitored anything. That means we will collect data from now on to find out what the
problem is. The additional value of an alert is not clear - and it for me not clear why we need additionally to install Prometheus considering
that until now we had no GKE monitoring at all. Please correct me if I am wrong.

upvoted 18 times

? ?  3ana  Most Recent ?  1ámonth, 1áweek ago

hi guys, for those who have the complete questions for this PCA exam, would you be kind enough to share it with me? I am scheduled to
take the exam this coming June, please send it to anashela.03@gmail.com. Thanks!

upvoted 1 times

? ?  JC0926 2ámonths, 2áweeks ago

Selected Answer: A

A. 1. Update your GKE cluster to use Cloud Operations for GKE. 2. Use the GKE Monitoring dashboard to investigate logs from affected
Pods.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

329/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

By updating your GKE cluster to use Cloud Operations for GKE (formerly known as Stackdriver), you enable monitoring and logging
without disrupting the application. The GKE Monitoring dashboard allows you to investigate logs from affected Pods, which helps you
diagnose the problem that customers have reported. This approach minimizes disruption to the application while providing the necessary
information to identify and resolve the issue

upvoted 1 times

? ?  kratosmat 2ámonths, 2áweeks ago

Selected Answer: A

As described here
https://cloud.google.com/stackdriver/docs/solutions/gke
is it possible to install prometheus as part of cloud operation suite.

upvoted 1 times

? ?  thamaster 6ámonths ago

Selected Answer: A

you want to minimize change so A is the best answer also you don't know what is the issue by reviewing the logs you can find something

upvoted 1 times

? ?  omermahgoub 6ámonths ago

The best option is D. 1. Create a new GKE cluster with Cloud Operations for GKE enabled, and deploy Prometheus. 2. Migrate the affected
Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Set an alert to trigger whenever the application returns an
error.

Here's why:

Option A does not involve creating a new GKE cluster, which means you will not be able to isolate the affected Pods from the rest of the
application. This can make it difficult to diagnose the issue without disrupting the entire application.

upvoted 3 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Thanks again for such a descriptive and well convinced explanation

upvoted 1 times

? ?  CosminCiuc 5ámonths ago

You cannot automatically migrate pods from one cluster to another. You would have to manually deploy the workloads on the new
cluster. And you have the problem of configuring the services in the new cluster. You will need to use new IP addresses for the services,
modify the DNS to direct the client applications to the services from the new cluster. Very, very complicated. I would exclude the answer
that propose creation of new GKE clusters.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

Option B involves deploying a new version of the application, which may or may not fix the issue. Additionally, this option does not
address the root cause of the issue or provide a way to monitor the application for future errors.

Option C involves deploying a new version of the application and setting an alert to trigger whenever the application returns an error.
However, this option does not involve creating a new GKE cluster or migrating the affected Pods to a separate cluster, which means
that the issue could continue to affect the entire application.

upvoted 1 times

? ?  gonlafer 6ámonths, 2áweeks ago

Selected Answer: C

Question states problem cannot be replicated. So alerting is required to review the right logs at right time. Hence A is not adequate
solution and C is the right one

upvoted 1 times

? ?  jaxclain 7ámonths ago

Selected Answer: A

I would also say A, Google recommended practices will always encourage not to select an option that will overkill what you need.

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Jailbreaker 7ámonths, 3áweeks ago

Selected Answer: A

"You have not been able to replicate the issue." is the key, as you can't replicate the issue, how come will you define the right alert?
A is definetly the right answer for me !

upvoted 3 times

? ?  zr79 8ámonths, 2áweeks ago

No need for Prometheus as we have monitoring option which is a managed service

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

330/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A - this is a simple question, no need for Prometheus and no need to build another cluster....!

upvoted 2 times

? ?  jagan79 10ámonths, 1áweek ago

Selected Answer: A

Use Promethues only in case of multi cloud solution hence A
Google Cloud Managed Service for Prometheus is Google Cloud's fully managed multi-cloud solution for Prometheus metrics. It lets you
globally monitor and alert on your workloads, using Prometheus, without having to manually manage and operate Prometheus at scale.

upvoted 3 times

? ?  patashish 10ámonths, 3áweeks ago

A is correct answer
Hint - minimal disruption

upvoted 4 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: A

https://cloud.google.com/blog/products/management-tools/using-logging-your-apps-running-kubernetes-engine

upvoted 1 times

? ?  deenee 11ámonths, 2áweeks ago

Selected Answer: A

A seems right. as first issue is with logging and then monitoring. Cloud logging and monitoring is deeply integrated with GKE. so instead
of using any other tool, cloud native solutions will have more weightage. Also, only alert might not help for debugging, logs would be
needed

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

331/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #123

Topic 1

You need to deploy a stateful workload on Google Cloud. The workload can scale horizontally, but each instance needs to read and write to the

same POSIX  lesystem. At high load, the stateful workload needs to support up to 100 MB/s of writes. What should you do?

A. Use a persistent disk for each instance.

B. Use a regional persistent disk for each instance.

C. Create a Cloud Filestore instance and mount it in each instance.

D. Create a Cloud Storage bucket and mount it in each instance using gcsfuse.

Correct Answer: D

Reference:

https://cloud.google.com/storage/docs/gcs-fuse

Community vote distribution

C (100%)

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago

Answer should be C,
https://cloud.google.com/storage/docs/gcs-fuse#notes

upvoted 28 times

? ?  elainexs 1áyear ago

"ôCloud Storage FUSE is an open sourceá[FUSE](http://fuse.sourceforge.net/)áadapter that allows you to mount Cloud Storage buckets as
file systems on Linux or macOS systems. It also provides a way for applications to upload and download Cloud Storage objects using
standard file system semantics. Cloud Storage FUSE can be run anywhere with connectivity to Cloud Storage, including Google
Compute Engine VMs or on-premises systems[**1**](https://cloud.google.com/storage/docs/gcs-fuse#f1-note)."

D says "gcsfuse", should be D

upvoted 3 times

? ?  Frollo 8ámonths ago
FUSE is not posix
upvoted 6 times

? ?  JeffClarke111 1áyear, 11ámonths ago

Agreed - C

upvoted 5 times

? ?  Urban_Life 1áyear, 6ámonths ago

https://cloud.google.com/filestore

upvoted 3 times

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago

IMHO d) is wrong, the correct answer is c).
The requirement is explicitly POSIX filesystem - using gcsfuse Cloud Storage still remains an object storage - IMHO gcsfuse brings a lot of
downsizes compared with Filestore and in the question there are no indications that a non-POSIX filesystem shall be used.

upvoted 15 times

? ?  coder36  Most Recent ?  1áweek, 5ádays ago

Could anyone please tell why A or B is wrong? Thanks

upvoted 1 times

? ?  BEE_HI_5 2ámonths ago

https://cloud.google.com/storage/docs/gcs-fuse#differences-and-limitations
Answer C because POSIX and FUSE are not compatible. Google recommends using Filestore to address POSIX file operations

upvoted 2 times

? ?  raselsys 2ámonths, 3áweeks ago

Selected Answer: C

Answer is C
https://cloud.google.com/storage/docs/gcs-fuse
Clearly mentioned to see filestore for POSIX

upvoted 2 times

? ?  CDL_Learner 7ámonths ago

GCFUSE does not work with POSIX File System , refer - https://cloud.google.com/storage/docs/gcs-fuse

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

332/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

So Answer should be C

upvoted 3 times

? ?  tomahawk003 7ámonths ago

Voting for D. Filestore may have throughput of up to 25 gb/S while question mentions 100 gb/s?

upvoted 1 times

? ?  steghe 5ámonths, 1áweek ago

Requirement asks for 100 MB/s not GB/s

upvoted 1 times

? ?  HenkH 7ámonths, 1áweek ago

Filestore, spelled with an L, doesn't exist (firestore, with an R, does), hence use the object store/bucket along with Fuse.

upvoted 2 times

? ?  HenkH 7ámonths, 1áweek ago

Sorry, fiLestore does exists, but a a NAS /filer solution:
https://cloud.google.com/architecture/filers-on-compute-engine#filestore

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

https://www.quobyte.com/storage-explained/posix-filesystem/

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: C

C. Create a Cloud Filestore instance and mount it in each instance.

upvoted 1 times

? ?  Mikado211 10ámonths, 4áweeks ago

Filestore is the best for posix+concurrential access
GCFuse isn't recommended by Google and you should always avoid it in any case.

C is the best

upvoted 4 times

? ?  deenee 11ámonths, 2áweeks ago

Selected Answer: C

FUSE can be used, but it comes with latency. Question states, huge workload like 100 MB/sec writes, then FUSE is not a god choice.
Filestore is much better solution.

upvoted 2 times

? ?  deenee 11ámonths, 2áweeks ago

Cloud Filestore: Cloud Filestore is a scalable and highly available shared file service fully managed by Google. Cloud Filestore provides
persistent storage ideal for shared workloads. It is best suited for enterprise applications requiring persistent, durable, shared storage
which is accessed by NFS or requires a POSIX compliant file system.
https://cloud.google.com/terms/services-20180724

upvoted 11 times

? ?  Sitender 11ámonths, 2áweeks ago

D is wrong: we can not mount Cloud Storage bucket inside O.S as a file system drive

upvoted 1 times

? ?  geofe_geo 10ámonths, 3áweeks ago

https://cloud.google.com/storage/docs/gcs-fuse

upvoted 1 times

? ?  AzureDP900 12ámonths ago

C is right

upvoted 1 times

? ?  ZLT 1áyear ago

Selected Answer: C

The answer is C. D is for macOS and Linux systems.

upvoted 1 times

? ?  yeahlon 1áyear, 4ámonths ago

Selected Answer: C

https://cloudplatform.googleblog.com/2018/06/New-Cloud-Filestore-service-brings-GCP-users-high-performance-file-storage.html

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

333/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

ôCloud Filestore was easy to provision and mount, and reliable for the kind of workload we have. Having a POSIX file system that we can
mount and use directly helps us speed-read our files, especially on new machines. We can also use the normal I/O features of any
language and donÆt have to use a specific SDK to use an object store."
- Charlie Rice, Chief Technology Officer, ever.ai

upvoted 6 times

? ?  luamail 8ámonths, 3áweeks ago

Cloud Storage FUSE Access: Authorization for files is governed by Cloud Storage permissions. POSIX-style access control does not work.
https://cloud.google.com/storage/docs/gcs-fuse#notes

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

334/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #124

Topic 1

Your company has an application deployed on Anthos clusters (formerly Anthos GKE) that is running multiple microservices. The cluster has both

Anthos Service

Mesh and Anthos Con g Management con gured. End users inform you that the application is responding very slowly. You want to identify the

microservice that is causing the delay. What should you do?

A. Use the Service Mesh visualization in the Cloud Console to inspect the telemetry between the microservices.

B. Use Anthos Con g Management to create a ClusterSelector selecting the relevant cluster. On the Google Cloud Console page for Google

Kubernetes Engine, view the Workloads and  lter on the cluster. Inspect the con gurations of the  ltered workloads.

C. Use Anthos Con g Management to create a namespaceSelector selecting the relevant cluster namespace. On the Google Cloud Console

page for Google Kubernetes Engine, visit the workloads and  lter on the namespace. Inspect the con gurations of the  ltered workloads.

D. Reinstall istio using the default istio pro le in order to collect request latency. Evaluate the telemetry between the microservices in the

Cloud Console.

Correct Answer: A

Community vote distribution

A (100%)

? ?  AnilKr  Highly Voted ?  1áyear, 10ámonths ago

The Anthos Service Mesh pages in the Google Cloud Console provide both summary and in-depth metrics, charts, and graphs that enable
you to observe service behavior. You can monitor the overall health of your services, or drill down on a specific service to set a service level
objective (SLO) or troubleshoot an issue.

https://cloud.google.com/service-mesh/docs/observability/explore-dashboard

upvoted 17 times

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is A

upvoted 13 times

? ?  megumin  Most Recent ?  7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam.

upvoted 3 times

? ?  Deepak31 7ámonths, 3áweeks ago

what is the answer

upvoted 2 times

? ?  AzureDP900 12ámonths ago

A is right.

upvoted 1 times

? ?  technodev 1áyear, 5ámonths ago

Got this question in my exam, answered A

upvoted 5 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

Thanks dear

upvoted 1 times

? ?  Deb2293 3ámonths, 3áweeks ago

How many question from this entire question bank were common?

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

335/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 4 times

? ?  AnilKr 1áyear, 10ámonths ago

Ans-A
https://cloud.google.com/service-mesh/docs/observability/explore-dashboard

upvoted 6 times

? ?  VishalB 1áyear, 11ámonths ago

Ans : A
Anthos Service MeshÆs robust tracing, monitoring, and logging features give you deep insights into how your services are performing, how
that performance affects other processes, and any issues that might exist.

upvoted 7 times

? ?  victory108 1áyear, 11ámonths ago

A. Use the Service Mesh visualization in the Cloud Console to inspect the telemetry between the microservices.

upvoted 4 times

? ?  Rom0817 1áyear, 12ámonths ago

Answer: A, Service Mesh
https://cloud.google.com/anthos/service-mesh

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

336/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #125

Topic 1

You are working at a  nancial institution that stores mortgage loan approval documents on Cloud Storage. Any change to these approval

documents must be uploaded as a separate approval  le, so you want to ensure that these documents cannot be deleted or overwritten for the

next 5 years. What should you do?

A. Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy.

B. Create the bucket with uniform bucket-level access, and grant a service account the role of Object Writer. Use the service account to upload

new  les.

C. Use a customer-managed key for the encryption of the bucket. Rotate the key after 5 years.

D. Create the bucket with  ne-grained access control, and grant a service account the role of Object Writer. Use the service account to upload

new  les.

Correct Answer: A

Reference:

https://cloud.google.com/storage/docs/using-bucket-lock

Community vote distribution

A (100%)

? ?  VishalB  Highly Voted ?  1áyear, 11ámonths ago

Answer A
o If a bucket has a retention policy, objects in the bucket can only be deleted or replaced once their age is greater than the retention
period.
o Once you lock a retention policy, you cannot remove it or reduce the retention period it has.

upvoted 25 times

? ?  azureaspirant  Highly Voted ?  1áyear, 4ámonths ago

2/15/21 exam
upvoted 7 times

? ?  examch  Most Recent ?  5ámonths, 3áweeks ago

Selected Answer: A

A is the correct answer,

You can include a retention policy when creating a new bucket, or you can add a retention policy to an existing bucket. Placing a retention
policy on a bucket ensures that all current and future objects in the bucket cannot be deleted or replaced until they reach the age you
define in the retention policy. Attempts to delete or replace objects whose age is less than the retention period fail with a 403 -
retentionPolicyNotMet error.

https://cloud.google.com/storage/docs/bucket-lock#retention-policy

upvoted 3 times

? ?  ecruells 6ámonths ago

It appeared in 12/12/22 Exam

upvoted 2 times

? ?  Deb2293 3ámonths, 3áweeks ago

How many question came from this question bank?

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam.

upvoted 2 times

? ?  cbarg 11ámonths, 3áweeks ago

Selected Answer: A

A is the only applicable answer

upvoted 1 times

? ?  AzureDP900 12ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

337/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is right

upvoted 1 times

? ?  MOSES3009 1áyear ago

A is correct

upvoted 1 times

? ?  ss909098 1áyear, 3ámonths ago

Selected Answer: A

A is correct. Create retention policy

upvoted 1 times

? ?  technodev 1áyear, 5ámonths ago

Got this question in my exam, answered A

upvoted 3 times

? ?  Sur_Nikki 1ámonth, 2áweeks ago

Thanks dear!

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for A

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

A is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 3 times

? ?  Chotebhaisahab 1áyear, 8ámonths ago

I agree with A
upvoted 1 times

? ?  AnilKr 1áyear, 10ámonths ago

Locking a retention policy is an irreversible action. Once locked, you must delete the entire bucket in order to "remove" the bucket's
retention policy. However, before you can delete the bucket, you must be able to delete all the objects in the bucket, which itself is only
possible if the all objects have reached the retention period set by the retention policy.

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

A. Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy.

upvoted 5 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

338/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #126

Topic 1

Your team will start developing a new application using microservices architecture on Kubernetes Engine. As part of the development lifecycle,

any code change that has been pushed to the remote develop branch on your GitHub repository should be built and tested automatically. When the

build and test are successful, the relevant microservice will be deployed automatically in the development environment. You want to ensure that

all code deployed in the development environment follows this process. What should you do?

A. Have each developer install a pre-commit hook on their workstation that tests the code and builds the container when committing on the

development branch. After a successful commit, have the developer deploy the newly built container image on the development cluster.

B. Install a post-commit hook on the remote git repository that tests the code and builds the container when code is pushed to the

development branch. After a successful commit, have the developer deploy the newly built container image on the development cluster.

C. Create a Cloud Build trigger based on the development branch that tests the code, builds the container, and stores it in Container Registry.

Create a deployment pipeline that watches for new images and deploys the new image on the development cluster. Ensure only the

deployment tool has access to deploy new versions.

D. Create a Cloud Build trigger based on the development branch to build a new container image and store it in Container Registry. Rely on

Vulnerability Scanning to ensure the code tests succeed. As the  nal step of the Cloud Build process, deploy the new container image on the

development cluster. Ensure only Cloud Build has access to deploy new versions.

Correct Answer: A

Community vote distribution

C (96%)

4%

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago

Answer should be C, obviously.

upvoted 42 times

? ?  AdGlad  Highly Voted ?  1áyear, 11ámonths ago

Questions say "relevant microservice will be deployed automatically in the development environment." Therefore A and B are out. D says
"Rely on Vulnerability Scanning to ensure the code tests succeed." Vulnerability Scanning is not test so D is out. The correct Answer is
therefore C.

upvoted 28 times

? ?  balajisreenivas  Most Recent ?  2ámonths, 4áweeks ago

Selected Answer: C

By elimination method, the answer is C.

upvoted 1 times

? ?  kopasz93 3ámonths ago

Selected Answer: C

The answer is C.
upvoted 1 times

? ?  Dr_Ramzus 5ámonths ago

Selected Answer: C

Clearly C

upvoted 1 times

? ?  omermahgoub 6ámonths ago

The correct answer is C: Create a Cloud Build trigger based on the development branch that tests the code, builds the container, and
stores it in Container Registry. Create a deployment pipeline that watches for new images and deploys the new image on the development
cluster. Ensure only the deployment tool has access to deploy new versions.

To automate the build and deployment process for your microservices in the development environment, you can use Cloud Build to set up
a trigger that listens for code pushes to the development branch on your GitHub repository. When a code change is pushed to the branch,
Cloud Build can test the code, build the container image, and store it in Container Registry. You can then create a deployment pipeline that
watches for new images in Container Registry and deploys them automatically on the development cluster. To ensure that only code that
has been properly tested and built is deployed in the development environment, you should ensure that only the deployment tool has
access to deploy new versions.

upvoted 5 times

? ?  omermahgoub 6ámonths ago

Option A is incorrect because installing a pre-commit hook on each developer's workstation does not ensure that the build and test
process is followed consistently for all code changes. It also does not provide a centralized way to track the deployments in the
development environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

339/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Option B is incorrect for the same reason. A post-commit hook on the remote repository does not provide a centralized way to manage
the build and deployment process for all code changes in the development environment.

Option D is incorrect because relying on Vulnerability Scanning alone is not sufficient to ensure that the code changes are properly
tested and built before being deployed in the development environment. A more comprehensive build and test process, such as the
one described in option C, is recommended to ensure the quality and reliability of the code being deployed.

upvoted 2 times

? ?  madmike123 6ámonths, 1áweek ago

Selected Answer: C

"any code change that has been pushed to the remote develop branch on your GitHub repository should be built"...this excludes A and B
since both happen locally before a push.
Answer 'D' only performs security scanning (no test) and is not automatically deployed which is what was requested.

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: C

C. Create a Cloud Build trigger based on the development branch that tests the code, builds the container, and stores it in Container
Registry. Create a deployment pipeline that watches for new images and deploys the new image on the development cluster. Ensure only
the deployment tool has access to deploy new versions.

upvoted 1 times

? ?  Amit_arch 9ámonths, 2áweeks ago

Selected Answer: D

C doesn't include test success. D should be the option.

upvoted 1 times

? ?  zr79 8ámonths, 2áweeks ago

It does include tests succuss, the question is using CI/CD

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

Vulnerability Scanning is not relevant though for the question.

upvoted 1 times

? ?  shayke 11ámonths ago

Selected Answer: C

C- the only answer

upvoted 2 times

? ?  gardislan18 11ámonths, 1áweek ago

Selected Answer: C

The answer should be C, we eliminate A and B because we should use Cloud Build trigger instead, now we eliminate D because the
deployment step is done manually and the requirement says automatically

So answer is C, it leverages Cloud Build trigger, tests the code, and uses deployment pipeline

upvoted 3 times

? ?  AzureDP900 12ámonths ago

C is right

upvoted 2 times

? ?  GPK 1áyear ago

25/06/2022 , C is correct

upvoted 5 times

? ?  Superr 1áyear, 1ámonth ago

Selected Answer: C

C fulfils all requirements

upvoted 1 times

? ?  gaojun 1áyear, 2ámonths ago

Go for C

upvoted 1 times

? ?  ss909098 1áyear, 3ámonths ago

Selected Answer: C

Yes C is the correct one

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

340/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

341/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #127

Topic 1

Your operations team has asked you to help diagnose a performance issue in a production application that runs on Compute Engine. The

application is dropping requests that reach it when under heavy load. The process list for affected instances shows a single application process

that is consuming all available CPU, and autoscaling has reached the upper limit of instances. There is no abnormal load on any other related

systems, including the database. You want to allow production tra c to be served again as quickly as possible. Which action should you

recommend?

A. Change the autoscaling metric to agent.googleapis.com/memory/percent_used.

B. Restart the affected instances on a staggered schedule.

C. SSH to each instance and restart the application process.

D. Increase the maximum number of instances in the autoscaling group.

Correct Answer: A

Reference:

https://cloud.google.com/blog/products/sap-google-cloud/best-practices-for-sap-app-server-autoscaling-on-google-cloud

Community vote distribution

D (82%)

Other

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago

Answer should be D.
I doubt it is intended to provide wrong answer.

upvoted 40 times

? ?  poseidon24 1áyear, 11ámonths ago

Agree.

Cannot be A), since changing the metric used for autoscaling will not solve the issue, the CPU is already over utilized, hence the unique
"workaround" meanwhile the application causing the issue is fixed (connection leaks, infinite loops, etc.) is to allow introducing new
nodes/workers/VMs.

upvoted 7 times

? ?  victorlie 1áyear, 10ámonths ago

why almost all answers are wrong?

upvoted 12 times

? ?  zr79 8ámonths, 2áweeks ago

to prevent us from memorizing the answers and hopefully, the site can not be shut down

upvoted 5 times

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is D

upvoted 12 times

? ?  red_panda  Most Recent ?  2áweeks, 5ádays ago

Selected Answer: D

Answer D is correct.
In order to prioritize the availability on production environment (as per question), first we need to increase the number of max instances
in the instance group, then, for sure we can investigate and restart application process.
Be careful, often the answer is in the question

upvoted 1 times

? ?  grejao 2ámonths, 4áweeks ago

I choose for A, but D is the best choice.

The trick is: "process that is consuming all available CPU" and "autoscaling has reached the upper limit of instances"
If the process is consuming all available CPU, we need to reconfigure our metrics for best tresholds (Option A)
AND
if the autoscaling reached the upper limit of instances, so we need to increase this limit (Option D),

BUT, after reached the upper limit of instances, it doesn't matter the tresholds, the process will consume all resources that have available.
So, option D is the best option.

upvoted 3 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

342/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Loved the way you made us travel through the roots of the question

upvoted 1 times

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: D

The application is dropping requests because the available CPU is exhausted. Autoscaling has reached the upper limit of instances, so it
cannot increase the number of instances to meet the demand. The best way to allow production traffic to be served again is to increase
the maximum number of instances in the autoscaling group.

This will allow autoscaling to increase the number of instances to meet the demand without exhausting the available CPU. Restarting the
affected instances or SSHing to each instance and restarting the application process will not solve the problem because the root cause is
that there are not enough instances to meet the demand.

upvoted 4 times

? ?  omermahgoub 6ámonths ago

The correct answer is D: Increase the maximum number of instances in the autoscaling group.

If the application is dropping requests under heavy load and the process list for affected instances shows a single application process
consuming all available CPU, increasing the maximum number of instances in the autoscaling group may help to alleviate the
performance issue. By adding more instances to the group, you can distribute the load across multiple instances, which should help to
reduce the strain on any single instance. This will allow production traffic to be served again more quickly.

upvoted 5 times

? ?  omermahgoub 6ámonths ago

Option A is incorrect because changing the autoscaling metric to agent.googleapis.com/memory/percent_used will not address the
root cause of the performance issue. The issue is related to CPU utilization, not memory usage.

Option B is incorrect because restarting the affected instances on a staggered schedule will not address the root cause of the
performance issue. It may provide temporary relief, but the issue is likely to recur once the instances are under heavy load again.

Option C is incorrect because restarting the application process on each instance will not address the root cause of the performance
issue. It may provide temporary relief, but the issue is likely to recur once the instances are under heavy load again. Increasing the
maximum number of instances in the autoscaling group is a more effective solution in this case.

upvoted 4 times

? ?  SureshbabuK 6ámonths, 2áweeks ago

Selected Answer: B

Given the is not no abnormal load, autoscaling will is not required, restarting should kill the single application process consuming excess
CPU

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  sanait100 6ámonths, 3áweeks ago

The keyword is "You want to allow production traffic to be served again as quickly as possible" so D should be the only answer so as to
resume production traffic and then you can do a root cause analysis and take further action depending upon the findings.

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: D

It all depends on how you want to troubleshoot the issue. Do you want to check the application before or after increasing the max number
of instances in the scaling group. I guess in real life people will ask for an increase in the max number of instances and if the application
process continues to consume all the CPU then they will probably stop/restart the app.
D is the only sensible option.
A is not an option
B you could restart but you dont know if that will fix the issue
C SSH assumes unix vm's (?)....!

upvoted 3 times

? ?  AzureDP900 8ámonths, 1áweek ago

autoscaling has reached the upper limit of instances. There is no abnormal load on any other related systems, including the database.
This so junk question, only D seems viable option.

upvoted 2 times

? ?  Jay_Krish 9ámonths, 3áweeks ago

Selected Answer: D

I feel increasing the autoscale limit seems to be the logical answer

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

343/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  ijazahmad722 10ámonths, 2áweeks ago

Selected Answer: D

D seems to be least wrong

upvoted 1 times

? ?  AzureDP900 12ámonths ago

Answer is D

upvoted 1 times

? ?  GPK 1áyear ago

exam 25/6/2022 .. agree its D

upvoted 7 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: D

Answered below. typo clearly *

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Question really states that the application on the container has reached upper limit and its time to spawn new instances hence D

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

344/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #128

Topic 1

You are implementing the infrastructure for a web service on Google Cloud. The web service needs to receive and store the data from 500,000

requests per second. The data will be queried later in real time, based on exact matches of a known set of attributes. There will be periods where

the web service will not receive any requests. The business wants to keep costs low. Which web service platform and database should you use for

the application?

A. Cloud Run and BigQuery

B. Cloud Run and Cloud Bigtable

C. A Compute Engine autoscaling managed instance group and BigQuery

D. A Compute Engine autoscaling managed instance group and Cloud Bigtable

Correct Answer: D

Community vote distribution

B (61%)

D (36%)

? ?  Enzian  Highly Voted ?  1áyear, 12ámonths ago

Any correct answer must involve Cloud Bigtable over BigQuery since Bigtable is optimized for heavy write loads. That leaves B and D. I
would suggest B b/c it is lower cost ("The business wants to keep costs low")

upvoted 68 times

? ?  pakilodi 1áyear, 6ámonths ago

Not only: occasionally there will be no requests. so Cloud Run will scale to zero

upvoted 16 times

? ?  Petya27 1ámonth ago

Plus, we are talking about a predefined set of queries. For any predefined list of (simple) queries, we use Bigtable, and for any
(complex) queries that we do not know ahead of time, we use BigQuery.

upvoted 1 times

? ?  AmitRBS 1áyear, 1ámonth ago

B. Agree. Additionally data need to store now so use Bigtable as question is not for analysing or data Analytics etc

upvoted 4 times

? ?  zanfo 1áyear, 3ámonths ago

the correct is B
upvoted 2 times

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

B is correct answer.

upvoted 16 times

? ?  JohnWick2020  Most Recent ?  3áweeks, 5ádays ago

Answer is B. Here are the clues:
Heavy writes = BigTable
Low cost (scales to zero) = Cloud Run.

BQ is ruled out because it's OLAP more for reads than write.

upvoted 2 times

? ?  non_90919 1ámonth, 1áweek ago

According to ChatGPT, "Option B (Cloud Run and Cloud Bigtable) is not the optimal choice because Cloud Bigtable is a NoSQL wide-column
database that is more suitable for handling large-scale, high-throughput analytical workloads rather than real-time querying based on
exact matches." So everyone is just saying something different lol

upvoted 1 times

? ?  BiddlyBdoyng 2áweeks, 5ádays ago

BigQuery has a 100k read/s quota, I can't write the quota for writes but would be confident BigQuery cannot handle 500k/s. Bigtable is
designed for such massive IO.

upvoted 1 times

? ?  Fu7ed 1ámonth, 2áweeks ago

Selected Answer: B

The problem is that I want a low cost, so I'll choose cloud run. Cloud run does not incur costs when not running, and auto scale is also
possible. I will choose Bigtable because I need to handle a large amount of throughput. So the answer is D. Please let me know if there are
any mistakes.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

345/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  balajisreenivas 2ámonths, 4áweeks ago

Selected Answer: B

B is correct since cost should be low.

upvoted 1 times

? ?  JC0926 3ámonths ago

Selected Answer: B

Based on the requirements and constraints mentioned, the recommended web service platform and database for the application would
be Cloud Run and Cloud Bigtable, option B.

Cloud Run is a serverless compute platform that allows for automatic scaling and provides a low-cost option for applications with
unpredictable or variable traffic patterns. Cloud Bigtable is a NoSQL database that is designed for high-throughput, low-latency workloads
and can handle large amounts of data. It is also fully managed, so it does not require significant administration overhead.

upvoted 1 times

? ?  JC0926 3ámonths, 1áweek ago

Selected Answer: B

Cloud Run is a serverless platform that allows you to run stateless containers on demand. It can scale quickly to handle a large number of
requests and automatically scales down to zero when there are no requests, making it a cost-effective option.

Cloud Bigtable is a NoSQL database that is designed for handling massive amounts of data with low latency and high throughput. It can
handle up to millions of operations per second, making it a suitable option for storing and querying the large volume of data generated
by the web service.

upvoted 2 times

? ?  Deb2293 3ámonths, 3áweeks ago

Selected Answer: B

Cloud Run can scale to 0 so should be B

upvoted 2 times

? ?  telp 4ámonths ago

Selected Answer: B

Cloud run scale to 0 so reduce cost when no activity.
Bigtable to support the heavy write.

upvoted 2 times

? ?  segkhachat 4ámonths, 2áweeks ago

MIG would not scale as fast as Run. With this high throughput I would rather use containerized solution instead of VMs as for each VM
instance you are going to have a separate OS. So you may need more memory with GCE which does not support scale from 0.

upvoted 1 times

? ?  segkhachat 4ámonths, 2áweeks ago

The only doubt I have is because of a mentioned 'infrastructure' keyword in the question.

upvoted 1 times

? ?  gcppandit 5ámonths ago

Selected Answer: D

I will go with the option D.
Both Cloud Run and Cloud Compute can be used to solve the problem. But the main challenge is to handle 500,000 requests per secons.
In case of Cloud Run the Maximum Number of instances are allowed to 100 and each cloud run container can handle maximum 1000
requests per seconds. So if we go with Cloud Run option we can maximum address 100,000 requests per second. In case of Compute
Engine (with proper external load balancer) does not have these type of limitations. So D will be better option just due to number of
requests.

Ref: https://cloud.google.com/run/docs/configuring/max-instances
Ref: https://cloud.google.com/run/docs/about-
concurrency#:~:text=By%20default%20each%20Cloud%20Run,to%20a%20maximum%20of%201000.

upvoted 6 times

? ?  nick_name_1 4ámonths, 1áweek ago

Run can scale to 1,000 instances without requesting a quota increase. Each instance can handle up to 1,000 concurrent requests.
500,000/sec is half the maximum for Run. Run scales to zero.

upvoted 2 times

? ?  CosminCiuc 5ámonths ago

https://cloud.google.com/run/docs/configuring/max-instances#limits
For instance using 1 CPU and 2GB memory, this maximum can be increased up to a limit of 1000 instances. The actual maximum limit
depends on the region of the Cloud Run service and its CPU and memory configurations.
If you want to specify a maximum number of instances greater than the maximum allowed in the region of the Cloud Run service, you
must request a quota increase.
I think answer B is the correct one.

upvoted 1 times

? ?  gcppandit 4ámonths, 4áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

346/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Both B and D will address the requirement. For B you need to change Google limits and for D you can set the limit to yourself. I will
go for the option that does not need any change at Google side.

upvoted 1 times

? ?  skyblue07 6ámonths ago

Selected Answer: A

Additionally, "based on exact matches of a known set of attributes" points to Big Query. Bigtable is a key-value database.

upvoted 2 times

? ?  gcppandit 4ámonths, 4áweeks ago

BiqQuery can not handle 500,000 concurrent requests. So you have to go for either B and D. Both can be used with some configuration
or change in limits.

upvoted 1 times

? ?  steghe 5ámonths ago

With BigTable you can build the key with the concatenation of the "known set of attributes" and so the exact value is found easily

upvoted 2 times

? ?  jay9114 6ámonths ago

Selected Answer: B

My reason for choosing Cloud Run? "Cloud Run only runs when requests come in, so you don't pay for time spent idling"

https://dev.to/pcraig3/cloud-run-vs-app-engine-a-head-to-head-comparison-using-facts-and-science-1225

upvoted 2 times

? ?  thamaster 6ámonths ago

Selected Answer: B

Cloud run is perfect to match high peak activity and scale to 0 to allow low cost when you don't need it.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

To handle 500,000 requests per second and store data that will be queried in real time based on exact matches of a known set of
attributes, I would recommend using option D: A Compute Engine autoscaling managed instance group and Cloud Bigtable.

Compute Engine autoscaling managed instance groups allow you to automatically increase or decrease the number of VM instances in a
group based on workload demand. This can help ensure that your application has the resources it needs to handle high traffic periods,
while minimizing costs during periods of low traffic.

Cloud Bigtable is a high-performance, NoSQL database designed for extremely large scale data storage and real-time data processing. It is
well-suited for applications that require fast queries and updates on large datasets, such as the one described in the question.

upvoted 4 times

? ?  omermahgoub 6ámonths ago

Cloud Run, on the other hand, is a fully managed serverless platform for deploying and scaling containerized applications. While it may
be a good choice for some applications, it may not be the best fit for an application that needs to handle high levels of traffic and
perform real-time queries on large datasets.

BigQuery is a fully managed, cloud-native data warehouse that is well-suited for analyzing large datasets using SQL. However, it may
not be the best choice for an application that needs to perform real-time queries on large datasets, as it is optimized for batch
processing rather than real-time querying.

upvoted 3 times

? ?  CDL_Learner 7ámonths ago

For Heavy Write Big Table is the Only Option . For Cost Saving , Cloud RUn is the correct Option . Hence B .

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

347/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #129

Topic 1

You are developing an application using different microservices that should remain internal to the cluster. You want to be able to con gure each

microservice with a speci c number of replicas. You also want to be able to address a speci c microservice from any other microservice in a

uniform way, regardless of the number of replicas the microservice scales to. You need to implement this solution on Google Kubernetes Engine.

What should you do?

A. Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to

address it from other microservices within the cluster.

B. Deploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to

address the Deployment from other microservices within the cluster.

C. Deploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the

microservice from other microservices within the cluster.

D. Deploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address name to address the

Pod from other microservices within the cluster.

Correct Answer: A

Community vote distribution

A (100%)

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is A

upvoted 22 times

? ?  PeppaPig  Highly Voted ?  1áyear, 10ámonths ago

Answer is A 100%
B is incorrect. Ingress comes with a HTTP(S) LB with external IP hence is not needed for communications within the cluster internally.

upvoted 13 times

? ?  GoReplyGCPExam  Most Recent ?  5ámonths, 3áweeks ago

Selected Answer: A

answer is A

upvoted 1 times

? ?  jay9114 6ámonths ago

Selected Answer: A

Benefit of using Service
Leveraging service allows for you to set up your environment with static IP addresses. So when your pods die and restart the IP address
associated with the deceased pod remains
for the new pod that replaces it (ephemeral). I think using "Service" is helpful if you are setting up your pods to be able to communicate
with specific pods in the cluster.

Benefit of using DNS for Service
Using DNS for your Service (static IP) you can look up Services and/or Pods by name instead of IP. Addressability by name instead of IP is
easier for me.
upvoted 4 times

? ?  jay9114 6ámonths ago

l
l
Benefit of using Service
Leveraging service allows for you to set up your environment with static IP addresses. So when your pods die and restart the IP address
associated with the deceased pod remains
for the new pod that replaces it (ephemeral). I think using "Service" is helpful if you are setting up your pods to be able to communicate
with specific pods in the cluster.

Benefit of using DNS for Service
Using DNS for your Service (static IP) you can look up Services and/or Pods by name instead of IP. Addressability by name instead of IP is
easier for me.
upvoted 1 times

? ?  Sreenivasa739 6ámonths, 2áweeks ago

A is ok

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

348/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

A is ok

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: A

A. Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to
address it from other microservices within the cluster.

upvoted 1 times

? ?  ACE_ASPIRE 10ámonths, 2áweeks ago

I got this question in exam.

upvoted 3 times

? ?  Sur_Nikki 1ámonth, 3áweeks ago

I have received this sentence from u in every comment posted by u

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: A

ngress comes with a HTTP(S) LB with external IP hence is not needed for communications within the cluster internally.Microservice as
Deployment - used to create replicas as per this request
DNS name - used as an alias service name for External name which is user for internal requests

upvoted 3 times

? ?  backhand 11ámonths, 1áweek ago

vote A

upvoted 1 times

? ?  AzureDP900 12ámonths ago

A is right, There is no need of Ingress here because all service need to communicate internally...

upvoted 1 times

? ?  JoeyCASD 1áyear, 1ámonth ago

Vote A
1. Based on the description "You want to be able to configure each microservice with a specific number of replicas.", It's a hint to use
either Deployment or StatefulSet based on the service type is stateless or stateful, since the option only has Deployment, thus Option C
and D is out.
2. Based on the description "You also want to be able to address a specific microservice from any other microservice in a uniform way,
regardless of the number of replicas the microservice scales to." the later part is the key point, which means the traffic direct to each
service is based on some certain rules, in K8S this means URL, which is Ingress with external HTTP LB.

upvoted 7 times

? ?  ss909098 1áyear, 3ámonths ago

Selected Answer: A

Of course it is A
upvoted 1 times

? ?  azureaspirant 1áyear, 4ámonths ago

2/15/21 exam
upvoted 6 times

? ?  deepnibm 1áyear, 4ámonths ago

is that 2/15/22*??

upvoted 1 times

? ?  hantanbl 1áyear, 5ámonths ago

this question came out in the exam

upvoted 5 times

? ?  technodev 1áyear, 5ámonths ago

Got this question in my exam, answered A

upvoted 5 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

349/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #130

Topic 1

Your company has a networking team and a development team. The development team runs applications on Compute Engine instances that

contain sensitive data. The development team requires administrative permissions for Compute Engine. Your company requires all network

resources to be managed by the networking team. The development team does not want the networking team to have access to the sensitive data

on the instances. What should you do?

A. 1. Create a project with a standalone VPC and assign the Network Admin role to the networking team. 2. Create a second project with a

standalone VPC and assign the Compute Admin role to the development team. 3. Use Cloud VPN to join the two VPCs.

B. 1. Create a project with a standalone Virtual Private Cloud (VPC), assign the Network Admin role to the networking team, and assign the

Compute Admin role to the development team.

C. 1. Create a project with a Shared VPC and assign the Network Admin role to the networking team. 2. Create a second project without a VPC,

con gure it as a Shared VPC service project, and assign the Compute Admin role to the development team.

D. 1. Create a project with a standalone VPC and assign the Network Admin role to the networking team. 2. Create a second project with a

standalone VPC and assign the Compute Admin role to the development team. 3. Use VPC Peering to join the two VPCs.

Correct Answer: C

Reference:

https://cloud.google.com/vpc/docs/shared-vpc

Community vote distribution

C (55%)

B (45%)

? ?  Nalo1  Highly Voted ?  1áyear, 6ámonths ago

Selected Answer: B

For the same project , same VPC, Network Admin role to the networking team, and Compute Admin role to the development team. What is
the need for another project?

upvoted 44 times

? ?  TonyKGH 4ádays ago

For full separation of the teams you will need to use a shared VPC in this case. If you compare the two roles you will see that Compute
Admin includes the permissions of the Network Admin so with option B you don't separate the teams as Compute Admin includes
compute.network.* permissions (and others). https://cloud.google.com/iam/docs/understanding-roles

upvoted 1 times

? ?  victory108  Highly Voted ?  1áyear, 11ámonths ago

C. 1. Create a project with a Shared VPC and assign the Network Admin role to the networking team. 2. Create a second project without a
VPC, configure it as a Shared VPC service project, and assign the Compute Admin role to the development team.

upvoted 22 times

? ?  Nick89GR 1áyear, 2ámonths ago

I do not understand why do we need to have a shared VPC. With B the dev team will not be able to make any network change and the
Network team will not be able to do any change on the CE. I would say B is the correct answer although C does not seem wrong

upvoted 15 times

? ?  medi01 2ámonths, 1áweek ago

Because Compute Admin has compute.* permissions, which includes Network Admin's.

upvoted 1 times

? ?  sfsdeniso 8ámonths, 1áweek ago

because dev team will have several projects - for dev, qa and prod per app they are developing so C is most scalable solution

upvoted 6 times

? ?  BeCalm 3ámonths, 3áweeks ago

How does 1 additional VPC solve what you're expressing -- that the dev team needs a dev and qa environment.

upvoted 1 times

? ?  red_panda  Most Recent ?  2áweeks, 4ádays ago

Selected Answer: B

B.
This is a confusing question. Sensitive data in a GCE cannot be view from network admin.
It's no needed another project or another VPC. It's okay the same project and the same VPC with different roles to each department team.

upvoted 2 times

? ?  BiddlyBdoyng 2áweeks, 5ádays ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

350/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

I read somewhere that a project should model a trust boundary. The below also states
https://cloud.google.com/vpc/docs/shared-vpc"
"Shared VPC lets organization administrators delegate administrative responsibilities, such as creating and managing instances, to Service
Project Admins while maintaining centralized control over network resources like subnets, routes, and firewalls."

upvoted 2 times

? ?  3ana 1ámonth, 1áweek ago

hi, can someone share a copy of the complete questions? please send it to anashela.03@gmail.com. Thanks!

upvoted 1 times

? ?  TheCloudGuruu 1ámonth, 2áweeks ago

Selected Answer: B

C would work but the extra effort is not justified

upvoted 3 times

? ?  mateuszma 1ámonth, 2áweeks ago

Selected Answer: C

Your company requires all network resources to be managed by the networking team. This is why C not B. Google best practice is to use
shared vpc, moreover its easier to manage from networking team point of view.

upvoted 1 times

? ?  felipekw 1ámonth, 3áweeks ago

Selected Answer: C

It's C, B could work but C is more future proof and follows Google's good practices.

upvoted 1 times

? ?  JC0926 2ámonths, 2áweeks ago

Selected Answer: C

C. 1. Create a project with a Shared VPC and assign the Network Admin role to the networking team. 2. Create a second project without a
VPC, configure it as a Shared VPC service project, and assign the Compute Admin role to the development team.

Using a Shared VPC allows you to separate network management from the management of Compute Engine instances. By creating a
project with a Shared VPC and assigning the Network Admin role to the networking team, you give them control over the network
resources. Then, create a second project without a VPC, configure it as a Shared VPC service project, and assign the Compute Admin role
to the development team. This way, the development team has administrative permissions for Compute Engine instances but does not
have access to network resources. This setup meets the requirements of both teams while maintaining the desired separation of
responsibilities.
upvoted 3 times

? ?  taer 2ámonths, 4áweeks ago

Selected Answer: C

This approach allows the networking team to manage the network resources in the Shared VPC host project while giving the development
team administrative permissions for Compute Engine instances in the service project. The development team's instances will use the
shared network resources, but the networking team won't have access to the sensitive data on those instances.

upvoted 1 times

? ?  mifrah 3ámonths ago

I vote for B.
Why not C: Step 2: Create a second VPC in the shared VPC service project? But all networks should be managed by the network team, so at
least the network team would also need Network Admin permissions in the service project.

upvoted 2 times

? ?  Jambalaja 3ámonths, 1áweek ago

Selected Answer: C

The answer is C, reason:
"Shared VPC lets organization administrators delegate administrative responsibilities, such as creating and managing instances, to Service
Project Admins while maintaining centralized control over network resources like subnets, routes, and firewalls."
https://cloud.google.com/vpc/docs/shared-vpc

upvoted 4 times

? ?  bharath2k5 3ámonths, 1áweek ago

Selected Answer: B

B shared vpc not necessary, and not given in requirement

upvoted 2 times

? ?  bharath2k5 3ámonths, 1áweek ago

Looks like option c is what recommended by google and there fore C is the anser https://cloud.google.com/iam/docs/job-
functions/networking#single_team_manages_security_network_for_organization

upvoted 2 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: B

C is an over-engineered version of B. What is the point of Compute Admin and Network Admin roles across two different VPC's when the
same roles can be granted on a single VPC.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

351/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  someCloudUser 4ámonths, 1áweek ago

Selected Answer: C

C is correct

upvoted 1 times

? ?  NodummyIQ 5ámonths, 4áweeks ago

The correct answer is D By creating two separate projects, you can ensure that the networking team only has access to the resources in
the first project, while the development team only has access to the resources in the second project. Using VPC Peering allows you to
connect the two VPCs and allow communication between them while still maintaining the separation of resources and permissions. This
allows the development team to have administrative permissions for Compute Engine, while still keeping the sensitive data on the
instances separate from the networking team.

upvoted 1 times

? ?  NodummyIQ 5ámonths, 4áweeks ago

The correct answer is D By creating two separate projects, you can ensure that the networking team only has access to the resources in
the first project, while the development team only has access to the resources in the second project. Using VPC Peering allows you to
connect the two VPCs and allow communication between them while still maintaining the separation of resources and permissions

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

352/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #131

Topic 1

Your company wants you to build a highly reliable web application with a few public APIs as the backend. You don't expect a lot of user tra c, but

tra c could spike occasionally. You want to leverage Cloud Load Balancing, and the solution must be cost-effective for users. What should you

do?

A. Store static content such as HTML and images in Cloud CDN. Host the APIs on App Engine and store the user data in Cloud SQL.

B. Store static content such as HTML and images in a Cloud Storage bucket. Host the APIs on a zonal Google Kubernetes Engine cluster with

worker nodes in multiple zones, and save the user data in Cloud Spanner.

C. Store static content such as HTML and images in Cloud CDN. Use Cloud Run to host the APIs and save the user data in Cloud SQL.

D. Store static content such as HTML and images in a Cloud Storage bucket. Use Cloud Functions to host the APIs and save the user data in

Firestore.

Correct Answer: B

Community vote distribution

D (61%)

B (20%)

Other

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago

Answer should be D,
https://cloud.google.com/load-balancing/docs/https/setting-up-https-serverless#gcloud:-cloud-functions
https://cloud.google.com/blog/products/networking/better-load-balancing-for-app-engine-cloud-run-and-functions

upvoted 50 times

? ?  mikesp 1áyear, 8ámonths ago

IMHO, i agree with you. Furthermore:
Cloud Storage buckets are a good choice for static web content. Cloud storage buckets behave like a CDN Network:
https://cloud.google.com/storage/docs/caching
So it is lower cost than CDN.

upvoted 5 times

? ?  dguillenca 1áyear, 11ámonths ago

D not use CDN, is D correct answer?

upvoted 1 times

? ?  PeppaPig 1áyear, 11ámonths ago

CDN is not needed here. You don't need to service users globally thus latency and locality isn't critical

upvoted 6 times

? ?  diluviouniv 1áyear, 11ámonths ago

Spanner is expensive

upvoted 8 times

? ?  letonphat 1áyear, 8ámonths ago

IMHO CDN is not storage solution to store static html or image

upvoted 4 times

? ?  Warlock7 1áyear, 4ámonths ago

You should look at this
https://cloud.google.com/storage/docs/caching

upvoted 1 times

? ?  BrunoTostes 1áyear, 8ámonths ago

but is it Cloud Functions used for hosting APIs?

upvoted 5 times

? ?  turbo8p 7ámonths, 2áweeks ago

Can be hosted. It's cost effective since you get charged on per call basis. If no traffic then no cost will be charged.

upvoted 3 times

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago

IMHO it is d), not b).
Reason is that you don't need Cloud Spanner just to store user data - FireStore is the better solution. Additionally, I see no indications
concerning the requirement to use GKE... Please correct me when I am wrong.

upvoted 16 times

? ?  Andrea67 1áyear, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

353/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

agree with u

upvoted 3 times

? ?  BiddlyBdoyng  Most Recent ?  2áweeks, 5ádays ago

The user of Spanner is mad (way to expensive for the requirement). CDN is not a storage solution. I suspect the charging model of
firestore is also better for this application, not to mention document databases typically preferred for storing user data.

upvoted 1 times

? ?  BiddlyBdoyng 2áweeks, 5ádays ago

The recommended answer is mad. Cloud Spanner for low volume occasional spike????

upvoted 1 times

? ?  Atanu 1ámonth ago

Option D doesn't says anything about load balancing. As the question says "You want to leverage Cloud Load Balancing"; maybe that's the
reason to go with Option B as GKE expose cloud load balancer.

upvoted 1 times

? ?  telp 3ámonths, 1áweek ago

Selected Answer: D

Answer D
static content html and images => cloud storage so A and B are eliminated.
The b is so expensive for the need of a few call API.
The cloud function can be used to answer API calls if there are a few calls and can scale if needed.
For user data storage, the recommended database for google is firestore.

upvoted 3 times

? ?  JC0926 3ámonths, 1áweek ago

Selected Answer: C

Explanation:

Cloud CDN: By using Cloud CDN, we can cache static content like HTML, images, and other files across Google's edge locations, which will
make the delivery of the static content faster and more reliable. This will help in reducing the latency for users and also minimize the load
on our web servers.

Cloud Run: Cloud Run provides an easy-to-use platform for hosting stateless HTTP-driven containers. It can automatically scale up or
down based on traffic demand, making it perfect for handling occasional traffic spikes. Cloud Run is also a cost-effective option since we
only pay for the compute resources used when the application is running.

Cloud SQL: Cloud SQL provides a managed relational database service that is highly available and scalable. It supports automatic failover
and backups, ensuring that our user data is safe and available at all times.

upvoted 2 times

? ?  JC0926 3ámonths, 1áweek ago

Cloud Functions: While Cloud Functions is a serverless compute platform that is easy to use, it is not ideal for hosting APIs that require
long-running connections, such as WebSockets. Cloud Functions are designed to run short-lived tasks in response to events, so they
may not provide the best performance for API endpoints that need to handle a lot of traffic.

upvoted 2 times

? ?  hicloud 3ámonths, 1áweek ago

the question mentioned leverage on Cloud Load Balancing, therefore B is most closest to the question by using GKE

upvoted 1 times

? ?  BeCalm 3ámonths, 4áweeks ago

Selected Answer: D

Spanner is expensive, GCS can be used as a storage layer for static web content and it is cheaper than CDN.

Why do some of these questions have obvious wrong answers?

upvoted 1 times

? ?  Gofu 4ámonths, 1áweek ago

Selected Answer: A

Why not A . App engine standard can handle sudden spike in traffic

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

D is the correct answer

upvoted 1 times

? ?  gonlafer 7ámonths ago

Selected Answer: B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

354/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Cloud functions is not for host API
Cloud CDN does not store.
Therefore B

upvoted 4 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is correct

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

D is correct, for global it is supported which is the case here, still in preview for internal and regional

upvoted 1 times

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: D

D for the simple reason that there is low traffic with occasional spikes. Also, Cloud CDN usually caches static content not store (wording).
In addition, you can use LB's with Storage buckets. No need for multizones and expensive spanner. Therefore the only remaining option is
D.

upvoted 1 times

? ?  zellck 9ámonths ago

Selected Answer: D

D is the answer.
upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

355/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #132

Topic 1

Your company sends all Google Cloud logs to Cloud Logging. Your security team wants to monitor the logs. You want to ensure that the security

team can react quickly if an anomaly such as an unwanted  rewall change or server breach is detected. You want to follow Google-recommended

practices. What should you do?

A. Schedule a cron job with Cloud Scheduler. The scheduled job queries the logs every minute for the relevant events.

B. Export logs to BigQuery, and trigger a query in BigQuery to process the log data for the relevant events.

C. Export logs to a Pub/Sub topic, and trigger Cloud Function with the relevant log events.

D. Export logs to a Cloud Storage bucket, and trigger Cloud Run with the relevant log events.

Correct Answer: C

Community vote distribution

C (100%)

? ?  kopper2019  Highly Voted ?  1áyear, 11ámonths ago

I think C using BigQuery can get expensive if you have somehow check the logs for anomalies

https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event

check there is a diagram

upvoted 36 times

? ?  AzureDP900 9ámonths, 1áweek ago

C is absolutely make sense, Thank you for sharing the link.

upvoted 2 times

? ?  PimSou 1áyear, 11ámonths ago

love you :)

upvoted 5 times

? ?  Urban_Life 1áyear, 6ámonths ago
cloud function is also key point

upvoted 2 times

? ?  poseidon24 1áyear, 11ámonths ago

Thanks for pointing out the reference. C is the correct one.

Nevertheless the question and all the answers are missleading, even C) sounds like sending all the logs to pub/sub, it should mention
about "filtering" prior to send to Pub/Sub.

upvoted 7 times

? ?  manmohan15  Highly Voted ?  1áyear, 12ámonths ago

c) is correct as quickly action is required for unwanted event/access should be actioned.

upvoted 9 times

? ?  Bhargav2000  Most Recent ?  2áweeks, 2ádays ago

One of your key employees received a job offer from another cloud company. S/he left the Organization without giving notice. His Google
Account was kept active for 3 weeks. How can you find out if the employee accessed any sensitive data after s/he left?

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C Is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

356/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  minmin2020 8ámonths, 2áweeks ago

Selected Answer: C

C - check https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event

upvoted 1 times

? ?  DrishaS4 10ámonths, 4áweeks ago

Selected Answer: C

https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event

upvoted 2 times

? ?  AzureDP900 12ámonths ago

Pub/Sub & Cloud Function serves the purpose , I am choosing C as right !

upvoted 1 times

? ?  ss909098 1áyear, 3ámonths ago

Selected Answer: C

C is the correct one

upvoted 1 times

? ?  azureaspirant 1áyear, 4ámonths ago

2/15/21 exam
upvoted 2 times

? ?  ahsangh 1áyear, 4ámonths ago

21 or 22 ?

upvoted 2 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: C

I got similar question on my exam. Answered C.

upvoted 3 times

? ?  DoVale 1áyear, 5ámonths ago

B is correct because exported logs can be analyzed in Bigquery to identity anomalies by executing scheduled queries on the exported
data.

upvoted 1 times

? ?  DoVale 1áyear, 5ámonths ago

B is correct because exported logs can be analyzed in Bigquery to identity anomalies by executing scheduled queries on the exported
data.

upvoted 1 times

? ?  ehgm 1áyear, 6ámonths ago

The logs already on Cloud Logging, we can just create a metric and an alert for it. No need any development.

upvoted 3 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C

upvoted 1 times

? ?  anjuagrawal 1áyear, 6ámonths ago

Vote C

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

357/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #133

Topic 1

You have deployed several instances on Compute Engine. As a security requirement, instances cannot have a public IP address. There is no VPN

connection between Google Cloud and your o ce, and you need to connect via SSH into a speci c machine without violating the security

requirements. What should you do?

A. Con gure Cloud NAT on the subnet where the instance is hosted. Create an SSH connection to the Cloud NAT IP address to reach the

instance.

B. Add all instances to an unmanaged instance group. Con gure TCP Proxy Load Balancing with the instance group as a backend. Connect to

the instance using the TCP Proxy IP.

C. Con gure Identity-Aware Proxy (IAP) for the instance and ensure that you have the role of IAP-secured Tunnel User. Use the gcloud

command line tool to ssh into the instance.

D. Create a bastion host in the network to SSH into the bastion host from your o ce location. From the bastion host, SSH into the desired

instance.

Correct Answer: D

Reference:

https://cloud.google.com/solutions/connecting-securely

Community vote distribution

C (68%)

D (32%)

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago

Answer is C.
https://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_with_ssh

upvoted 53 times

? ?  meh009 1áyear, 8ámonths ago

100% Agree. I use IAP all the time which allows me to reduce exposure to VM from public internet. Ans is C

upvoted 9 times

? ?  mikesp 1áyear, 8ámonths ago

Agee too. Bastion host violates security requirements due to it has public IP :)

upvoted 7 times

? ?  ShadowLord 10ámonths, 1áweek ago

https://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_with_ssh

"IAP TCP forwarding allows you to establish an encrypted tunnel over which you can forward SSH, RDP, and other traffic to VM
instances"
But Options C says ,,,,, SSH from IAP .. which is not true.

upvoted 2 times

? ?  ank82  Highly Voted ?  1áyear, 11ámonths ago

And D seems correct, bastion host is specifically used for this purpose, using option C user can connect through cloud only.
By using a bastion host, you can connect to an VM that does not have an external IP address. This approach allows you to connect to a
development environment or manage the database instance for your external application, for example, without configuring additional
firewall rules.
https://cloud.google.com/solutions/connecting-securely

upvoted 15 times

? ?  learner311 1áyear, 2ámonths ago

C. no network connection between office and cloud. Can't use bastion. What C fails to say or specify is if you are either using cloud shell
gcloud or you downloaded the sdk on local. Dumb question without clarification. Assuming silly test writers conflate gcloud always
being used in cloud shell. So you are in cloud shell, you have internal access since the shell resides inside the VPC network with all
perms.

upvoted 2 times

? ?  orest 11ámonths, 1áweek ago

" There is no VPN connection between Google Cloud and your office". If there would be no network connection betweek office and
the cloud you could not use any of google services

upvoted 1 times

? ?  ShadowLord 10ámonths, 1áweek ago

But you can always SSH to bastion host from internet .. as ports are open usually

https://cloud.google.com/iap/docs/using-tcp-forwarding#tunneling_with_ssh

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

358/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

"IAP TCP forwarding allows you to establish an encrypted tunnel over which you can forward SSH, RDP, and other traffic to VM
instances". it is Traffic forwarding ...
But Options C says ,,,,, SSH from IAP .. which is not true.

upvoted 1 times

? ?  turbo8p 7ámonths, 2áweeks ago

If you're looking for word precision then:

Option C says "Use the gcloud command line tool to ssh into the instance. Most Voted"

So I think C is still correct.

upvoted 1 times

? ?  eascen 1áyear, 8ámonths ago

Except the policy is no machines can have public IP's, how do you connect to the bastion?

upvoted 5 times

? ?  elainexs 1áyear ago

It's never mentioned that there's no public IP in all GCP services, it just said instances no public IP, which is very normal. that's why
bastion inward, and NAT outward.

upvoted 1 times

? ?  BiddlyBdoyng  Most Recent ?  2áweeks, 5ádays ago

I think it's Bastion host. In my org (large bluechip) all connections are via bastion host to provide a single point of audit and control.

upvoted 2 times

? ?  kapa900 4ádays, 5áhours ago

Instances cannot have public IP bastian host will still need IP

upvoted 1 times

? ?  Atanu 1ámonth ago

Selected Answer: D

Bastion host service is specifically designed for this purpose. No need to do over-engineering too much here.

upvoted 1 times

? ?  mraza 1ámonth, 3áweeks ago

Selected Answer: D

As per ChatGPT:
Since instances cannot have a public IP address, the best option is to use a bastion host to access the instance securely. Therefore, option
D is the correct choice.

Here's what you would do:

Create a new instance that will serve as a bastion host. Assign it a static IP address.
Configure the firewall rules for the bastion host to allow incoming SSH traffic from your office location.
Connect to the bastion host via SSH from your office location.
Once connected to the bastion host, use SSH to connect to the desired instance on the same network.
This way, you can securely access the instance without violating the security requirements.

upvoted 1 times

? ?  jlambdan 3ámonths ago

Selected Answer: C

https://cloud.google.com/iap/docs/using-tcp-forwarding

upvoted 1 times

? ?  VarunGo 3ámonths, 3áweeks ago

Selected Answer: C

As per chatGPT answer is C.
Identity-Aware Proxy (IAP) is a Google Cloud service that provides secure access to VM instances without exposing them to the internet. It
allows you to establish a secure SSH connection to a VM instance via the Google Cloud Console or the gcloud command-line tool, using
OAuth 2.0-based authentication and authorization. With IAP, you can set up secure, encrypted tunnels to your VM instances, without the
need for a VPN or an external bastion host.

By configuring IAP for the instance and ensuring that you have the IAP-secured Tunnel User role, you can securely access the instance
using the gcloud command-line tool to SSH into the instance, without violating the security requirements.

upvoted 3 times

? ?  stock28_CA 4ámonths ago

there is a new question that is very similar to this one. Two subnets in the gcp cloud, A has public IP and B ONLY has private IP. how does B
reach out to external resources on the public internet?

upvoted 1 times

? ?  gcppandit 4ámonths, 4áweeks ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

359/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Lets eliminate the options one by one ->
A. NAT allows internet access from VM that does not have external not internet to VM with private IP. So does not meet the requirement.
B. L4 Load Balancer does not route to a specific instance. So does not meet the requirement.
D. Possible option but the Bastion Host has to be exposed to internet via External IP (and firewall rule) so does not meet the security
standard.
C. Is the only option where you have have only Private IPs and still access the instances.

upvoted 2 times

? ?  GopeshSahu 5ámonths ago

Answer C or D ?? I will go for C
C -> SSH IAP
D-> Bastion host
Sorry to say but Google Cloud Documentation are extremely poor and sometime its not very clear which answer is correct
https://cloud.google.com/solutions/connecting-securely

This is the reason Google cloud is struggling to expand it stake among its competitors AWS and Azure . .. I am using GCP unfortunately
Google do not have proper documentation which caused confusion most of time. Even Google Cloud Tech Youtube channel by Google
Techies are extremely poor to explain it service compare to AWS and Azure. Poor GCP.

upvoted 2 times

? ?  BARBYNUVOLA11 5ámonths, 1áweek ago

c. ╚ possibile connettersi a istanze Linux che non dispongono di un indirizzo IP esterno mediante tunneling del traffico SSH tramite IAP.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

good option for connecting to a specific Compute Engine instance without violating the security requirement of no public IP addresses
would be to create a bastion host in the network and SSH into the bastion host from your office location. From the bastion host, you can
then SSH into the desired instance.

Option D, "Create a bastion host in the network to SSH into the bastion host from your office location. From the bastion host, SSH into the
desired instance," would be the correct choice for this scenario.

A bastion host is a secure instance that is accessible from the public internet, and is used as a jump host for accessing other instances in
the network. By SSHing into the bastion host from your office location, you can then use the bastion host to access the desired instance
without violating the security requirement of no public IP addresses.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

Option C, "Configure Identity-Aware Proxy (IAP) for the instance and ensure that you have the role of IAP-secured Tunnel User. Use the
gcloud command line tool to ssh into the instance," would not be a good choice because it would not allow you to access the instance
directly. IAP allows you to securely access applications running on Compute Engine instances, but it does not allow you to access the
instances themselves.

upvoted 1 times

? ?  sameer2803 6ámonths ago

This link clearly says we can SSH with IAP
https://cloud.google.com/blog/products/identity-security/cloud-iap-enables-context-aware-access-to-vms-via-ssh-and-rdp-without-
bastion-hosts
upvoted 1 times

? ?  omermahgoub 6ámonths ago

Option A, "Configure Cloud NAT on the subnet where the instance is hosted. Create an SSH connection to the Cloud NAT IP address
to reach the instance," would not be a good choice because it would not allow you to access the instance directly. Cloud NAT allows
instances without a public IP address to access the internet, but it does not allow incoming connections.

Option B, "Add all instances to an unmanaged instance group. Configure TCP Proxy Load Balancing with the instance group as a
backend. Connect to the instance using the TCP Proxy IP," would not be a good choice because it would not allow you to access the
instance directly. TCP Proxy Load Balancing allows incoming connections to be distributed among instances in an instance group,
but it does not allow you to access a specific instance.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: D

Answer is D
Bastion hosts provide an external facing point of entry into a network containing private network instances. This host can provide a single
point of fortification or audit and can be started and stopped to enable or disable inbound SSH. By using a bastion host, you can connect
to an VM that does not have an external IP address. This approach allows you to connect to a development environment or manage the
database instance for your external application without configuring additional firewall rules.
https://cloud.google.com/solutions/connecting-securely

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  ale_brd_ 6ámonths, 4áweeks ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

360/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

i vote for c

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C --> https://cloud.google.com/solutions/connecting-securely#cloud_iap

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

361/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #134

Topic 1

Your company is using Google Cloud. You have two folders under the Organization: Finance and Shopping. The members of the development team

are in a

Google Group. The development team group has been assigned the Project Owner role on the Organization. You want to prevent the development

team from creating resources in projects in the Finance folder. What should you do?

A. Assign the development team group the Project Viewer role on the Finance folder, and assign the development team group the Project

Owner role on the Shopping folder.

B. Assign the development team group only the Project Viewer role on the Finance folder.

C. Assign the development team group the Project Owner role on the Shopping folder, and remove the development team group Project Owner

role from the Organization.

D. Assign the development team group only the Project Owner role on the Shopping folder.

Correct Answer: C

Reference:

https://cloud.google.com/resource-manager/docs/creating-managing-folders

Community vote distribution

C (89%)

6%

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

It is C

upvoted 31 times

? ?  milan74  Highly Voted ?  1áyear, 11ámonths ago

Answer C is correct.
Answer A and B are both overridden by the less-restrictive permission on Organization level.
Answer D permission was already there on Organization level, and does not remove the project owner permission on the other folder

upvoted 11 times

? ?  AugustoKras011111  Most Recent ?  4ámonths, 2áweeks ago

C seems right, you have to remove the role from organization level

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is correct

upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: C

It says "The development team has received Project Owner Role in the Organization level"
This means the only answer viable to not be able to access Finance folder resources is to remove that initial role that they were assigned
and assign the new one, hence only C mentions removing the role.
Remember IAM roles are hereditary downwards, therefor even if we assign Project Viewer to that Group, they still keep their Project
Owner in the Project, unless removed.

upvoted 8 times

? ?  AzureDP900 9ámonths, 1áweek ago
C is perfect for this scenario.

upvoted 1 times

? ?  backhand 10ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

362/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

selected answer C
basic concept
upvoted 2 times

? ?  szefco 11ámonths, 3áweeks ago

Selected Answer: C

C is correct

upvoted 1 times

? ?  vs_s 1áyear ago

Selected Answer: C

C is right

upvoted 1 times

? ?  dragos_dragos62000 1áyear, 1ámonth ago

Selected Answer: C

Answer is C, adding Project Viewer on folder doesn't overwrite the Project Owner at org level, so you have to delete it.

upvoted 2 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: A

A is better than B. And Correct.

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: B

It is obviously B
upvoted 1 times

? ?  azureaspirant 1áyear, 4ámonths ago

2/15/21 exam
upvoted 1 times

? ?  AWS2GCP 1áyear, 3ámonths ago

give some context on your 2/15/21 post in every discussion

upvoted 4 times

? ?  satamex 11ámonths ago

I guess he got this question on that day exam.

upvoted 2 times

? ?  saggianki 1áyear, 2ámonths ago

Seriously :D

upvoted 1 times

? ?  AzureDP900 12ámonths ago

what do u mean by the date, It doesn't help anyone

upvoted 1 times

? ?  haroldbenites 1áyear, 6ámonths ago

Go for C

upvoted 1 times

? ?  anjuagrawal 1áyear, 6ámonths ago

Vote C. Permissions are inherited and union of from parent. Owner at org level need to be removed to disallow from creating the
resources in one of the below folders.

upvoted 3 times

? ?  Bobch 1áyear, 6ámonths ago

Selected Answer: C

Answer C

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

363/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #135

Topic 1

You are developing your microservices application on Google Kubernetes Engine. During testing, you want to validate the behavior of your

application in case a speci c microservice should suddenly crash. What should you do?

A. Add a taint to one of the nodes of the Kubernetes cluster. For the speci c microservice, con gure a pod anti-a nity label that has the name

of the tainted node as a value.

B. Use Istio's fault injection on the particular microservice whose faulty behavior you want to simulate.

C. Destroy one of the nodes of the Kubernetes cluster to observe the behavior.

D. Con gure Istio's tra c management features to steer the tra c away from a crashing microservice.

Correct Answer: C

Community vote distribution

B (84%)

C (16%)

? ?  TotoroChina  Highly Voted ?  1áyear, 12ámonths ago

Answer is B.
application crash, not node.

upvoted 36 times

? ?  XDevX 1áyear, 12ámonths ago
I see it the same way - it is b)

upvoted 5 times

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago
I think that c) is not the correct answer.
I am not a GKE or Kubernetes expert, so maybe I am wrong.
My understanding is, that in Kubernetes a microservice can run on pods on different nodes and one node can contain pods running
differend microservices - so to kill one node will not kill a microservice but several pods running on that node. Please correct me if I am
wrong.

upvoted 17 times

? ?  BiddlyBdoyng  Most Recent ?  2áweeks, 5ádays ago

I think perhaps C is correct due to the very specific test scenario. Chaos testing would be great for generally resilience test but it wants to
test the behaviour of the app when the microservice crashed. Shutting down the microservice seems the simplest way to test this
scenario.

upvoted 1 times

? ?  BeCalm 3ámonths, 4áweeks ago

IsnÆt Istio used in the context of Anthos vs. GKE?

upvoted 1 times

? ?  roaming_panda 5ámonths, 3áweeks ago

Selected Answer: B

Istio fault injection :https://istiobyexample.dev/fault-injection/

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: B

B is the correct answer

upvoted 1 times

? ?  ale_brd_ 6ámonths, 4áweeks ago

Selected Answer: B

Istio is replicated and replaced by Anthos service mesh, please update the answer.

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

destroying a node is not really testing microservices failure

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

364/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  DoodleDo 8ámonths, 3áweeks ago

B is the right answer - reference - https://istio.io/latest/docs/tasks/traffic-management/fault-injection/

upvoted 2 times

? ?  satyagade 9ámonths, 2áweeks ago

Selected Answer: B

B is the answer
upvoted 2 times

? ?  Jay_Krish 9ámonths, 3áweeks ago

I too think it is B and seems logical.. if you want to simulate a failure of the part of the application then inject some failure code and
observe the behaviour. But the comments here says Istio is depricated.. does this mean it is totally not being used? Any help here will be
helpful for the exam.

upvoted 1 times

? ?  chickennuggets 10ámonths, 2áweeks ago

It should be B) ANTHOS fault injection (anthos is replacing Istio which is deprecated)

upvoted 3 times

? ?  Jabree12 1áyear ago

Seems as if B is the correct answer since we want to see what happens if a specific microservice fails and not a whole node, just swap
ISTIO for Anthos Service Mesh which is the new option.

upvoted 4 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

Sorry C is the answen.,

upvoted 1 times

? ?  nkit 1áyear, 2ámonths ago

CAUTION- ISTIO is deprecated in GKE, choose wisely.

upvoted 2 times

? ?  brvinod 1áyear, 3ámonths ago

Selected Answer: B

https://istio.io/latest/docs/tasks/traffic-management/fault-injection/

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

365/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #136

Topic 1

Your company is developing a new application that will allow globally distributed users to upload pictures and share them with other selected

users. The application will support millions of concurrent users. You want to allow developers to focus on just building code without having to

create and maintain the underlying infrastructure. Which service should you use to deploy the application?

A. App Engine

B. Cloud Endpoints

C. Compute Engine

D. Google Kubernetes Engine

Correct Answer: A

Reference:

https://cloud.google.com/terms/services

Community vote distribution

A (100%)

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

A, App Engine, you just want you people dedicated to the App

upvoted 26 times

? ?  Rzla  Highly Voted ?  1áyear, 9ámonths ago

AppEngine is regional. Millions of distributed global users = GkE.

upvoted 13 times

? ?  joe2211 1áyear, 7ámonths ago

But "focus on just building code without having to create and maintain the underlying infrastructure" => A right

upvoted 6 times

? ?  Aninina  Most Recent ?  7ámonths, 1áweek ago

Selected Answer: A

App Engine would work

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 12ámonths ago

App Engine is right choice, Developer can focus on developing code rather than worry about infrastructure. A is right

upvoted 3 times

? ?  Jabree12 1áyear ago

The users are global but doesn't mean that app can't be regional. A is the correct answer it seems.

upvoted 1 times

? ?  learner311 1áyear, 2ámonths ago

Another dumb question. Does the company have an operations team? K8s is specifically meant for appcode devs to be abstracted away
from infra and your devops/ops handles the infra making it so all a dev has to do is say "here's my built container image that was built in a
fully automated fashion that was completely abstracted from me after I merged my pull request". Then again, test writes are def going for
app engine here.
upvoted 1 times

? ?  azureaspirant 1áyear, 4ámonths ago

2/15/21 exam
upvoted 2 times

? ?  mshry 1áyear, 4ámonths ago

Selected Answer: A

I am seeing some confusion in answers for questions relating to requests or concurrent sessions being served by a solution. I think we
ought not to look at API rate limits being synonymous to data handling limits, as these are programming limits to that API. An analogy
would be comparing the control plane request limits to the data limits.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

366/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  haroldbenites 1áyear, 4ámonths ago

go for A

upvoted 1 times

? ?  anjuagrawal 1áyear, 6ámonths ago

Why not Cloud Endpoint?

upvoted 1 times

? ?  Urban_Life 1áyear, 6ámonths ago
Not to manage infra so it's A

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 2 times

? ?  ravisar 1áyear, 7ámonths ago

In GKE, you have to create underlying infrastructure. It is not PAAS. Only app engine provide capability for developers to focus on code.
The GKE, you need to configure many other items Apart from Code. So A seems to be more accurate. Regarding global nature, the app
engine application servers users globally. I agree that there may be little latency for regions other than NA, however since the focus of the
question is "Code", I would select A.

upvoted 4 times

? ?  XAliX 1áyear, 7ámonths ago

App Engine support limited numbers of languages, they does not mention which interface, so for flexibity i will go for D

upvoted 1 times

? ?  danielfootc 1áyear, 8ámonths ago

I would say it's App Engine for developers to focus on code.

upvoted 2 times

? ?  JasonL_GCP 1áyear, 8ámonths ago

Can anyone tell me why the answer to the following question is D? I am not seeing any difference between this question and the one
below.

Your company has announced that they will be outsourcing operations functions. You want to allow developers to easily stage new
versions of a cloud-based application in the production environment and allow the outsourced operations team to autonomously promote
staged versions to production. You want to minimize the operational overhead of the solution. Which Google Cloud product should you
migrate to?

A. App Engine
B. GKE On-Prem
C. Compute Engine
D. Google Kubernetes Engine

upvoted 1 times

? ?  JasonL_GCP 1áyear, 8ámonths ago

By considering the number of concurrent requests, D is the appropriate answer.

upvoted 2 times

? ?  BobbyFlash 1áyear, 6ámonths ago

Question 110. Answer is A. Please try to review the discussion there and not to hijack this one.

upvoted 2 times

? ?  rottzy 1áyear, 9ámonths ago

millions of users from a single region? then its App engine!

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

367/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #137

Topic 1

Your company provides a recommendation engine for retail customers. You are providing retail customers with an API where they can submit a

user ID and the

API returns a list of recommendations for that user. You are responsible for the API lifecycle and want to ensure stability for your customers in

case the API makes backward-incompatible changes. You want to follow Google-recommended practices. What should you do?

A. Create a distribution list of all customers to inform them of an upcoming backward-incompatible change at least one month before

replacing the old API with the new API.

B. Create an automated process to generate API documentation, and update the public API documentation as part of the CI/CD process when

deploying an update to the API.

C. Use a versioning strategy for the APIs that increases the version number on every backward-incompatible change.

D. Use a versioning strategy for the APIs that adds the su x ?ÇDEPRECATED?Ç to the current API version number on every backward-

incompatible change. Use the current version number for the new API.

Correct Answer: A

Community vote distribution

C (100%)

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

It is C

upvoted 33 times

? ?  sebafranek 1áyear, 10ámonths ago

https://cloud.google.com/apis/design/versioning

upvoted 9 times

? ?  p_inkfreud 5ámonths, 3áweeks ago

more specifically: https://cloud.google.com/apis/design/versioning#release-based_versioning

upvoted 2 times

? ?  VishalB  Highly Voted ?  1áyear, 11ámonths ago

Answer C
All Google API interfaces must provide a major version number, which is encoded at the end of the protobuf package, and included as the
first part of the URI path for REST APIs. If an API introduces a breaking change, such as removing or renaming a field, it must increment its
API version number to ensure that existing user code does not suddenly break.

upvoted 20 times

? ?  BiddlyBdoyng  Most Recent ?  2áweeks, 5ádays ago

Maybe A on top of C but are we really going to mandate customers only have 1 month to be ready? I don't think so, use the version
strategy and let them migrate when ready!

upvoted 1 times

? ?  Atanu 1ámonth ago

Anyways option A is also a recommended best practice. Its version incompatible change afterall.

upvoted 1 times

? ?  gu_pp 2ámonths, 1áweek ago

Selected Answer: C

Clearly C

upvoted 1 times

? ?  bharath2k5 3ámonths, 1áweek ago

Selected Answer: C

c version numbers cannot be changed as given in d

upvoted 1 times

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: C

The correct answer is: C. Use a versioning strategy for the APIs that increases the version number on every backward-incompatible
change.

A versioning strategy for the APIs that increases the version number on every backward-incompatible change is the best way to ensure
stability for your customers in case the API makes backward-incompatible changes. This will allow you to track the changes that have been
made to the API and allow your customers to easily identify the latest version of the API.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

368/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  zerg0 4ámonths, 4áweeks ago

Selected Answer: C

See @seafranek comment

upvoted 1 times

? ?  HenkH 6ámonths, 1áweek ago

If 'A' is the correct answer, one would wonder why.
The new version might be back-ward incompatible but does not necessarily mean its a breaking one. In that case A might be sufficient,
although C remains mandatoryBest Practice imho.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Balaji_Sakthi 8ámonths ago

option C

upvoted 1 times

? ?  Aswincoder 9ámonths, 1áweek ago

Selected Answer: C

Answer is C as per https://cloud.google.com/apis/design/versioning

upvoted 4 times

? ?  AzureDP900 12ámonths ago

C is right answer
upvoted 2 times

? ?  Jabree12 1áyear ago

A does not ensure stability for anyone. It's just a notification.

upvoted 4 times

? ?  kummimhk 1áyear ago

Selected Answer: C

versioning can be done using API gateway APIGEE

upvoted 4 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

I am with C. But looking at depri called option in D made me worder there is something more to backward in compatable.
https://cloud.google.com/endpoints/docs/openapi/lifecycle-management

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

369/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #138

Topic 1

Your company has developed a monolithic, 3-tier application to allow external users to upload and share  les. The solution cannot be easily

enhanced and lacks reliability. The development team would like to re-architect the application to adopt microservices and a fully managed

service approach, but they need to convince their leadership that the effort is worthwhile. Which advantage(s) should they highlight to leadership?

A. The new approach will be signi cantly less costly, make it easier to manage the underlying infrastructure, and automatically manage the

CI/CD pipelines.

B. The monolithic solution can be converted to a container with Docker. The generated container can then be deployed into a Kubernetes

cluster.

C. The new approach will make it easier to decouple infrastructure from application, develop and release new features, manage the underlying

infrastructure, manage CI/CD pipelines and perform A/B testing, and scale the solution if necessary.

D. The process can be automated with Migrate for Compute Engine.

Correct Answer: C

Community vote distribution

C (88%)

13%

? ?  Mitra123  Highly Voted ?  1áyear, 12ámonths ago

decoupling, new features, CI/CD, A/B testing, scaling is the advantage so C

upvoted 21 times

? ?  kopper2019  Highly Voted ?  1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 12 times

? ?  bishalsainju 1áyear, 7ámonths ago

Hey man do you know if in second attempt as well, we get these same questions?

upvoted 4 times

? ?  YAS007 1áyear ago

+ 1 Please let me know as well.

upvoted 2 times

? ?  tannV 1áyear, 1ámonth ago

+1. Please let me know as well.

upvoted 2 times

? ?  PST21  Most Recent ?  3ámonths, 4áweeks ago

Here they need to convince why the effort is worthwhile - to prove that you will talk on enhancements hence C. All of these lead to cost
reduction/saving eventually.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  FenixRa73 7ámonths, 1áweek ago

I had the same question on the exam, my choice was A, passed

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  samsonakala 8ámonths ago

C most definitely. Following the best practice.

upvoted 1 times

? ?  Sitender 11ámonths, 2áweeks ago

C is better: Management like to understand business benefit in simple language. They don't bother about the technology

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

370/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  AzureDP900 12ámonths ago

C clearly states all of the benefits, C is right.

upvoted 2 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: B

There is very less discussion for this question.
.I thing we are forgetting reliablity or HA.
I am voting for B
upvoted 2 times

? ?  9xnine 1áyear ago

HA is not mentioned in the question, reliability is, but decoupling the architecture improves reliability.

upvoted 1 times

? ?  ryzior 1áyear, 1ámonth ago

Devs will talk to their managers - so more technically, then the managers will talk to their managers (maybe CxO level) and they will talk
'money'. So I think the correct answer is C but A is still quite possible in very flat organizations. Again the question is asked in some
context which is not expressed :(

upvoted 2 times

? ?  ryzior 1áyear, 1ámonth ago

and CxOs are not interested in 'easier' or 'automatically' which are parts of A answer, the job to make things easier/less cumbersome
are on the low level managers or leads of the streams :)

upvoted 1 times

? ?  wilwong 1áyear, 2ámonths ago

Selected Answer: C

C to decouple infrastructure from application, the development team want a fully managed service approach

upvoted 2 times

? ?  learner311 1áyear, 2ámonths ago

C. "develop and release new features". Work as a dev in any environment. All leadership cares about is "first to market" and differentiators.

upvoted 2 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: C

I got similar question on my exam. Answered C.

upvoted 5 times

? ?  technodev 1áyear, 5ámonths ago

Got this question in my exam, answered C

upvoted 2 times

? ?  pgarg2000 1áyear, 5ámonths ago

do you have the same response to every question

upvoted 6 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I vote A.
Management is concerned about costs.
A is the only option that touches on costs.

upvoted 4 times

? ?  anjuagrawal 1áyear, 6ámonths ago

The answer should be between A or C. Choice is to be made between cost effective or Scaling respectively. They have to convince
leadership on the effort and not cost so I think we can go for C.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

371/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #139

Topic 1

Your team is developing a web application that will be deployed on Google Kubernetes Engine (GKE). Your CTO expects a successful launch and

you need to ensure your application can handle the expected load of tens of thousands of users. You want to test the current deployment to

ensure the latency of your application stays below a certain threshold. What should you do?

A. Use a load testing tool to simulate the expected number of concurrent users and total requests to your application, and inspect the results.

B. Enable autoscaling on the GKE cluster and enable horizontal pod autoscaling on your application deployments. Send curl requests to your

application, and validate if the auto scaling works.

C. Replicate the application over multiple GKE clusters in every Google Cloud region. Con gure a global HTTP(S) load balancer to expose the

different clusters over a single global IP address.

D. Use Cloud Debugger in the development environment to understand the latency between the different microservices.

Correct Answer: B

Community vote distribution

A (83%)

B (17%)

? ?  kopper2019  Highly Voted ?  1áyear, 11ámonths ago

21 NEw Qs - July 12, 2021
# 15. An application development team has come to you for advice.They are planning to write and deploy an HTTP(S) API using Go 1.12.
The API will have a very unpredictable workload and must remain reliable during peaks in traffic. They want to minimize operational
overhead for this application. What approach should you recommend?

a. Use a Managed Instance Group when deploying to Compute Engine
b. Develop an application with containers, and deploy to Google Kubernetes Engine (GKE)
c. Develop the application for App Engine standard environment
d. Develop the application for App Engine Flexible environment using a custom runtime

Answer C, , please share you answers

upvoted 22 times

? ?  jay9114 4ámonths, 3áweeks ago

I vote C

upvoted 1 times

? ?  deep_ROOT 1áyear, 11ámonths ago

C.
https://cloud.google.com/appengine/docs/the-appengine-environments

upvoted 7 times

? ?  muhasinem 1áyear, 11ámonths ago

C is ok.

upvoted 5 times

? ?  namanp12345 1áyear, 10ámonths ago

Answer A

upvoted 1 times

? ?  namanp12345 1áyear, 10ámonths ago

Sorry C

upvoted 1 times

? ?  juccjucc  Highly Voted ?  1áyear, 11ámonths ago

Anyone can tell please if at the new exam there are also questions from the old set(before question 115)?

upvoted 13 times

? ?  kopper2019 1áyear, 9ámonths ago

old ones are not removed

upvoted 4 times

? ?  ale183 1áyear, 9ámonths ago

really ? the old ones are are still on exam ? from 1-100 ? how about old case study questions ?

upvoted 2 times

? ?  kubinho  Most Recent ?  1áweek, 6ádays ago

Admin should really deleting the wrong answers.. I think 90% of people here just copying explanations and really have no idea what they
are talking about. every second question has different answer with different "voting"answer..

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

372/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  BiddlyBdoyng 2áweeks, 5ádays ago

Using Curl seems weird, how is Curl going to inject the load? Curl would be fine if we just wanted to test the underlying latency of the
system.

upvoted 1 times

? ?  gu_pp 2ámonths, 1áweek ago

Selected Answer: A

It is A.
You want to TEST the deployment, not changing anything (yet).

upvoted 1 times

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: A

The correct answer is: A. Use a load testing tool to simulate the expected number of concurrent users and total requests to your
application, and inspect the results.

A load testing tool can be used to simulate the expected number of concurrent users and total requests to your application. This will allow
you to test how your application handles the expected load and to identify any potential problems.

Enabling autoscaling on the GKE cluster and enabling horizontal pod autoscaling on your application deployments will not help you to test
the latency of your application. This will only help to ensure that your application can handle the expected load.

upvoted 3 times

? ?  MaryMei 4ámonths ago

Selected Answer: B

is curl returns latency info?

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  sanait100 6ámonths, 3áweeks ago

Also, there is no expected numbers, the users can be tens of thousands so whatever testing you do with A may not be sufficient so it's
better to keep autoscaling for whatsoever load comes in and curl test ensures autoscaling happens when required

upvoted 2 times

? ?  sanait100 6ámonths, 3áweeks ago

The question specifically asks that your CTO expects a successful launch and you need to ensure your application can handle the expected
load of tens of thousands of users. In A, you are just testing and not taking an action. In B, you are not only testing with curl commands to
check for latency but also taking action to enable the cluster to acquire more resources. So, I will go with B.

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is the correct, no load ==> no latency checking

upvoted 1 times

? ?  AHUI 8ámonths, 2áweeks ago

Ans A: the question asks you want to test

upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

I was going for B.
However analyzing question it says "test the current deployment to ensure the latency"
The only test answer is the stress test / performance test simulating the users workload.

upvoted 1 times

? ?  pp0709 10ámonths ago

Selected Answer: B

Both A and B are practically right answers. From an exam standpoint, I will choose B, as the question explicitly calls out GKE

upvoted 2 times

? ?  tycho 10ámonths, 2áweeks ago

I think it wrong for application to be in pod it should be a deployment, and deployment would scale

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

373/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  AzureDP900 12ámonths ago

This is pretty easy one, other options are not good except Option A is right. Performance test can simulate the user experience.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

374/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #140

Topic 1

Your company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the application is simple, it

was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered that the application cannot process the

messages in real time. Most of them wait for minutes before being processed. You need to scale the elaboration process that is I/O-intensive.

What should you do?

A. Use kubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50 to con gure Kubernetes autoscaling deployment.

B. Con gure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric.

C. Use the --enable-autoscaling  ag when you create the Kubernetes cluster.

D. Con gure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric.

Correct Answer: C

Reference:

https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler

Community vote distribution

D (83%)

B (17%)

? ?  Rzla  Highly Voted ?  1áyear, 9ámonths ago

Answer is D. num_undelivered_messages metric can indicate if subscribers are keeping up with message submissions.
https://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog

upvoted 34 times

? ?  GopeshSahu 5ámonths ago

https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub

upvoted 2 times

? ?  PATILDXB 7ámonths ago

The provided link is not relevant to kubernetes, but pertains to cloud pub/sub....the num_undelivered_messages metric is not available
for kubernetes autoscaling...C is correct

upvoted 2 times

? ?  rishab86 1áyear, 9ámonths ago

D is correct !

upvoted 2 times

? ?  aut0pil0t  Highly Voted ?  9ámonths, 4áweeks ago

Selected Answer: D

Direct answer - D

https://cloud.google.com/kubernetes-engine/docs/samples/container-pubsub-horizontal-pod-autoscaler

apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
name: pubsub
spec:
minReplicas: 1
maxReplicas: 5
metrics:
- external:
metric:
name: pubsub.googleapis.com|subscription|num_undelivered_messages
selector:
matchLabels:
resource.labels.subscription_id: echo-read
target:
type: AverageValue
averageValue: 2
type: External
scaleTargetRef:
apiVersion: apps/v1
kind: Deployment
name: pubsub
upvoted 13 times

? ?  sampon279  Most Recent ?  1áday, 17áhours ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

375/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Answer D

upvoted 1 times

? ?  BiddlyBdoyng 2áweeks, 5ádays ago

B & C refer to using auto scaler with custom metrics but

"Cluster autoscaler makes these scaling decisions based on the resource requests (rather than actual resource utilization) of Pods running
on that node pool's nodes. "

A makes no sense as it defines a min and max that we don't know

D seems like part of the solution so D.

upvoted 1 times

? ?  natpilot 2ámonths, 2áweeks ago

is D - https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub

upvoted 1 times

? ?  WFCheong 5ámonths, 2áweeks ago

Selected Answer: B

I vot for B. Configure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric. as the metric
should be based on latency instead of num_undelivered_messages metric in D.

upvoted 2 times

? ?  gcppandit 4ámonths, 4áweeks ago

The problems says PULL request and B is related to PUSH request. I do not think it is related.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  ale_brd_ 6ámonths, 4áweeks ago

Selected Answer: D

D is the correct one

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  newuser111 8ámonths ago

Selected Answer: D

D

https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub

upvoted 2 times

? ?  bossdellacert 10ámonths, 1áweek ago

Selected Answer: D

This seems relevant
https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub
even if it uses Deployment + HorizontalPodAutoscaler which is not mentioned in the context of the question/answer

upvoted 2 times

? ?  tycho 10ámonths, 2áweeks ago

I think it wrong for application to be in pod it should be a deployment, and deployment would scale

upvoted 2 times

? ?  AzureDP900 12ámonths ago

Option D is most suitable for given requirement.

upvoted 1 times

? ?  AzureDP900 12ámonths ago

https://cloud.google.com/pubsub/docs/monitoring#monitoring_subscriptions_with_filters

upvoted 1 times

? ?  DP_GCP 1áyear ago

D is correct, please check below code:
gcloud compute instance-groups managed set-autoscaling \
our-instance-group \
--zone=us-central1-a \
--max-num-replicas=100 \

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

376/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

--min-num-replicas=0 \
--update-stackdriver-metric=pubsub.googleapis.com/subscription/num_undelivered_messages \
--stackdriver-metric-filter="resource.type = pubsub_subscription AND resource.labels.subscription_id = our-subscription" \
--stackdriver-metric-single-instance-assignment=15

upvoted 2 times

? ?  ridyr 1áyear, 1ámonth ago

Selected Answer: D

I'm going with D, but not certain
Question says; app pulls(subscription) messages. B is push metric?

upvoted 3 times

? ?  wilwong 1áyear, 2ámonths ago

Selected Answer: D

It's for io, not cpu, so A is wrong, not for push B is wrong, C can't fix the problem, Just D is correct.

upvoted 3 times

? ?  user1324567 1áyear, 5ámonths ago

D is ok.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

377/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #141

Topic 1

Your company is developing a web-based application. You need to make sure that production deployments are linked to source code commits and

are fully auditable. What should you do?

A. Make sure a developer is tagging the code commit with the date and time of commit.

B. Make sure a developer is adding a comment to the commit that links to the deployment.

C. Make the container tag match the source code commit hash.

D. Make sure the developer is tagging the commits with latest.

Correct Answer: A

Community vote distribution

C (100%)

? ?  djosani  Highly Voted ?  1áyear, 10ámonths ago

Developer shouldn't tag or comment every commit with some specific data, like timestamps or something else. There might be an app
version, but it's not mentioned. I'd go with C as it's an automated, error-less approach that answers the question.

upvoted 31 times

? ?  Urban_Life 1áyear, 6ámonths ago

@Kopper2019- what do you think about ans C?

upvoted 2 times

? ?  victory108  Highly Voted ?  1áyear, 10ámonths ago

C. Make the container tag match the source code commit hash.

upvoted 13 times

? ?  amxexam 1áyear, 9ámonths ago

Not sure how the container tag match with the commit will help to audit, can someone explain?

upvoted 2 times

? ?  ynoot 1áyear, 7ámonths ago

if you got the commit hash from the container you can check the corresponding commit in the git repository. So the change, that
was made and deployed into your environment can be audited.

upvoted 8 times

? ?  BiddlyBdoyng  Most Recent ?  2áweeks, 5ádays ago

Really C should say image?
We have to seperate systems: source code repo & container repo.
How do we link the two together? C is the only attempt at solving the problem.

upvoted 1 times

? ?  WFCheong 5ámonths, 2áweeks ago

Selected Answer: C

Agreed with C instead of A with them.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  KumarSelvaraj 7ámonths, 1áweek ago

Answer is C

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 2 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is correct "By design, the Git commit hash is immutable and references a specific version of your software." as per
https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

378/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: C

C is the answer.

https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash
You can use this commit hash as a version number for your software, but also as a tag for the Docker image built from this specific version
of your software. Doing so makes Docker images traceable: because in this case the image tag is immutable, you instantly know which
specific version of your software is running inside a given container.

upvoted 3 times

? ?  AzureDP900 12ámonths ago

Every Git commit with timestamp A doesn't make since. C is right

upvoted 3 times

? ?  munnysh 1áyear ago

Selected Answer: C

No manual intervention is preferred in automatic deployments. Only automating the container tag to match the commit hash will be fully
auditable with the help of the scm.

upvoted 4 times

? ?  ridyr 1áyear, 1ámonth ago

Selected Answer: C

From: https://cloud.google.com/architecture/best-practices-for-building-containers
Under: Tagging using the Git commit hash (bottom of page almost)

"In this case, a common way of handling version numbers is to use the Git commit SHA-1 hash (or a short version of it) as the version
number. By design, the Git commit hash is immutable and references a specific version of your software.

You can use this commit hash as a version number for your software, but also as a tag for the Docker image built from this specific version
of your software. Doing so makes Docker images traceable: because in this case the image tag is immutable, you instantly know which
specific version of your software is running inside a given container."

upvoted 7 times

? ?  SCVinod 1áyear, 3ámonths ago

It's got to be A. Option C talks about containers whereas there is no mention of containers in the question.

upvoted 3 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: C

I got similar question on my exam. Answered C.

upvoted 5 times

? ?  Narinder 1áyear, 5ámonths ago

I think answer is A.

In Git, tag is used to mark release points (v1.0, v2.0 and so on). You can tag the release based on the time stamp and using git show <tag-
name> command, you can see the commit detailed history.

Reference: https://git-scm.com/book/en/v2/Git-Basics-Tagging

C could be the correct answer for the case if you are going with container based solution which is not mentioned anywhere in the
question.

upvoted 4 times

? ?  ks100 1áyear, 5ámonths ago

Selected Answer: C

should be C.
If A is correct, can the site provide some reference to the reason?

upvoted 1 times

? ?  tmnd91 1áyear, 5ámonths ago

Selected Answer: C

Humans are unreliable

upvoted 6 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

379/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #142

Topic 1

An application development team has come to you for advice. They are planning to write and deploy an HTTP(S) API using Go 1.12. The API will

have a very unpredictable workload and must remain reliable during peaks in tra c. They want to minimize operational overhead for this

application. Which approach should you recommend?

A. Develop the application with containers, and deploy to Google Kubernetes Engine.

B. Develop the application for App Engine standard environment.

C. Use a Managed Instance Group when deploying to Compute Engine.

D. Develop the application for App Engine  exible environment, using a custom runtime.

Correct Answer: B

Community vote distribution

B (100%)

? ?  vladik820  Highly Voted ?  1áyear, 10ámonths ago

B is ok

upvoted 21 times

? ?  SweetieS  Highly Voted ?  1áyear, 10ámonths ago

B is ok.
https://cloud.google.com/appengine/docs/the-appengine-environments

upvoted 11 times

? ?  cugena 1áyear, 9ámonths ago

Source code is written in specific versions of the supported programming languages:
Python 2.7, Python 3.7, Python 3.8, Python 3.9
Java 8, Java 11
Node.js 10, Node.js 12, Node.js 14, Node.js 16 (preview)
PHP 5.5, PHP 7.2, PHP 7.3, and PHP 7.4
Ruby 2.5, Ruby 2.6, and Ruby 2.7
Go 1.11, Go 1.12, Go 1.13, Go 1.14, Go 1.15, and Go 1.16 (preview)

upvoted 6 times

? ?  cugena 1áyear, 9ámonths ago

Intended to run for free or at very low cost, where you pay only for what you need and when you need it. For example, your application
can scale to 0 instances when there is no traffic.

Experiences sudden and extreme spikes of traffic which require immediate scaling.

upvoted 4 times

? ?  sampon279  Most Recent ?  1áday, 17áhours ago

Selected Answer: B

App engine standard provides go env.

upvoted 1 times

? ?  AugustoKras011111 3ámonths, 4áweeks ago

Selected Answer: B

App Engine Std. Can run this Go version and Scales to 0.

upvoted 1 times

? ?  zerg0 4ámonths, 2áweeks ago

Selected Answer: B

Standard AppEngine Environment supports Go 1.2. The AppEngine can be low cost if no or low traffic. It has free quotas.

upvoted 1 times

? ?  zerg0 4ámonths, 4áweeks ago

Selected Answer: B

AppEngine scales well, only dev effort. No infrastructure. go is supported in the standard distribution.

upvoted 1 times

? ?  NodummyIQ 5ámonths, 4áweeks ago

The answer is A. B option is not correct. It is not recommended to use App Engine Standard environment for an HTTP(S) API with a very
unpredictable workload because App Engine Standard environment has certain limitations and constraints that may not be suitable for an
API with an unpredictable workload. For example, App Engine Standard environment has a maximum request timeout of 60 seconds,
which may not be sufficient for an API with a very unpredictable workload.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

380/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  AzureDP900 12ámonths ago

B is correct ..https://cloud.google.com/appengine/docs/the-appengine-environments

Experiences sudden and extreme spikes of traffic which require immediate scaling.

upvoted 1 times

? ?  munnysh 1áyear ago

Selected Answer: B

https://cloud.google.com/appengine/docs/the-appengine-environments App engine standard environment support go 1.13 and also
handles the unpredictable load.

upvoted 3 times

? ?  TitaniumBurger 1áyear, 4ámonths ago

B. Unpredictable traffic & low overhead.

upvoted 2 times

? ?  tmnd91 1áyear, 5ámonths ago

Selected Answer: B

App Engine standard has autoscaling out of the box, supports Go 1.12 and can scale down to 0 to save money

upvoted 6 times

? ?  lxgywil 1áyear, 5ámonths ago

B is ok.

upvoted 1 times

? ?  PhuocT 1áyear, 6ámonths ago

Selected Answer: B

B is the right answer

upvoted 2 times

? ?  phantomsg 1áyear, 6ámonths ago

Selected Answer: B

AppEngine Standard supports Go language now. Fully-managed service - So no operational overhead and pay-only-for-what-you-use
model.

upvoted 2 times

? ?  rajadhav 1áyear, 6ámonths ago

B is correct answer.

upvoted 2 times

? ?  Bert_77 1áyear, 6ámonths ago

Selected Answer: B

B will be the best option. App engine standard supports Go 1.12, can scale quickly during peaks and even scale to 0 when not used, no
management overhead

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

381/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #143

Topic 1

Your company is designing its data lake on Google Cloud and wants to develop different ingestion pipelines to collect unstructured data from

different sources.

After the data is stored in Google Cloud, it will be processed in several data pipelines to build a recommendation engine for end users on the

website. The structure of the data retrieved from the source systems can change at any time. The data must be stored exactly as it was retrieved

for reprocessing purposes in case the data structure is incompatible with the current processing pipelines. You need to design an architecture to

support the use case after you retrieve the data. What should you do?

A. Send the data through the processing pipeline, and then store the processed data in a BigQuery table for reprocessing.

B. Store the data in a BigQuery table. Design the processing pipelines to retrieve the data from the table.

C. Send the data through the processing pipeline, and then store the processed data in a Cloud Storage bucket for reprocessing.

D. Store the data in a Cloud Storage bucket. Design the processing pipelines to retrieve the data from the bucket.

Correct Answer: D

Community vote distribution

D (100%)

? ?  vladik820  Highly Voted ?  1áyear, 10ámonths ago

D is ok
The data needs to be stored as it is retrieved. This would mean that any processing should be done after it is stored.

upvoted 24 times

? ?  MaxNRG  Highly Voted ?  1áyear, 8ámonths ago

D, store RAW unstructured data as-is in Cloud Storage, and then define how to process it.
Classical Data Lake ELT (Extract -> Load -> Transform )

upvoted 5 times

? ?  BeCalm  Most Recent ?  3ámonths, 2áweeks ago

What is the point of data being in a lake and then being dumped into GCS without processing. What purpose is served with GCS being a
copy of lake?

upvoted 1 times

? ?  jlambdan 2ámonths, 4áweeks ago
here gcs is the lake. Not a copy.
The data warehouse will be what comes out of the pipelines.

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 2 times

? ?  jmblancof 7ámonths, 4áweeks ago

D is ok

upvoted 2 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: D

D is ok
The data needs to be stored as it is retrieved. This would mean that any processing should be done after it is stored in GCS.

upvoted 2 times

? ?  AzureDP900 12ámonths ago

storing and retrieving data in cloud storage solve the purpose of this use case. D is perfect answer.

upvoted 1 times

? ?  snwbr 1áyear, 3ámonths ago

Although... wouldn't be Bigtable or Datastore better than GCS?

upvoted 1 times

? ?  wykofc 11ámonths, 1áweek ago

Both BigTable and DataStore are NoSQL Databases, qns mentioned that data structure may change anytime

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

382/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: D

I got similar question on my exam. Answered D.

upvoted 4 times

? ?  technodev 1áyear, 5ámonths ago

Got this question in my exam, answered D

upvoted 4 times

? ?  lxgywil 1áyear, 5ámonths ago

D is ok

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

D is the correct answer

upvoted 1 times

? ?  pakilodi 1áyear, 7ámonths ago

Selected Answer: D

D is correct

upvoted 1 times

? ?  TheCloudBoy77 1áyear, 7ámonths ago

D - Data must be stored as it is before and after so use Cloud storage and then build pipelines as needed.

upvoted 2 times

? ?  danielfootc 1áyear, 8ámonths ago

I would select D as well.

upvoted 2 times

? ?  AnilKr 1áyear, 9ámonths ago

D is correct.

upvoted 2 times

? ?  amxexam 1áyear, 9ámonths ago

"After the data is stored in Google Cloud, it will be processed in several data pipelines to build a recommendation engine"

So first store then process in the pipeline.

So we need to store first then process it.
Will eliminate A and C.

The second point big data table needs a fixed schema to work so it won't work.
Will eliminate B

Hence D

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

383/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #144

Topic 1

You are responsible for the Google Cloud environment in your company. Multiple departments need access to their own projects, and the

members within each department will have the same project responsibilities. You want to structure your Google Cloud environment for minimal

maintenance and maximum overview of

IAM permissions as each department's projects start and end. You want to follow Google-recommended practices. What should you do?

A. Grant all department members the required IAM permissions for their respective projects.

B. Create a Google Group per department and add all department members to their respective groups. Create a folder per department and

grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders.

C. Create a folder per department and grant the respective members of the department the required IAM permissions at the folder level.

Structure all projects for each department under the respective folders.

D. Create a Google Group per department and add all department members to their respective groups. Grant each group the required IAM

permissions for their respective projects.

Correct Answer: B

Community vote distribution

B (100%)

? ?  Manh  Highly Voted ?  1áyear, 9ámonths ago

it's B

upvoted 16 times

? ?  victory108  Highly Voted ?  1áyear, 10ámonths ago

B. Create a Google Group per department and add all department members to their respective groups. Create a folder per department
and grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders.

upvoted 8 times

? ?  megumin  Most Recent ?  7ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: B

B is the answer.

https://cloud.google.com/resource-manager/docs/access-control-folders#best-practices-folders-iam
Use groups whenever possible to manage principals.

https://cloud.google.com/resource-manager/docs/creating-managing-folders
A folder can contain projects, other folders, or a combination of both. Organizations can use folders to group projects under the
organization node in a hierarchy. For example, your organization might contain multiple departments, each with its own set of Google
Cloud resources. Folders allow you to group these resources on a per-department basis.

upvoted 3 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: B

B is most appropriate for the use case and principle of least privilege.

upvoted 1 times

? ?  AzureDP900 12ámonths ago

B is most appropriate for the use case and principle of least privilege.

upvoted 1 times

? ?  coutcin 1áyear, 1ámonth ago

Selected Answer: B

B is correct

upvoted 1 times

? ?  lxgywil 1áyear, 5ámonths ago

B is ok

upvoted 1 times

? ?  edilramos 1áyear, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

384/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B is ideal for minimal maintenance and maximum overview of IAM permissions as each department's projects start and end.
Manage the users inside Groups will turn it easer.

upvoted 4 times

? ?  anjuagrawal 1áyear, 6ámonths ago

Voted B

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

B is the correct answer

upvoted 1 times

? ?  nqthien041292 1áyear, 7ámonths ago

Selected Answer: B

Vote B

upvoted 2 times

? ?  danielfootc 1áyear, 8ámonths ago

I would select B.
upvoted 2 times

? ?  AnilKr 1áyear, 9ámonths ago

B is correct, folder restructure per department and IAM permission for Group is recommended.

upvoted 4 times

? ?  Sonu_xyz 1áyear, 9ámonths ago

Answer is B

upvoted 2 times

? ?  diaga2 1áyear, 9ámonths ago

Yes, B

upvoted 4 times

? ?  serious_user 1áyear, 10ámonths ago

B is ok

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

385/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #145

Topic 1

Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. You have separate clusters for

development, staging, and production. You have discovered that the team is able to deploy a Docker image to the production cluster without  rst

testing the deployment in development and then staging. You want to allow the team to have autonomy but want to prevent this from happening.

You want a Google Cloud solution that can be implemented quickly with minimal effort. What should you do?

A. Con gure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given environment.

B. Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker image was tested in an

earlier environment.

C. Con gure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous

integration pipeline.

D. Create a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the given environment.

Correct Answer: C

Community vote distribution

C (100%)

? ?  diaga2  Highly Voted ?  1áyear, 9ámonths ago

C is s fine.

upvoted 15 times

? ?  [Removed]  Highly Voted ?  1áyear, 4ámonths ago

Selected Answer: C

I got similar question on my exam. Answered C.

upvoted 9 times

? ?  Deb2293  Most Recent ?  3ámonths, 2áweeks ago

Selected Answer: C

C it is

upvoted 1 times

? ?  omermahgoub 6ámonths ago

A good option for quickly implementing a solution to prevent deployments to the production cluster without first testing in development
and staging would be to configure binary authorization policies for the development, staging, and production clusters. You can then
create attestations as part of the continuous integration pipeline.

Option C, "Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the
continuous integration pipeline," would be the correct choice for this scenario.

Binary authorization is a feature of Google Kubernetes Engine that allows you to enforce policies on the images that are deployed to your
clusters. By configuring binary authorization policies for the development, staging, and production clusters, you can ensure that only
images that have been attested by an authorized entity are allowed to be deployed to those clusters. You can create the attestations as
part of the continuous integration pipeline, which will allow you to verify that the image has been tested before it is deployed to the next
environment.
upvoted 4 times

? ?  omermahgoub 6ámonths ago

Option A, "Configure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given
environment," would not be a good choice because it would not prevent the deployment of the container to the cluster in the first
place.

Option D, "Create a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the given
environment," would also not be a good choice because it would not prevent the deployment of the container to the cluster in the first
place.

Option B, "Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker image
was tested in an earlier environment," would be a good option, but it would not be as effective as using binary authorization policies,
as it would rely on the team following the policy rather than enforcing it automatically.

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

386/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Thornadoo 11ámonths ago

Why not A? Need something to be implemented quickly is what the q asks.

upvoted 1 times

? ?  AzureDP900 12ámonths ago

C is right..

Binary Authorization implements a policy model, where a policy is a set of rules that governs the deployment of container images. Rules in
a policy provide specific criteria that an image must satisfy before it can be deployed.

For more information about the Binary Authorization policy model and other concepts, see Key concepts.

upvoted 2 times

? ?  AzureDP900 12ámonths ago

https://cloud.google.com/binary-authorization/docs/overview#policy_model

upvoted 1 times

? ?  yogi_508 1áyear, 6ámonths ago

where the case study questions are available in this website?

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

C is the correct answer
https://cloud.google.com/binary-authorization/docs/overview

upvoted 3 times

? ?  Jimjiang 1áyear, 8ámonths ago

C is fine

upvoted 1 times

? ?  danielfootc 1áyear, 8ámonths ago
I think C is the correct answer.

upvoted 1 times

? ?  AnilKr 1áyear, 9ámonths ago

C is correct, binary authorization is the solution.

upvoted 2 times

? ?  victory108 1áyear, 10ámonths ago

C. Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the
continuous integration pipeline.

upvoted 2 times

? ?  serious_user 1áyear, 10ámonths ago

C is ok

upvoted 2 times

? ?  vladik820 1áyear, 10ámonths ago

C is ok

upvoted 2 times

? ?  SweetieS 1áyear, 10ámonths ago

Sorry, it's C : Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of
the continuous integration pipeline.

upvoted 3 times

? ?  SweetieS 1áyear, 10ámonths ago

D is ok.
https://cloud.google.com/binary-authorization/docs/overview

upvoted 1 times

? ?  cugena 1áyear, 9ámonths ago

You meant C I guess

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

387/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #146

Topic 1

Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it takes to complete

this activity, the overall cost, and database load. The bandwidth between the on-premises environment and Google Cloud is 1 Gbps. You want to

follow Google-recommended practices. What should you do?

A. Develop a Data ow job to read data directly from the database and write it into Cloud Storage.

B. Use the Data Transfer appliance to perform an o ine migration.

C. Use a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud Storage.

D. Compress the data and upload it with gsutil -m to enable multi-threaded copy.

Correct Answer: A

Community vote distribution

D (46%)

B (45%)

10%

? ?  pr2web  Highly Voted ?  1áyear, 9ámonths ago

This is pretty simple.
Time to transfer using Transfer Appliance: 1-3 weeks (I've used it twice and had a 2-3 week turnaround total)
Time to transfer using 1Gbps : 30 hours (https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-
datasets)

Answer is D, using gsutil

upvoted 69 times

? ?  Petya27 1ámonth ago

Plus, you use the Transfer Appliance for data, which is over 40 TB, and 10 TB is way below that.

upvoted 1 times

? ?  ckw_1206 4ámonths, 1áweek ago

make sense!

upvoted 1 times

? ?  moota 4ámonths, 3áweeks ago

According to https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets
> The expected turnaround time for a network appliance to be shipped, loaded with your data, shipped back, and rehydrated on
Google Cloud is 20 days. If your online transfer timeframe is calculated to be substantially more than this timeframe, consider Transfer
Appliance.

upvoted 4 times

? ?  PleeO 1áyear, 9ámonths ago

according to your real case, D definitely is a correct answer. However this is the GCP exam, I would go with B - Use data transfer
appliance -> the precise answer

upvoted 19 times

? ?  mikesp 1áyear, 8ámonths ago

I agree. "Google-recommended practices" means than if more than 1TB, then Transfer Appliance. The main drawback of option D is
that gsutil does not enable to manage bandwidth and corporate connection could be congested by transfer and affect other
business activities.

upvoted 15 times

? ?  gingerbeer  Highly Voted ?  1áyear, 9ámonths ago

No perfect answer as B and D both have flaws. B is time latency as transfer appliance usually takes weeks; D gsutil applies for less than
1TB. The answer should be storage transfer service for on-premises data, which is not available here.

If have to choose one I go for B

upvoted 17 times

? ?  RitwickKumar 10ámonths, 1áweek ago

Storage transfer service is for online data. It can't serve the purpose if you don't have the connectivity established between on prem
and gcp. Which is what we can't assume ourselves in this question.

upvoted 1 times

? ?  DS2023  Most Recent ?  1áweek, 6ádays ago

Selected Answer: D

Ans: D, Refer the link, it gave a calculation, as per which approximately 30 hours/1 Day needed to transfer 10 TB of Data on 1 GPBS
bandwidth. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

388/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  BiddlyBdoyng 2áweeks, 5ádays ago

I'd go for D. Seems the fastest (1day) and simplest.
"The expected turnaround time for a network appliance to be shipped, loaded with your data, shipped back, and rehydrated on Google
Cloud is 20 days. If your online transfer timeframe is calculated to be substantially more than this timeframe, consider Transfer
Appliance."

upvoted 1 times

? ?  TheCloudGuruu 1ámonth, 2áweeks ago

Selected Answer: D

Answer is D, transfer appliance will take too long

upvoted 1 times

? ?  medi01 2ámonths, 1áweek ago

Selected Answer: A

Maximum object size in Cloud Storage per:

https://cloud.google.com/storage/quotas#object_size

Is 5TB. Now, whether compressed version of DB dump will be smaller than that, is not guaranteed. So suddenly A is an option.

upvoted 1 times

? ?  Armanna 2ámonths, 2áweeks ago

-m
Causes supported operations (acl ch, acl set, cp, mv, rm, rsync, and setmeta) to run in parallel. This can significantly improve performance
if you are performing operations on a large number of files over a reasonably fast network connection.

https://cloud.google.com/storage/docs/gsutil/addlhelp/GlobalCommandLineOptions

So, D is not correct, B is correct

upvoted 1 times

? ?  GCP_Tanu 2ámonths, 3áweeks ago

Selected Answer: D

Use the calc at ttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets

upvoted 1 times

? ?  lokiinaction 2ámonths, 3áweeks ago

Selected Answer: A

I am thinking of A:
for D transferring 10 TB with 1Gbps takes 3 hours: 10000/60/60 , but the problem is with -m option, database files are normally quite
huge, which are not really good option to use multi threading
Transfer Appliance is not required.
Only meaninful option for me is A

upvoted 2 times

? ?  kripya 1áday, 19áhours ago

Took the exam yesterday and gsutil option was not even there. Only 2 viable options were dataflow or transfer appliance. Question was
to select most cost effective option, so went with dataflow. I did pass the test in first attempt.

upvoted 1 times

? ?  musumusu 3ámonths ago

Answer D,
10 Tb limit of gsutil, and compression reduce size by 60 to 80 percent.
Transfer appliance used when data is so big and bad internet connection

upvoted 1 times

? ?  medi01 2ámonths, 1áweek ago

gsutil limit (rather CS limit) is 5TB per file.

How much of it will be compessed, depends on data, you cannot guarantee it will be 50%+.

upvoted 1 times

? ?  JC0926 3ámonths ago

Selected Answer: D

Key: minimize the time it takes to complete this activity,
So it's D

upvoted 1 times

? ?  Ralucas 3ámonths ago

It says to minimize the time for the operation. That is why I am leaning towards D.
Plus after reading the documentation here (https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-
datasets) we are in the scenario: "Enough bandwidth to meet your project deadline for more than 1 TB of data" and we DO NOT have the
"Storage Transfer service" option available to pick. My 2 cents.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

389/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  medi01 2ámonths, 1áweek ago

And there it says that gsutil is for less than 1Tb.

upvoted 1 times

? ?  telp 3ámonths, 1áweek ago

Selected Answer: B

Answer B because it's a google exam. The rule for best practices is for more than 1 Tb to transfer data => use the google transfer
appliance.

But we all know in reality with 1gbs, we will use classic upload with gsutils.

Ref. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options

upvoted 3 times

? ?  Deb2293 3ámonths, 1áweek ago

Selected Answer: B

Should be B. Google would recommend using their appliance.

upvoted 1 times

? ?  cert2020 3ámonths, 2áweeks ago

Answer is B.
Although the time taken for transfer of 10TB with 1 GBPS is 30 hours, Google recommends to use GSUTIL for data transfers less than 1 TB.
Perhaps Storage Transfer Service would have been better choice but not listed.
Ref. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options

upvoted 3 times

? ?  Deb2293 3ámonths, 2áweeks ago

Selected Answer: B

To minimize the time it takes to complete the activity, the overall cost, and database load when migrating a 10-TB on-premises database
export into Cloud Storage, the best option is to use the Data Transfer Appliance to perform an offline migration. This solution involves
shipping a physical device that can transfer large amounts of data quickly and securely to Google Cloud.

upvoted 1 times

? ?  segkhachat 3ámonths, 2áweeks ago

Selected Answer: D

With gsutil it will take 20 mins

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

390/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #147

Topic 1

Your company has an enterprise application running on Compute Engine that requires high availability and high performance. The application has

been deployed on two instances in two zones in the same region in active-passive mode. The application writes data to a persistent disk. In the

case of a single zone outage, that data should be immediately made available to the other instance in the other zone. You want to maximize

performance while minimizing downtime and data loss.

What should you do?

A. 1. Attach a persistent SSD disk to the  rst instance. 2. Create a snapshot every hour. 3. In case of a zone outage, recreate a persistent SSD

disk in the second instance where data is coming from the created snapshot.

B. 1. Create a Cloud Storage bucket. 2. Mount the bucket into the  rst instance with gcs-fuse. 3. In case of a zone outage, mount the Cloud

Storage bucket to the second instance with gcs-fuse.

C. 1. Attach a regional SSD persistent disk to the  rst instance. 2. In case of a zone outage, force-attach the disk to the other instance.

D. 1. Attach a local SSD to the  rst instance disk. 2. Execute an rsync command every hour where the target is a persistent SSD disk attached

to the second instance. 3. In case of a zone outage, use the second instance.

Correct Answer: B

Community vote distribution

C (90%)

10%

? ?  juma_david  Highly Voted ?  1áyear, 10ámonths ago

Answer C
https://cloud.google.com/compute/docs/disks/repd-failover

upvoted 39 times

? ?  [Removed]  Highly Voted ?  1áyear, 8ámonths ago

C is right answer.
C. 1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the other instance.
gcs-fuse is slower than of regional SSD PD.

**** Admin: You need to correct lots of questions. Some of the marked answers are nonsense, these must be revisited based on experts
comments.

upvoted 35 times

? ?  DS2023  Most Recent ?  1áweek, 6ádays ago

Selected Answer: C

Ans: C, please check - https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk

upvoted 1 times

? ?  dbsmk 3ámonths ago

https://cloud.google.com/compute/docs/disks/repd-failover

Seems C is correct

upvoted 1 times

? ?  examch 5ámonths, 3áweeks ago

C is the correct answer,

https://cloud.google.com/compute/docs/disks/repd-failover#zonal_failures

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

Answer is C

upvoted 1 times

? ?  zetalexg 6ámonths, 2áweeks ago

Admins please take some time and redo the answers, put them to match at least the most voted ones, would help a lot.

upvoted 6 times

? ?  ashrafh 7ámonths, 1áweek ago

Selected Answer: C

Regional persistent disk is a storage option that provides synchronous replication of data between two zones in a region. Regional
persistent disks can be a good building block to use when you implement HA services in Compute Engine.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

391/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is the right answer

upvoted 1 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: C

You want to maximize performance while minimizing downtime and data loss

upvoted 1 times

? ?  RitwickKumar 10ámonths, 1áweek ago

Selected Answer: C

Inline with the current architecture itself "The application writes data to a persistent disk."

upvoted 1 times

? ?  AzureDP900 12ámonths ago

C is right..

https://cloud.google.com/compute/docs/disks/high-availability-regional-persistent-disk
Regional persistent disk is a storage option that provides synchronous replication of data between two zones in a region. Regional
persistent disks can be a good building block to use when you implement HA services in Compute Engine.

The benefit of regional persistent disks is that in the event of a zonal outage, where your virtual machine (VM) instance might become
unavailable, you can usually force attach a regional persistent disk to a VM instance in a secondary zone in the same region. To perform
this task, you must either start another VM instance in the same zone as the regional persistent disk that you are force attaching, or
maintain a hot standby VM instance in that zone. A hot standby is a running VM instance that is identical to the one you are using. The two
instances have the same data.

The force-attach operation executes in less than one minute.

upvoted 4 times

? ?  munnysh 1áyear ago

Selected Answer: C

C would be the answer. Storage bucket in comparison of SSD is not high performance

upvoted 1 times

? ?  sarath 1áyear ago

Selected Answer: B

As availability is immediate (storage being redundant across zones), so cheaper compared to regional persistent disk.
If performance(IOPS) was a requirement, C would've been apt.

upvoted 3 times

? ?  conmarch2022 1áyear, 2ámonths ago

C In the event that the primary zone fails, you can fail over your regional persistent disk to a VM instance in another zone by using the --
force-attach flag with the attach-disk command.

upvoted 1 times

? ?  sri7 1áyear, 3ámonths ago

Selected Answer: C

C is the right answer.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

392/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #148

Topic 1

You are designing a Data Warehouse on Google Cloud and want to store sensitive data in BigQuery. Your company requires you to generate the

encryption keys outside of Google Cloud. You need to implement a solution. What should you do?

A. Generate a new key in Cloud Key Management Service (Cloud KMS). Store all data in Cloud Storage using the customer-managed key option

and select the created key. Set up a Data ow pipeline to decrypt the data and to store it in a new BigQuery dataset.

B. Generate a new key in Cloud KMS. Create a dataset in BigQuery using the customer-managed key option and select the created key.

C. Import a key in Cloud KMS. Store all data in Cloud Storage using the customer-managed key option and select the created key. Set up a

Data ow pipeline to decrypt the data and to store it in a new BigQuery dataset.

D. Import a key in Cloud KMS. Create a dataset in BigQuery using the customer-supplied key option and select the created key.

Correct Answer: D

Community vote distribution

D (71%)

C (24%)

6%

? ?  SweetieS  Highly Voted ?  1áyear, 10ámonths ago

D is OK

upvoted 16 times

? ?  SR23222 1áweek, 6ádays ago

But CSEK is not supported in BigQuery

upvoted 1 times

? ?  alexandercamachop  Highly Voted ?  9ámonths, 2áweeks ago

Selected Answer: D

The answer is easy. It says keys must be left outside of Google Cloud.
This automatically eliminates A / B.
Now the C option says decrypts before storing it in BigQuery which the point is to encrypt the data while been in BigQuery, D is the only
possible answer.
upvoted 12 times

? ?  jits1984  Most Recent ?  2ámonths, 1áweek ago

Selected Answer: C

C - as BigQuery doesn't support Customer Supplier Keys.

upvoted 1 times

? ?  n_nana 3ámonths, 2áweeks ago

Selected Answer: C

BigQuery doesn't support CSEK

upvoted 1 times

? ?  nandoD 1ámonth, 4áweeks ago

If you want to control encryption yourself, you can use customer-managed encryption keys (CMEK) for BigQuery.
https://cloud.google.com/bigquery/docs/customer-managed-encryption

upvoted 1 times

? ?  SR23222 1áweek, 6ádays ago

There is a difference between customer managed and customer supplied. Link that you have shared talks about customer managed
and not customer supplied

upvoted 1 times

? ?  medi01 2ámonths, 1áweek ago
BG DOES support CSEK.

upvoted 1 times

? ?  n_nana 3ámonths, 2áweeks ago

Sorry even C is not correct, why to store the data in bq without encryption.
data should be passed encrypted from storage to bq.
then Answer is B
upvoted 1 times

? ?  AugustoKras011111 3ámonths, 3áweeks ago

Selected Answer: D

Key work: "keys outside of Google Cloud" so you have to import the key. between C and D I go with D.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

393/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  smachnio 5ámonths, 2áweeks ago

Selected Answer: D

D is correct. I had this question on the exam toaday and I go with D.
Explanation is - Generate the key outside the GCP so C and D are correct.
"Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset" is not correct becuase it means that data exist
on GCP what is not correct. Only D is correct.

upvoted 1 times

? ?  examch 5ámonths, 3áweeks ago

Selected Answer: C

Yes, BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK). Answer must be either A or C, since the say
generate key outside Google Cloud, import the key, hence I go for the answer C.
https://cloud.google.com/bigquery/docs/customer-managed-encryption#before_you_begin

https://cloud.google.com/kms/docs/importing-a-key

upvoted 2 times

? ?  NodummyIQ 5ámonths, 4áweeks ago

Answer D is incorrect because BigQuery does not support the use of customer-supplied keys to encrypt data at rest. Instead, you can use
customer-managed encryption keys in Cloud KMS to encrypt the data in BigQuery. To do this, you can either generate a new key in Cloud
KMS (answer A) or import an existing key (answer C). Once you have a key in Cloud KMS, you can create a BigQuery dataset and select the
key as the customer-managed key for the dataset. This will enable BigQuery to use the key to encrypt the data in the dataset.

upvoted 3 times

? ?  examch 5ámonths, 3áweeks ago

Yes, BigQuery and BigLake tables don't support Customer-Supplied Encryption Keys (CSEK). Answer must be either A or C, since the say
generate key outside Google Cloud, import the key, hence I go for the answer C.

upvoted 1 times

? ?  examch 5ámonths, 3áweeks ago

https://cloud.google.com/bigquery/docs/customer-managed-encryption#before_you_begin

upvoted 1 times

? ?  examch 5ámonths, 3áweeks ago

https://cloud.google.com/kms/docs/importing-a-key

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

Correct answer is D

upvoted 1 times

? ?  ale_brd_ 6ámonths, 3áweeks ago

Selected Answer: D

answer is D
https://cloud.google.com/bigquery/docs/customer-managed-encryption

upvoted 1 times

? ?  tomahawk003 6ámonths, 4áweeks ago

Answer D.
Questions says "...design data warehouse..." - would prefer BigQuery

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

D seems right to me

upvoted 1 times

? ?  midgoo 10ámonths ago

Selected Answer: D

For those that saying BigQuery does not support CSEK, read the below. You will need to import you CSEK and it will become CMEK. From
there you can use it for BigQuery
https://cloud.google.com/bigquery/docs/customer-managed-encryption

upvoted 4 times

? ?  kiappy81 10ámonths, 1áweek ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

394/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

I spent last 2 hours verifying this info on documentation and this is my result: CSEK works only with GCE e GCS, whereas Bq and Dataflow
are fine with CMEK.
CMEK generates a key inside google, in KMS, whereas CSEK generates the key on prem and then you import in KMS.

upvoted 2 times

? ?  melono 8ámonths, 2áweeks ago

Cloud Dataflow do not currently support objects encrypted with customer-supplied encryption keys.
https://cloud.google.com/storage/docs/encryption/customer-supplied-keys

upvoted 1 times

? ?  melono 8ámonths, 2áweeks ago

Id go for D
https://www.trendmicro.com/cloudoneconformity/knowledge-base/gcp/BigQuery/dataset-encryption-cmek.html#

upvoted 1 times

? ?  6721sora 10ámonths ago

The question clearly says that the keys should be generated outside of Google cloud
Answer is D

upvoted 2 times

? ?  bossdellacert 10ámonths, 1áweek ago

Selected Answer: D

That's my idea:
Scenario 1. there are Google generated and managed encryption key, which are the default with BigQuery and used to encrypt the data at
rest (no action/management needed by the customer)
Scenario 2. Then there are CMEK (customer-managed encryption keys): customer generate encryption keys and then manage them within
GCP with Cloud Key Management Service (Cloud KMS), here keys are provided by the customer, managed with Cloud KMS and then
provided to BigQuery to encrypt their data at rest
Scenario 3. At the end there are CSEK: customer generate and manage keys outside GCP and have to provide those keys every time is
needed through API call to the service. BigQuery seems do not support this.

I think here the correct answer could be D, because we can be in scenario 2 which is supported by BigQuery.

upvoted 2 times

? ?  bchmni 10ámonths, 2áweeks ago

Selected Answer: C

Because BigQuery doesn't support CSEK. As of today it can be used in GCS and GCE only.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

395/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #149

Topic 1

Your organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to rotate the encryption

key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow Google-recommended practices for

security. What should you do?

A. Create a key with Cloud Key Management Service (KMS). Encrypt the data using the encrypt method of Cloud KMS.

B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.

C. Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket.

D. Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature.

Correct Answer: D

Community vote distribution

B (81%)

Other

? ?  victory108  Highly Voted ?  1áyear, 9ámonths ago

B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.

upvoted 30 times

? ?  SweetieS  Highly Voted ?  1áyear, 10ámonths ago

B is OK
https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys#add-object-key

upvoted 9 times

? ?  BiddlyBdoyng  Most Recent ?  2áweeks, 4ádays ago

It says customer wants to manage the rotation not the supplying of key. Hence B not D. Seen some people say with customer managed
you cannot rotate but this document suggests you can https://cloud.google.com/storage/docs/encryption/customer-managed-keys#key-
rotation.

upvoted 1 times

? ?  jlambdan 2ámonths, 3áweeks ago

B does not allow to rotate assymetric key.
https://cloud.google.com/kms/docs/key-rotation
=> Cloud Key Management Service does not support automatic rotation of asymmetric keys. See Considerations for asymmetric keys
below.

I go for D.

upvoted 1 times

? ?  medi01 2ámonths, 1áweek ago
GC uses symmetric key.

upvoted 1 times

? ?  JC0926 3ámonths, 1áweek ago

Selected Answer: B

B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.

To rotate the encryption key used to encrypt data in a Cloud Storage bucket, it is recommended to use Cloud KMS. You can create a new
key version, set it as the primary version, and update the bucket's default KMS key to the new key version. This allows you to rotate the
encryption key while still allowing access to the data. You can then process the data in Dataproc while the encryption key is being rotated.
This approach provides security and compliance with regulations, as well as easy key rotation without disrupting access to data.

upvoted 1 times

? ?  JC0926 3ámonths, 1áweek ago

Selected Answer: B

Your organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to rotate the
encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow Google-recommended
practices for security. What should you do?
A. Create a key with Cloud Key Management Service (KMS). Encrypt the data using the encrypt method of Cloud KMS.
B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.
C. Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket.
D. Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature.

upvoted 1 times

? ?  examch 5ámonths, 3áweeks ago

Selected Answer: B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

396/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B is the correct answer, we can encrypt the data in the bucket using CMEK. And the key can be rotated as per requirement.
https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys#add-object-key

https://cloud.google.com/storage/docs/samples/storage-rotate-encryption-key#storage_rotate_encryption_key-python

upvoted 1 times

? ?  nhorcajada 7ámonths, 1áweek ago

Selected Answer: C

B is ok

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  KongsMom 8ámonths ago

B. rotation and dataproc ... trendmicro talk about this in https://www.trendmicro.com/cloudoneconformity/knowledge-
base/gcp/Dataproc/enable-encryption-with-cmks-for-dataproc-clusters.html

Ensure that your Google Cloud Dataproc clusters on Compute Engine are encrypted with Customer-Managed Keys (CMKs) in order to
control the cluster data encryption/decryption process. You can create and manage your own Customer-Managed Keys (CMKs) with Cloud
Key Management Service (Cloud KMS). Cloud KMS provides secure and efficient encryption key management, controlled key rotation, and
revocation mechanisms.
This rule resolution is part of the Conformity Security & Compliance tool for GCP.

upvoted 1 times

? ?  RitwickKumar 10ámonths, 1áweek ago

Selected Answer: B

As per question: " your company must be able to rotate the encryption key"
It is easily possible with KMS: https://cloud.google.com/kms/docs/rotating-keys#kms-create-key-rotation-schedule-gcloud

upvoted 3 times

? ?  Ric350 10ámonths, 4áweeks ago

"Your company must be able to rotate the encryption key" is the requirement which eliminates CMEK and why you need a CSEK. You have
to use a boto config file to do this and is part of one of the labs.

upvoted 2 times

? ?  AzureDP900 12ámonths ago

Answer B

upvoted 1 times

? ?  MarcExams 1áyear ago

Selected Answer: B

For security reasons you want to create the key in GCP [KMS] then set the encryption at the bucket as the data is inside the bucket.

upvoted 1 times

? ?  MarcExams 1áyear ago

Selected Answer: D

For security reasons you want to create the key in GCP [KMS] then set the encryption at the bucket as the data is inside the bucket.

upvoted 1 times

? ?  MarcExams 1áyear ago

SORRY MEANT TO SAY ANSWER IS B.

upvoted 1 times

? ?  nkit 1áyear, 2ámonths ago

Selected Answer: B

"Set the encryption key on the bucket to the Cloud KMS key"

upvoted 1 times

? ?  azureaspirant 1áyear, 4ámonths ago

2/15/21

upvoted 3 times

? ?  cloudmon 1áyear, 2ámonths ago

must be 22!

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

397/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #150

Topic 1

Your team needs to create a Google Kubernetes Engine (GKE) cluster to host a newly built application that requires access to third-party services

on the internet.

Your company does not allow any Compute Engine instance to have a public IP address on Google Cloud. You need to create a deployment

strategy that adheres to these guidelines. What should you do?

A. Con gure the GKE cluster as a private cluster, and con gure Cloud NAT Gateway for the cluster subnet.

B. Con gure the GKE cluster as a private cluster. Con gure Private Google Access on the Virtual Private Cloud (VPC).

C. Con gure the GKE cluster as a route-based cluster. Con gure Private Google Access on the Virtual Private Cloud (VPC).

D. Create a Compute Engine instance, and install a NAT Proxy on the instance. Con gure all workloads on GKE to pass through this proxy to

access third-party services on the Internet.

Correct Answer: B

Reference:

https://cloud.google.com/architecture/prep-kubernetes-engine-for-prod

Community vote distribution

A (98%)

? ?  ACE_ASPIRE  Highly Voted ?  1áyear, 9ámonths ago

Cloud NAT is the correct answer

upvoted 28 times

? ?  RitwickKumar  Highly Voted ?  10ámonths, 1áweek ago

Selected Answer: A

** Admins: More than 60% of the answers you have selected are wrong. Please correct them ASAP. I must appreciate community here for
taking out time to share their perspective and help fellow learners.

"B" can never be an answer here as the Private Google Access enables internal access to Google APIs only whereas in question the ask is
"access to third-party services on the internet"

upvoted 19 times

? ?  jlambdan 2ámonths, 3áweeks ago

This is most likely on purpose. Otherwise google will do something in order for the exam dump to be shutdown.

upvoted 3 times

? ?  GoReplyGCPExam  Most Recent ?  1áday, 19áhours ago

Selected Answer: A

Cloud NAT A

upvoted 1 times

? ?  DS2023 1ámonth ago

Selected Answer: A

Cloud NAT allows the resources in private subnet to access the internetùfor updates, patching, config management, and moreùin a
controlled and efficient manner.

upvoted 1 times

? ?  LaxmanTiwari 3áweeks, 6ádays ago

Yeah agree as GKE admin

upvoted 1 times

? ?  DS2023 1ámonth ago

Selected Answer: A. Cloud NAT allows the resources in private subnet to access the internetùfor updates, patching, config management,
and moreùin a controlled and efficient manner.

upvoted 1 times

? ?  dbsmk 2ámonths, 3áweeks ago

A.

https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#workloads_on_private_clusters_unable_to_access_internet

upvoted 2 times

? ?  JC0926 3ámonths, 1áweek ago

Selected Answer: B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

398/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Private Google Access allows resources in a VPC network to access Google Cloud services without an external IP address. By configuring
the GKE cluster as a private cluster, the nodes and services inside the cluster will not have a public IP address, and only resources within
the VPC network will be able to communicate with them. With Private Google Access enabled, the GKE cluster can access third-party
services on the internet via Google APIs and services without requiring a public IP address.

Therefore, the correct option is:

B. Configure the GKE cluster as a private cluster. Configure Private Google Access on the Virtual Private Cloud (VPC).

upvoted 1 times

? ?  r1ck 4ámonths, 1áweek ago
answer should be "B"
https://cloud.google.com/vpc/docs/private-access-options

upvoted 2 times

? ?  examch 5ámonths, 3áweeks ago

Selected Answer: A

A is the correct answer,

Granting private nodes outbound internet access
To provide outbound internet access for your private nodes, such as to pull images from an external registry, use Cloud NAT to create and
configure a Cloud Router. Cloud NAT lets private clusters establish outbound connections over the internet to send and receive packets.

The Cloud Router allows all your nodes in the region to use Cloud NAT for all primary and alias IP ranges. It also automatically allocates
the external IP addresses for the NAT gateway.

For instructions to create and configure a Cloud Router, refer to Create a Cloud NAT configuration using Cloud Router in the Cloud NAT
documentation.

https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#private-nodes-outbound

upvoted 3 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: A

A is the correct answer

upvoted 2 times

? ?  KongsMom 8ámonths ago

B is right,,, ,
https://cloud.google.com/vpc/docs/configure-private-google-access

By default, when a Compute Engine VM lacks an external IP address assigned to its network interface, it can only send packets to other
internal IP address destinations. You can allow these VMs to connect to the set of external IP addresses used by Google APIs and services
by enabling Private Google Access on the subnet used by the VM's network interface.
Private Google Access also allows access to the external IP addresses used by App Engine, including third-party App Engine-based
services.

upvoted 3 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is the correct answer as per this https://cloud.google.com/nat/docs/overview

upvoted 1 times

? ?  rorz 9ámonths, 3áweeks ago

Selected Answer: A

Cloud NAT is the correct

upvoted 3 times

? ?  cbarg 10ámonths, 2áweeks ago

Selected Answer: A

Cloud NAT to be able to reach internet services

upvoted 2 times

? ?  binpan 11ámonths, 3áweeks ago
sorry Answer is B. without NAT

upvoted 1 times

? ?  AMohanty 10ámonths, 4áweeks ago

How do u access internet from a Private cluster w/o NAT ?

upvoted 2 times

? ?  binpan 11ámonths, 3áweeks ago

Answer is A
private GKE cluster does not have any external IP by default and when you enable Private Google Access the using the --enable-master-
authorized-networks you can specify the external IPs to which it can connect. You do not need NAT in this case.
reference - https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

399/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  luamail 8ámonths, 3áweeks ago

Only A is valid por Google Ip, for other Ip user Cloud Nat
https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept

upvoted 2 times

? ?  binpan 11ámonths, 3áweeks ago

sorry answer is B. no NAT required.

upvoted 1 times

? ?  Matalf 11ámonths, 1áweek ago

the answer is A:

not B for this reason:
Private Google Access:
Google Cloud resources without external IP addresses.
Use this option to connect to Google APIs and services without giving your Google Cloud resources external IP addresses.
from https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters

upvoted 1 times

? ?  ryzior 1áyear ago

Selected Answer: A

https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept
"If you want to provide outbound internet access for certain private nodes, you can use Cloud NAT or manage your own NAT gateway."
since using managed services is the recommended way I'd go with A (Cloud NAT GW)

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

400/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #151

Topic 1

Your company has a support ticketing solution that uses App Engine Standard. The project that contains the App Engine application already has a

Virtual Private

Cloud (VPC) network fully connected to the company's on-premises environment through a Cloud VPN tunnel. You want to enable the App Engine

application to communicate with a database that is running in the company's on-premises environment. What should you do?

A. Con gure private Google access for on-premises hosts only.

B. Con gure private Google access.

C. Con gure private services access.

D. Con gure serverless VPC access.

Correct Answer: B

Community vote distribution

D (89%)

11%

? ?  Roncy  Highly Voted ?  1áyear, 9ámonths ago

D is right , refer to https://cloud.google.com/vpc/docs/serverless-vpc-access#use_cases

upvoted 37 times

? ?  cloudguy2 1áyear, 6ámonths ago

D) is correct. Use case example: Your serverless environment needs to access data from your on-premises database through Cloud
VPN.

upvoted 6 times

? ?  Besss  Highly Voted ?  1áyear, 9ámonths ago

D. Configuring serverless VPC access App Engine can connect to the VPC and then through VPN tunnel to the on-prem DB

upvoted 13 times

? ?  natpilot  Most Recent ?  2ámonths, 1áweek ago

D is Right . You can use a Serverless VPC Access connector to let Cloud Run, App Engine standard, and Cloud Functions environments send
packets to the internal IPv4 addresses of resources in a VPC network. Serverless VPC Access also supports sending packets to other
networks connected to the selected VPC network.

upvoted 1 times

? ?  JC0926 3ámonths, 1áweek ago

Selected Answer: D

Private Google Access (option B) is used to enable VM instances in a VPC network to reach Google APIs and services using an internal IP
address, but it does not allow communication to on-premises resources.

Private Services Access (option C) allows you to access supported Google Cloud services through private IP addresses rather than public IP
addresses, but it does not help in communicating with on-premises resources.

Configuring Private Google Access for on-premises hosts only (option A) is not a valid option as this configuration is not available.

upvoted 3 times

? ?  Mohtasham9 4ámonths, 1áweek ago

C. Configure private services access.
To enable an App Engine application to communicate with a database running in the company's on-premises environment over a VPC
network that is fully connected to the company's on-premises environment through a Cloud VPN tunnel, the recommended approach is to
use Private Service Access (PSA). Therefore, the correct answer is C. Configure private services access.

Private Service Access (PSA) allows you to create private connections between your VPC network and services like Cloud SQL, Cloud
Storage, and other Google APIs and services. With PSA, you can access these services using their private IP addresses, which are only
accessible from within your VPC network, and not over the public internet. This provides better security and reduces the risk of data
exfiltration or unauthorized access.

upvoted 1 times

? ?  SLChief 4ámonths, 2áweeks ago

D is right. Configuring serverless VPC access is the option for app engine to have Google private access

upvoted 1 times

? ?  [Removed] 4ámonths, 3áweeks ago

Selected Answer: D

You can use a Serverless VPC Access connector to let Cloud Run, App Engine standard, and Cloud Functions environments send packets to
the internal IPv4 addresses of resources in a VPC network. Serverless VPC Access also supports sending packets to other networks
connected to the selected VPC network.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

401/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

https://cloud.google.com/vpc/docs/private-access-options

upvoted 1 times

? ?  jay9114 4ámonths, 3áweeks ago

Upvote if there was no mention of "serverless VPC access" in the training videos and study guides you used to prepare for this exam.

upvoted 6 times

? ?  GopeshSahu 5ámonths ago

Selected Answer: B

I am surprised 95% selected option D without understanding the use case. Very basic ask

AppEngine ->Private Google Access->On-Prem DB
Google Private Access to so enable any Services no matter running in VPC to connect to on-prem DB via VPN tunnel.
https://cloud.google.com/vpc/docs/private-google-access-hybrid
https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid

AppEngine -> Serveless-VPV-Access -> Any GCP Resources/Services(with private IPs)

upvoted 3 times

? ?  gcppandit 4ámonths, 4áweeks ago

Private Google Access provides access to Google Services via Private IP and this can be used to call the App Engine from On-Prem. Here
the usecase is exactly the opposite. Here only option to set up the Serverless VPC access to allow Serverless components to access
Private resources (including on-Prem if proper VPN is already setup)

upvoted 2 times

? ?  jake_edman 5ámonths ago

I still think it is D - the example you linked to for Private Google Access is to allow on-prem resources to contact Google Services, not
the other way round.
https://cloud.google.com/vpc/docs/private-google-access-hybrid

But the example others link to explicitly says a use case is "Your serverless environment needs to access data from your on-premises
database through Cloud VPN
https://cloud.google.com/vpc/docs/serverless-vpc-access#use_cases

upvoted 2 times

? ?  examch 5ámonths, 3áweeks ago

Selected Answer: D

D is the correct answer,

Serverless VPC Access

bookmark_border
Serverless VPC Access makes it possible for you to connect directly to your Virtual Private Cloud network from serverless environments
such as Cloud Run, App Engine, or Cloud Functions. Configuring Serverless VPC Access allows your serverless environment to send
requests to your VPC network using internal DNS and internal IP addresses (as defined by RFC 1918 and RFC 6598). The responses to these
requests also use your internal network.

There are two main benefits to using Serverless VPC Access:

Requests sent to your VPC network are never exposed to the internet.
Communication through Serverless VPC Access can have less latency compared to the internet.

https://cloud.google.com/vpc/docs/serverless-vpc-access#use_case

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  goofball 6ámonths, 2áweeks ago

Ans = D because App Engine doesn't fall under the approved services for B
.
https://cloud.google.com/vpc/docs/private-services-access

upvoted 1 times

? ?  TonytheTiger 6ámonths, 3áweeks ago

Ans B : How can some many people be WRONG
https://cloud.google.com/vpc/docs/private-access-options
Private Google Access for on-premises hosts - Connect to Google APIs and services, from your on-premises network, through a Cloud VPN
tunnel or Cloud Interconnect by using one of the Private Google Access-specific domains and VIPs.

upvoted 3 times

? ?  Medofree 5ámonths, 3áweeks ago

Sorry but you are Wrong. You need that AppEngine access a database located on-promisses, so AppEngine needs to access the VPC
which is already connected to the on-premisses, to do so AppEngine uses VPC Serverless Access

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

402/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

D is the correct answer https://cloud.google.com/vpc/docs/serverless-vpc-access#use_cases

upvoted 1 times

? ?  melono 8ámonths, 2áweeks ago

What about A?
https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: D

D is the answer.

https://cloud.google.com/vpc/docs/serverless-vpc-access
Serverless VPC Access makes it possible for you to connect directly to your Virtual Private Cloud network from serverless environments
such as Cloud Run, App Engine, or Cloud Functions.

upvoted 4 times

? ?  zellck 9ámonths, 2áweeks ago

https://cloud.google.com/vpc/docs/serverless-vpc-access#use_cases
You can use Serverless VPC Access to access Compute Engine VM instances, Memorystore instances, and any other resources with
internal DNS or internal IP address. Some examples are:
- Your serverless environment needs to access data from your on-premises database through Cloud VPN.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

403/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #152

Topic 1

Your company is planning to upload several important  les to Cloud Storage. After the upload is completed, they want to verify that the uploaded

content is identical to what they have on-premises. You want to minimize the cost and effort of performing this check. What should you do?

A. 1. Use Linux shasum to compute a digest of  les you want to upload. 2. Use gsutil -m to upload all the  les to Cloud Storage. 3. Use gsutil

cp to download the uploaded  les. 4. Use Linux shasum to compute a digest of the downloaded  les. 5. Compare the hashes.

B. 1. Use gsutil -m to upload the  les to Cloud Storage. 2. Develop a custom Java application that computes CRC32C hashes. 3. Use gsutil ls -

L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded  les. 4. Compare the hashes.

C. 1. Use gsutil -m to upload all the  les to Cloud Storage. 2. Use gsutil cp to download the uploaded  les. 3. Use Linux diff to compare the

content of the  les.

D. 1. Use gsutil -m to upload the  les to Cloud Storage. 2. Use gsutil hash -c FILE_NAME to generate CRC32C hashes of all on-premises  les.

3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded  les. 4. Compare the hashes.

Correct Answer: C

Community vote distribution

D (95%)

5%

? ?  vladik820  Highly Voted ?  1áyear, 10ámonths ago

D is ok .
https://cloud.google.com/storage/docs/gsutil/commands/hash

upvoted 34 times

? ?  Bahubali1988  Highly Voted ?  9ámonths, 3áweeks ago

Seems most of the questions are having wrong answers.. If there is no discussion , its highly difficult to get the right answers.

upvoted 13 times

? ?  BiddlyBdoyng  Most Recent ?  2áweeks, 4ádays ago

Downloading before hashing cannot be right. The upload might be fine but if the download could corrupt

upvoted 1 times

? ?  MelaBro 1ámonth, 1áweek ago

The correct answer should be D: https://cloud.google.com/storage/docs/gsutil/commands/hash

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  Nuwan_SriLanka 8ámonths, 1áweek ago

Selected Answer: D

Calculate hashes on local files, which can be used to compare with gsutil ls -L output. If a specific hash option is not provided, this
command calculates all gsutil-supported hashes for the files.

Note that gsutil automatically performs hash validation when uploading or downloading files, so this command is only needed if you want
to write a script that separately checks the hash.

If you calculate a CRC32c hash for files without a precompiled crcmod installation, hashing will be very slow. See gsutil help crcmod for
details.
https://cloud.google.com/storage/docs/gsutil/commands/hash

upvoted 4 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

D is the right answer per this doc https://cloud.google.com/storage/docs/gsutil/commands/hash

upvoted 1 times

? ?  Jay_Krish 9ámonths, 3áweeks ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

404/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

All those who answered D.. can one of you if you're genuine tell how is this even possible - The second step in the option D?
2. Use gsutil hash -c FILE_NAME to generate CRC32C hashes of all on-premises files.

upvoted 2 times

? ?  Jay_Krish 9ámonths, 3áweeks ago

Reading again it's probably not C because it talks about Linux commands but what if the environment is Windows..
but I still have my doubts on D if someone could clarify?

upvoted 2 times

? ?  binpan 11ámonths, 3áweeks ago

Correct Answer C
A- digest comparison does not gurantee file contents are same. moreover lot of extra steps. - not correct
B - custom Java code - lot of effort - not correct
D - gs util cannot be used for creating hash for on prem files stored on on prem filestore/database. Not correct
C - not the best option but right answer for the options available.

upvoted 2 times

? ?  luamail 8ámonths, 3áweeks ago

dowload file has cost, C no is a option

upvoted 1 times

? ?  SIMMEAT 10ámonths, 3áweeks ago

there is a hash options in gsutil for local files.
https://cloud.google.com/storage/docs/gsutil/commands/hash

upvoted 1 times

? ?  kaito789 11ámonths ago

D is correct. you only need gs util to generate hash for cloud storage. you would use your own utility to create ash for on prem and
then compare the two.

upvoted 1 times

? ?  AzureDP900 12ámonths ago

D is correct , there is no need to build custom java script.

upvoted 1 times

? ?  Superr 1áyear, 1ámonth ago

Selected Answer: D

D seems valid
upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: D

I am eliminating tedious approaches that is downloading and doing custom coding so A B C are eliminated.
D is the solution.
upvoted 1 times

? ?  cmamiusa 1áyear, 2ámonths ago

Selected Answer: D

D makes sense
upvoted 2 times

? ?  brvinod 1áyear, 2ámonths ago

Selected Answer: D

https://cloud.google.com/storage/docs/gsutil/commands/hash.
Also, it is easy to eliminate wrong answers. A&C talks about re-downloading the files backl to on-premise to compare. Makes no sense.
Between B&D, B talks about creating custom Java program. Usually, out of box is preferred. So, D.

upvoted 5 times

? ?  [Removed] 1áyear, 4ámonths ago

Selected Answer: D

I got similar question on my exam. Answered D.

upvoted 4 times

? ?  haroldbenites 1áyear, 4ámonths ago

Go for D

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

405/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #153

Topic 1

You have deployed an application on Anthos clusters (formerly Anthos GKE). According to the SRE practices at your company, you need to be

alerted if request latency is above a certain threshold for a speci ed amount of time. What should you do?

A. Install Anthos Service Mesh on your cluster. Use the Google Cloud Console to de ne a Service Level Objective (SLO), and create an alerting

policy based on this SLO.

B. Enable the Cloud Trace API on your project, and use Cloud Monitoring Alerts to send an alert based on the Cloud Trace metrics.

C. Use Cloud Pro ler to follow up the request latency. Create a custom metric in Cloud Monitoring based on the results of Cloud Pro ler, and

create an Alerting policy in case this metric exceeds the threshold.

D. Con gure Anthos Con g Management on your cluster, and create a yaml  le that de nes the SLO and alerting policy you want to deploy in

your cluster.

Correct Answer: A

Reference:

https://cloud.google.com/anthos/docs/tutorials/manage-slos

Community vote distribution

A (100%)

? ?  vladik820  Highly Voted ?  1áyear, 10ámonths ago

A is ok.
https://cloud.google.com/service-mesh/docs/observability/slo-overview

upvoted 20 times

? ?  examch  Most Recent ?  5ámonths, 3áweeks ago

Selected Answer: A

Cloud Monitoring can trigger an alert when a Service is on track to violate an SLO. You can create an alerting policy based on the rate of
consumption of your error budget. All alerts on error budgets have the same basic condition: a specified percentage of the error budget
for the compliance period is consumed in a lookback period, which is a time period, such as the previous 60 minutes. When you create the
alerting policy, Anthos Service Mesh automatically sets most of the conditions for the alert based on the settings in the SLO. You specify
the lookback period and the consumption percentage.

https://cloud.google.com/service-mesh/docs/observability/alert-policy-slo

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 2 times

? ?  Jay_Krish 9ámonths, 3áweeks ago

Selected Answer: A

A seems correct
upvoted 2 times

? ?  RitwickKumar 10ámonths, 1áweek ago

Selected Answer: A

https://cloud.google.com/service-mesh/docs/observability/alert-policy-slo

upvoted 2 times

? ?  igor_nov1 10ámonths, 3áweeks ago

Use the Google Cloud Console to define a Service Level Objective (SLO)
WAAAAT ?
How Console help you to define SLO?

upvoted 1 times

? ?  AMohanty 10ámonths, 4áweeks ago

Specific Purpose of Cloud Trace API is to get info regarding Latency.
Would go with B.
upvoted 1 times

? ?  AzureDP900 12ámonths ago

A is right

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

406/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  sivre 1áyear, 2ámonths ago

Why not B....
Cloud Trace is a distributed tracing system that collects latency data from the applications and displays it in near real-time. It allows you to
follow a sample request through your distributed system, observe the network calls and profile your system end to end.
Note that Cloud Trace is disabled by default.
The Anthos Service Mesh pages provide a link to the traces in the Cloud Trace page in the Cloud Console.
https://cloud.google.com/service-mesh/docs/observability/accessing-traces
In Anthos clusters you need to install Anthos service mesh? From this link you need to install it only on GKE and on-premises platforms
https://cloud.google.com/service-mesh/docs/observability/accessing-traces

upvoted 2 times

? ?  kimharsh 1áyear ago

Can you create an Alert when you use Cloud Trace?

upvoted 2 times

? ?  Shawnn 3ámonths, 2áweeks ago

yep, you can

upvoted 1 times

? ?  ryzior 1áyear ago

I think A is about monitoring and alerting without any further investigation, while Trace is for finding the root cause/detective
purposes, when you look into a call and track this call step by step through each endpoint, the call is going through.

upvoted 1 times

? ?  [Removed] 1áyear, 4ámonths ago

I got same question on my exam.

upvoted 2 times

? ?  haroldbenites 1áyear, 4ámonths ago

Go for A

upvoted 1 times

? ?  technodev 1áyear, 5ámonths ago

Got this question in my exam, answered A

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 2 times

? ?  nqthien041292 1áyear, 7ámonths ago

Selected Answer: A

Vote A

upvoted 1 times

? ?  zt00 1áyear, 9ámonths ago

A, https://cloud.google.com/service-mesh/docs/observability/alert-policy-slo

upvoted 1 times

? ?  diaga2 1áyear, 9ámonths ago

Yes, A is fine!

upvoted 1 times

? ?  victory108 1áyear, 10ámonths ago

A. Install Anthos Service Mesh on your cluster. Use the Google Cloud Console to define a Service Level Objective (SLO), and create an
alerting policy based on this SLO.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

407/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #154

Topic 1

Your company has a stateless web API that performs scienti c calculations. The web API runs on a single Google Kubernetes Engine (GKE)

cluster. The cluster is currently deployed in us-central1. Your company has expanded to offer your API to customers in Asia. You want to reduce

the latency for users in Asia.

What should you do?

A. Create a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the public IPs to the

Cloud DNS zone.

B. Use a global HTTP(s) load balancer with Cloud CDN enabled.

C. Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer.

D. Increase the memory and CPU allocated to the application in the cluster.

Correct Answer: B

Community vote distribution

C (76%)

A (18%)

6%

? ?  vladik820  Highly Voted ?  1áyear, 10ámonths ago

C is ok .
https://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci

upvoted 31 times

? ?  rishab86 1áyear, 8ámonths ago

After going through the link I feel its C

upvoted 3 times

? ?  mikesp 1áyear, 8ámonths ago

Mee too.
CDN does not make sense

upvoted 2 times

? ?  Lk9876  Highly Voted ?  1áyear, 9ámonths ago

I'm not sure about C. kubemci is deprecated and is not part anymore of cloud sdk in favor of ingress for anthos. I'll go with A

upvoted 9 times

? ?  Rzla 1áyear, 9ámonths ago

Problem with A is that a service load bancer is not l7 https. The question is outdated, the answer will have been C. Now it would be
Anthos multi cluster ingress -https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress

upvoted 9 times

? ?  cotam 1áyear, 8ámonths ago

That's actually not true. Service of type: LoadBalancer, is a service from "K8s" point of view, which creates L7 HTTP(S) Load Balancer.

upvoted 3 times

? ?  MikeB19 1áyear, 9ámonths ago

I think either a or c is correct. I chose c base on the article ref in the chat. Do u have supporting article ref kubemci is deprecated? I also
found some chatter about kubemci being deprecated but couldnÆt find anything offical

upvoted 1 times

? ?  Linus11 1áyear, 8ámonths ago

It is hee -- https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress

upvoted 1 times

? ?  BiddlyBdoyng  Most Recent ?  2áweeks, 4ádays ago

The problem with B is the question very much infers we are dealing with dynamic content & not static.

upvoted 1 times

? ?  BiddlyBdoyng 2áweeks, 4ádays ago

It's A or C but I think A might be better.
A is a simpler solution. Cloud DNS allows you to add multiple targets and part of its decision making is the latency.
https://cloud.google.com/dns/docs/zones/zones-overview
Tough but I think A because it's only one API being exposed. Ingress comes into its own when exposing many services.

upvoted 1 times

? ?  r1ck 4ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

408/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

kubemci - deprecated
https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress

upvoted 2 times

? ?  omermahgoub 6ámonths ago

A good option for reducing latency for users in Asia accessing the web API would be to create a second GKE cluster in asia-southeast1 and
use kubemci to create a global HTTP(s) load balancer.

Option C, "Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer," would be the correct
choice for this scenario.

By creating a second GKE cluster in asia-southeast1, you can reduce latency for users in Asia by serving the API from a closer location. You
can then use kubemci, a command-line tool that simplifies the process of creating a global HTTP(s) load balancer, to expose the APIs from
both clusters through a single global IP address. This allows users to access the API with low latency, regardless of their location.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

Option A, "Create a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the public
IPs to the Cloud DNS zone," would not be a good choice because it would not provide a single global IP address for users to access the
API, which would increase latency and complexity.

Option B, "Use a global HTTP(s) load balancer with Cloud CDN enabled," would not be a good choice because it would not allow you to
serve the API from a closer location for users in Asia.

Option D, "Increase the memory and CPU allocated to the application in the cluster," would not be a good choice because it would not
address the issue of latency for users in Asia accessing the API.

upvoted 2 times

? ?  ale_brd_ 6ámonths, 3áweeks ago

Selected Answer: C

Answer is C but kubemci is deprecated, now you have to go with:
Multi Cluster Ingress is a cloud-hosted controller for Google Kubernetes Engine (GKE) clusters. It's a Google-hosted service that supports
deploying shared load balancing resources across clusters and across regions. To deploy Multi Cluster Ingress across multiple clusters,
complete Setting up Multi Cluster Ingress then see Deploying Ingress across multiple clusters.
https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress

upvoted 4 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is correct, however, this question is an old question and need to be updated to use the ingress for global HTTPS LB

upvoted 2 times

? ?  melono 8ámonths, 1áweek ago

I understand that if a GKE cluster houses an API in US, Another cluster will house another API in Asia. So 2 APIs (load balanced or
whatever) right?
upvoted 1 times

? ?  AMEJack 8ámonths, 3áweeks ago

Kubemci is now deprecated for ingress for Anthos
https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress

upvoted 1 times

? ?  Rolazo 8ámonths, 2áweeks ago

We all know that Kubenci is deprecated. But it wasnt when this question was created. The truly concern is what happened when a
question is outdated? Will it change its response?

upvoted 1 times

? ?  melono 8ámonths, 1áweek ago

It will be changed or not be in the exam bro, that is what will happen

upvoted 1 times

? ?  muneebarshad 9ámonths, 2áweeks ago

Selected Answer: A

This is an old question and answers are not updated .... Given that Google's best practice is to use "Multi-Cluster Ingress". I would go with
Option A . kubemci is not recommended for multi-cluster solution .

upvoted 3 times

? ?  shekarcfc 9ámonths, 3áweeks ago

Selected Answer: B

B, two reasons: 1. Kubemci is deprecated. 2. Considering the cost of having additional GKE.
GKE is costlier than CDN.

upvoted 1 times

? ?  desertlotus1211 10ámonths, 3áweeks ago

https://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

409/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Answer C

upvoted 1 times

? ?  abdelilahfa 11ámonths ago

Selected Answer: C

C is the correct answer based on
https://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci

upvoted 1 times

? ?  AzureDP900 12ámonths ago

C is fine after reading the Google docs!

upvoted 1 times

? ?  xfall12 1áyear, 1ámonth ago

ANSWER : A kubemci is deprecated

upvoted 2 times

? ?  mark_af 1áyear, 2ámonths ago

Selected Answer: C

C is ok.
Cluster is regional and you need a Global LB to balance

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

410/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #155

Topic 1

You are migrating third-party applications from optimized on-premises virtual machines to Google Cloud. You are unsure about the optimum CPU

and memory options. The applications have a consistent usage pattern across multiple weeks. You want to optimize resource usage for the

lowest cost. What should you do?

A. Create an instance template with the smallest available machine type, and use an image of the third-party application taken from a current

on-premises virtual machine. Create a managed instance group that uses average CPU utilization to autoscale the number of instances in the

group. Modify the average CPU utilization threshold to optimize the number of instances running.

B. Create an App Engine  exible environment, and deploy the third-party application using a Docker le and a custom runtime. Set CPU and

memory options similar to your application's current on-premises virtual machine in the app.yaml  le.

C. Create multiple Compute Engine instances with varying CPU and memory options. Install the Cloud Monitoring agent, and deploy the third-

party application on each of them. Run a load test with high tra c levels on the application, and use the results to determine the optimal

settings.

D. Create a Compute Engine instance with CPU and memory options similar to your application's current on-premises virtual machine. Install

the Cloud Monitoring agent, and deploy the third-party application. Run a load test with normal tra c levels on the application, and follow the

Rightsizing Recommendations in the Cloud Console.

Correct Answer: A

Reference:

https://avinetworks.com/docs/18.2/server-autoscaling-in-gcp/

Community vote distribution

D (69%)

A (21%)

10%

? ?  pr2web  Highly Voted ?  1áyear, 9ámonths ago

Answer is D.

https://cloud.google.com/migrate/compute-engine/docs/4.9/concepts/planning-a-migration/cloud-instance-rightsizing?hl=en

"Rightsizing provides two types of recommendations:

1. Performance-based recommendations: Recommends Compute Engine instances based on the CPU and RAM currently allocated to the
on-premises VM. This recommendation is the default.

2. Cost-based recommendations: Recommends Compute Engine instances based on:
- The current CPU and RAM configuration of the on-premises VM.
- The average usage of this VM during a given period. To use this option, you must activate rightsizing monitoring with vSphere for this
group of VMs and allow time for Migrate for Compute Engine to analyze usage.

upvoted 48 times

? ?  melono 8ámonths, 1áweek ago

The point:
2. Cost-based recommendations: Recommends Compute Engine instances based on:
The current CPU and RAM configuration of the on-premises VM.

upvoted 1 times

? ?  cloudmon  Highly Voted ?  1áyear, 2ámonths ago

Selected Answer: D

It's definitely D. See the reference at the following link that says "The recommendation algorithm is suited to workloads that follow weekly
patterns", which matches the part of the questions that says "consistent usage pattern over multiple weeks":
https://cloud.google.com/compute/docs/instances/apply-machine-type-recommendations-for-instances

Option A also has two problems;
1. It only focuses on CPU, but the question says "CPU and memory"
2. The question does not mention anything about horizontal scalability

upvoted 7 times

? ?  cloudmon 1áyear, 2ámonths ago

Another (less obvious) reason for choosing D: I've noticed a pattern in these exams that the cloud provider wants to advertise and
promote anything that they consider to be a cool feature of their platform. In this case, they are promoting their recommendation
engine. If there's even an option that sounds like it's advertising a relevant managed service from the cloud provider, then that's usually
one to consider.
upvoted 2 times

? ?  cloudmon 1áyear, 2ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

411/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

I also find the following wording in option A to be a bit iffy: "an image of the third-party application taken from a current on-
premises virtual machine". That seems a bit vague in terms of what the image format would be.

upvoted 1 times

? ?  WinSxS  Most Recent ?  3ámonths, 2áweeks ago

Selected Answer: D

Option D would be the best option to optimize resource usage for the lowest cost when migrating third-party applications from optimized
on-premises virtual machines to Google Cloud.

upvoted 2 times

? ?  AugustoKras011111 3ámonths, 3áweeks ago

Selected Answer: D

Answer is D. Similar than third-party convince me...

upvoted 1 times

? ?  beehive 5ámonths, 3áweeks ago

why most of the answers selected by host is INCORRECT? Is it intentional to misguide the folks?

upvoted 5 times

? ?  thamaster 6ámonths ago

Selected Answer: D

i choose D as it's best practice create an instance with similar configuration as on premise and check metrics

upvoted 1 times

? ?  shefalia 6ámonths, 1áweek ago

Selected Answer: D

D is the right one because of Rightsizing option from GCP

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

I agree with D is the most accurate

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

D is correct

upvoted 1 times

? ?  SerGCP 9ámonths ago

Selected Answer: D

A, application may not support horizontal scaling and may not run in instances whith small cpu
B, dockerize third-party applications is not a requirement....Complex and costly
C, too expensive
D, simple and works

upvoted 2 times

? ?  shekarcfc 9ámonths, 3áweeks ago

Selected Answer: A

A, the benefit of moving to cloud is scaling based on load, start with min infra and scale-up based on usage.

upvoted 3 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: D

A - you cannot expect application to behavior similar in 2 different envior met without a test.
B - App Engine is costly
C- Varing cpu and memory cannot be doone.
D- correa.

upvoted 3 times

? ?  Meyucho 1áyear, 3ámonths ago

Selected Answer: A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

412/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

The D is not correct. The MIG image was made before the third party software was installed so any new VM on the group will start without
this software. Answer is A

upvoted 4 times

? ?  haroldbenites 1áyear, 4ámonths ago

Go for D

upvoted 1 times

? ?  anjuagrawal 1áyear, 4ámonths ago

Selected Answer: A

The options can be A , B or D. B is not true because is says CPU and Memory options are not clear. Hence, configuration of app.ymp in
AppEngine Flexible will not be viable. Option D is not correct because it says create a virtual machine whereas there are many VMs and
also scaling would not be possible. So, A is the correct answer.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

413/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #156

Topic 1

Your company has a Google Cloud project that uses BigQuery for data warehousing. They have a VPN tunnel between the on-premises

environment and Google

Cloud that is con gured with Cloud VPN. The security team wants to avoid data ex ltration by malicious insiders, compromised code, and

accidental oversharing.

What should they do?

A. Con gure Private Google Access for on-premises only.

B. Perform the following tasks: 1. Create a service account. 2. Give the BigQuery JobUser role and Storage Reader role to the service account.

3. Remove all other IAM access from the project.

C. Con gure VPC Service Controls and con gure Private Google Access.

D. Con gure Private Google Access.

Correct Answer: A

Reference:

https://cloud.google.com/vpc-service-controls/docs/overview

Community vote distribution

C (100%)

? ?  Craigenator  Highly Voted ?  1áyear, 7ámonths ago

Without the discussion this site would be useless, many thanks to all that participate. Majority of answers are wrong...

upvoted 53 times

? ?  VarunGo 3áweeks, 5ádays ago
you can used chatGPT now

upvoted 2 times

? ?  diaga2  Highly Voted ?  1áyear, 9ámonths ago

C is the recommended one https://cloud.google.com/vpc-service-controls/docs/overview

upvoted 31 times

? ?  Mrinalini19  Most Recent ?  4ámonths ago

Selected Answer: C

C is correct

upvoted 1 times

? ?  examch 5ámonths, 3áweeks ago

Selected Answer: C

C is the correct answer,

To secure data from exfiltration by malicious insiders, compromised code or accidental oversharing, we use VPC Service controls

https://cloud.google.com/vpc-service-controls/docs/overview

For private access options, connect to services in VPC networks we use private service endpoints or VPC network peering.

https://cloud.google.com/vpc/docs/private-access-options#connect-services

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 2 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is the right answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

414/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  AzureDP900 8ámonths, 2áweeks ago

I will go with C
upvoted 1 times

? ?  nkit 1áyear, 2ámonths ago

Selected Answer: C

Going by definition- VPC Service Controls improves your ability to mitigate the risk of data exfiltration from Google Cloud services such as
Cloud Storage and BigQuery.

hence C is correct

upvoted 6 times

? ?  dangcpped 1áyear, 2ámonths ago

Selected Answer: C

C is the recommended
https://cloud.google.com/vpc-service-controls/docs/overview

upvoted 2 times

? ?  kimharsh 1áyear, 4ámonths ago

I don't get it , C is correct because of the "VPC service Control", But Privet Google access is not for on On-premises, A is for On-premises =
https://cloud.google.com/vpc/docs/private-access-options

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: C

I agree C.
The link that wroted in Reveral Solution means C.

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer
https://cloud.google.com/vpc-service-controls/docs/overview

upvoted 2 times

? ?  sapsant 1áyear, 6ámonths ago

Selected Answer: C

https://cloud.google.com/vpc-service-controls/docs/overview

upvoted 2 times

? ?  pakilodi 1áyear, 7ámonths ago

Selected Answer: C

Vote C

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 2 times

? ?  dmc123 1áyear, 7ámonths ago

VPC Service Control and Private Google Access

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

415/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #157

Topic 1

You are working at an institution that processes medical data. You are migrating several workloads onto Google Cloud. Company policies require

all workloads to run on physically separated hardware, and workloads from different clients must also be separated. You created a sole-tenant

node group and added a node for each client. You need to deploy the workloads on these dedicated hosts. What should you do?

A. Add the node group name as a network tag when creating Compute Engine instances in order to host each workload on the correct node

group.

B. Add the node name as a network tag when creating Compute Engine instances in order to host each workload on the correct node.

C. Use node a nity labels based on the node group name when creating Compute Engine instances in order to host each workload on the

correct node group.

D. Use node a nity labels based on the node name when creating Compute Engine instances in order to host each workload on the correct

node.

Correct Answer: C

Reference:

https://cloud.google.com/compute/docs/nodes/provisioning-sole-tenant-vms

Community vote distribution

D (100%)

? ?  pr2web  Highly Voted ?  1áyear, 9ámonths ago

Answer is D.

Y'all not reading the fine details. The question is about aligning EACH client to their dedicated nodes (D), not to a node group (C).

https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#default_affinity_labels

The above reference clearly articulates the default affinity label for node group and node name. Unless we're thinking about growing each
client to their own dedicated node groups (not in the current requirement), then the answer is not C, rather D.

Compute Engine assigns two default affinity labels to each node:

A label for the node group name:
Key: compute.googleapis.com/node-group-name
Value: Name of the node group.
A label for the node name:
Key: compute.googleapis.com/node-name
Value: Name of the individual node.

upvoted 52 times

? ?  Binoz  Highly Voted ?  1áyear, 10ámonths ago

D. Afinity should be set at node level, not node-group as every client has its own node in the group

upvoted 17 times

? ?  MikeB19 1áyear, 9ámonths ago
ThatÆs what i thought too

upvoted 7 times

? ?  Andras2k  Most Recent ?  4ámonths, 4áweeks ago

I had this question recently (end of jan 2023) and went with answer D. After doing some investigation, that seems to be the right answer
to me.

upvoted 2 times

? ?  LaxmanTiwari 1ámonth ago

Preparing for exam and gone through the concept make sense the answer is D

upvoted 1 times

? ?  beehive 5ámonths, 3áweeks ago

Answer is D.
Ref: you can't specify node affinity labels on a node group.>> https://cloud.google.com/compute/docs/nodes/sole-tenant-
nodes#node_templates

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

416/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

D is the correct answer, VMs must be associated to a specific node within the node-group, so you must use the node name label to
provision the VM.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

D is right, Node is right choice instead of node group

upvoted 1 times

? ?  deenee 11ámonths, 2áweeks ago

D : https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes
Node affinity labels are key-value pairs assigned to nodes, and are inherited from a node template. Affinity labels let you:

Control how individual VM instances are assigned to nodes.
Control how VM instances created from a template, such as those created by a managed instance group, are assigned to nodes.
Group sensitive VM instances on specific nodes or node groups, separate from other VMs.

upvoted 4 times

? ?  slars2k 1áyear, 3ámonths ago

I go with C as I believe single-tenant node group meant for only one client

upvoted 2 times

? ?  Skr6266 1áyear, 3ámonths ago

Answer is D since it is clearly documented as
When you create a VM, you request sole-tenancy by specifying node affinity or anti-affinity, referencing one or more node affinity labels.
You specify custom node affinity labels when you create a node template, and Compute Engine automatically includes some default
affinity labels on each node. By specifying affinity when you create a VM, you can schedule VMs together on a specific node or nodes in a
node group. By specifying anti-affinity when you create a VM, you can ensure that certain VMs are not scheduled together on the same
node or nodes in a node group.

Node affinity labels are key-value pairs assigned to nodes, and are inherited from a node template. Affinity labels let you:

Control how individual VM instances are assigned to nodes.
Control how VM instances created from a template, such as those created by a managed instance group, are assigned to nodes.
Group sensitive VM instances on specific nodes or node groups, separate from other VMs.

upvoted 1 times

? ?  azureaspirant 1áyear, 4ámonths ago

2/15/21

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

They must 'Use node affinity labels based on the node name'.
Because 'added a node for each client'.
So I chose D.
Ty guys.

upvoted 1 times

? ?  AmitMittal 1áyear, 6ámonths ago

D is abs right
upvoted 1 times

? ?  SamGCP 1áyear, 6ámonths ago

Selected Answer: D

https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes#default_affinity_labels

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: D

D is the correct answer.

upvoted 1 times

? ?  [Removed] 1áyear, 6ámonths ago

Selected Answer: D

D is right.
There is very thin difference node group vs node label. Marked C is wrong.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

417/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

418/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #158

Topic 1

Your company's test suite is a custom C++ application that runs tests throughout each day on Linux virtual machines. The full test suite takes

several hours to complete, running on a limited number of on-premises servers reserved for testing. Your company wants to move the testing

infrastructure to the cloud, to reduce the amount of time it takes to fully test a change to the system, while changing the tests as little as possible.

Which cloud infrastructure should you recommend?

A. Google Compute Engine unmanaged instance groups and Network Load Balancer

B. Google Compute Engine managed instance groups with auto-scaling

C. Google Cloud Dataproc to run Apache Hadoop jobs to process each test

D. Google App Engine with Google StackDriver for logging

Correct Answer: B

Google Compute Engine enables users to launch virtual machines (VMs) on demand. VMs can be launched from the standard images or

custom images created by users.

Managed instance groups offer autoscaling capabilities that allow you to automatically add or remove instances from a managed instance

group based on increases or decreases in load. Autoscaling helps your applications gracefully handle increases in tra c and reduces cost

when the need for resources is lower.

Incorrect Answers:

B: There is no mention of incoming IP data tra c for the custom C++ applications.

C: Apache Hadoop is not  t for testing C++ applications. Apache Hadoop is an open-source software framework used for distributed storage

and processing of datasets of big data using the MapReduce programming model.

D: Google App Engine is intended to be used for web applications.

Google App Engine (often referred to as GAE or simply App Engine) is a web framework and cloud computing platform for developing and

hosting web applications in Google-managed data centers.

Reference:

https://cloud.google.com/compute/docs/autoscaler/

Community vote distribution

B (100%)

? ?  AWS56  Highly Voted ?  3áyears, 7ámonths ago

B, https://cloud.google.com/compute/docs/autoscaler/

upvoted 24 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 7 times

? ?  RVivek  Most Recent ?  4ámonths, 3áweeks ago

Selected Answer: B

Changing the tests as little as possible rules out C & D.
Test takes several hours and you need to improve perfromace. Autocaling with MIG will do it
Unmanaged group cannot autosacle. Load balancer will not improve perfromance

upvoted 1 times

? ?  SerGCP 8ámonths ago

Why not A? the custom APP may be not supporto autoscaling....

upvoted 2 times

? ?  RVivek 4ámonths, 3áweeks ago

Changing the tests as little as possible rules out C & D.
Test takes several hours and you need to improve perfromace. Autocaling with MIG will do it
Unmanaged group cannot autosacle. Load balancer will not improve perfromance

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

B is the right answer

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is right

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

419/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  Pime13 1áyear, 5ámonths ago

Selected Answer: B

choose b

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: B

B is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 3 times

? ?  Cloudguy123 1áyear, 10ámonths ago

New Question
Your company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the application is
simple, it was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered that the application cannot
process the messages in real time. Most of them wait for minutes before being processed. You need to scale the elaboration process that
is 1/0-intensive. What should you do?

A. Usekubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50 to
configure Kubernetes autoscaling deployment.
B. Configure a Kubemetes autoscaling deployment based on the
subscription/push_request_latencies metric.
C. Use the --enable-autoscaling flag when you create the Kubernetes cluster.
D. Configure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric.

upvoted 2 times

? ?  PeppaPig 1áyear, 10ámonths ago

D is the answer
upvoted 4 times

? ?  PleeO 1áyear, 9ámonths ago
D is the correct answer
https://cloud.google.com/kubernetes-engine/docs/tutorials/autoscaling-metrics#pubsub

upvoted 2 times

? ?  Cloudguy123 1áyear, 10ámonths ago

New Question
Your organization has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to rotate the
encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow Google-recommended
practices for security What should you do?

A. Create a key with Cloud Key Management Service (KMS) Encrypt the data using the encrypt method of Cloud KMS.
B. Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.
C. Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket.
D. Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature.

Answer Please
upvoted 2 times

? ?  fahad01hbti 1áyear, 10ámonths ago

it is B
https://cloud.google.com/storage/docs/encryption/using-customer-managed-keys

upvoted 2 times

? ?  rottzy 1áyear, 8ámonths ago

why are you repeating questions from this series as comments?

upvoted 2 times

? ?  GCP_New 1áyear, 11ámonths ago

Are we getting questions from 1-100 in the exam?

upvoted 2 times

? ?  bishalsainju 1áyear, 7ámonths ago

Yeah I have the same question in mind.

upvoted 2 times

? ?  AnilKr 1áyear, 11ámonths ago

Google Compute Engine and with MIG for auto-scaling

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

420/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  bala786 1áyear, 11ámonths ago

Agree with Option B.Google Compute Managed instance groups with auto-scaling

upvoted 2 times

? ?  [Removed] 2áyears ago

agree with B

upvoted 3 times

? ?  victory108 2áyears, 1ámonth ago

B. Google Compute Engine managed instance groups with auto-scaling

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

B is correct

upvoted 2 times

? ?  sidbet 2áyears, 2ámonths ago

B should be the one

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

421/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #159

Topic 1

A lead software engineer tells you that his new application design uses websockets and HTTP sessions that are not distributed across the web

servers. You want to help him ensure his application will run properly on Google Cloud Platform.

What should you do?

A. Help the engineer to convert his websocket code to use HTTP streaming

B. Review the encryption requirements for websocket connections with the security team

C. Meet with the cloud operations team and the engineer to discuss load balancer options

D. Help the engineer redesign the application to use a distributed user session service that does not rely on websockets and HTTP sessions.

Correct Answer: C

Google Cloud Platform (GCP) HTTP(S) load balancing provides global load balancing for HTTP(S) requests destined for your instances.

The HTTP(S) load balancer has native support for the WebSocket protocol.

Incorrect Answers:

A: HTTP server push, also known as HTTP streaming, is a client-server communication pattern that sends information from an HTTP server to a

client asynchronously, without a client request. A server push architecture is especially effective for highly interactive web or mobile

applications, where one or more clients need to receive continuous information from the server.

Reference:

https://cloud.google.com/compute/docs/load-balancing/http/

Community vote distribution

C (100%)

? ?  AWS56  Highly Voted ?  3áyears, 7ámonths ago

I agree with C

upvoted 15 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 8 times

? ?  [Removed] 1áyear, 8ámonths ago

The key line from the link above:

Session affinity for WebSockets works the same as for any other request. For information, see Session affinity.

upvoted 4 times

? ?  fraloca 2áyears, 5ámonths ago

https://cloud.google.com/load-balancing/docs/https#websocket_support

upvoted 5 times

? ?  lynx256  Highly Voted ?  2áyears, 3ámonths ago

IMO C is ok.
Beside the reasons mentioned above regarding why A, B and D are wrong, there are also:
A and D are wrong because are abot changing the app - whereas in the task "You want to help him ensure his application will run properly
on GCP" (not REDESIGN/CHANGE).
B is wrong because you don't have to "Review the encryption requirements for websocket connections with the security team"...

upvoted 5 times

? ?  ashrafh 7ámonths, 1áweek ago

thanks

upvoted 1 times

? ?  examch  Most Recent ?  5ámonths, 3áweeks ago

Selected Answer: C

C is the correct answer,
Google Cloud HTTP(S)-based load balancers have native support for the WebSocket protocol when you use HTTP or HTTPS as the protocol
to the backend. The load balancer does not need any configuration to proxy WebSocket connections.

https://cloud.google.com/load-balancing/docs/https#websocket_support

upvoted 2 times

? ?  NodummyIQ 5ámonths, 4áweeks ago

The answer is D. C. is not the best answer because it does not address the issue of websockets and HTTP sessions not being distributed
across the web servers. While load balancer options may be relevant to the overall operation of the application, they do not address the

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

422/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

specific issue of ensuring that the websockets and HTTP sessions are properly distributed. A better solution would be to help the engineer
redesign the application to use a distributed user session service that does not rely on websockets and HTTP sessions, as this would
address the issue of session distribution. Alternatively, the engineer could consider converting their websocket code to use HTTP
streaming, which could potentially help with session distribution.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is fine.

upvoted 1 times

? ?  ijazahmad722 10ámonths, 2áweeks ago

Selected Answer: C

I agree with C
upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 4 times

? ?  Cloudguy123 1áyear, 10ámonths ago

New Case Study Question- TerramEarth

For this question, refer to the TerramEarth case study.

You are building a microservice-based application for TerramEarth.
The application is based on Docker containers. You want to follow Google-recommended practices to build the application continuously
and store the build artifacts. What should you do?

upvoted 2 times

? ?  Cloudguy123 1áyear, 10ámonths ago

A) Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build container images for each microservice, and
tag them using the code commit hash. Push the images to the Container Registry.

B)Configure a trigger in Cloud Build for new source changes. The trigger invokes build jobs and build container images for the
microservices. Tag the images with a version number, and push them to Cloud Storage.

C) Create a Scheduler job to check the repo every minute. For any new change, invoke Cloud Build to build container images for the
microservices. Tag the images using the current timestamp, and push them to the Container Registry.

D) Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build one container image, and tag the image with
the label 'latest' Push the image to the Container Registry

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A looks good to me.

upvoted 2 times

? ?  nickojul 1áyear, 10ámonths ago

A is ok

upvoted 5 times

? ?  DreamerK 1áyear, 11ámonths ago

Why D is wrong is the wording "doesn't rely on". This means the application needs to use other protocols instead of http or websocket.
This is not realistic and requires too much application refactoring. Actually a distributed session service is possible with http or websocket
as long as the session information is stored in shared storage such as nosql database or redis that can be accessed by all web servers. In
this sense, D is wrong answer.

upvoted 1 times

? ?  AnilKr 1áyear, 11ámonths ago

C is fine. Global HTTP(S) load Balancer supports webSockets.

upvoted 1 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 3 times

? ?  giovy_82 1áyear, 11ámonths ago

I also agree with C. the answer D could be ok but the question says "his new application design " so it means that the app has just been
developed and deployed so there's no convenience to redesign it from scratch to avoid use of sessions and websocket.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

423/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  ashrafh 7ámonths, 1áweek ago

from where you got the "new" word? ha ha

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

C. Meet with the cloud operations team and the engineer to discuss load balancer options

upvoted 1 times

? ?  un 2áyears, 1ámonth ago

C is correct

upvoted 1 times

? ?  Ausias18 2áyears, 3ámonths ago

Answer is C

upvoted 1 times

? ?  Rathul 2áyears, 3ámonths ago

Agree with option C

upvoted 2 times

? ?  padma29 2áyears, 3ámonths ago

Hello Rathul,

Have you cleared the Cloud Architect Exam? Was this site useful??

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

424/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #160

Topic 1

The application reliability team at your company this added a debug feature to their backend service to send all server events to Google Cloud

Storage for eventual analysis. The event records are at least 50 KB and at most 15 MB and are expected to peak at 3,000 events per second. You

want to minimize data loss.

Which process should you implement?

A. ?Çó Append metadata to  le body ?Çó Compress individual  les ?Çó Name  les with serverName ?Ç" Timestamp ?Çó Create a new bucket if

bucket is older than 1 hour and save individual  les to the new bucket. Otherwise, save  les to existing bucket.

B. ?Çó Batch every 10,000 events with a single manifest  le for metadata ?Çó Compress event  les and manifest  le into a single archive  le

?Çó Name  les using serverName ?Ç" EventSequence ?Çó Create a new bucket if bucket is older than 1 day and save the single archive  le to

the new bucket. Otherwise, save the single archive  le to existing bucket.

C. ?Çó Compress individual  les ?Çó Name  les with serverName ?Ç" EventSequence ?Çó Save  les to one bucket ?Çó Set custom metadata

headers for each object after saving

D. ?Çó Append metadata to  le body ?Çó Compress individual  les ?Çó Name  les with a random pre x pattern ?Çó Save  les to one bucket

Correct Answer: D

Community vote distribution

D (89%)

11%

? ?  rishab86  Highly Voted ?  2áyears ago

answer is definitely D
https://cloud.google.com/storage/docs/request-rate#naming-convention
"A longer randomized prefix provides more effective auto-scaling when ramping to very high read and write rates. For example, a 1-
character prefix using a random hex value provides effective auto-scaling from the initial 5000/1000 reads/writes per second up to roughly
80000/16000 reads/writes per second, because the prefix has 16 potential values. If your use case does not need higher rates than this, a
1-character randomized prefix is just as effective at ramping up request rates as a 2-character or longer randomized prefix."
Example:
my-bucket/2fa764-2016-05-10-12-00-00/file1
my-bucket/5ca42c-2016-05-10-12-00-00/file2
my-bucket/6e9b84-2016-05-10-12-00-01/file3

upvoted 30 times

? ?  kopper2019  Highly Voted ?  1áyear, 12ámonths ago

- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview
QUESTION 6
For this question, refer to the Helicopter Racing League (HRL) case study. A recent finance audit of cloud infrastructure noted an
exceptionally high number of Compute Engine instances are allocated to do video encoding and transcoding. You suspect that these
Virtual Machines are zombie machines that were not deleted after their workloads completed. You need to quickly get a list of which VM
instances are idle. What should you do?
A. Log into each Compute Engine instance and collect disk, CPU, memory, and network usage statistics for analysis.
B. Use the gcloud compute instances list to list the virtual machine instances that have the idle: true label set.
C. Use the gcloud recommender command to list the idle virtual machine instances.
D. From the Google Console, identify which Compute Engine instances in the managed instance groups are no longer responding to
health check probes.

upvoted 5 times

? ?  cloudstd 1áyear, 12ámonths ago

answer: C

upvoted 8 times

? ?  KS1911 1áyear, 11ámonths ago

I have my exam scheduled after 3 days. Would there be more questions coming on ExamTopics?

upvoted 3 times

? ?  kravenn 1áyear, 10ámonths ago

answer C

upvoted 2 times

? ?  juccjucc 1áyear, 12ámonths ago

is it C?

upvoted 1 times

? ?  cloudstd 1áyear, 12ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

425/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

this is not 100% accurate. you should investigate if you doubt if is incorrect
https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations

upvoted 3 times

? ?  Papafel 1áyear, 11ámonths ago
The correct answer is A

upvoted 1 times

? ?  matmuh 1áyear, 7ámonths ago

Absulatly C

upvoted 1 times

? ?  ptsironis  Most Recent ?  1ámonth ago

Selected Answer: B

Why not option B??

upvoted 1 times

? ?  nunopires2001 5ámonths ago

I was thinking correct answer was A, because we should have some kind of bucket rotation in order to avoid hiting the max size of a
bucket.
However it seems there is no size limit for a GCP cloud bucket, so I will have to agree with community and stick to answer D.

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

D is the correct answer
https://cloud.google.com/storage/docs/request-rate#naming-convention

upvoted 1 times

? ?  Pime13 1áyear, 5ámonths ago

D: https://cloud.google.com/storage/docs/request-rate#naming-convention

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: D

D is the correct answer

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 5 times

? ?  amxexam 1áyear, 9ámonths ago

Request admin to intervene and delete the hijacking of the question by kopper2019

upvoted 4 times

? ?  Examster1 1áyear, 9ámonths ago

Use the material for study dude! Hello? Anyone home?

upvoted 5 times

? ?  Arad 1áyear, 7ámonths ago

it looks like this website does not have any admin

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview
QUESTION 5
For this question, refer to the Helicopter Racing League (HRL) case study. HRL is looking for a cost- effective approach for storing their race
data such as telemetry. They want to keep all historical records, train models using only the previous season's data, and plan for data
growth in terms of volume and information collected. You need to propose a data solution. Considering HRL business requirements and
the goals expressed by CEO S. Hawke, what should you do?
A. Use Firestore for its scalable and flexible document-based database. Use collections to aggregate race data by season and event.
B. Use Cloud Spanner for its scalability and ability to version schemas with zero downtime. Split race data using season as a primary key.
C. Use BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season.
D. Use Cloud SQL for its ability to automatically manage storage increases and compatibility with MySQL. Use separate database instances
for each season.
upvoted 3 times

? ?  cloudstd 1áyear, 12ámonths ago

answer: C

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

426/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Papafel 1áyear, 11ámonths ago

Yes answer is C
upvoted 2 times

? ?  juccjucc 1áyear, 12ámonths ago

is it C?
all these questions are from the new exam? why they are here in the comments and not as questions in the list?

upvoted 2 times

? ?  kopper2019 1áyear, 12ámonths ago

because exam was not updated so I added the Qs but they added this new Qs as normal now we have 218 Qs

upvoted 4 times

? ?  Roncy 1áyear, 9ámonths ago

Hey Kopper, when would you provide the new set of questions ?

upvoted 1 times

? ?  kravenn 1áyear, 10ámonths ago

answer: C

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview
QUESTION 4
For this question, refer to the Helicopter Racing League (HRL) case study. HRL wants better prediction accuracy from their ML prediction
models. They want you to use GoogleÆs AI Platform so HRL can understand and interpret the predictions. What should you do?

A. Use Explainable AI.
B. Use Vision AI.
C. Use Google CloudÆs operations suite.
D. Use Jupyter Notebooks.

upvoted 3 times

? ?  cloudstd 1áyear, 12ámonths ago

answer: A

upvoted 4 times

? ?  juccjucc 1áyear, 12ámonths ago

is it A?

upvoted 2 times

? ?  Papafel 1áyear, 11ámonths ago

Yes answer is A
upvoted 1 times

? ?  kravenn 1áyear, 10ámonths ago

answer A

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview
QUESTION 3
For this question, refer to the Helicopter Racing League (HRL) case study. The HRL development team releases a new version of their
predictive capability application every Tuesday evening at 3 a.m. UTC to a repository. The security team at HRL has developed an in-house
penetration test Cloud Function called Airwolf. The security team wants to run Airwolf against the predictive capability application as soon
as it is released every Tuesday. You need to set up Airwolf to run at the recurring weekly cadence. What should you do?
A. Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function.
B. Set up a Cloud Logging sink and a Cloud Storage bucket that triggers a Cloud Function.
C. Configure the deployment job to notify a Pub/Sub queue that triggers a Cloud Function.
D. Set up Identity and Access Management (IAM) and Confidential Computing to trigger a Cloud Function.

upvoted 2 times

? ?  Amrit123 1áyear, 8ámonths ago

C, is the right answer. The scheduler would run without a trigger even though the release has not been done. If you read (application
as soon as it is released ), the time is not certain. So, the answer is C. Check out the last 30 questions, would give a better idea as there
is a separate discussion

upvoted 2 times

? ?  esc 1áyear, 12ámonths ago

answer : A

upvoted 5 times

? ?  vchrist 1áyear, 7ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

427/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

why A? Does Cloud Storage make sense ?

upvoted 1 times

? ?  jask 1áyear, 9ámonths ago

in option A what is the use of Cloud storage bucket? In my opinion answer is C.

upvoted 3 times

? ?  Papafel 1áyear, 11ámonths ago

Answer is A

upvoted 2 times

? ?  cloudmon 1áyear, 2ámonths ago

I would go with C
https://cloud.google.com/source-repositories/docs/code-change-notification

upvoted 1 times

? ?  BiddlyBdoyng 2áweeks, 3ádays ago

It's probably C due to pub sub on Cloud Deploy rather than source repos
https://cloud.google.com/deploy/docs/subscribe-deploy-notifications

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview

QUESTION 2
For this question, refer to the Helicopter Racing League (HRL) case study. Recently HRL started a new regional racing league in Cape Town,
South Africa. In an effort to give customers in Cape Town a better user experience, HRL has partnered with the Content Delivery Network
provider, Fastly. HRL needs to allow traffic coming from all of the Fastly IP address ranges into their Virtual Private Cloud network (VPC
network). You are a member of the HRL security team and you need to configure the update that will allow only the Fastly IP address
ranges through the External HTTP(S) load balancer. Which command should you use?

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

A. gcloud compute security-policies rules update 1000 \
--security-policy from-fastly \
--src-ip-ranges * \
--action ôallowö
B. gcloud compute firewall rules update sourceiplist-fastly \
--priority 100 \
--allow tcp:443
C. gcloud compute firewall rules update hir-policy \
--priority 100 \
--target-tags=sourceiplist-fastly \
--allow tcp:443
D. gcloud compute security-policies rules update 1000 \
--security-policy hir-policy \
--expression ôevaluatePreconfiguredExpr(æsourceiplist-fastlyÆ)ö \
--action ôallowö
upvoted 1 times

? ?  GrandAM 3ámonths, 1áweek ago

The gcloud compute firewall rules update command can be used to update the firewall rules for a specific VPC network.

Option A, "gcloud compute security-policies rules update," is not applicable in this scenario as it's used to define and manage
security policies in a centralized manner.

Option B, "gcloud compute firewall rules update sourceiplist-fastly," allows traffic from all TCP ports and it does not restrict traffic to
specific IP address ranges.

Option D, "gcloud compute security-policies rules update," is also not applicable in this scenario as it's used to define and manage
security policies in a centralized manner.

Therefore, Option C, "gcloud compute firewall rules update hir-policy --priority 100 --target-tags=sourceiplist-fastly --allow tcp:443,"
would be the right command to allow only the Fastly IP address ranges through the External HTTP(S) load balancer.

upvoted 1 times

? ?  cloudstd 1áyear, 12ámonths ago

answer: D

upvoted 6 times

? ?  Papafel 1áyear, 11ámonths ago

Answer is A

upvoted 2 times

? ?  matmuh 1áyear, 7ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

428/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is incorrect : To match all IPs specify *
https://cloud.google.com/sdk/gcloud/reference/compute/security-policies/rules/update

upvoted 1 times

? ?  kravenn 1áyear, 10ámonths ago

answer D

upvoted 4 times

? ?  kopper2019 1áyear, 12ámonths ago

- New Q, 06/2021
Helicopter Racing League Testlet 1
Company overview

Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship
and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to
stream the races all over the world with live telemetry and predictions throughout each race.

Solution concept

HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race
predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their
content, both real-time and recorded, closer to their users.

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago
Existing technical environment

HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video
recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud.
Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are
hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:

- Existing content is stored in an object storage service on their existing public cloud provider.
Video encoding and transcoding is performed on VMs created for each job.
Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

Business requirements
HRLÆs owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their
requirements are:

Support ability to expose the predictive models to partners. Increase predictive capabilities during and before races:
? Race results
? Mechanical failures
? Crowd sentiment
Increase telemetry and create additional insights. Measure fan engagement with new predictions. Enhance global availability and
quality of the broadcasts. Increase the number of concurrent viewers.
Minimize operational complexity. Ensure compliance with regulations.
Create a merchandising revenue stream.

Technical requirements
Maintain or increase prediction throughput and accuracy. Reduce viewer latency.
Increase transcoding performance.
Create real-time analytics of viewer consumption patterns and engagement. Create a data mart to enable processing of large
volumes of race data.

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

Executive statement

Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want
enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to
predict race outcomes but lacks the facility to support real- time predictions during races and the capacity to process season-long
results.

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

QUESTION 1
For this question, refer to the Helicopter Racing League (HRL) case study. Your team is in charge of creating a payment card
data vault for card numbers used to bill tens of thousands of viewers, merchandise consumers, and season ticket holders. You
need to implement a custom card tokenization service that meets the following requirements:
ò It must provide low latency at minimal cost.
ò It must be able to identify duplicate credit cards and must not store plaintext card numbers.
ò It should support annual key rotation.

Which storage approach should you adopt for your tokenization service?

A. Store the card data in Secret Manager after running a query to identify duplicates.
B. Encrypt the card data with a deterministic algorithm stored in Firestore using Datastore mode.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

429/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Encrypt the card data with a deterministic algorithm and shard it across multiple Memorystore instances.
D. Use column-level encryption to store the data in Cloud SQL.

upvoted 2 times

? ?  SPNBLUE 1áyear, 11ámonths ago

Why D ?

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

- New Q, 06/2021 Mountkirk Games, C Testlet 1 Company overview
QUESTION 7
Your development team has created a mobile game app. You want to test the new mobile app on Android and iOS devices with a variety of
configurations. You need to ensure that testing is efficient and cost- effective. What should you do?

A. Upload your mobile app to the Firebase Test Lab, and test the mobile app on Android and iOS devices.
B. Create Android and iOS VMs on Google Cloud, install the mobile app on the VMs, and test the mobile app.
C. Create Android and iOS containers on Google Kubernetes Engine (GKE), install the mobile app on the containers, and test the mobile
app.
D. Upload your mobile app with different configurations to Firebase Hosting and test each configuration.

upvoted 1 times

? ?  cloudstd 1áyear, 12ámonths ago

answer: A

upvoted 5 times

? ?  abhtri 1áyear, 9ámonths ago

yes ans A

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

- New Q, 06/2021 Mountkirk Games, C Testlet 1 Company overview
QUESTION 6
Mountkirk Games wants you to secure the connectivity from the new gaming application platform to Google Cloud. You want to
streamline the process and follow Google-recommended practices. What should you do?

A. Configure Workload Identity and service accounts to be used by the application platform.
B. Use Kubernetes Secrets, which are obfuscated by default. Configure these Secrets to be used by the application platform.
C. Configure Kubernetes Secrets to store the secret, enable Application-Layer Secrets Encryption, and use Cloud Key Management Service
(Cloud KMS) to manage the encryption keys. Configure these Secrets to be used by the application platform.
D. Configure HashiCorp Vault on Compute Engine, and use customer managed encryption keys and Cloud Key Management Service
(Cloud KMS) to manage the encryption keys. Configure these Secrets to be used by the application platform.

upvoted 1 times

? ?  kravenn 1áyear, 10ámonths ago

answer A, https://cloud.google.com/blog/products/containers-kubernetes/introducing-workload-identity-better-authentication-for-
your-gke-applications

upvoted 2 times

? ?  hello_aws 1áyear, 11ámonths ago

https://www.examtopics.com/discussions/google/view/56645-exam-professional-cloud-architect-topic-5-question-6/

upvoted 3 times

? ?  cloudstd 1áyear, 12ámonths ago

answer: A

upvoted 2 times

? ?  kamilC 1áyear, 11ámonths ago

why not D?

upvoted 2 times

? ?  vimal1206 1áyear, 11ámonths ago

Yes, the answer is D

upvoted 1 times

? ?  siri7676 1áyear, 11ámonths ago

why D?

upvoted 1 times

? ?  rottzy 1áyear, 8ámonths ago

if workload on GKE - answer is A

upvoted 2 times

? ?  sai953 1áyear, 11ámonths ago

@kopper2019 have you mentioned answers for all of these questions anywhere? Can you please help with the correct answers?

upvoted 1 times

? ?  kopper2019 1áyear, 12ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

430/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

- New Q, 06/2021
Mountkirk Games, C Testlet 1
Company overview

QUESTION 5
Your development teams release new versions of games running on Google Kubernetes Engine (GKE) daily. You want to create service
level indicators (SLIs) to evaluate the quality of the new versions from the userÆs perspective. What should you do?

A. Create CPU Utilization and Request Latency as service level indicators.
B. Create GKE CPU Utilization and Memory Utilization as service level indicators.
C. Create Request Latency and Error Rate as service level indicators.
D. Create Server Uptime and Error Rate as service level indicators.

upvoted 2 times

? ?  vchrist 1áyear, 7ámonths ago

C - Mountkirk Games Study Case, need low latency

upvoted 1 times

? ?  kbouwmee 1áyear, 12ámonths ago

answer: C, because these impact the users perspective

upvoted 9 times

? ?  trismegistus 1áyear, 8ámonths ago

C and D seem equally viable to me. Server up time and request latency would both directly impact the user. I think I'll go with C
ultimately because GKE, being a cluster, should be able to survive the loss of any single server.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

431/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #161

Topic 1

A recent audit revealed that a new network was created in your GCP project. In this network, a GCE instance has an SSH port open to the world.

You want to discover this network's origin.

What should you do?

A. Search for Create VM entry in the Stackdriver alerting console

B. Navigate to the Activity page in the Home section. Set category to Data Access and search for Create VM entry

C. In the Logging section of the console, specify GCE Network as the logging section. Search for the Create Insert entry

D. Connect to the GCE instance using project SSH keys. Identify previous logins in system logs, and match these with the project owners list

Correct Answer: C

Incorrect Answers:

A: To use the Stackdriver alerting console we must  rst set up alerting policies.

B: Data access logs only contain read-only operations.

Audit logs help you determine who did what, where, and when.

Cloud Audit Logging returns two types of logs:
? Admin activity logs
? Data access logs: Contains log entries for operations that perform read-only operations do not modify any data, such as get, list, and
aggregated list methods.

Community vote distribution

C (100%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

When you search for Create Insert, it displays a JSON code string that contains the creators e-mail

upvoted 13 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 13 times

? ?  clouddude  Highly Voted ?  3áyears, 1ámonth ago

I am going to go with C. Answer A doesn't seem to fit because the matter of when a VM was created.
Answer B focuses on Data Access logs which doesn't seem to fit since the matter of creating a network firewall rule
is an Admin activity, not a data access activity.
D focuses on who logged in which is good to know but doesn't answer the question of how the network was created.
C focuses on logging, the selection of network events, and the Create/Insert entry.

upvoted 11 times

? ?  AugustoKras011111  Most Recent ?  4ámonths ago

Selected Answer: C

C is ok to me!
upvoted 1 times

? ?  NodummyIQ 5ámonths, 4áweeks ago

Option C is incorrect because the GCE Network logs are not the correct place to search for the creation of a VM instance. The correct place
to search for this information is the Activity page, as specified in option B.

upvoted 1 times

? ?  n_nana 5ámonths, 2áweeks ago

Question is asking about network origin creation not VM creation. that's why is C

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is the right answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

432/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  AzureDP900 8ámonths, 2áweeks ago

C is right

upvoted 1 times

? ?  cloudmon 1áyear, 2ámonths ago

Sorry to gripe again, but why on Earth would anybody need to remember this from the top of their mind. You will never be in a situation in
which you need to remember this without looking at the available options in the console (or simply Googling it, lol).

upvoted 6 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  Bobch 1áyear, 6ámonths ago

Selected Answer: C

Vote C

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 2 times

? ?  muneebarshad 1áyear, 10ámonths ago

In Logs Explorer , Filter "resource.type="gce_firewall_rule" and Query insert Create

You would see below and email address
"methodName": "v1.compute.firewalls.insert",
"authorizationInfo": [
{
"permission": "compute.firewalls.create",

upvoted 2 times

? ?  bala786 1áyear, 11ámonths ago

Option C is correct, because logging section is the correct choice to get this details

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

C - In the Logging section of the console, specify GCE Network as the logging section. Search for the Create Insert entry

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

C is correct

upvoted 2 times

? ?  Ausias18 2áyears, 3ámonths ago

Answer is C

upvoted 2 times

? ?  willan 2áyears, 5ámonths ago

Agree..C

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

433/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #162

Topic 1

You want to make a copy of a production Linux virtual machine in the US-Central region. You want to manage and replace the copy easily if there

are changes on the production virtual machine. You will deploy the copy as a new instance in a different project in the US-East region.

What steps must you take?

A. Use the Linux dd and netcat commands to copy and stream the root disk contents to a new virtual machine instance in the US-East region.

B. Create a snapshot of the root disk and select the snapshot as the root disk when you create a new virtual machine instance in the US-East

region.

C. Create an image  le from the root disk with Linux dd command, create a new virtual machine instance in the US-East region

D. Create a snapshot of the root disk, create an image  le in Google Cloud Storage from the snapshot, and create a new virtual machine

instance in the US-East region using the image  le the root disk.

Correct Answer: D

Community vote distribution

D (62%)

B (38%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

D is correct. A and B are talking about appending the file system to a new VM, not setting it at the root in a new VM set. Option C is not
offered within the GCP because the image must be on the GCP platform to run the gcloud of Google Console instructions to create a VM
with the image.
upvoted 27 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 10 times

? ?  Sudipta  Highly Voted ?  3áyears, 5ámonths ago

Why Not B.
https://cloud.google.com/compute/docs/instances/create-start-instance#createsnapshot
This clearly tells we can use snapshot to create a VM instance, and only need a custom image if we need to create many instances. Here
we are creating only one.

upvoted 13 times

? ?  Jack_in_Large 2áyears, 10ámonths ago

You can't use the snapshot created by another project

upvoted 10 times

? ?  noussy 2áyears, 9ámonths ago

According to the documentation we can now https://cloud.google.com/compute/docs/disks/create-snapshots

upvoted 7 times

? ?  ArthurL20 2áyears, 1ámonth ago

Only if its in the same zone: https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots
"Note: The disk must be in the same zone as the instance."

But this is not the case here, we have:
Different zones and different project hence, you must use a bucket.

upvoted 13 times

? ?  JasminL 2áyears, 6ámonths ago

I think the question has 2 different answers now as Google improve the snapshot function.
Quoted from the link:
'You can create snapshots from disks even while they are attached to running instances. Snapshots are global resources, so you
can use them to restore data to a new disk or instance within the same project. You can also share snapshots across projects.'

upvoted 9 times

? ?  VSMu 4ámonths, 3áweeks ago

B would have been the answer in the current context. But as I read carefully, it doesnt mention the step of sharing snapshot
across projects. It directly expects to use the snapshot. Hence D may be the right answer!

upvoted 2 times

? ?  PST21  Most Recent ?  3ámonths, 3áweeks ago

B is incorrect as it doesnt create an image uses the snapshot and hence D is the only corret option

upvoted 2 times

? ?  AugustoKras011111 4ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

434/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: D

D seems better, but B actually works too.

upvoted 1 times

? ?  romandrigo 4ámonths ago

Selected Answer: D

https://cloud.google.com/compute/docs/instances/copy-vm-between-projects#zonal-boot-disk

upvoted 1 times

? ?  Clauther 5ámonths ago

Selected Answer: B

B is the right one as of 01/2023

upvoted 1 times

? ?  n_nana 5ámonths, 2áweeks ago

Selected Answer: B

Currently , It is possible to create VM from snapshot within same project, different project or even different organisation. so answer B is
more straight forward.

upvoted 2 times

? ?  n_nana 5ámonths, 2áweeks ago

https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots
https://cloud.google.com/compute/docs/disks/create-snapshots#sharing_snapshots_across_orgs

upvoted 2 times

? ?  examch 5ámonths, 3áweeks ago

Selected Answer: B

B is the correct answer,

We can create VM from snapshot across zones and regions, please read through the link,

https://cloud.google.com/compute/docs/instances/moving-instance-across-zones#moving-an-instance-manually

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  arpitshah20 7ámonths, 2áweeks ago

Selected Answer: D

D is correct

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

D is the right answer, B would be a good answer if mentioned to share the snapshot with other project

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I will go with D
upvoted 1 times

? ?  riyer1 9ámonths, 1áweek ago

The generic flow goes like this:
snapshot --> Image --> Instance Template --> MIG

The instance is created after the image without an instance template in this question. That will also work.

upvoted 1 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: B

B must be the correct one

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

435/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

vote D

upvoted 2 times

? ?  Omni_Omnom 1áyear, 7ámonths ago

D is the correct answer. Key word - different project in the same region. https://cloud.google.com/compute/docs/instances/copy-vm-
between-projects

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

436/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #163

Topic 1

Your company runs several databases on a single MySQL instance. They need to take backups of a speci c database at regular intervals. The

backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance.

How should you con gure the storage?

A. Con gure a cron job to use the gcloud tool to take regular backups using persistent disk snapshots.

B. Mount a Local SSD volume as the backup location. After the backup is complete, use gsutil to move the backup to Google Cloud Storage.

C. Use gcs se to mount a Google Cloud Storage bucket as a volume directly on the instance and write backups to the mounted location using

mysqldump.

D. Mount additional persistent disk volumes onto each virtual machine (VM) instance in a RAID10 array and use LVM to create snapshots to

send to Cloud Storage

Correct Answer: B

Community vote distribution

B (77%)

C (23%)

? ?  hannibal1969  Highly Voted ?  3áyears, 7ámonths ago

I think it's B. If you use a tool like GCFUSE it will write immediatly to GCS which is a cost benefit because you don't need intermediate
storage. In this case however "Quickly as possible" key for understanding. GCFUSE will write to GCS which is much slower than writing
directly to an added SSD. During the write to GCS it would also execute reads for a longer period on the production database. Therefor
writing to the extra SSD would be my recommended solution. Offloading from the SSD to GCS would not impact the running database
because the data is already separated.

upvoted 47 times

? ?  raf2121 1áyear, 10ámonths ago

Point for Discussion
Can local SSD be mounted in a running instance.

upvoted 2 times

? ?  pr2web 1áyear, 9ámonths ago

In addition the mysqldump command can be run with local ssd as destination, and NOT impacting existing DB performance by
using the --databases dbname --single-transaction flag.

That said, this boils down to which is quicker to complete, a local SSD dump and gsutil to gs://bucket
OR
gcsfuse the bucket directly on the VM, and mysqldump with the --single-transaction command

upvoted 2 times

? ?  blitzzzz 11ámonths ago

who what to backup to a local SSD. If your instance is down, you lost all data.

upvoted 4 times

? ?  RVivek 4ámonths, 3áweeks ago

Locall SSD is used for backup to improve performnace. Later the backup will be moved to Cloud storage using gsutil

upvoted 1 times

? ?  SerGCP 9ámonths ago

The local SSD can be created only during the VM creation process.
After than you can mount disk for in the destination path for export mysqldump. gsutil is the supported tool that you may used to
migrate the dump to bucket.

upvoted 1 times

? ?  JasonL_GCP 1áyear, 8ámonths ago

Good point, Because Local SSDs are located on the physical machine where your virtual machine instance is running, they can be
created only during the instance creation process

upvoted 1 times

? ?  heelhook_ambassador 1áyear, 7ámonths ago

Thanks!

upvoted 1 times

? ?  kvenkatasudhakar 1áyear, 7ámonths ago

We cannot attach and mount a local SSD to a running instance. I think it's C (GCFUSE)

upvoted 3 times

? ?  Rathish  Highly Voted ?  3áyears, 4ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

437/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Ans: B
Persistent Disk snapshot not required: "They need to take backups of a specific database at regular intervals."

"The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance."

This can be achieved by using both Local SSD & GCS Fuse (mounting GCS as directory), but as the question stats needs to complete as
quickly as possible.

General Rule: Any addition of components introduce a latency. I could not get write throughput of GCS & Local SSD, even if we consider
both provides same throughput, streaming data through network to GCS Bucket introduce latency. Attached Local SSD has advantage in
this case, since there is no network involved.

From Local SSD to GCS bucket - copy job does not impact the mysql data disk.

upvoted 15 times

? ?  JC0926  Most Recent ?  3ámonths, 1áweek ago

Selected Answer: B

Option B would be the best choice for this scenario. Mounting a Local SSD volume as the backup location would ensure high performance
and minimal impact on disk performance, while also allowing for quick backups. After the backup is complete, using gsutil to move the
backup to Google Cloud Storage would provide a reliable and secure storage location for the backups. This approach is also cost-effective,
as Local SSD volumes are less expensive than persistent disks.

upvoted 1 times

? ?  NodummyIQ 5ámonths, 4áweeks ago

B is incorrect. The Local SSD volumes are only available on certain instance types, and they are not suitable for long-term storage as they
are ephemeral and are deleted when the instance is deleted or stopped. For long-term storage, it is recommended to use persistent disks
or Google Cloud Storage.

upvoted 1 times

? ?  RVivek 4ámonths, 3áweeks ago

I guess umissed the second paryt of the answer B whic says "After the backup is complete, use gsutil to move the backup to Google
Cloud Storage"
upvoted 1 times

? ?  omermahgoub 6ámonths ago

Option B is the most appropriate solution in this case. Mounting a Local SSD volume as the backup location will allow the backups to be
taken quickly and efficiently, as Local SSDs have very high I/O performance and low latencies. Additionally, using gsutil to move the
backups to Google Cloud Storage after they have been taken will provide a secure and durable storage location for the backups.

A, configuring a cron job to use the gcloud tool to take regular backups using persistent disk snapshots, may not be the most efficient
option because persistent disks have relatively lower I/O performance compared to Local SSDs.

C, using gcsfuse to mount a Google Cloud Storage bucket as a volume directly on the instance and writing the backups to the mounted
location using mysqldump, may not be the most efficient option because the backups would need to be transferred over the network,
which could impact the performance of the backups.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

D, mounting additional persistent disk volumes onto each VM instance in a RAID10 array and using LVM to create snapshots to send to
Cloud Storage, may not be the most efficient option because it would require additional disk space and setup, and LVM snapshots may
not be as fast as Local SSDs for taking backups.

upvoted 1 times

? ?  minmin2020 8ámonths ago

Selected Answer: C

Gcsfuse needs local storage for caching, usually local/non-persistent disks are used for this purpose. With gcsfuse you can have the
backend storage mounted as a filesystem on the server. Mysqldump allows for hot database backups.
Option C provides the automated solution needed to backup and store the database.
Option B is the manual version where you need to mount the local SSD, run the backup and then transfer it to a bucket manually.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I will go with B
upvoted 2 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: B

B is the answer.

https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up
When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups
and then push them to a Cloud Storage bucket.

upvoted 7 times

? ?  ashrafh 7ámonths, 1áweek ago

best answer thank you

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

438/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Pradeepkumar 10ámonths, 2áweeks ago

https://cloud.google.com/compute/docs/instances/sql-server/best-practices#backing_up

When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups
and then push them to a Cloud Storage bucket.

Though it is mentioned for SQL Server, the best practices are common for most of the databases. Also it is assumed that the Local SSD are
already mounted while creating the VM

upvoted 4 times

? ?  xman3 11ámonths, 1áweek ago

Selected Answer: C

>backups of a specific database

upvoted 2 times

? ?  Ric350 11ámonths, 2áweeks ago

B - I think this will clear things up. Local SSD is ATTACHED when CREATING the VM. The local SSDs are just LOCATED (on the physical host)
where the VM is running. See here.
https://cloud.google.com/compute/docs/disks/add-local-ssd#create_local_ssd

You can have a VM with locally attached SSD in an unformatted and unmounted state or just not mounted! Maybe it was umounted and
now needs to be re-mounted? Answer B says to MOUNT the local SSD. MOUNTING the SSD is done when the VM is RUNNING! We need to
assume the VM was built with locally attached SSD but not formatted and mounted yet. See here.
https://cloud.google.com/compute/docs/disks/add-local-ssd#format_and_mount_a_local_ssd_device!

upvoted 3 times

? ?  n_nana 5ámonths, 2áweeks ago

This clear confusion, Thank you.

upvoted 1 times

? ?  Ric350 11ámonths, 2áweeks ago

Also, When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your
backups and then push them to a Cloud Storage bucket. See here under "formatting secondary disks, backing up."
https://cloud.google.com/compute/docs/instances/sql-server/best-practices#formatting_secondary_disks

upvoted 3 times

? ?  Nirca 11ámonths, 3áweeks ago

Selected Answer: B

It is B. Writing to Local SSD and the fasted method. (Expansive too) Coping from SSD to GCS is slow, yet not affecting the database.

upvoted 1 times

? ?  H_S 1áyear ago

Selected Answer: C

The only way to have a specific data base backup is mysqldump
mates trust me I am a data person it can't be anything else than C

upvoted 3 times

? ?  cmamiusa 1áyear, 2ámonths ago

Selected Answer: B

Backing up
Best practice:
https://cloud.google.com/compute/docs/instances/sql-server/best-practices
When taking regular database backups, be careful not to consume too many persistent disk IOPS. Use the local SSD to stage your backups
and then push them to a Cloud Storage bucket.

upvoted 4 times

? ?  SAMBIT 1áyear, 3ámonths ago

https://cloud.google.com/database-migration/docs/mysql/mysql-dump

upvoted 2 times

? ?  SAMBIT 1áyear, 3ámonths ago

Mysqldump does export. All other means do a backup. Backup doesnÆt let select specific DB which is the ask. So, answer is C.

upvoted 2 times

? ?  SAMBIT 1áyear, 3ámonths ago

So every day one will attach a SSD & run gsutil to complete the work. Not practical. Hence option B is rejected.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

439/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #164

Topic 1

You are helping the QA team to roll out a new load-testing tool to test the scalability of your primary cloud services that run on Google Compute

Engine with Cloud

Bigtable.

Which three requirements should they include? (Choose three.)

A. Ensure that the load tests validate the performance of Cloud Bigtable

B. Create a separate Google Cloud project to use for the load-testing environment

C. Schedule the load-testing tool to regularly run against the production environment

D. Ensure all third-party systems your services use is capable of handling high load

E. Instrument the production services to record every transaction for replay by the load-testing tool

F. Instrument the load-testing tool and the target services with detailed logging and metrics collection

Correct Answer: ABF

Community vote distribution

ABF (56%)

BEF (33%)

11%

? ?  rishab86  Highly Voted ?  2áyears ago

after reading link: https://cloud.google.com/bigtable/docs/performance
A:Run your typical workloads against Bigtable :Always run your own typical workloads against a Bigtable cluster when doing capacity
planning, so you can figure out the best resource allocation for your applications.
B. Create a separate Google Cloud project to use for the load-testing environment
F : The most important/standard factor of testing, you gather logs and metrics in TEST environment for further scaling.

upvoted 24 times

? ?  AK2020 2áyears ago

There is no relevance to D here. So ABF

upvoted 4 times

? ?  mikesp 1áyear, 8ámonths ago

I agree. It is important to verity that current BitTable cluster can deal with incoming traffic:
A cluster must have enough nodes to support its current workload and the amount of data it stores. Otherwise, the cluster might not
be able to handle incoming requests, and latency could go up.
So although it is a managed service, it does not auto-scale.

upvoted 1 times

? ?  PeppaPig  Highly Voted ?  1áyear, 10ámonths ago

AB&F
Creating a separate project is highly recommended. It gives you total isolation from your product environment, and make sure it will not
share the resources with your product env such as service quota

upvoted 7 times

? ?  PeppaPig 1áyear, 10ámonths ago

You won't want load testing to consume the service quotas in your product project
https://cloud.google.com/docs/quota

upvoted 1 times

? ?  jlambdan  Most Recent ?  2ámonths, 3áweeks ago

Selected Answer: BEF

Here is my take, I respectfully disagree with ya all :)

A. Ensure that the load tests validate the performance of Cloud Bigtable Most Voted
=> not the requirement
B. Create a separate Google Cloud project to use for the load-testing environment Most Voted
=> yes, you don't want to use production quota.
C. Schedule the load-testing tool to regularly run against the production environment
=> yes please kill the prod !
D. Ensure all third-party systems your services use is capable of handling high load
=> well, that is what we shall test, so, it was more the task of the development team, not the QA team.
E. Instrument the production services to record every transaction for replay by the load-testing tool
=> yes, this way you can build your test dataset with realistic behavior.
F. Instrument the load-testing tool and the target services with detailed logging and metrics collection Most Vo
=> yes, otherwise you test for nothing, you have no data at the end to evaluate the system's performance.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

440/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  omermahgoub 6ámonths ago

Answer ADF
A: It is important to ensure that the load-testing tool is able to accurately test the performance of Cloud Bigtable in order to ensure that it
can handle the expected load.

D: It is important to ensure that all third-party systems that your primary cloud services rely on are able to handle the expected load in
order to avoid any potential bottlenecks or failures.

F: Instrumenting the load-testing tool and the target services with detailed logging and metrics collection can provide valuable insights
into the performance and behavior of the system under test, allowing the QA team to identify any potential issues or bottlenecks.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

Why not B, C and E:
B: creating a separate Google Cloud project to use for the load-testing environment, could also be a good idea by not necessary in
order to ensure that the load tests do not impact the performance of the production environment.

C: scheduling the load-testing tool to regularly run against the production environment, is not recommended, as this could potentially
impact the performance of the production environment and could lead to unexpected behavior or issues.

E: instrumenting the production services to record every transaction for replay by the load-testing tool, could also be a useful
requirement, as it would allow the QA team to accurately replay real-world workloads during the load tests in order to more accurately
simulate the expected production environment.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: ABF

ABF is the correct answer

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: ABF

A, B, F are the correct answer

upvoted 1 times

? ?  andras 8ámonths, 1áweek ago

why testing Bigtable... it's per definition of Google would absorb practically any load... don't you trust Google? :-)

upvoted 3 times

? ?  AzureDP900 8ámonths, 2áweeks ago

ABF is right

upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: ADF

There is no necessary reason for running it in a separate project.
A we have to test Bigtable.
F Important to record all the outputs and be able to review it.
D Important to stress test third party solutions or change it.

upvoted 1 times

? ?  AMEJack 8ámonths, 3áweeks ago

It is Google best practice to create a separate project for testing

upvoted 2 times

? ?  kiappy81 9ámonths, 2áweeks ago

hi, in the sentence it's underline that you what to test the scalability of your primary CLOUD SERVICS, so I think that D is not required.
For me it's ABF
upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: ABF

ABF is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: ABF

Vote ABF

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

BCF
1) B - you need to have a separate project for Load-Testing tool. That would at least separate role based access - dev and test. Also, test will
have their own code/project/config for testing, so no any chance of collision.
2) C - testing on production? because per Google recommendation, BigTable should be tested on production instances (not on

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

441/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

development) and for at least 10 min / 300 GB of data. Check "Testing Performance with Cloud Bigtable" here.
I understand this as a requirement for integration test for projects using BigTable. Testing of BigTable on Dev instances won't give proper
results.
3) F - collecting of metrics would be useful anyway....
Why not A, D, E?
1) A - isolation testing of BigTable likely doesn't make sense, if anyway integration test will need to run. That's covered in C.
2) D - testing of 3rd party tools in isolated mode likely is a one time effort (only useful when upgrading these tools). No point to run them
regularly.
3) E - collecting metrics on production env just to replay them on load-testing tool? What's a point? We need test max load anyway...

upvoted 2 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 1 times

? ?  gionny 1áyear, 11ámonths ago

Hi Kopper please send to giodionisi@yahoo.it

upvoted 1 times

? ?   recloud 1áyear, 11ámonths ago

Hi Kopper please send to thecloudit@outlook.com

upvoted 1 times

? ?  k_grdn 1áyear, 11ámonths ago

@kopper2019 - email k_grdn@tiscali.co.uk, many thanks.

upvoted 1 times

? ?  victory108 1áyear, 12ámonths ago

A. Ensure that the load tests validate the performance of Cloud Bigtable
B. Create a separate Google Cloud project to use for the load-testing environment
F. Instrument the load-testing tool and the target services with detailed logging and metrics collection

upvoted 3 times

? ?  kopper2019 1áyear, 12ámonths ago

all New Questions released in June 2021 are in Question number 3 or share you email

upvoted 3 times

? ?  Calvinchin0802 1áyear, 11ámonths ago

calvinchin0802@gmail.com

upvoted 1 times

? ?  digomelo 1áyear, 11ámonths ago

digodmeloo@gmail.com

upvoted 1 times

? ?  gcpexam_ca 1áyear, 12ámonths ago

are the new questions appeared in the real exam occurred after May 1st?

upvoted 1 times

? ?  Braindump 1áyear, 11ámonths ago

Did any one got the new questions from Kopper? Plz share at nad_far@rocketmail.com

upvoted 1 times

? ?  MrXBasit 2áyears ago
Answer should be BEF

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

442/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #165

Topic 1

Your customer is moving their corporate applications to Google Cloud Platform. The security team wants detailed visibility of all projects in the

organization. You provision the Google Cloud Resource Manager and set up yourself as the org admin.

What Google Cloud Identity and Access Management (Cloud IAM) roles should you give to the security team?

A. Org viewer, project owner

B. Org viewer, project viewer

C. Org admin, project browser

D. Project owner, network admin

Correct Answer: B

Community vote distribution

B (100%)

? ?  shandy  Highly Voted ?  3áyears, 7ámonths ago

A is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.

B is correct because:-Org viewer grants the security team permissions to view the organization's display name.
-Project viewer grants the security team permissions to see the resources within projects.

C is not correct because Org admin is too broad. The security team does not need to be able to make changes to the organization.

D is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.

upvoted 27 times

? ?  Pr44 6ámonths, 1áweek ago

I agree.

upvoted 1 times

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

B is the best answer because according to Google documentation i is best to use predefined roles and give the every team the least
amount of access. (https://cloud.google.com/iam/docs/using-iam-securely) The question states the security must be able to view things,
and the viewer role allows just that.

upvoted 12 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 6 times

? ?  surajkrishnamurthy  Most Recent ?  6ámonths, 2áweeks ago

Selected Answer: B

B is the correct answer

upvoted 1 times

? ?  allen_y_q_huang 7ámonths ago

Agree B as security team does not need Project owner permission, but why need to grant project viewer after granting organization
viewer?

upvoted 1 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: B

B. Org viewer, project viewer!

upvoted 1 times

? ?  mahima123k 11ámonths, 2áweeks ago

Very similar question was presented on 15 July 2022 exam

upvoted 3 times

? ?  Bill76 11ámonths, 2áweeks ago

Are the 260 exam topic questions enough to pass the exam?

upvoted 3 times

? ?  methamode 1áyear, 2ámonths ago

Selected Answer: B

B is the answer!

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

443/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  Surls 1áyear, 6ámonths ago

Selected Answer: B

B is correct

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: B

B is the correct answer

upvoted 1 times

? ?  nqthien041292 1áyear, 6ámonths ago

Selected Answer: B

Vote B

upvoted 1 times

? ?  mudot 1áyear, 7ámonths ago

Selected Answer: B

A is not correct because Project owner is too broad. The security team does not need to be able to make changes to projects.

B is correct because:
-Organization viewer grants the security team permissions to view the organization's display name.
-Project viewer grants the security team permissions to see the resources within projects.

C is not correct because Organization Administrator is too broad. The security team does not need to be able to make changes to the
organization.

D is not correct because Project Owner is too broad. The security team does not need to be able to make changes to projects.

upvoted 1 times

? ?  bala786 1áyear, 11ámonths ago

Option B is correct as per Least Privilege

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago
B. Org viewer, project viewer

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

B is correct

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

B is ok

upvoted 1 times

? ?  Ausias18 2áyears, 3ámonths ago

Answer is B

upvoted 1 times

? ?  Joyjit_Deb 2áyears, 4ámonths ago

"B" is definitely the correct answer.
Question talks about visibility for security team.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

444/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #166

Topic 1

Your company places a high value on being responsive and meeting customer needs quickly. Their primary business objectives are release speed

and agility. You want to reduce the chance of security errors being accidentally introduced.

Which two actions can you take? (Choose two.)

A. Ensure every code check-in is peer reviewed by a security SME

B. Use source code security analyzers as part of the CI/CD pipeline

C. Ensure you have stubs to unit test all interfaces between components

D. Enable code signing and a trusted binary repository integrated with your CI/CD pipeline

E. Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline

Correct Answer: BE

Community vote distribution

BE (48%)

DE (32%)

BD (16%)

2%

? ?  PeppaPig  Highly Voted ?  1áyear, 10ámonths ago

B&E
Code signing only verifies the author. In other words it only check who you are, but not what have you done

upvoted 36 times

? ?  robotgeek 1áyear, 7ámonths ago

I understand that would be a requirement for security

upvoted 2 times

? ?  Ishu_awsguy 9ámonths, 3áweeks ago

But when we select E , it might auto include B . SOme VA scanning tools also do SAST.
So why choose B and E in that case.
D makes more sense with E .
Authorised repo will add an additional layer of security with verified images and artifacts in it.

upvoted 1 times

? ?  rishab86  Highly Voted ?  2áyears ago

I think answer is D & E.

upvoted 30 times

? ?  AK2020 2áyears ago

Agree with this. https://cloud.google.com/container-registry/docs/container-analysis

upvoted 3 times

? ?  ravisar 1áyear, 7ámonths ago

Here the question is to provide solution for "Speed and Agility". The Binary authorization prevent unauthorized deployments in
production for GKE, Anthos Servicemesh and Cloud run, however will add delay in deployment process. So D may not be suitable in
this scenario. Answer is B&E.

upvoted 8 times

? ?  Ishu_awsguy 9ámonths, 3áweeks ago

Speed will nit get hampered if the images are verified and attested. Checks need to be there. If you argument would be true than
why to introduce VA scanner , as that will also induce delay in deployment.
when we select E , it might auto include B . Some VA scanning tools also do SAST.
So why choose B and E in that case.
D makes more sense with E .
Authorised repo will add an additional layer of security with verified images and artifacts in it.
Answer - D & E
upvoted 1 times

? ?  red_panda  Most Recent ?  1áweek, 2ádays ago

Selected Answer: BE

B and E is the answer for me also.

upvoted 1 times

? ?  mateuszma 1ámonth, 2áweeks ago

Selected Answer: DE

here you can find why: https://cloud.google.com/blog/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

445/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  JC0926 3ámonths, 1áweek ago

Selected Answer: B

B) Using source code security analyzers as part of the CI/CD pipeline can help identify security vulnerabilities and issues early in the
development process. This can help reduce the risk of security errors being accidentally introduced and ensure that security is integrated
into the development process from the beginning.

E) Running a vulnerability security scanner as part of the CI/CD pipeline can help identify vulnerabilities and issues in the code and
infrastructure before they are deployed to production. This can help reduce the risk of security errors being accidentally introduced and
ensure that security is integrated into the development process from the beginning.

upvoted 1 times

? ?  WinSxS 3ámonths, 2áweeks ago

Selected Answer: BE

B. Use source code security analyzers as part of the CI/CD pipeline
E. Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline

These actions ensure that security is integrated into the development and deployment processes and helps catch security issues early in
the software development lifecycle.

upvoted 1 times

? ?  PST21 3ámonths, 3áweeks ago

ChatGPT says B & E :-)

upvoted 2 times

? ?  nick_name_1 4ámonths, 1áweek ago

B? There is no category of product called "source code security analyzer"

upvoted 1 times

? ?  essadequeiroz 3ámonths, 3áweeks ago

Polaris, BlackDuck, etc

upvoted 1 times

? ?  telp 4ámonths, 2áweeks ago

Selected Answer: BE

For me, Option D, enabling code signing and a trusted binary repository integrated with your CI/CD pipeline. It's to ensure that the correct
code is put in production but not link to security errors.

upvoted 1 times

? ?  xval 4ámonths, 2áweeks ago

a, b
e is the same as b but source code scanning if more efficient

upvoted 1 times

? ?  izekc 4ámonths, 3áweeks ago

B,D should be correct

upvoted 1 times

? ?  Medofree 5ámonths, 3áweeks ago

Selected Answer: AB

Remember : "You want to reduce the chance of security errors being accidentally introduced".
A & B
A: By doing a peer-review you are reducing the risk of introducing security error.
B: Source code analyzer may detect security errors

Not E : because the question is not about vulnerabilities but about introducing security errors by the developers

upvoted 1 times

? ?  Wael216 6ámonths ago

Selected Answer: BE

B & E
code signing only checks the author not what he commits

upvoted 1 times

? ?  omermahgoub 6ámonths ago

B. Use source code security analyzers as part of the CI/CD pipeline: By using source code security analyzers as part of the CI/CD pipeline,
you can automatically detect and alert on security vulnerabilities in the code as it is being developed, which can help prevent security
errors from being introduced.

E. Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD) pipeline: By running a
vulnerability security scanner as part of the CI/CD pipeline, you can automatically detect and alert on security vulnerabilities in the
application as it is being deployed, which can help prevent security errors from being introduced.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

446/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Option A, ensuring every code check-in is peer reviewed by a security SME, could be a good way to prevent security errors from being
introduced, as peer review can help catch mistakes before they are committed. However, it may not be practical to always have a
security SME available to review every code check-in, especially if the company places a high value on release speed and agility.

Option C, ensuring you have stubs to unit test all interfaces between components, is not directly related to preventing security errors
from being introduced. While unit testing can help ensure that the code is working correctly, it is not specifically focused on security.

Option D, enabling code signing and a trusted binary repository integrated with your CI/CD pipeline, could be a good way to ensure
that only trusted code is being deployed. However, it may not be sufficient on its own to prevent security errors from being introduced,
as it does not directly address vulnerabilities in the code itself.

upvoted 3 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: BE

BE is the correct answer

upvoted 1 times

? ?  ale_brd_ 6ámonths, 2áweeks ago

Selected Answer: BE

B & E are the correct ones.

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: BE

B & E seems right, code signing is not needed here

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

447/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #167

Topic 1

You want to enable your running Google Kubernetes Engine cluster to scale as demand for your application changes.

What should you do?

A. Add additional nodes to your Kubernetes Engine cluster using the following command: gcloud container clusters resize CLUSTER_Name ?Ç"

-size 10

B. Add a tag to the instances in the cluster with the following command: gcloud compute instances add-tags INSTANCE - -tags enable-

autoscaling max-nodes-10

C. Update the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update mycluster - -enable-

autoscaling - -min-nodes=1 - -max-nodes=10

D. Create a new Kubernetes Engine cluster with the following command: gcloud alpha container clusters create mycluster - -enable-

autoscaling - -min-nodes=1 - -max-nodes=10 and redeploy your application

Correct Answer: C

Community vote distribution

C (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Agree C

upvoted 23 times

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

A is incorrect because there is supposed to be two hypens "--" not one before size
(https://cloud.google.com/sdk/gcloud/reference/container/clusters/resize). B is incorrect because it just adds a string to the cluster
(https://cloud.google.com/sdk/gcloud/reference/compute/instances/add-tags). "C" is just as wrong as "A" because the documentation says
it should be "--max-nodes" followed by "--min-nodes" (https://cloud.google.com/sdk/gcloud/reference/alpha/container/clusters/update),
also the alpha command no longer works but it used to and is still up on google docs. This goes for "D" as well but D talks about making
another, which doesn't have to be done because one it already up. So the debate is between A and C, and C used to work so C was chosen,
although C also has spaces which never worked... So this question is an absolute thug tactic by a Google team to steal from the Google
kingdom preventing the establishment of their library by failing people that actually know the science behind the technology. When you
see this question at a test center I'd select C.

upvoted 11 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 9 times

? ?  tartar 2áyears, 10ámonths ago

To enable autoscaling for an existing node pool, run the following command:

gcloud container clusters update cluster-name --enable-autoscaling \
--min-nodes 1 --max-nodes 10 --zone compute-zone --node-pool default-pool

https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-autoscaler

upvoted 9 times

? ?  svjl 2áyears, 6ámonths ago

You didn't check the documentation.

upvoted 3 times

? ?  AugustoKras011111  Most Recent ?  4ámonths ago

Selected Answer: C

no need to create a new one, just update!

upvoted 4 times

? ?  zerg0 4ámonths, 3áweeks ago

Selected Answer: C

See the cli docs
upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

448/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  gonlafer 6ámonths, 2áweeks ago

Selected Answer: C

It's C

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

ok for C

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I agree with C
upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 2 times

? ?  Bobch 1áyear, 6ámonths ago

Selected Answer: C

C looks OK

upvoted 1 times

? ?  TheCloudBoy77 1áyear, 7ámonths ago

C - cluster is already running so use update instead of create new cluster.

upvoted 5 times

? ?  [Removed] 1áyear, 8ámonths ago

Answer should be C. Now alpha command is not needed. seems question is older and now kubernets command is not with alpha.
gcloud container clusters update cluster-name --enable-autoscaling ....

upvoted 4 times

? ?  Examster1 1áyear, 9ámonths ago

This couldnÆt be C, you shouldnÆt use alpha commands in a production(app) workload.

upvoted 1 times

? ?  kopper2019 1áyear, 11ámonths ago

C is the way to go min and max and done

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

C. Update the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update mycluster - -
enable- autoscaling - -min-nodes=1 - -max-nodes=10

upvoted 3 times

? ?  Amber25 2áyears, 1ámonth ago

Answer- C.
Update command and autoscaling tag will update existing running kubernetes cluster.

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

C is correct

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

449/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #168

Topic 1

Your marketing department wants to send out a promotional email campaign. The development team wants to minimize direct operation

management. They project a wide range of possible customer responses, from 100 to 500,000 click-through per day. The link leads to a simple

website that explains the promotion and collects user information and preferences.

Which infrastructure should you recommend? (Choose two.)

A. Use Google App Engine to serve the website and Google Cloud Datastore to store user data.

B. Use a Google Container Engine cluster to serve the website and store data to persistent disk.

C. Use a managed instance group to serve the website and Google Cloud Bigtable to store user data.

D. Use a single Compute Engine virtual machine (VM) to host a web server, backend by Google Cloud SQL.

Correct Answer: AC

Reference:

https://cloud.google.com/storage-options/

Community vote distribution

AC (100%)

? ?  rishab86  Highly Voted ?  2áyears ago

A & C seems to be the correct answer.

upvoted 27 times

? ?  victory108  Highly Voted ?  1áyear, 12ámonths ago

A. Use Google App Engine to serve the website and Google Cloud Datastore to store user data.
C. Use a managed instance group to serve the website and Google Cloud Bigtable to store user data.

upvoted 8 times

? ?  J19G 1áyear, 8ámonths ago

Why not D?

upvoted 1 times

? ?  Bert_77 1áyear, 6ámonths ago

Because a single GCE instance might not be able to handle the unpredictable load

upvoted 5 times

? ?  zerg0  Most Recent ?  4ámonths, 3áweeks ago

Selected Answer: AC

Cloud Data store and Big Table are the only solutions that can handle 500000 clicks

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

450/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  omermahgoub 6ámonths ago

A: Google App Engine is a fully managed platform for building and running web applications and APIs. It can automatically scale to meet
high traffic demands, making it a good choice for serving the website for the promotional email campaign. Google Cloud Datastore can
also scale automatically to meet high traffic demands, making it a good choice for storing user data.

C: A managed instance group are managed as a single entity and can automatically scale up or down based on demand. This makes it a
good choice for serving the website for the promotional email campaign. Google Cloud Bigtable is a fully managed, high-performance
NoSQL database that can store and serve large amounts of structured data with low latency. It is designed to scale horizontally and can
handle high traffic demands, making it a good choice for storing user data.

upvoted 3 times

? ?  omermahgoub 6ámonths ago

B, using a Google Container Engine cluster to serve the website and store data to persistent disk, could be a valid solution as well.
However, persistent disks may not be able to scale horizontally to meet high traffic demands, which could impact the performance of
the website.

D, using a single Compute Engine VM to host a web server, backed by Google Cloud SQL, would not be a good choice for this scenario.
A single VM would not be able to scale to meet the wide range of possible traffic levels for the promotional email campaign, and
Google Cloud SQL may not be able to scale horizontally to meet high traffic demands.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A and C is right choice, D is saying single VM

upvoted 2 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: AC

AC (100%) !!!

upvoted 2 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: AC

A & C seems to be the correct answer.

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: AC

AC is the correct answer.

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

A only, choose two - App Engine + Datastore

Use GAE to serve the website and Google Datastore to store user data.

GCE û is too complex solution with specific OS to maintain.
GKE û is for microservices apps, and Persistent Disk is not good solution for relational data storage;
GAE û is fast and reliable solution, you write just code and run it on fully managed service. DataStore also matches perfectly since intended
for storing user profiles, key-value pairs.

upvoted 1 times

? ?  alan9999 2áyears ago

A & B with less operations management. Also Containers and App Engine as the clicks varies.

upvoted 3 times

? ?  AK2020 2áyears ago

But user data storing in persistent disks? Not correct to me. Seems A & C

upvoted 3 times

? ?  poseidon24 1áyear, 11ámonths ago

"Google Container Engine" does not exist, only "GKE", but operating a Kubernetes cluster is not easy, in that case, an option could be
Cloud Run.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

451/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #169

Topic 1

Your company just  nished a rapid lift and shift to Google Compute Engine for your compute needs. You have another 9 months to design and

deploy a more cloud-native solution. Speci cally, you want a system that is no-ops and auto-scaling.

Which two compute products should you choose? (Choose two.)

A. Compute Engine with containers

B. Google Kubernetes Engine with containers

C. Google App Engine Standard Environment

D. Compute Engine with custom instance types

E. Compute Engine with managed instance groups

Correct Answer: BC

B: With Container Engine, Google will automatically deploy your cluster for you, update, patch, secure the nodes.

Kubernetes Engine's cluster autoscaler automatically resizes clusters based on the demands of the workloads you want to run.

C: Solutions like Datastore, BigQuery, AppEngine, etc are truly NoOps.

App Engine by default scales the number of instances running up and down to match the load, thus providing consistent performance for your

app at all times while minimizing idle instances and thus reducing cost.

Note: At a high level, NoOps means that there is no infrastructure to build out and manage during usage of the platform. Typically, the

compromise you make with

NoOps is that you lose control of the underlying infrastructure.

Reference:

https://www.quora.com/How-well-does-Google-Container-Engine-support-Google-Cloud-Platform%E2%80%99s-NoOps-claim

Community vote distribution

BC (67%)

CE (33%)

? ?  PeppaPig  Highly Voted ?  1áyear, 11ámonths ago

I would go with B&C
Cloud-native, less-ops and auto-scaling all get addressed

upvoted 17 times

? ?  kinghin  Highly Voted ?  1áyear, 2ámonths ago

Why E is incorrect? can't MIG also perform autoscaling? Also it needs fewer administration as GKE

upvoted 8 times

? ?  AhmedH7793 9ámonths, 1áweek ago

No ops = Serverless / Almost Serverless MIG is not.

upvoted 4 times

? ?  JC0926  Most Recent ?  2ámonths, 2áweeks ago

Selected Answer: BC

Option B, Google Kubernetes Engine (GKE) with containers, is a managed Kubernetes service that automatically manages and scales
containerized applications. GKE handles cluster management tasks like scaling, upgrades, and security patches, allowing you to focus on
the application itself.

Option C, Google App Engine Standard Environment, is a fully managed platform for building and deploying applications. It automatically
scales applications based on demand and provides a no-ops experience. With App Engine Standard Environment, you don't need to worry
about infrastructure management, as Google handles it for you.

upvoted 2 times

? ?  jlambdan 2ámonths, 3áweeks ago

Selected Answer: BC

B: GKE with autopilot mode for workload not requiring ingress or egress. Otherwise you will need some ops work IMHO.
C: app engine for workload requiring ingress. It comes with autoscaling features and rolling update features without being as heavy as
gke.

upvoted 1 times

? ?  Deb2293 3ámonths, 2áweeks ago

Selected Answer: CE

I would still go for C & E. My take is GKE still requires some operational overhead for managing the Kubernetes cluster and ensuring high
availability of the workloads.
Hence C & E would be most suitable one.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

452/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  telp 4ámonths, 2áweeks ago

Selected Answer: BC

No ops: use container or gcp product without mangement.
So not VM possible in the answer

upvoted 1 times

? ?  habros 6ámonths, 4áweeks ago

Selected Answer: BC

App Engine standard = container based (can even go to zero)
App Engine flexible = VM based (minimum 1)
No ops: container > VM

upvoted 2 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: BC

B & C seems right to me, E needs lots of Ops to build image, instance template and instance group, ... maintain your image always

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B. Google Kubernetes Engine with containers
C. Google App Engine Standard Environmen

upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: BC

No ops = Serverless / Almost Serverless, less operational management overhead.
Kubernetes and App Engine are the only one that gives us that flexibility, plus is modernizing apps

upvoted 2 times

? ?  6721sora 9ámonths, 3áweeks ago

Selected Answer: CE

C and E
GKE is absolutely nor no-ops.
MIG can be closest to no-ops among the other options

upvoted 5 times

? ?  jabrrJ68w02ond1 9ámonths, 4áweeks ago

Selected Answer: BC

B&C seem to be right for this question. In reality, whoever really proposes B as an option never ran Kubernetes in production.

upvoted 2 times

? ?  JoeyCASD 1áyear, 1ámonth ago

Vote A and B
However I think option B should address more specifically, like GKE - autopilot mode.

upvoted 1 times

? ?  JoeyCASD 1áyear, 1ámonth ago
Correct the answer for B and C

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: BC

BC are the correct answers

upvoted 1 times

? ?  Bobch 1áyear, 6ámonths ago

Selected Answer: BC

Agree B and C
upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

Correct Answer: BC
B: With Container Engine, Google will automatically deploy your cluster for you, update, patch, secure the nodes.
Kubernetes Engine's cluster autoscaler automatically resizes clusters based on the demands of the workloads you want to run.
C: Solutions like Datastore, BigQuery, AppEngine, etc are truly NoOps.
App Engine by default scales the number of instances running up and down to match the load, thus providing consistent performance for
your app at all times while minimizing idle instances and thus reducing cost.

upvoted 3 times

? ?  MaxNRG 1áyear, 8ámonths ago

Note: At a high level, NoOps means that there is no infrastructure to build out and manage during usage of the platform. Typically, the
compromise you make with NoOps is that you lose control of the underlying infrastructure.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

453/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

https://www.quora.com/How-well-does-Google-Container-Engine-support-Google-Cloud-Platform%E2%80%99s-NoOps-claim
B û Google Container Engine (autoscaling)
C û Google AppEngine Standard Environment (no ops)
You should understand this Q as following: after Lift-n-Shift parts of the monolith should be moved to managed services (e.g. REST API)
running on GAE; and other micro-services will run in containers / pods.

upvoted 2 times

? ?  Rzla 1áyear, 9ámonths ago

B & C. Although GKE standard is definitely not no-ops!

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

454/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #170

Topic 1

One of your primary business objectives is being able to trust the data stored in your application. You want to log all changes to the application

data.

How can you design your logging system to verify authenticity of your logs?

A. Write the log concurrently in the cloud and on premises

B. Use a SQL database and limit who can modify the log table

C. Digitally sign each timestamp and log entry and store the signature

D. Create a JSON dump of each log entry and store it in Google Cloud Storage

Correct Answer: C

Community vote distribution

C (100%)

? ?  get2dd  Highly Voted ?  2áyears, 4ámonths ago

Correct answer is C (verified from Question Bank in Whizlabs.com)

Feedback
C (Correct answer) - Digitally sign each timestamp and log entry and store the signature.
Answer A, B, and D donÆt have any added value to verify the authenticity of your logs. Besides, Logs are mostly suitable for exporting to
Cloud storage, BigQuery, and PubSub. SQL database is not the best way to be exported to nor store log data.
Simplified Explanation
To verify the authenticity of your logs if they are tampered with or forged, you can use a certain algorithm to generate digest by hashing
each timestamp or log entry and then digitally sign the digest with a private key to generate a signature. Anybody with your public key can
verify that signature to confirm that it was made with your private key and they can tell if the timestamp or log entry was modified. You
can put the signature files into a folder separate from the log files. This separation enables you to enforce granular security policies.

upvoted 26 times

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

C is correct and common practice

upvoted 23 times

? ?  omermahgoub  Most Recent ?  6ámonths ago

I would recommend option C, digitally signing each timestamp and log entry and storing the signature. Digitally signing a log entry
involves creating a cryptographic hash of the log entry and a timestamp, and then encrypting the hash using a private key. The encrypted
hash, known as the signature, can be stored along with the log entry in a secure manner. To verify the authenticity of the log entry, you
can use the public key associated with the private key used to create the signature to decrypt the signature and recreate the hash. If the
recreated hash matches the original hash, it indicates that the log entry has not been tampered with and is authentic.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

Writing the log concurrently in the cloud and on premises, would not necessarily help to verify the authenticity of the logs, so A is not
an option

B, using a SQL database and limiting who can modify the log table, could help to prevent unauthorized modification of the logs, but it
would not necessarily provide a way to verify the authenticity of the logs if they are modified by an authorized user.

Option D, creating a JSON dump of each log entry and storing it in Google Cloud Storage, would not necessarily help to verify the
authenticity of the logs.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Digitally signing is correct. C is right option!

upvoted 1 times

? ?  GMats 1áyear, 6ámonths ago

C is correct.You can use deterministic algorithm to validate hash values.

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

455/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

C û Digitally sign each timestamp and log entry and store the signature.
This is fun Q where all options are technically correct. But, the point is to find most efficient. Since, Q asks about verification of log entry -
then you don't need to dub it. Using of much shorter timestamp-hash pair will address the request. So, when reading log from original
source, you also read hash for this timestamp and then verify the entry's body.
BTW, this is one of general purpose questions, which is not directly related to GCP. Just checks your attentiveness
A - is about duplication, can work, but redundant;
B / D - both have similar design, but donÆt allow verification of entry. No cross-checking of entry. E.g. person having access to log can
change it in one place.
C - storing log in one place, and hash-code in another. So, even if "trusted" person has modified original log, then it will break
correspondence with hash code in other storage. That storage should be available only for authentication program (via service account).

upvoted 5 times

? ?  Wonka 1áyear, 5ámonths ago

@MaxNRG, very clearly articulated elimination technique. BTW are these questions appearing in actual exam?

upvoted 1 times

? ?  Neo_ACE 1áyear, 8ámonths ago

If you attended recently, Please update some new questions too. It would be great help

upvoted 2 times

? ?  aviratna 2áyears ago

C is correct

upvoted 1 times

? ?  Amrit00009 2áyears, 1ámonth ago
C seems like the right answer

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

C. Digitally sign each timestamp and log entry and store the signature

upvoted 1 times

? ?  Amber25 2áyears, 1ámonth ago

C (Correct answer) - Digitally sign each timestamp and log entry and store the signature.

Other options are possible to export logs but won't be able to verify authenticity of logs

upvoted 2 times

? ?  un 2áyears, 1ámonth ago

C is correct

upvoted 1 times

? ?  mrhege 2áyears, 2ámonths ago

I'm on the fence between C and D. C is a good practice but D can do the job as well as versioned objects might be able to do job at some
level... Now, C tells that only the signature would be stored which is obviously not enough, but the owner of versioned objects might be
tampered too... IDK

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

IMO - C is ok

upvoted 1 times

? ?  Ausias18 2áyears, 3ámonths ago

Answers is C

upvoted 1 times

? ?  bnlcnd 2áyears, 5ámonths ago

I think this question should require 2 answers. In that case, C + D is correct. For only one choice, C is better than D.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

456/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #171

Topic 1

Your company has a Google Workspace account and Google Cloud Organization. Some developers in the company have created Google Cloud

projects outside of the Google Cloud Organization.

You want to create an Organization structure that allows developers to create projects, but prevents them from modifying production projects. You

want to manage policies for all projects centrally and be able to set more restrictive policies for production projects.

You want to minimize disruption to users and developers when business needs change in the future. You want to follow Google-recommended

practices. Now should you design the Organization structure?

A. 1. Create a second Google Workspace account and Organization. 2. Grant all developers the Project Creator IAM role on the new

Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on both Organizations. 5.

Additionally, set the production policies on the original Organization.

B. 1. Create a folder under the Organization resource named ?ÇProduction.2 Ç?. Grant all developers the Project Creator IAM role on the new

Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on the Organization. 5. Additionally,

set the production policies on the ?ÇProduction?Ç folder.

C. 1. Create folders under the Organization resource named ?ÇDevelopment?Ç and ?ÇProduction.2 Ç?. Grant all developers the Project Creator

IAM role on the ?ÇDevelopment?Ç folder. 3. Move the developer projects into the ?ÇDevelopment?Ç folder. 4. Set the policies for all projects on

the Organization. 5. Additionally, set the production policies on the ?ÇProduction?Ç folder.

D. 1. Designate the Organization for production projects only. 2. Ensure that developers do not have the Project Creator IAM role on the

Organization. 3. Create development projects outside of the Organization using the developer Google Workspace accounts. 4. Set the policies

for all projects on the Organization. 5. Additionally, set the production policies on the individual production projects.

Correct Answer: D

Reference:

https://cloud.google.com/resource-manager/docs/creating-managing-organization

Community vote distribution

C (97%)

? ?  cloudmon  Highly Voted ?  1áyear, 2ámonths ago

Selected Answer: C

C, because managing multiple organizations is not a Google best practice

upvoted 12 times

? ?  AugustoKras011111  Most Recent ?  4ámonths ago

Selected Answer: C

C, Bcuz manage multiple organizations is not a Google best practice

upvoted 1 times

? ?  OttomanSheikhIran 5ámonths, 2áweeks ago
clearly C. Two orgs is a BAD practice

upvoted 1 times

? ?  omermahgoub 6ámonths ago

I would recommend option C, creating two folders under the Organization resource named "Development" and "Production" and placing
developer and production projects in the respective folders. This approach would allow you to centrally manage policies for all projects,
while also being able to set more restrictive policies for production projects. It would also allow you to easily move projects between the
Development and Production folders as business needs change, without disrupting users or developers.

Option D, designating the Organization for production projects only, would not allow developers to create projects within the Organization
and could lead to confusion around project ownership and management. It would also make it more difficult to move projects between
development and production environments.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

457/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  omermahgoub 6ámonths ago

Option A, creating a second Google Workspace account and Organization, would not be a recommended practice as it would create
unnecessary complexity and make it more difficult to manage policies and move projects between environments.

Option B, creating a single folder under the Organization resource and placing all projects in that folder, would not allow you to set
different policies for development and production projects.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C Is the Correct Answer

upvoted 1 times

? ?  ashrafh 7ámonths, 1áweek ago
all 4 answers seems stupid

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is the best option

upvoted 1 times

? ?  6721sora 9ámonths, 3áweeks ago

Selected Answer: C

C is Ok

upvoted 2 times

? ?  cloudinit 10ámonths, 2áweeks ago

Selected Answer: C

I don't think anyone can create projects outside the organization using the workspace account as it redirects the users into the
organization.
upvoted 2 times

? ?  gardislan18 11ámonths, 1áweek ago

Answer is C
A - you only want to create and Organization structure not Google Workspace
B - best practice is to move your projects to a folders
D - developers are allowed to create projects

upvoted 1 times

? ?  szefco 11ámonths, 3áweeks ago

Selected Answer: C

C makes most sense in this scenario

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: D

D is better than C.

upvoted 1 times

? ?  sjmsummer 1áyear, 5ámonths ago

Selected Answer: C

C seems to be more organized solution than D.

upvoted 3 times

? ?  technodev 1áyear, 5ámonths ago

Selected Answer: C

I would go with C

upvoted 2 times

? ?  Aiffone 1áyear, 5ámonths ago

i'd go with C. option D is not a recommended practice

upvoted 1 times

? ?  victory108 1áyear, 5ámonths ago

C. 1. Create folders under the Organization resource named ?ÇDevelopment?Ç and ?ÇProduction.2 Ç?. Grant all developers the Project
Creator IAM role on the ?ÇDevelopment?Ç folder. 3. Move the developer projects into the ?ÇDevelopment?Ç folder. 4. Set the policies for all
projects on the Organization. 5. Additionally, set the production policies on the ?ÇProduction?Ç folder.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

458/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

459/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #172

Topic 1

Your company has an application running on Compute Engine that allows users to play their favorite music. There are a  xed number of instances.

Files are stored in Cloud Storage, and data is streamed directly to users. Users are reporting that they sometimes need to attempt to play popular

songs multiple times before they are successful. You need to improve the performance of the application. What should you do?

A. 1. Mount the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances. 2. Serve music  les directly from the backend

Compute Engine instance.

B. 1. Create a Cloud Filestore NFS volume and attach it to the backend Compute Engine instances. 2. Download popular songs in Cloud

Filestore. 3. Serve music  les directly from the backend Compute Engine instance.

C. 1. Copy popular songs into CloudSQL as a blob. 2. Update application code to retrieve data from CloudSQL when Cloud Storage is

overloaded.

D. 1. Create a managed instance group with Compute Engine instances. 2. Create a global load balancer and con gure it with two backends: ?

ùï Managed instance group ?ùï Cloud Storage bucket 3. Enable Cloud CDN on the bucket backend.

Correct Answer: A

Reference:

https://cloud.google.com/compute/docs/logging/usage-export

Community vote distribution

D (97%)

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: D

The correct answer is: D. Create a managed instance group with Compute Engine instances. Create a global load balancer and configure it
with two backends: Managed instance group, Cloud Storage bucket. Enable Cloud CDN on the bucket backend.

This solution will improve the performance of the application by:

Automatically scaling the number of Compute Engine instances to meet demand.
Distributing traffic across multiple instances to reduce load on each instance.
Caching popular songs in memory to reduce the number of times that they need to be loaded from Cloud Storage.
Using a global load balancer to distribute traffic evenly across all regions.
Using Cloud CDN to deliver files to users from a location that is closer to them.
This solution is the most efficient and cost-effective way to improve the performance of the application.

upvoted 4 times

? ?  zerg0 4ámonths, 3áweeks ago

Selected Answer: D

The Cloud CDN is the best practice for the content caching.

upvoted 3 times

? ?  omermahgoub 6ámonths ago

I would recommend option D, creating a managed instance group with Compute Engine instances and a global load balancer with two
backends: the managed instance group and the Cloud Storage bucket, and enabling Cloud CDN on the bucket backend. This approach
would allow you to scale the number of instances in the managed instance group as needed to handle the demand for the application,
and would also use the Cloud CDN to improve the performance of the application by caching the music files closer to the users.

Option A, mounting the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances, would not provide a way to scale
the number of instances to handle increased demand for the application.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

Option B, creating a Cloud Filestore NFS volume and attaching it to the backend Compute Engine instances, would not provide a way to
scale the number of instances to handle increased demand for the application.

Option C, copying popular songs into CloudSQL as a blob and updating the application code to retrieve data from CloudSQL when
Cloud Storage is overloaded, would not provide a way to scale the number of instances to handle increased demand for the
application. Additionally, using CloudSQL to store and serve music files may not be the most appropriate use case for the service, as it
is designed for storing and querying structured data, rather than serving large files.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D Is the Correct Answer

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

460/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

upvoted 1 times

? ?  adelynllllllllll 8ámonths, 1áweek ago

Selected Answer: A

I think it should be A,

upvoted 1 times

? ?  adelynllllllllll 8ámonths, 1áweek ago

Why not A, I think it should be A, since it mentioned popular songs and file store is faster then the cloud storage, I vote for A

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I will go with D
upvoted 1 times

? ?  jabrrJ68w02ond1 9ámonths, 4áweeks ago

Selected Answer: D

Do not trust the official answers here, D is correct. In special for this question, never use gcsfuse in production. Performance is bad and
reliability is trashy - Google states it themselves.

upvoted 4 times

? ?  szefco 11ámonths, 3áweeks ago

D is correct

upvoted 2 times

? ?  JoeyCASD 1áyear, 1ámonth ago

Remember Gcsfuse performance is not good, reference
https://cloud.google.com/storage/docs/gcs-fuse#notes

upvoted 3 times

? ?  nkit 1áyear, 2ámonths ago

Selected Answer: D

A is wrong because you can't be serving files directly from Compute Engine instance.
GCS + CDN is best option

upvoted 4 times

? ?  cloudmon 1áyear, 2ámonths ago

Selected Answer: D

D for sure

upvoted 1 times

? ?  Tan1234 1áyear, 4ámonths ago

Clarification on D: Question states there are a fixed number of instances. So creating MIG

upvoted 3 times

? ?  technodev 1áyear, 5ámonths ago

Selected Answer: D

Got this question in my exam, answered D

upvoted 3 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: D

most logical answer is D.

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

B&C are out.
Use A gcsfuse is not recommended. Warning on the gcsfuse reference page.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

461/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #173

Topic 1

The operations team in your company wants to save Cloud VPN log events for one year. You need to con gure the cloud infrastructure to save the

logs. What should you do?

A. Set up a  lter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.

B. Enable the Compute Engine API, and then enable logging on the  rewall rules that match the tra c you want to save.

C. Set up a Cloud Logging Dashboard titled Cloud VPN Logs, and then add a chart that queries for the VPN metrics over a one-year time

period.

D. Set up a  lter in Cloud Logging and a topic in Pub/Sub to publish the logs.

Correct Answer: A

Reference:

https://cloud.google.com/network-connectivity/docs/vpn/how-to/viewing-logs-metrics

Community vote distribution

A (100%)

? ?  Deb2293 3ámonths, 2áweeks ago

Selected Answer: A

Archival storage: Cloud Storage is the best

upvoted 2 times

? ?  tdotcat 5ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago
I would like to go with Option A

upvoted 1 times

? ?  6721sora 9ámonths, 3áweeks ago

Selected Answer: A

Logs needed for a year. Coldline or Archive storage classes available.
A seems fine

upvoted 3 times

? ?  exam9391 11ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  [Removed] 1áyear, 3ámonths ago

A should be right.

upvoted 1 times

? ?  technodev 1áyear, 5ámonths ago

It should be D, CDN serves the purpose.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

462/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  technodev 1áyear, 5ámonths ago

Ignore A is correct. CDN was an answer to a diff question.

upvoted 1 times

? ?  victory108 1áyear, 5ámonths ago

A. Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.

upvoted 1 times

? ?  edilramos 1áyear, 6ámonths ago

Selected Answer: A

A Is correct.
Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.

Filter in Cloud Loggin for specific content, and Cloud Storage for storage.

upvoted 4 times

? ?  StelSen 1áyear, 6ámonths ago

Option-A is correct. Need cloud storage bucket for long time storage.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

463/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #174

Topic 1

You are working with a data warehousing team that performs data analysis. The team needs to process data from external partners, but the data

contains personally identi able information (PII). You need to process and store the data without storing any of the PIIE data. What should you

do?

A. Create a Data ow pipeline to retrieve the data from the external sources. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud

DLP) API to remove any PII data. Store the result in BigQuery.

B. Create a Data ow pipeline to retrieve the data from the external sources. As part of the pipeline, store all non-PII data in BigQuery and store

all PII data in a Cloud Storage bucket that has a retention policy set.

C. Ask the external partners to upload all data on Cloud Storage. Con gure Bucket Lock for the bucket. Create a Data ow pipeline to read the

data from the bucket. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in

BigQuery.

D. Ask the external partners to import all data in your BigQuery dataset. Create a data ow pipeline to copy the data into a new table. As part of

the Data ow bucket, skip all data in columns that have PII data

Correct Answer: A

Community vote distribution

A (74%)

C (26%)

? ?  StelSen  Highly Voted ?  1áyear, 6ámonths ago

Option-A is correct. Although Option-C sounds good, ultimately we should not store PI data at all as per question says.

upvoted 43 times

? ?  edilramos  Highly Voted ?  1áyear, 6ámonths ago

Selected Answer: A

The correct answer is A.
Option C seems to be an option, but there are two non-conformities there. In addition to storing personal data in the GCS, it is being
improperly retained.

upvoted 14 times

? ?  BiddlyBdoyng  Most Recent ?  2áweeks ago

The problem with C is the data is stored in the bucket with the PII data even though the BigQuery data has it removed?

upvoted 1 times

? ?  AugustoKras011111 4ámonths ago

Selected Answer: A

option A, the question say dont store data...

upvoted 1 times

? ?  someCloudUser 4ámonths, 1áweek ago

Selected Answer: A

A is correct.

upvoted 1 times

? ?  telp 4ámonths, 2áweeks ago

Selected Answer: A

Answer A. The question say do not store PII data so need to remove it before storing.

upvoted 1 times

? ?  rotorclear 4ámonths, 2áweeks ago

Selected Answer: A

Answer should be A because the question emphasises on processing the data without storing it. That rules out C.

upvoted 1 times

? ?  RVivek 4ámonths, 3áweeks ago

Selected Answer: A

C -- is wrong because PII data is uploaded and the bucket is locked which means the data cannot be deleted
B and D are wron as they do not use Data loss prevention to protect data

upvoted 1 times

? ?  dataqueen_3110 4ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

464/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

PII --> Cloud DLP. So that narrows the choices down to A or C. C says "Ask the external partners to upload all data on Cloud Storage" which
is not generally a feasible or recommended practice. Also, we cannot store PII anywhere, including in GCS. Answer is A.

upvoted 1 times

? ?  Wael216 6ámonths ago

Selected Answer: A

A i s correct, C sounds good but storing the data in GCS is already a violation of the PII requirements

upvoted 1 times

? ?  omermahgoub 6ámonths ago

I would recommend option A, creating a Dataflow pipeline to retrieve the data from the external sources and using the Cloud Data Loss
Prevention (Cloud DLP) API to remove any PII data. Storing the result in BigQuery would allow the data warehousing team to easily
perform analysis on the data.

Option C, using Bucket Lock to protect the data and using the Cloud DLP API to remove PII data, would protect the data from
unauthorized access, but would not allow the data warehousing team to easily perform analysis on the data.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

Option B, storing non-PII data in BigQuery and PII data in a Cloud Storage bucket with a retention policy set, would not fully protect the
PII data and could potentially lead to data breaches.

Option D, copying the data into a new table and skipping columns with PII data, would not fully protect the PII data and could
potentially lead to data breaches. It would also require the data warehousing team to manually skip certain columns when performing
analysis, which could be time-consuming and error-prone.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: A

A Is the Correct Answer

upvoted 1 times

? ?  ardit 6ámonths, 3áweeks ago

Selected Answer: A

A is the right one.

upvoted 1 times

? ?  jaxclain 7ámonths ago

Selected Answer: A

Of course the correct answer is A, not sure how some people think C is valid, probably trolling trying to confuse some here.

upvoted 2 times

? ?  Aninina 7ámonths, 1áweek ago

Selected Answer: A

It's A, not C because we cannot ask the external partners to upload all data on Cloud Storage

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I will go with A
upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

465/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #175

Topic 1

You want to allow your operations team to store logs from all the production projects in your Organization, without including logs from other

projects. All of the production projects are contained in a folder. You want to ensure that all logs for existing and new production projects are

captured automatically. What should you do?

A. Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project.

B. Create an aggregated export on the Organization resource. Set the log sink to be a Cloud Storage bucket in an operations project.

C. Create log exports in the production projects. Set the log sinks to be a Cloud Storage bucket in an operations project.

D. Create log exports in the production projects. Set the log sinks to be BigQuery datasets in the production projects, and grant IAM access to

the operations team to run queries on the datasets.

Correct Answer: B

Reference:

https://cloud.google.com/logging/docs/audit

Community vote distribution

A (97%)

? ?  simbu1299  Highly Voted ?  1áyear, 6ámonths ago

The correct answer is A

upvoted 15 times

? ?  technodev  Highly Voted ?  1áyear, 5ámonths ago

Selected Answer: A

A is the right answer.

upvoted 9 times

? ?  Atanu  Most Recent ?  4áweeks, 1áday ago

The admin must have failed this exam multiple times. How can one select option B here.

upvoted 2 times

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: A

The correct answer is: A. Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an
operations project.

This solution will allow the operations team to store logs from all the production projects in your Organization, without including logs
from other projects. All of the production projects are contained in a folder, so you can create an aggregated export on the Production
folder. You can then set the log sink to be a Cloud Storage bucket in an operations project. This will allow the operations team to store all
of the logs from the production projects in one place.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

Option B is not the correct solution because it creates an aggregated export on the Organization resource, which will capture logs from all
projects in the Organization, including those outside the Production folder.

The best option to achieve the desired result is to create an aggregated export on the Production folder. Set the log sink to be a Cloud
Storage bucket in an operations project. This will allow the operations team to store logs from all the production projects in the
Organization, without including logs from other projects. Additionally, this setup will automatically capture logs for existing and new
production projects.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

466/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Option A is the correct solution because it allows you to create an aggregated export on the Production folder, which will capture logs
from all the production projects contained in the folder. Setting the log sink to a Cloud Storage bucket in an operations project will allow
the operations team to store the logs in a central location.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

Option C is not the correct solution because it requires you to create log exports in each production project, which can be time-
consuming and error-prone. Additionally, setting the log sink to a Cloud Storage bucket in an operations project will not automatically
capture logs for new production projects.

Option D is not the correct solution because it requires you to create log exports in each production project, which can be time-
consuming and error-prone. Additionally, storing the logs in BigQuery datasets in the production projects will not allow the operations
team to easily access the logs. Instead, they would need to be granted IAM access to run queries on the datasets.

upvoted 3 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is the right answer https://cloud.google.com/logging/docs/export/aggregated_sinks

upvoted 5 times

? ?  AzureDP900 8ámonths, 2áweeks ago
I will choose A as right answer

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: A

A is the answer.

https://cloud.google.com/logging/docs/export/aggregated_sinks
Aggregated sinks combine and route log entries from the Google Cloud resources contained by an organization or folder.

upvoted 3 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: A

A, is the only one that creates the policy in the Production Folder.
Hence doing what the question says "make sure existing / all future projects gets automatically logs sabed"

upvoted 2 times

? ?  exam9391 11ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 3 times

? ?  cloudmon 1áyear, 2ámonths ago

Selected Answer: A

The correct answer is A

upvoted 3 times

? ?  kinghin 1áyear, 2ámonths ago

Anyone can explain the difference between A and C? Both options look similar...

upvoted 1 times

? ?  9xnine 1áyear ago

Projects vs. folder. IF you create the export on the folder it will apply to all new projects under that folder.

upvoted 8 times

? ?  jabrrJ68w02ond1 9ámonths, 4áweeks ago

Now this is a really cool feature, thanks for the explanation!

upvoted 2 times

? ?  AsadZaidi 1áyear, 4ámonths ago

"without including logs from other projects. " << Chose A as answer

upvoted 2 times

? ?  kongae 1áyear, 5ámonths ago

Selected Answer: A

Don't understand why we need to choose at organization level, Please explain if B is correct

upvoted 5 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

467/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  OrangeTiger 1áyear, 5ámonths ago

'All of the production projects are contained in a folder.'
'existing and new production projects are captured automatically'
A can both.

upvoted 1 times

? ?  AJapieGuru 1áyear, 5ámonths ago

Go for A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

468/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #176

Topic 1

Your company has an application that is running on multiple instances of Compute Engine. It generates 1 TB per day of logs. For compliance

reasons, the logs need to be kept for at least two years. The logs need to be available for active query for 30 days. After that, they just need to be

retained for audit purposes. You want to implement a storage solution that is compliant, minimizes costs, and follows Google-recommended

practices. What should you do?

A. 1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object

Lifecycle rule to move  les into a Coldline Cloud Storage bucket after one month. 4. Con gure a retention policy at the bucket level using

bucket lock.

B. 1. Write a daily cron job, running on all instances, that uploads logs into a Cloud Storage bucket. 2. Create a sink to export logs into a

regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move  les into a Coldline Cloud Storage bucket after one month.

C. 1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a partitioned BigQuery table. 3. Set a

time_partitioning_expiration of 30 days.

D. 1. Create a daily cron job, running on all instances, that uploads logs into a partitioned BigQuery table. 2. Set a time_partitioning_expiration

of 30 days.

Correct Answer: C

Community vote distribution

A (97%)

? ?  gggsrs  Highly Voted ?  1áyear, 5ámonths ago

The answer is A.

The practice for managing logs generated on Compute Engine on Google Cloud is to install the Cloud Logging agent and send them to
Cloud Logging.

The sent logs will be aggregated into a Cloud Logging sink and exported to Cloud Storage.
The reason for using Cloud Storage as the destination for the logs is that the requirement in question requires setting up a lifecycle based
on the storage period.
In this case, the log will be used for active queries for 30 days after it is saved, but after that, it needs to be stored for a longer period of
time for auditing purposes.

If the data is to be used for active queries, we can use BigQuery's Cloud Storage data query feature and move the data past 30 days to
Coldline to build a cost-optimal solution.

Therefore, the correct answer is as follows
1. Install the Cloud Logging agent on all instances.
Create a sync that exports the logs to the region's Cloud Storage bucket.
3. Create an Object Lifecycle rule to move the files to the Coldline Cloud Storage bucket after one month. 4.
4. set up a bucket-level retention policy using bucket locking."

upvoted 17 times

? ?  gggsrs 1áyear, 5ámonths ago

https://cloud.google.com/logging/docs/agent/logging/installation
https://cloud.google.com/logging/docs/export/configure_export_v2
https://cloud.google.com/bigquery/external-data-cloud-storage

upvoted 4 times

? ?  someCloudUser  Most Recent ?  4ámonths ago

Selected Answer: A

A is correct

upvoted 1 times

? ?  someCloudUser 4ámonths, 1áweek ago

Selected Answer: A

A is correct.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

A. 1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object
Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention policy at the bucket level using
bucket lock.

This approach would allow you to use Cloud Logging to collect and export the logs from the Compute Engine instances into a Cloud
Storage bucket. You can then use an Object Lifecycle rule to automatically move the logs from the regional bucket to a Coldline bucket

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

469/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

after one month, which will reduce storage costs for logs that are not actively being queried. By configuring a retention policy using
bucket lock, you can ensure that the logs are retained for at least two years for audit purposes. This approach follows Google-
recommended practices for storing logs and minimizing costs.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: A

A Is the Correct Answer

upvoted 1 times

? ?  habros 6ámonths, 4áweeks ago

Selected Answer: A

A is perfect answerà the rest doesnÆt sound rational

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I agree with A, There is no need of BigQuery.

upvoted 1 times

? ?  manis68 8ámonths, 2áweeks ago

Selected Answer: A

A is fine

upvoted 1 times

? ?  Imran109 10ámonths, 2áweeks ago

For compliance reasons, the logs need to be kept for at least two years... In Bigquery time partitioned after 30 days ..how the logs be
present for 2 years ..Hence going with A

upvoted 1 times

? ?  harutheorochimaru 10ámonths, 4áweeks ago

Selected Answer: A

A is a no-brainer
upvoted 1 times

? ?  exam9391 11ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  H_S 1áyear ago

Selected Answer: A

when a partition expires, the data in the partition is no longer available

upvoted 2 times

? ?  shasha_zhang 1áyear, 1ámonth ago

Selected Answer: A

for big query, when a partition expires, the data in the partition is no longer available for querying. BigQuery will eventually deletes the
expired partition which doesn't meet the requirement described.

upvoted 4 times

? ?  cloudmon 1áyear, 2ámonths ago

Selected Answer: A

The answer is A.
upvoted 3 times

? ?  sergaebi 1áyear, 2ámonths ago

Selected Answer: A

Voting for A

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

470/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #177

Topic 1

Your company has just recently activated Cloud Identity to manage users. The Google Cloud Organization has been con gured as well. The

security team needs to secure projects that will be part of the Organization. They want to prohibit IAM users outside the domain from gaining

permissions from now on. What should they do?

A. Con gure an organization policy to restrict identities by domain.

B. Con gure an organization policy to block creation of service accounts.

C. Con gure Cloud Scheduler to trigger a Cloud Function every hour that removes all users that don't belong to the Cloud Identity domain from

all projects.

D. Create a technical user (e.g., crawler@yourdomain.com), and give it the project owner role at root organization level. Write a bash script

that: ?Çó Lists all the IAM rules of all projects within the organization. ?Çó Deletes all users that do not belong to the company domain. Create

a Compute Engine instance in a project within the Organization and con gure gcloud to be executed with technical user credentials. Con gure

a cron job that executes the bash script every hour.

Correct Answer: D

Reference:

https://sysdig.com/blog/gcp-security-best-practices/

Community vote distribution

A (100%)

? ?  Foto lico  Highly Voted ?  1áyear, 5ámonths ago

Selected Answer: A

https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains

upvoted 14 times

? ?  kimharsh  Highly Voted ?  1áyear, 1ámonth ago

LOL , if we give this question to someone who know nothing about GCP they will select A

upvoted 11 times

? ?  Atanu  Most Recent ?  4áweeks, 1áday ago

Selected Answer: A

Option D is just to create confusion only.

upvoted 1 times

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: A

The correct answer is: A. Configure an organization policy to restrict identities by domain.

This solution will allow the security team to secure projects that will be part of the Organization by prohibiting IAM users outside the
domain from gaining permissions.

The other options are not as efficient or effective. Option B would not be efficient, as it would block the creation of all service accounts,
which are necessary for some applications. Option C would not be effective, as it would not prevent IAM users from gaining permissions,
as it would only remove users that do not belong to the Cloud Identity domain from all projects. Option D would not be efficient, as it
would require a Compute Engine instance to be created and a cron job to be configured, which would add complexity and cost to the
solution.

upvoted 1 times

? ?  someCloudUser 4ámonths ago

Selected Answer: A

A is correct

upvoted 1 times

? ?  omermahgoub 6ámonths ago

The security team should configure an organization policy to restrict identities by domain. This will allow them to specify a list of allowed
domains, and prevent users from outside those domains from gaining permissions in the Organization.

Alternatively, the security team could configure an organization policy to block creation of service accounts. This would prevent the
creation of new service accounts, which could be used to grant permissions to users outside the domain.

The other options are not recommended. Option C involves manually removing users every hour, which could be time-consuming and
error-prone. Option D involves creating a technical user and writing a bash script to delete users, which is not a recommended approach.
It would be more secure and efficient to use an organization policy to restrict identities by domain.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

471/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: A

A Is the Correct Answer

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is the correct answer
https://cloud.google.com/resource-manager/docs/organization-policy/restricting-domains

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is right

upvoted 1 times

? ?  manis68 8ámonths, 2áweeks ago

Selected Answer: A

A is OK

upvoted 1 times

? ?  exam9391 11ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 3 times

? ?  azureaspirant 1áyear, 4ámonths ago

2/15/21 exam
upvoted 5 times

? ?  blk_rook 1áyear, 5ámonths ago

Selected Answer: A

must restrict the access, not clean up every hour. see reference from Fotofilico

upvoted 3 times

? ?  AJapieGuru 1áyear, 5ámonths ago

Go for A

upvoted 1 times

? ?  GauravLahoti 1áyear, 5ámonths ago

Correct Answer is A

upvoted 2 times

? ?  brushek 1áyear, 5ámonths ago

Selected Answer: A

vote A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

472/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #178

Topic 1

Your company has an application running on Google Cloud that is collecting data from thousands of physical devices that are globally distributed.

Data is published to Pub/Sub and streamed in real time into an SSD Cloud Bigtable cluster via a Data ow pipeline. The operations team informs

you that your Cloud

Bigtable cluster has a hotspot, and queries are taking longer than expected. You need to resolve the problem and prevent it from happening in the

future. What should you do?

A. Advise your clients to use HBase APIs instead of NodeJS APIs.

B. Delete records older than 30 days.

C. Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.

D. Double the number of nodes you currently have.

Correct Answer: C

Community vote distribution

C (100%)

? ?  StelSen  Highly Voted ?  1áyear, 6ámonths ago

Option-C is correct: https://cloud.google.com/bigtable/docs/schema-design#row-keys

upvoted 12 times

? ?  omermahgoub  Highly Voted ?  6ámonths ago

C. Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.

The RowKey is used to sort data within a Cloud Bigtable cluster. If the keys are not evenly spread across the alphabet, it can result in a
hotspot and slow down queries. To prevent this from happening in the future, you should review your RowKey strategy and ensure that
keys are evenly spread across the alphabet. This will help to distribute the data evenly across the cluster and improve query performance.
Other potential solutions to consider include adding more nodes to the cluster or optimizing your query patterns. However, deleting
records older than 30 days or advising clients to use HBase APIs instead of NodeJS APIs would not address the issue of a hotspot in the
cluster.

upvoted 7 times

? ?  AugustoKras011111  Most Recent ?  4ámonths ago

Selected Answer: C

C in the only answer that make sense.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C Is the Correct Answer

upvoted 1 times

? ?  gonlafer 6ámonths, 2áweeks ago

Selected Answer: C

C
https://cloud.google.com/bigtable/docs/overview#load-balancing

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is the right answer

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is better option
upvoted 1 times

? ?  Jeyakumar 11ámonths ago
Option-C is the better one.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

473/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Nirca 11ámonths, 3áweeks ago

The issue described is with "querying" meaning reading. Not writing. C: distributing across the Alphabet is good for Writing.

upvoted 2 times

? ?  JoeyCASD 1áyear, 1ámonth ago

Suggest to study the following reference, it's important to design the row key pattern in Bigtable.
https://cloud.google.com/bigtable/docs/overview#architecture
https://cloud.google.com/bigtable/docs/overview#load-balancing

upvoted 6 times

? ?  AjayPrajapati 1áyear, 5ámonths ago

C looks good, I dont think we have to control number of nodes in Big table

upvoted 3 times

? ?  AJapieGuru 1áyear, 5ámonths ago

Vote C

upvoted 2 times

? ?  victory108 1áyear, 5ámonths ago

C. Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.

upvoted 1 times

? ?  GMats 1áyear, 6ámonths ago

C is answer.Hot key/partitions are created due to improper row key design.

upvoted 2 times

? ?  spoxman 1áyear, 6ámonths ago

Selected Answer: C

correct row-key strategy improves performance

upvoted 1 times

? ?  ACasper 1áyear, 6ámonths ago

I think the ans is D

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

474/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #179

Topic 1

Your company has a Google Cloud project that uses BigQuery for data warehousing. There are some tables that contain personally identi able

information (PII).

Only the compliance team may access the PII. The other information in the tables must be available to the data science team. You want to

minimize cost and the time it takes to assign appropriate access to the tables. What should you do?

A. 1. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an appropriate

project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.

B. 1. From the dataset where you have the source data, create materialized views of tables that you want to share, excluding PII. 2. Assign an

appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.

C. 1. Create a dataset for the data science team. 2. Create views of tables that you want to share, excluding PII. 3. Assign an appropriate

project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize

the view to access the source dataset.

D. 1. Create a dataset for the data science team. 2. Create materialized views of tables that you want to share, excluding PII. 3. Assign an

appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5.

Authorize the view to access the source dataset.

Correct Answer: C

Reference:

https://cloud.google.com/blog/topics/developers-practitioners/bigquery-admin-reference-guide-data-governance?skip_cache=true

Community vote distribution

C (57%)

A (36%)

7%

? ?  [Removed]  Highly Voted ?  1áyear, 1ámonth ago

Selected Answer: A

Materialized view is too costly for the requirement. So B & D is out.

To protect PII, there is no need to create another dataset. Creating a view on the original dataset should be sufficient. In addition,
according to https://cloud.google.com/bigquery/docs/view-access-controls, view access can be granted at the 'dataset' level.

upvoted 13 times

? ?  ashrafh 7ámonths, 1áweek ago

A is correct

Giving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view lets you share query
results with particular users and groups without giving them access to the underlying tables. You can also use the view's SQL query to
restrict the columns (fields) the users are able to query. In this tutorial, you create an authorized view.

https://cloud.google.com/bigquery/docs/share-access-views

upvoted 2 times

? ?  ashrafh 7ámonths, 1áweek ago

Sorry I mean C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

475/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  zellck 9ámonths, 2áweeks ago

Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the
authorized view without simultaneously granting access to the underlying data.

upvoted 4 times

? ?  [Removed]  Highly Voted ?  1áyear, 5ámonths ago

Selected Answer: C

C is correct here. You need view to avoid PII data. So materialized view is not needed.

upvoted 10 times

? ?  melono 8ámonths, 1áweek ago

also can't query data from a view, so A not.

upvoted 1 times

? ?  melono 8ámonths, 1áweek ago

https://cloud.google.com/bigquery/docs/share-access-views

upvoted 1 times

? ?  JC0926  Most Recent ?  2ámonths, 3áweeks ago

Selected Answer: C

Option A is not the best choice because it doesn't involve creating a separate dataset for the data science team. Creating a separate
dataset provides better organization and access control management for different teams.

In option C, you create a separate dataset specifically for the data science team and then create views that exclude PII. This allows for
more granular access controls and a better separation of concerns. By authorizing the view to access the source dataset, you ensure that
the data science team can only access the non-PII data through the views, maintaining privacy and compliance.

upvoted 1 times

? ?  JC0926 3ámonths, 1áweek ago

Selected Answer: A

Option C are not appropriate because creating a new dataset is not necessary in this scenario. Creating views of the tables that exclude PII
is a simpler and more cost-effective solution. Additionally, authorizing the view to access the source dataset is not necessary because the
view already contains the relevant data.

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

C = A + One additional step to create the dataset which is not necessary so the answer is A

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

Selected Answer: A

No need for materialized view which is an operational overhead.

upvoted 1 times

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: A

A. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an appropriate
project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view. This
solution will minimize cost and the time it takes to assign appropriate access to the tables. The other options are not as efficient or
effective.

upvoted 1 times

? ?  AugustoKras011111 4ámonths ago

Selected Answer: C

I vote for C. Option C provides better security option.

upvoted 2 times

? ?  szagarella 4ámonths, 1áweek ago

Selected Answer: A

A is my answer.
upvoted 1 times

? ?  telp 4ámonths, 2áweeks ago

Selected Answer: C

Agree with C from the link with google best practice
https://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view
Create a dataset where you can store your view
After creating your source dataset, you create a new, separate dataset to store the authorized view that you share with your data analysts.
In a later step, you grant the authorized view access to the data in the source dataset. Your data analysts then have access to the
authorized view, but not direct access to the source data.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

476/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Jeena345 4ámonths, 2áweeks ago

Selected Answer: A

Materialized views costs more than normal ones. Creating a new dataset is not cost-effective. You can use authorized views to restrict data
access.
From Google doc:

Giving a view access to a dataset is also known as creating an authorized view in BigQuery. An authorized view lets you share query results
with particular users and groups without giving them access to the underlying tables. You can also use the view's SQL query to restrict the
columns (fields) the users are able to query.

Users need the bigquery.tables.getData permission on all tables and views that their query references. In addition, when querying a view
users need this permission on all underlying tables and views. However, if you are using authorized views or authorized datasets, you
don't need to give users access to the underlying source data.

Reference:

https://cloud.google.com/bigquery/docs/share-access-views

https://cloud.google.com/bigquery/docs/table-access-controls#required_permission_to_query_tables_and_views

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is the correct answer https://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view

upvoted 4 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is right

upvoted 1 times

? ?  ppandey96 9ámonths, 1áweek ago

Selected Answer: C

https://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: C

Answer is C. You need to create a new dataset based on a query from the source dataset first before you can create the authorised view.

https://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view

After creating your source dataset, you create a new, separate dataset to store the authorized view that you share with your data analysts.
In a later step, you grant the authorized view access to the data in the source dataset. Your data analysts then have access to the
authorized view, but not direct access to the source data.

upvoted 2 times

? ?  zellck 9ámonths, 2áweeks ago

Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the
authorized view without simultaneously granting access to the underlying data.

upvoted 1 times

? ?  kiappy81 9ámonths, 2áweeks ago

Selected Answer: C

if you watch this video there is a useful explanation of authorized views that, IMHO, matches with the question.
https://cloud.google.com/bigquery/docs/authorized-views

upvoted 2 times

? ?  GordonLeo 8ámonths, 1áweek ago

Thank you Kiappy to share the video. That video clearly confirms the right answer is C

upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: A

Requirements: "Cost and Time"
Materialized views are very expensive. Therefor, B / D discarted.
Between A & C:
C works but creates a new dataset, hence we need time / cost for the new dataset, which is not needed.
A seems like the best answer.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

477/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #180

Topic 1

Your operations team currently stores 10 TB of data in an object storage service from a third-party provider. They want to move this data to a

Cloud Storage bucket as quickly as possible, following Google-recommended practices. They want to minimize the cost of this data migration.

Which approach should they use?

A. Use the gsutil mv command to move the data.

B. Use the Storage Transfer Service to move the data.

C. Download the data to a Transfer Appliance, and ship it to Google.

D. Download the data to the on-premises data center, and upload it to the Cloud Storage bucket.

Correct Answer: B

Reference:

https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets

Community vote distribution

B (100%)

? ?  amxexam  Highly Voted ?  1áyear, 1ámonth ago

Selected Answer: B

I would have voted C looking at 10 TB at first glance. But the senario other cloud storge changes the look out. You cannot wre master
applica here. As you wont be able to remove and store data like on prem. Hence is or Starge Transfer service.

upvoted 8 times

? ?  someCloudUser  Most Recent ?  4ámonths, 1áweek ago

Selected Answer: B

Correct answer is B

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago
B is right, please refer this link
https://cloud.google.com/storage-transfer-service

upvoted 2 times

? ?  cloudmon 1áyear, 2ámonths ago

B
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options

upvoted 4 times

? ?  SAMBIT 1áyear, 3ámonths ago

Current storage is object store so mostly a cloud providerà thus storage transfer service

upvoted 3 times

? ?  technodev 1áyear, 5ámonths ago

Got this question in my exam, answered B

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

478/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  AjayPrajapati 1áyear, 5ámonths ago

B
https://cloud.google.com/storage-transfer-service

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I think B or C.
A&D are out. Becaus tha data size is 10TB.

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Can the third-party provider use B StrageTransferService?
If can,B is correct.
There is not enough information.

upvoted 1 times

? ?  [Removed] 1áyear, 5ámonths ago

B Storage Transfer Service is correct.

upvoted 1 times

? ?  victory108 1áyear, 5ámonths ago

B. Use the Storage Transfer Service to move the data.

upvoted 1 times

? ?  spoxman 1áyear, 6ámonths ago

@StelSen: re. B. Being a cloud provider is not a requirement for using STS

upvoted 1 times

? ?  edilramos 1áyear, 6ámonths ago

Selected Answer: B

B: Storage Transfer Service
https://cloud.google.com/storage-transfer-service

upvoted 1 times

? ?  StelSen 1áyear, 6ámonths ago

Very Tricky and don't have enough details to answer the question. Use this Guide (https://cloud.google.com/architecture/migration-to-
google-cloud-transferring-your-large-datasets#transfer-options) and let's try to eliminate options.

A. Use the gsutil mv command to move the data. (We have 10TB, hence rejected)
B. Use the Storage Transfer Service to move the data. (Source might not be Cloud provider. Hence rejecting it. If source is AWS/Azure then
this is the answer)
C. Download the data to a Transfer Appliance, and ship it to Google. (I don't think we can use Transfer Appliance at Third party service
providers DC. Assuming this 3rd party is not a cloud provider)
D. Download the data to the on-premises data center, and upload it to the Cloud Storage bucket. (This seems better assuming DC has
good bandwidth such as 1 Gbbs)

upvoted 4 times

? ?  Dhiraj03 1áyear ago
B is the answer
upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

479/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #181

Topic 1

You have a Compute Engine managed instance group that adds and removes Compute Engine instances from the group in response to the load on

your application. The instances have a shutdown script that removes REDIS database entries associated with the instance. You see that many

database entries have not been removed, and you suspect that the shutdown script is the problem. You need to ensure that the commands in the

shutdown script are run reliably every time an instance is shut down. You create a Cloud Function to remove the database entries. What should

you do next?

A. Modify the shutdown script to wait for 30 seconds before triggering the Cloud Function.

B. Do not use the Cloud Function. Modify the shutdown script to restart if it has not completed in 30 seconds.

C. Set up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud Logging.

D. Modify the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue.

Correct Answer: A

Community vote distribution

C (83%)

Other

? ?  Tesla  Highly Voted ?  8ámonths ago

Actually C is correct but Wrong also in a way .. Sink cannot trigger a cloud function directly. It need Pub/Sub which then will trigger Cloud
Function.

upvoted 7 times

? ?  omermahgoub  Highly Voted ?  6ámonths ago

The correct answer is C: Set up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in
Cloud Logging.

In this scenario, you want to ensure that the commands in the shutdown script are run reliably every time an instance is shut down. One
way to do this is by setting up a Cloud Monitoring sink that triggers a Cloud Function after an instance removal log message arrives in
Cloud Logging. This will allow you to use the Cloud Function to perform the necessary tasks (such as removing database entries) when an
instance is shut down, and it will ensure that these tasks are performed reliably and consistently.

Option A: Modifying the shutdown script to wait for 30 seconds before triggering the Cloud Function is not a reliable solution, as it relies
on the shutdown script being able to run for at least 30 seconds before the instance is shut down.

upvoted 5 times

? ?  omermahgoub 6ámonths ago

Option B: Modifying the shutdown script to restart if it has not completed in 30 seconds is also not a reliable solution, as it may not be
feasible to restart the script if the instance has already been shut down.

Option D: Modifying the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue is not a reliable
solution, as it relies on the shutdown script being able to run for at least 30 seconds before the instance is shut down, and it also
requires additional infrastructure (a Pub/Sub queue) to be set up and maintained.

upvoted 2 times

? ?  Ery  Most Recent ?  4ámonths, 3áweeks ago

Selected Answer: C

cCorrect Ans is C
upvoted 1 times

? ?  samirzubair 6ámonths ago

Correct Ans is C
upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C Is the Correct Answer

upvoted 2 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: C

C is ok

upvoted 2 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is the right answer

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

480/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

c is correct

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is right

upvoted 2 times

? ?  charlie_lee 9ámonths, 1áweek ago

Selected Answer: D

use pub/sub trigger cloud function

upvoted 2 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: C

C is the answer as shutdown script is run based on best effort and not a reliable method.

https://cloud.google.com/compute/docs/shutdownscript#limitations
Compute Engine executes shutdown scripts only on a best-effort basis. In rare cases, Compute Engine cannot guarantee that the
shutdown script will complete.

upvoted 5 times

? ?  kuboraam 9ámonths, 3áweeks ago

Selected Answer: C

C would be the cleanest solution. Although at this time, Cloud Monitoring sink cannot trigger a cloud function directly, it can be done via
Pub/Sub. Still better than solution D.

upvoted 4 times

? ?  ramzez4815 9ámonths, 3áweeks ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  [Removed] 9ámonths, 3áweeks ago

Selected Answer: C

C looks not a professional way, but can make sure the work being done.

upvoted 1 times

? ?  rorz 9ámonths, 3áweeks ago

Selected Answer: C

Set up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud Logging.

upvoted 1 times

? ?  jabrrJ68w02ond1 9ámonths, 4áweeks ago

Selected Answer: A

A seems to be correct.

A is incorrect, since GCP does not guarantee the shutdown time [1].
B is incorrect, restarting a shutdown script after the shutdown period is not possible.
C is going to take some seconds but will do the job.
D is also not possible because GCP does not guarantee the shutdown period of at least 30 seconds [1].

[1] https://cloud.google.com/compute/docs/instances/deleting-instance?hl=en#delete_timeout

"If you choose to run a shutdown script during this period, your shutdown script must finish running within this time period so that the
operating system has time to complete its shutdown and flush buffers to disk.

Note: Compute Engine does not guarantee the length of these shutdown periods and we recommend that you do not create any hard
dependencies on these time limits."

upvoted 2 times

? ?  kiappy81 9ámonths, 3áweeks ago

I cannot see that cloud function can be considered as a supported destination of a cloud monitoring sink, so I cannot select C as the
answer.
https://cloud.google.com/logging/docs/export/configure_export_v2#supported-destinations
Instead it's possible to trigger a cloud function by a pub/sub quewe, so I'd prefer option D

upvoted 1 times

? ?  jabrrJ68w02ond1 9ámonths, 4áweeks ago

Edit: Of course C is correct, I messed the vote.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

481/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #182

Topic 1

You are managing several projects on Google Cloud and need to interact on a daily basis with BigQuery, Bigtable, and Kubernetes Engine using the

gcloud CL tool. You are travelling a lot and work on different workstations during the week. You want to avoid having to manage the gcloud CLI

manually. What should you do?

A. Use Google Cloud Shell in the Google Cloud Console to interact with Google Cloud.

B. Create a Compute Engine instance and install gcloud on the instance. Connect to this instance via SSH to always use the same gcloud

installation when interacting with Google Cloud.

C. Install gcloud on all of your workstations. Run the command gcloud components auto-update on each workstation

D. Use a package manager to install gcloud on your workstations instead of installing it manually.

Correct Answer: A

Reference:

https://cloud.google.com/sdk/gcloud

Community vote distribution

A (100%)

? ?  alexandercamachop  Highly Voted ?  9ámonths, 2áweeks ago

Selected Answer: A

First discard:
C / D are totally not it. Since they will do so much work and it says "do not want to manage gcoud CLI manually"
Then B is not a good cost option, besides at the end you are managing your gcloud cli manually. So A is the only left correct answer.
It even saves your $HOME files.

upvoted 7 times

? ?  surajkrishnamurthy  Most Recent ?  6ámonths, 2áweeks ago

Selected Answer: A

A Is the Correct Answer

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is right

upvoted 1 times

? ?  kiappy81 9ámonths, 3áweeks ago

Selected Answer: A

If you're using Cloud Shell, the gcloud CLI is available automatically and you don't need to install it. commadn gcloud components auto-
update doesn't exist but the command gcloud components update ensure that you are using updated version of your components.

upvoted 4 times

? ?  jabrrJ68w02ond1 9ámonths, 4áweeks ago

Selected Answer: A

A is the correct solution, as the only requirement for your workstation is to have a browser installed.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

482/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #183

Topic 1

Your company recently acquired a company that has infrastructure in Google Cloud. Each company has its own Google Cloud organization. Each

company is using a Shared Virtual Private Cloud (VPC) to provide network connectivity for its applications. Some of the subnets used by both

companies overlap. In order for both businesses to integrate, the applications need to have private network connectivity. These applications are

not on overlapping subnets. You want to provide connectivity with minimal re-engineering. What should you do?

A. Set up VPC peering and peer each Shared VPC together.

B. Migrate the projects from the acquired company into your company's Google Cloud organization. Re-launch the instances in your

companies Shared VPC.

C. Set up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs.

D. Con gure SSH port forwarding on each application to provide connectivity between applications in the different Shared VPCs.

Correct Answer: C

Community vote distribution

C (76%)

A (17%)

7%

? ?  6721sora  Highly Voted ?  9ámonths, 3áweeks ago

Selected Answer: C

VPC peering cannot be established between VPCs if there is IP range overlap. C is ok since you can establish VPN across these VPCs and
only include the applications required IP ranges as its mentioned that they do not overlap

upvoted 13 times

? ?  ilcasta73  Highly Voted ?  9ámonths, 3áweeks ago

It's A
The applications need to have private network connectivity.
These applications ARE NOT on overlapping subnets

upvoted 6 times

? ?  medi01 2ámonths, 1áweek ago

But networks are... which will stop the peering.

upvoted 2 times

? ?  jlambdan  Most Recent ?  1ámonth, 1áweek ago

Selected Answer: C

looks like the following best practice: https://cloud.google.com/architecture/best-practices-vpc-design#shared-service

Cloud VPN is another alternative. Because Cloud VPN establishes reachability through managed IPsec tunnels, it doesn't have the
aggregate limits of VPC Network Peering. Cloud VPN uses a VPN Gateway for connectivity and doesn't consider the aggregate resource
use of the IPsec peer. The drawbacks of Cloud VPN include increased costs (VPN tunnels and traffic egress), management overhead
required to maintain tunnels, and the performance overhead of IPsec.

upvoted 1 times

? ?  kratosmat 2ámonths, 2áweeks ago

Selected Answer: C

It seems to be C because VPN peering use BGP protocol that manages the overlaps.
https://cloud.google.com/network-connectivity/docs/vpn/how-to/configuring-peer-gateway

upvoted 1 times

? ?  HD2023 2ámonths, 3áweeks ago

Selected Answer: C

omermahgoub said it best. C

upvoted 1 times

? ?  rr4444 3ámonths, 2áweeks ago

Selected Answer: B

B to reorg it all under one org cos it's a mess.
You cant have shared RFC1918 ranges between peered networks OR VPNs... Don't know why everyone thinks VPNs avoid that problem.
https://cloud.google.com/network-connectivity/docs/vpn/how-to/creating-ha-vpn2 "You can connect two VPC networks together as long
as the primary and secondary subnet IP address ranges in each network don't overlap."

upvoted 1 times

? ?  BiddlyBdoyng 1áweek, 5ádays ago

This applies to static routes only. "A dynamic route can overlap with a subnet route in a peer network. For dynamic routes, the
destination ranges that overlap with a subnet route from the peer network are silently dropped. Google Cloud uses the subnet route.".

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

483/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

https://cloud.google.com/vpc/docs/vpc-peering

upvoted 1 times

? ?  RVivek 4ámonths, 3áweeks ago

Selected Answer: C

Please check answer from BalaGCPArch

"https://cloud.google.com/vpc/docs/vpc-peering#overlapping_subnets_at_time_of_peering
Overlapping subnets at time of peering
At the time of peering, Google Cloud checks to see if there are any subnets with overlapping IP ranges between the two VPC networks or
any of their peered networks. If there is an overlap, peering is not established. Since a full mesh connectivity is created between VM
instances, subnets in the peered VPC networks can't have overlapping IP ranges as this would cause routing issues."

upvoted 1 times

? ?  omermahgoub 6ámonths ago

VPC peering is generally possible even if there are overlapping subnets between the two VPCs. However, there are some considerations to
keep in mind if there's overlapping subnets:
1. You will not be able to route traffic between the overlapping subnets. If needed, you will have to use a different method (such as a
Cloud VPN connection or a Cloud Router) to connect the VPCs.
2. You will need to ensure that the overlapping subnets are not used by any resources in either VPC. This means that you will need to
either modify the existing network configuration to avoid using the overlapping subnets, or you will need to create new subnets that do
not overlap.
3. You may need to update any existing firewall rules or routes that refer to the overlapping subnets to ensure that they are still valid after
the VPCs are peered.

In the question, you want to provide private network connectivity between the two companies' applications, which are not on overlapping
subnets. However, there is overlap in the subnets used by both companies, which means that you will not be able to use VPC peering to
connect the two VPCs.

upvoted 5 times

? ?  omermahgoub 6ámonths ago

One solution in this case would be to set up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs. This will allow you to create
a secure, private network connection between the two VPCs, and it will allow the applications in each company's Shared VPC to
communicate with each other over the private connection.

The correct answer is C: Set up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs.

Option A: Setting up VPC peering and peering each Shared VPC together would not be a viable solution in this case, because the
subnets used by both companies overlap, and VPC peering does not support overlapping subnets.

upvoted 5 times

? ?  colombrican 6ámonths, 2áweeks ago

Selected Answer: C

Answer is C
A is wrong because you cannot peer VPCs with overlapping subnets:
https://cloud.google.com/vpc/docs/vpc-peering#interaction-subnet-subnet
IPv4 subnet routes in peered VPC networks can't overlap:
- Peering prohibits identical IPv4 subnet routes. For example, two peered VPC networks can't both have an IPv4 subnet route whose
destination is 100.64.0.0/10.
- Peering prohibits a subnet route from being contained within a peering subnet route. For example, if the local VPC network has a subnet
route whose destination is 100.64.0.0/24, then none of the peered VPC networks can have a subnet route whose destination is
100.64.0.0/10.

B and D are ruled out because it breaks the requirement "with minimal re-engineering" to the applications

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: A

A is ok: The applications are not on overlapping subnets. So use VPC peering.
You want to provide connectivity with minimal re-engineering. VPC Network Peering
accomplishes this.
https://cloud.google.com/vpc/docs/vpc-peering

upvoted 3 times

? ?  BalaGCPArch 6ámonths, 3áweeks ago

C should be the Answer : same explaination goes here

"https://cloud.google.com/vpc/docs/vpc-peering#overlapping_subnets_at_time_of_peering

Overlapping subnets at time of peering
At the time of peering, Google Cloud checks to see if there are any subnets with overlapping IP ranges between the two VPC networks
or any of their peered networks. If there is an overlap, peering is not established. Since a full mesh connectivity is created between VM
instances, subnets in the peered VPC networks can't have overlapping IP ranges as this would cause routing issues."

upvoted 5 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

484/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C is correct answer

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is best option
upvoted 1 times

? ?  charlie_lee 9ámonths, 1áweek ago

Selected Answer: A

These applications ARE NOT on overlapping subnets

upvoted 2 times

? ?  SerGCP 9ámonths ago

https://cloud.google.com/vpc/docs/vpc-peering#overlapping_subnets_at_time_of_peering

Overlapping subnets at time of peering
At the time of peering, Google Cloud checks to see if there are any subnets with overlapping IP ranges between the two VPC networks
or any of their peered networks. If there is an overlap, peering is not established. Since a full mesh connectivity is created between VM
instances, subnets in the peered VPC networks can't have overlapping IP ranges as this would cause routing issues.

upvoted 3 times

? ?  sTree100 9ámonths, 1áweek ago

the answer is A
upvoted 1 times

? ?  kiappy81 9ámonths, 2áweeks ago

Selected Answer: B

Can someone explain my why not B?
It's fine to eliminate A due to overlapping and also D because is out of discussion, but why C is better than B?

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

you need minimal re-engineering. migrating projects and relaunching instances will be a significant effort.

upvoted 4 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: C

Google Documentation " When a VPC subnet is created or a subnet IP range is expanded, Google Cloud performs a check to make sure
the new subnet range does not overlap with IP ranges "
I know the subnets where the application is hosted, does not overlap, however it will not allow a VPC peering because of that overlap, so
the only possible answer is C.

https://cloud.google.com/vpc/docs/vpc-peering

upvoted 1 times

? ?  hisunilarora 9ámonths, 3áweeks ago

Selected Answer: C

https://cloud.google.com/vpc/docs/vpc-peering
Read the section - Overlapping subnets at the time of peering. VPC peering is not possible with overlapping subnets. So, A is wrong and C
is the correct answer.

upvoted 2 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

" At the time of peering, Google Cloud checks to see if there are any subnets with overlapping IP ranges between the two VPC networks
or any of their peered networks. If there is an overlap, peering is not established. Since a full mesh connectivity is created between VM
instances, subnets in the peered VPC networks can't have overlapping IP ranges as this would cause routing issues."
Agree, it has to be C

upvoted 1 times

? ?  Ishu_awsguy 9ámonths, 2áweeks ago

Wrong.
The applications are not on overlapping subnets. So use VPC peering

upvoted 1 times

? ?  BalaGCPArch 6ámonths, 3áweeks ago

But Application resides in the the VPC which has Subnet overlapping and this peering they mention in option A is VPC, so it chould
be Option C

upvoted 1 times

? ?  Ishu_awsguy 9ámonths, 2áweeks ago

A is the correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

485/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #184

Topic 1

You are managing several internal applications that are deployed on Compute Engine. Business users inform you that an application has become

very slow over the past few days. You want to  nd the underlying cause in order to solve the problem. What should you do  rst?

A. Inspect the logs and metrics from the instances in Cloud Logging and Cloud Monitoring.

B. Change the Compute Engine Instances behind the application to a machine type with more CPU and memory.

C. Restore a backup of the application database from a time before the application became slow.

D. Deploy the applications on a managed instance group with autoscaling enabled. Add a load balancer in front of the managed instance

group, and have the users connect to the IP of the load balancer.

Correct Answer: D

Community vote distribution

A (100%)

? ?  aut0pil0t  Highly Voted ?  9ámonths, 4áweeks ago

Selected Answer: A

First thing to do is to inspect logs and monitoring to see what is happening

upvoted 16 times

? ?  AugustoKras011111  Most Recent ?  4ámonths ago

Selected Answer: A

Key Word "find the underlying cause", so the answer is A.

upvoted 2 times

? ?  RVivek 4ámonths, 3áweeks ago

Selected Answer: A

Question mentions "You want to find the underlying cause in order to solve the problem"
B, C and D are attempt to solve the problem without finding the cause

upvoted 4 times

? ?  Wael216 6ámonths ago

this has nothing to do with "gcp" in real, this is SRE instinct

upvoted 3 times

? ?  omermahgoub 6ámonths ago

When an application becomes slow, the first step you should take is to gather information about the underlying cause of the problem.
One way to do this is by inspecting the logs and metrics from the instances where the application is deployed. Google Cloud Platform
(GCP) provides tools such as Cloud Logging and Cloud Monitoring that can help you to collect and analyze this information.

By reviewing the logs and metrics from the instances, you may be able to identify issues such as resource shortages (e.g. CPU, memory, or
disk), network problems, or application errors that are causing the performance issues. Once you have identified the underlying cause of
the problem, you can take steps to resolve it.

The correct answer is A: Inspect the logs and metrics from the instances in Cloud Logging and Cloud Monitoring.

upvoted 3 times

? ?  omermahgoub 6ámonths ago

Option D: Deploying the applications on a managed instance group with autoscaling enabled and adding a load balancer in front of the
managed instance group may help to improve the performance of the application, but it is not necessarily the first step you should
take. You should first try to understand the underlying cause of the performance issues before making changes to the deployment
architecture.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

Option B: Changing the Compute Engine instances behind the application to a machine type with more CPU and memory may help
to improve the performance of the application, but it is not necessarily the first step you should take. You should first try to
understand the underlying cause of the performance issues before making changes to the instances.

Option C: Restoring a backup of the application database from a time before the application became slow may help to resolve the
performance issues if the problem is related to the database. However, it is not necessarily the first step you should take, as there
may be other issues causing the performance problems.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: A

A Is the Correct Answer

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

486/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

This is no brainer question, I would choose A

upvoted 2 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: A

Agree with A.
First remove any non possible answers: B / C.
Then we have A or D left.
But D does a good action / recommended action but it says "what do we do first" which is always troubleshoot.

upvoted 3 times

? ?  rorz 9ámonths, 3áweeks ago

Selected Answer: A

First thing would be to inspect the logs

upvoted 1 times

? ?  EricG77 9ámonths, 4áweeks ago

Answe is A. I would agree the question is stating "You want to find the underlying cause in order to solve the problem."
Everything else is making changes without understanding the issues at hand

upvoted 1 times

? ?  jabrrJ68w02ond1 9ámonths, 4áweeks ago

A is the only answer that is really caring about **analyzing** the underlying problem before **touching** anything.

upvoted 2 times

? ?  rhage_56 9ámonths, 4áweeks ago

Selected Answer: A

key word is "You want to find the underlying cause in order to solve the problem"

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

487/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #185

Topic 1

Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. When releasing new versions of the

application via a rolling deployment, the team has been causing outages. The root cause of the outages is miscon gurations with parameters that

are only used in production. You want to put preventive measures for this in the platform to prevent outages. What should you do?

A. Con gure liveness and readiness probes in the Pod speci cation.

B. Con gure health checks on the managed instance group.

C. Create a Scheduled Task to check whether the application is available.

D. Con gure an uptime alert in Cloud Monitoring.

Correct Answer: B

Community vote distribution

A (100%)

? ?  jabrrJ68w02ond1  Highly Voted ?  9ámonths, 4áweeks ago

Selected Answer: A

A: Configuring the right liveness and readiness probes prevents outages when rolling out a new ReplicaSet of a Deployment, because
Pods are only getting traffic when they are considered ready.
B: With GKE, you do not deal with MIGs.
C: Does not use GKE tools and is therefore not the best option.
D: Does alert you but does not prevent the outage.

upvoted 13 times

? ?  khadar 9ámonths, 3áweeks ago

more explanation in the below link..https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-setting-
up-health-checks-with-readiness-and-liveness-probes

upvoted 6 times

? ?  red_panda  Most Recent ?  1áweek, 2ádays ago

Selected Answer: A

A without any doubts

upvoted 1 times

? ?  mimicha1 1áweek, 5ádays ago

D. Configure an uptime alert in Cloud Monitoring.
Configuring an uptime alert in Cloud Monitoring will notify the team when the application becomes unavailable. This will help in detecting
outages before they occur and mitigate the risks of releasing new versions with misconfigurations.
While configuring liveness and readiness probes in the Pod specification and configuring health checks on the managed instance group
are important for ensuring that the application is running, they do not prevent outages caused by misconfigurations with production
parameters.
Creating a Scheduled Task to check whether the application is available is also useful, but it is not preventive in nature. By the time a
scheduled task detects an outage, the damage may have already been done.

upvoted 1 times

? ?  mimicha1 1áweek, 5ádays ago

why not A ?
* Configuring liveness and readiness probes in the Pod specification is important to detect when a container in a Pod becomes
unresponsive or starts experiencing problems. However, it does not directly prevent outages caused by misconfigurations with
parameters that are only used in production.

Liveness and readiness probes can help to detect issues with the application, but they do not provide information about the health of
the underlying infrastructure. Misconfigurations with parameters that are only used in production can cause problems with the
infrastructure itself, which may not be detected by liveness and readiness probes.

In summary, while configuring liveness and readiness probes is important, it should be done in addition to other preventive measures
such as configuring an uptime alert in Cloud Monitoring to ensure timely detection of outages and reduce their impact on the
application.

upvoted 1 times

? ?  CGS22 3ámonths, 3áweeks ago

Selected Answer: A

Configure liveness and readiness probes in the Pod specification.

This will help to prevent outages by ensuring that only healthy Pods are serving traffic. The liveness probe will check that the Pod is
running and responding to requests. The readiness probe will check that the Pod is ready to serve traffic, such as by checking that the
application is installed and configured.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

488/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  omermahgoub 6ámonths ago

Liveness and readiness probes are used to determine the health of a Pod. Liveness probes are used to determine whether a Pod is
running, and readiness probes are used to determine whether a Pod is able to receive traffic.

By configuring liveness and readiness probes in the Pod specification, you can help to prevent outages when releasing new versions of the
application via a rolling deployment. If a Pod fails a liveness or readiness probe, it will be restarted, which can help to prevent issues
caused by misconfigured parameters or other problems.
The correct answer is A: Configure liveness and readiness probes in the Pod specification.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

Option B: Configuring health checks on the managed instance group is not relevant in this scenario, as the application is running in a
GKE cluster, not on a managed instance group.

Option C: Creating a Scheduled Task to check whether the application is available may help to detect outages, but it will not prevent
them from occurring. To prevent outages, you should focus on identifying and addressing the root cause of the problem.

Option D: Configuring an uptime alert in Cloud Monitoring may help to detect outages, but it will not prevent them from occurring. To
prevent outages, you should focus on identifying and addressing the root cause of the problem.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: A

A Is the Correct Answer

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is best answer
upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: A

A is the answer.

Kubernetes Health Checks with Readiness and Liveness Probes
https://www.youtube.com/watch?v=mxEvAPQRwhw

upvoted 2 times

? ?  rhage_56 9ámonths, 4áweeks ago

Selected Answer: A

B is out since MIGs relate to compute engine. D and C are both not preventive measures.

upvoted 1 times

? ?  spET_1024 9ámonths, 4áweeks ago

Option A is correct. Since it is regarding GKE and the application deployed in GKE cluster. Therefore, managed instance group does not
have anything to do.
So, right answer is:
A. Configure liveness and readiness probes in the Pod specification.

upvoted 2 times

? ?  aut0pil0t 9ámonths, 4áweeks ago

Selected Answer: A

A.
There are no MIGs in GKE. Only thing that makes sense is to have good readiness probes

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

489/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #186

Topic 1

Your company uses Google Kubernetes Engine (GKE) as a platform for all workloads. Your company has a single large GKE cluster that contains

batch, stateful, and stateless workloads. The GKE cluster is con gured with a single node pool with 200 nodes. Your company needs to reduce the

cost of this cluster but does not want to compromise availability. What should you do?

A. Create a second GKE cluster for the batch workloads only. Allocate the 200 original nodes across both clusters.

B. Con gure CPU and memory limits on the namespaces in the cluster. Con gure all Pods to have a CPU and memory limits.

C. Con gure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Con gure the cluster to use

node auto scaling.

D. Change the node pool to use preemptible VMs.

Correct Answer: B

Community vote distribution

C (100%)

? ?  jabrrJ68w02ond1  Highly Voted ?  9ámonths, 4áweeks ago

Selected Answer: C

A: Is not necessary because you can have multiple node pools with different configurations.
B: Optimizes resource usage of CPU/memory in your existing node pool but does not necessarily improve cost - still an option that should
be considered.
C: This looks really good. Autoscaling workloads and the node pools makes your whole infrastructure more elastic and gives you the
option to rely on the same node pool.
D: This might not be a good option for every type of workload. Batch and stateless workloads can often handle this quite well, but stateful
workloads are not well-suited for operation on preemptible VMs.

Since only one answer is accepted, I'll choose C.

upvoted 12 times

? ?  ramzez4815  Highly Voted ?  9ámonths, 3áweeks ago

Selected Answer: C

C is the correct answer as it doesn't involve major changes to the current Kubernetes configuration

upvoted 8 times

? ?  AugustoKras011111  Most Recent ?  4ámonths ago

Selected Answer: C

Answer C. Use HorizontalPodAutoscaler.

upvoted 1 times

? ?  zerg0 4ámonths, 3áweeks ago

Selected Answer: C

HorizontalPodAutoscaler is the way

upvoted 1 times

? ?  tdotcat 5ámonths, 2áweeks ago

Selected Answer: C

c is good

upvoted 1 times

? ?  omermahgoub 6ámonths ago

The correct answer is C: Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads.
Configure the cluster to use node auto scaling.

One way to reduce the cost of a Google Kubernetes Engine (GKE) cluster without compromising availability is to use horizontal pod
autoscalers (HPA) and node auto scaling.

HPA allows you to automatically scale the number of Pods in a deployment based on the resource usage of the Pods. By configuring HPA
for stateless workloads and for compatible stateful workloads, you can ensure that the number of Pods is automatically adjusted based on
the actual resource usage, which can help to reduce costs.

Node auto scaling allows you to automatically add or remove nodes from the node pool based on the resource usage of the cluster. By
configuring node auto scaling, you can ensure that the cluster has the minimum number of nodes needed to meet the resource
requirements of the workloads, which can also help to reduce costs.

upvoted 3 times

? ?  omermahgoub 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

490/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A: Creating a second GKE cluster for the batch workloads only and allocating the 200 original nodes across both clusters would not
necessarily help to reduce costs, as the total number of nodes in the clusters would remain the same.

B: Configuring CPU and memory limits on the namespaces in the cluster and configuring all Pods to have CPU and memory limits may
help to reduce costs, but it is not sufficient on its own. You should also use HPA and node auto scaling to ensure that the cluster is
properly sized based on the actual resource usage.

D: Changing the node pool to use preemptible VMs may help to reduce costs, but it is not sufficient on its own. Preemptible VMs can be
terminated at any time, which may not be suitable for all workloads. You should also use HPA and node auto scaling to ensure that the
cluster is properly sized based on the actual resource usage.

upvoted 2 times

? ?  ale_brd_ 6ámonths, 1áweek ago

Selected Answer: C

C is the correct answer as it doesn't involve major changes to the current Kubernetes configuration

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is correct

upvoted 1 times

? ?  rorz 9ámonths, 3áweeks ago

Selected Answer: C

C is correc

upvoted 1 times

? ?  aswani 9ámonths, 3áweeks ago

Selected Answer: C

Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use
node auto scaling

upvoted 2 times

? ?  spET_1024 9ámonths, 4áweeks ago

Option C is correct. Since, the company does not want to compromise availability of the application so, HPA is suitable option for
autoscaling pods. Keeping the cost optimization in mind, nodes of the GKE cluster also needs to be autoscaled. Therefore, the correct
option is,
C. Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use
node auto scaling.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

491/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #187

Topic 1

Your company has a Google Cloud project that uses BigQuery for data warehousing on a pay-per-use basis. You want to monitor queries in real

time to discover the most costly queries and which users spend the most. What should you do?

A. 1. In the BigQuery dataset that contains all the tables to be queried, add a label for each user that can launch a query. 2. Open the Billing

page of the project. 3. Select Reports. 4. Select BigQuery as the product and  lter by the user you want to check.

B. 1. Create a Cloud Logging sink to export BigQuery data access logs to BigQuery. 2. Perform a BigQuery query on the generated table to

extract the information you need.

C. 1. Create a Cloud Logging sink to export BigQuery data access logs to Cloud Storage. 2. Develop a Data ow pipeline to compute the cost of

queries split by users.

D. 1. Activate billing export into BigQuery. 2. Perform a BigQuery query on the billing table to extract the information you need.

Correct Answer: A

Community vote distribution

B (78%)

D (17%)

6%

? ?  kuboraam  Highly Voted ?  9ámonths, 3áweeks ago

Selected Answer: B

I choose B because of "real-time". Otherwise, D seems to be the most relevant and flexible.

upvoted 9 times

? ?  VSMu 4ámonths, 3áweeks ago

D also can be continuous https://cloud.google.com/billing/docs/how-to/export-data-bigquery#setup. I think D is the right answer.

upvoted 6 times

? ?  omermahgoub  Highly Voted ?  6ámonths ago

The correct answer is D: 1. Activate billing export into BigQuery. 2. Perform a BigQuery query on the billing table to extract the information
you need.

To monitor queries in real time and discover the most costly queries and which users spend the most in BigQuery, you can activate billing
export into BigQuery. This will allow you to access detailed billing information for your Google Cloud project in real time, including
information about the cost of BigQuery queries.

Once you have activated billing export into BigQuery, you can perform a BigQuery query on the billing table to extract the information you
need. This will allow you to analyze the data and identify the most costly queries and the users who are spending the most.

upvoted 6 times

? ?  omermahgoub 6ámonths ago

1. Activate billing export into BigQuery: This will allow you to access a table in BigQuery that contains information about the cost of
queries run in your project. To enable billing export, follow these steps:
- Go to the Billing page of your Google Cloud project.
- In the top left and select "billing Export."
- Follow the prompts to enable billing export and specify the BigQuery dataset and table where you want the data to be exported.
- Perform a BigQuery query on the billing table: Once you have enabled billing export and the data has started flowing into the
specified BigQuery table, you can run a query on the table to extract the information you need. For example, you could use a query like
the following to get the total cost of queries split by user:

SELECT user, SUM(cost) as total_cost
FROM your-project.your-dataset.billing_export
GROUP BY user
ORDER BY total_cost DESC


upvoted 5 times

? ?  omermahgoub 6ámonths ago

This query will give you a list of users and the total cost of the queries they have run, sorted by the highest cost first. You can then
use this information to identify the most costly queries and users and take steps to optimize them if necessary.

Option A: Adding labels to the BigQuery dataset and filtering the Reports page in the project's Billing page would not provide real-
time information about BigQuery query costs.

Option B: Exporting BigQuery data access logs to BigQuery and performing a query on the generated table would not provide
information about the cost of BigQuery queries.

Option C: Exporting BigQuery data access logs to Cloud Storage and developing a Dataflow pipeline to compute the cost of queries
split by users would not provide real-time information about BigQuery query costs.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

492/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  red_panda  Most Recent ?  1áweek, 2ádays ago

Selected Answer: D

For me, most simple and also available in real time is D

upvoted 1 times

? ?  JC0926 2ámonths, 3áweeks ago

Selected Answer: B

Option D might seem like a reasonable choice, but it doesn't provide real-time monitoring of queries, which is the requirement mentioned
in the question.

Activating billing export to BigQuery provides detailed billing information for your Google Cloud project. However, this method doesn't
provide real-time insights into query costs and user expenditures, as billing data is typically updated once per day.

On the other hand, option B allows you to monitor queries in real-time by exporting BigQuery data access logs directly to another
BigQuery table, enabling you to analyze the most costly queries and user expenses as they happen.

upvoted 1 times

? ?  JC0926 3ámonths ago

Selected Answer: D

D
Exporting BigQuery data access logs to BigQuery and performing a query on the generated table would not provide information about
the cost of BigQuery queries.

upvoted 1 times

? ?  msmamrs 3ámonths, 1áweek ago

Selected Answer: D

B won't give costs

upvoted 2 times

? ?  rr4444 3ámonths, 2áweeks ago

Selected Answer: D

D, cos B won't give costs

upvoted 2 times

? ?  tdotcat 5ámonths, 2áweeks ago

Selected Answer: B

B seems backed by google's blog

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: B

B is ok

upvoted 2 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

B is the correct answer https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring
A is incorrect as there is not billing page for a project, its billing account that handles all org billing.

upvoted 5 times

? ?  jlambdan 1ámonth, 1áweek ago

"details about the query that was executed, like the SQL code, the job ID and, most important, the user who executed the query and the
amount of data that was processed. With that information, you can compute the total cost of the query using a simple multiplication
equation: cost per TB processed * numbers of TB processed" means it will be an estimation, not the real numbers.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I will go with B
upvoted 1 times

? ?  ShadowLord 9ámonths, 1áweek ago

Selected Answer: A

It also need details on which user spend more time .. Seems like answer is A considering both aspects
As this need both query as well time spent by users

upvoted 2 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: B

B is my answer.

https://cloud.google.com/blog/products/data-analytics/taking-a-practical-approach-to-bigquery-cost-monitoring

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

493/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  ShadowLord 9ámonths, 1áweek ago

It also need details on which user spend more time .. Seems like answer is A considering both aspects

upvoted 2 times

? ?  ShadowLord 9ámonths, 1áweek ago

After more detailed consideration .. B seems alright

upvoted 1 times

? ?  6721sora 9ámonths, 3áweeks ago

Selected Answer: B

B because D is billing export which, I'm guessing, is not likely to contain user level data...A seems too far fetched and complicated even if
technically feasible.

upvoted 2 times

? ?  [Removed] 9ámonths, 3áweeks ago

Selected Answer: B

I would choose B, since 1. need to get result in real time. 2 . The result need to be categorized by specific queries, and users. So only log
can achieve that.
upvoted 4 times

? ?  ilcasta73 9ámonths, 4áweeks ago

I think it is D

upvoted 4 times

? ?  ilcasta73 9ámonths, 3áweeks ago

https://cloud.google.com/billing/docs/how-to/export-data-bigquery

upvoted 3 times

? ?  khadar 9ámonths, 3áweeks ago

more explanation for this question in the below link..https://helpcenter.itopia.com/en/articles/1306325-enable-billing-export-to-
bigquery

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

494/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #188

Topic 1

Your company and one of its partners each have a Google Cloud project in separate organizations. Your company's project (prj-a) runs in Virtual

Private Cloud

(vpc-a). The partner's project (prj-b) runs in vpc-b. There are two instances running on vpc-a and one instance running on vpc-b. Subnets de ned in

both VPCs are not overlapping. You need to ensure that all instances communicate with each other via internal IPs, minimizing latency and

maximizing throughput. What should you do?

A. Set up a network peering between vpc-a and vpc-b.

B. Set up a VPN between vpc-a and vpc-b using Cloud VPN.

C. Con gure IAP TCP forwarding on the instance in vpc-b, and then launch the following gcloud command from one of the instances in vpc-a

gcloud: gcloud compute start-iap-tunnel INSTANCE_NAME_IN_VPC_8 22 \ --local-host-port=localhost:22

D. 1. Create an additional instance in vpc-a. 2. Create an additional instance in vpc-b. 3. Install OpenVPN in newly created instances. 4.

Con gure a VPN tunnel between vpc-a and vpc-b with the help of OpenVPN.

Correct Answer: A

Community vote distribution

A (100%)

? ?  zellck  Highly Voted ?  9ámonths, 2áweeks ago

Selected Answer: A

definitely A.

https://cloud.google.com/vpc/docs/vpc-peering
Google Cloud VPC Network Peering allows internal IP address connectivity across two Virtual Private Cloud (VPC) networks regardless of
whether they belong to the same project or the same organization.

upvoted 8 times

? ?  megumin  Most Recent ?  7ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 2 times

? ?  jake_edman 7ámonths, 1áweek ago

Selected Answer: A

Clearly A as the IPs do not overlap

upvoted 2 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is the correct answer as per https://cloud.google.com/vpc/docs/vpc-peering

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is right answer
upvoted 2 times

? ?  6721sora 9ámonths, 3áweeks ago

Selected Answer: A

Clearly A

upvoted 2 times

? ?  rorz 9ámonths, 3áweeks ago

Selected Answer: A

A - VPC peering should be good

upvoted 1 times

? ?  aswani 9ámonths, 3áweeks ago

Selected Answer: A

It should be A
upvoted 1 times

? ?  kiappy81 9ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

495/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

network peering is fine

upvoted 1 times

? ?  ilcasta73 9ámonths, 4áweeks ago

It should be A
upvoted 1 times

? ?  rhage_56 9ámonths, 4áweeks ago

Selected Answer: A

peering is better as both orgs are in GCP

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

496/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #189

Topic 1

You want to store critical business information in Cloud Storage buckets. The information is regularly changed, but previous versions need to be

referenced on a regular basis. You want to ensure that there is a record of all changes to any information in these buckets. You want to ensure

that accidental edits or deletions can be easily rolled back. Which feature should you enable?

A. Bucket Lock

B. Object Versioning

C. Object change noti cation

D. Object Lifecycle Management

Correct Answer: B

Reference:

https://cloud.google.com/storage/docs/object-versioning

Community vote distribution

B (100%)

? ?  namesgeo 3ámonths, 2áweeks ago

Object Versioning is a feature that allows you to store multiple versions of an object in Cloud Storage. Hence, answer should be B

upvoted 3 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  samsonakala 8ámonths ago

Most definitely B
upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

B is the correct answer as per https://cloud.google.com/storage/docs/object-versioning

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B - Object versioning

upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: B

Object versioning, super important to be able to rollback in case of any deletion.

upvoted 4 times

? ?  kiappy81 9ámonths, 3áweeks ago

Selected Answer: B

https://cloud.google.com/storage/docs/object-versioning

upvoted 3 times

? ?  khadar 9ámonths, 3áweeks ago

I too got this question in 10-09-22 exam with similar option and result is pass

upvoted 7 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

497/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #190

Topic 1

You have a Compute Engine application that you want to autoscale when total memory usage exceeds 80%. You have installed the Cloud

Monitoring agent and con gured the autoscaling policy as follows:
? Metric identi er: agent.googleapis.com/memory/percent_used
? Filter: metric.label.state = 'used'
? Target utilization level: 80
? Target type: GAUGE
You observe that the application does not scale under high load. You want to resolve this. What should you do?

A. Change the Target type to DELTA_PER_MINUTE.

B. Change the Metric identi er to agent.googleapis.com/memory/bytes_used.

C. Change the  lter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state =

'slab'.

D. Change the  lter to metric.label.state = 'free' and the Target utilization to 20.

Correct Answer: A

Community vote distribution

C (47%)

A (38%)

D (15%)

? ?  Mahmoud_E  Highly Voted ?  8ámonths, 1áweek ago

Selected Answer: C

C is correct answer:
A. Change the Target type to DELTA_PER_MINUTE. (in this case the utlization tagret need to be in minutes which is not the case its
percentage % and not time based.
B. Change the Metric identifier to agent.googleapis.com/memory/bytes_used. (not applicable)
C. Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND
metric.label.state = 'slab'. (this gives total memory used)
D. Change the filter to metric.label.state = 'free' and the Target utilization to 20. (you would still need to change the the percent_used to
percent_free)

https://stackoverflow.com/questions/69267526/what-is-disk-data-cached-in-the-memory-usage-chart-metrics-of-gcp-compute-in

https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics

upvoted 22 times

? ?  kiappy81  Highly Voted ?  9ámonths, 3áweeks ago

Selected Answer: A

TARGET_TYPE: the value type for the metric.
gauge: the autoscaler computes the average value of the data collected in the last couple of minutes and compares that to the utilization
target.
delta-per-minute: the autoscaler calculates the average rate of growth per minute and compares that to the utilization target.
delta-per-second: the autoscaler calculates the average rate of growth per second and compares that to the utilization target. For accurate
comparisons, if you set the utilization target in seconds, use delta-per-second as the target type. Likewise, use delta-per-minute for a
utilization target in minutes.

upvoted 13 times

? ?  sssss1  Most Recent ?  1áday, 20áhours ago

Selected Answer: C

Key words: "when total memory usage exceeds 80%"

Correct answer: C

source: https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage

upvoted 1 times

? ?  kapa900 4ádays, 2áhours ago

Selected Answer: A

https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage

upvoted 1 times

? ?  kapa900 4ádays, 2áhours ago

Selected Answer: C

C
https://cloud.google.com/compute/docs/autoscaler/scaling-cloud-monitoring-metrics#autoscale_based_on_memory_usage

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

498/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  red_panda 1áweek, 2ádays ago

Selected Answer: A

A is correct. With C we are autoscale with totally sum of memory, and we do not want to scale except for the memory used

upvoted 1 times

? ?  TheCloudGuruu 1ámonth, 1áweek ago

Selected Answer: A

A is the best option

upvoted 2 times

? ?  mateuszma 2ámonths, 1áweek ago

Selected Answer: D

Right answer is D

upvoted 1 times

? ?  JC0926 2ámonths, 3áweeks ago

Selected Answer: D

D. Change the filter to metric.label.state = 'free' and the Target utilization to 20.

The original filter you used (metric.label.state = 'used') is not suitable for measuring memory usage, as it only considers the "used"
memory and ignores other aspects such as buffered, cached, and slab memory.

By changing the filter to metric.label.state = 'free' and adjusting the target utilization level to 20, you will be able to monitor the free
memory, which will provide a more accurate representation of memory usage. When the free memory drops to 20% or below, the
autoscaling policy will be triggered, allowing the application to scale under high load.

upvoted 2 times

? ?  JC0926 2ámonths, 2áweeks ago

B. Change the Metric identifier to agent.googleapis.com/memory/bytes_used:
This option would change the metric from a percentage-based metric to an absolute bytes-based metric. While this could potentially
work, it would require adjusting the target utilization level to represent an absolute number of bytes instead of a percentage. The
original question asked to resolve the issue with minimal changes, and changing the target utilization level would require additional
configuration.

C. Change the filter to metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND
metric.label.state = 'slab':
The filter in this option is incorrect, as it tries to require all states to be true at the same time, which is not possible. You would need to
use the OR operator instead of AND for this to make sense. However, even with the OR operator, this option is not ideal, as it would be
monitoring the sum of used, buffered, cached, and slab memory. We are specifically interested in the percentage of used memory to
trigger autoscaling.

upvoted 1 times

? ?  JC0926 2ámonths, 2áweeks ago

A. Change the Target type to DELTA_PER_MINUTE:
This is not appropriate because the target type "DELTA_PER_MINUTE" is used to measure the change in the metric value per minute. In
this case, we want to monitor the percentage of memory used, not how it changes over time. Using "GAUGE" as the target type is
correct as it represents the current value of the metric.

upvoted 1 times

? ?  Certolony 3ámonths, 1áweek ago

Well... in Metrics Explorer, this filter with AND-ing of several memory types will give no result, because these memory types are disjoint so
you cannot get usage of memory which is of type used and cached in the same time! It should be aggregated and summed up, not AND-
ed.
Free memory type makes no sense, as we would require signal to scale up when metrics goes below target and not above (I have tested
it).
Bytes_used gives just huge number of used memory bytes.
And DELTA_PER_MINUTE also is wrong, because mem utilization is a guage metrics type. When it is set, an error, that specified metric does
not exist.
There is no valid answer...?

upvoted 3 times

? ?  Deb2293 3ámonths, 2áweeks ago

Selected Answer: C

Option C proposes changing the filter to include multiple states ('used', 'buffered', 'cached', and 'slab') instead of just 'used'. This approach
could potentially provide a more comprehensive view of the memory usage and ensure that the autoscaling policy is triggered when the
application's memory usage reaches 80%

upvoted 2 times

? ?  nick_name_1 4ámonths, 1áweek ago

C.
"To configure autoscaling based on the percent of used memory, specify the percent_used metric provided by the memory Ops Agent
metrics. You should filter the metric by state to use only the used memory state. If you do not specify the filter, then the autoscaler takes
the sum of memory usage by all memory states labeled as buffered, cached, free, slab, and used."

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

499/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  pepigeon 4ámonths, 1áweek ago

A would not prevent you running out of memory, it only responds to quick increases/decreases of usage
B we could change the specs of the server at some point, definitely want to use %
C as someone else mentioned if you add all these up that's the total amount of memory your instance has
D seems like the best option because your used memory can stay stable whilst cached and buffered can increase. You can verify this in
Metrics explorer. Free always changes unlike used.

upvoted 2 times

? ?  Mannpal 4ámonths, 3áweeks ago

how to get full access 192 to 273

upvoted 1 times

? ?  r1ck 4ámonths, 1áweek ago

purchase subscriber access

upvoted 3 times

? ?  moota 4ámonths, 3áweeks ago

Selected Answer: C

I vote for C to account for buffered, cached and slabs.

upvoted 1 times

? ?  WFCheong 5ámonths, 2áweeks ago

Selected Answer: A

kiappy81 Highly Voted 4 months, 1 week ago
Selected Answer: A
TARGET_TYPE: the value type for the metric.
gauge: the autoscaler computes the average value of the data collected in the last couple of minutes and compares that to the utilization
target.
delta-per-minute: the autoscaler calculates the average rate of growth per minute and compares that to the utilization target.
delta-per-second: the autoscaler calculates the average rate of growth per second and compares that to the utilization target. For accurate
comparisons, if you set the utilization target in seconds, use delta-per-second as the target type. Likewise, use delta-per-minute for a
utilization target in minutes.

upvoted 4 times

? ?  thamaster 6ámonths ago

Selected Answer: C

we are using percent so why use delta time? answer D is discarded as you need to change memory/percent_used also, B is nonsense.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

500/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #191

Topic 1

You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must communicate over a

private network with applications in a non-Google Cloud environment. The expected average throughput is 200 kbps. The business requires:
? as close to 100% system availability as possible
? cost optimization
You need to design the connectivity between the locations to meet the business requirements. What should you provision?

A. An HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway

B. Two Classic Cloud VPN gateways connected to two on-premises VPN gateways Con gure each Classic Cloud VPN gateway to have two

tunnels, each connected to different on-premises VPN gateways

C. Two HA Cloud VPN gateways connected to two on-premises VPN gateways Con gure each HA Cloud VPN gateway to have two tunnels,

each connected to different on-premises VPN gateways

D. A single Cloud VPN gateway connected to an on-premises VPN gateway

Correct Answer: A

Community vote distribution

A (79%)

14%

7%

? ?  BiddlyBdoyng 1áweek, 5ádays ago

A: looks an exact match for the requirements, 99.99% availability

B: Is a manual implementation of HA, not optimizing cost

C: Is behoynd HA, no longer optimizing cost.

D: Does not provide close to 100% as possible

upvoted 1 times

? ?  AugustoKras011111 3ámonths, 3áweeks ago

Selected Answer: A

Use HA (High Availability) VPN as required in the question. A is better aswer.

upvoted 2 times

? ?  ale_brd_ 6ámonths, 1áweek ago

Selected Answer: A

both A and C are possible solutions but A is cheaper.

upvoted 2 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: A

A is ok

upvoted 2 times

? ?  winter0w 7ámonths, 1áweek ago

Selected Answer: D

Correct Answer is D,
You cannot migrate an existing Cloud VPN tunnel or tunnels on a Classic VPN gateway to an HA VPN gateway. Instead, you need to create
new tunnels and delete the old ones.
https://cloud.google.com/network-connectivity/docs/vpn/how-to/moving-to-ha-vpn#general_guidelines

upvoted 2 times

? ?  minmin2020 8ámonths, 1áweek ago

Selected Answer: A

A is true only if the on-prem (peer) gateway has two separate external P addresses. The HA VPN gateway uses two tunnels, one tunnel to
each external IP address on the peer device as described in https://cloud.google.com/network-
connectivity/docs/vpn/concepts/topologies#configurations_that_support_9999_availability

C is a complete solution that provides full redundancy of the on-prem gateway. This is probably more expensive and having two HA VPN
Gateways is an unusual configuration as the online documentation only describes using one HA VPN Gateway

A appears to be correct with assumptions...!

upvoted 3 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

501/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

A satisfty both requriements

upvoted 3 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Satisfy both requirements for close to 100% availability and cost containment

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is right

upvoted 1 times

? ?  ForkMeSoftly 9ámonths, 2áweeks ago

Selected Answer: A

best explained in https://jayendrapatil.com/tag/classic-vpn-vs-ha-vpn/
HA VPN provides an SLA of 99.99% service availability, when configured with two interfaces and two external IP addresses.

upvoted 2 times

? ?  alvinlxw 9ámonths, 2áweeks ago

Selected Answer: A

To meet the 99.99% SLA on the Google Cloud side, there must be a tunnel from each of the two interfaces on the HA VPN gateway to the
corresponding interfaces on the peer gateway.

upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: A

A can provide 99.99% availability as well, and no need for C which will be more expensive.

https://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#1-peer-1-address

upvoted 3 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: A

A can provide 99.99% availability as well, and no need for C which will be more expensive.

https://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies#1-peer-1-address

upvoted 2 times

? ?  Nirca 9ámonths, 3áweeks ago

Selected Answer: C

C is full mash. real HR with redundancy on the on premises site

upvoted 1 times

? ?  kuboraam 9ámonths, 3áweeks ago

Selected Answer: A

I choose A. Gives you 99.99% availability, and is certainly cheaper than B, C and is more reliable than D.

https://cloud.google.com/network-connectivity/docs/vpn/concepts/topologies

upvoted 3 times

? ?  aswani 9ámonths, 3áweeks ago

Selected Answer: C

https://cloud.google.com/static/network-connectivity/docs/vpn/images/ha-vpn-gcp-to-on-prem-2-a.svg

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

502/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 2 - Testlet 1

Question #1

Introductory Info

Company Overview -

Topic 2

JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One

of the company's core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon

output by 50% over the next 5 years.

Company Background -

JencoMart started as a general store in 1931, and has grown into one of the world's leading brands, known for great value and customer service.

Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart

has little presence in Asia, but considers that market key for future growth.

Solution Concept -

JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the

cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no

longer supported.

Existing Technical Environment -

JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.

JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.

Application: Customer loyalty portal

LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.

Database -

Oracle Database stores user pro les

- 20 TB

- Complex table structure

- Well maintained, clean data

- Strong backup strategy

PostgreSQL database stores user credentials

- Single-homed in US West

- No redundancy

- Backed up every 12 hours

- 100% uptime service level agreement (SLA)

- Authenticates all users

Compute -

30 machines in US West Coast, each machine has:

- Twin, dual core CPUs

- 32 GB of RAM

- Twin 250 GB HDD (RAID 1)

20 machines in US East Coast, each machine has:

- Single, dual-core CPU

- 24 GB of RAM

- Twin 250 GB HDD (RAID 1)

Storage -

Access to shared 100 TB SAN in each location

Tape backup every week

Business Requirements -

Optimize for capacity during peak periods and value during off-peak periods

Guarantee service availability and support

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

503/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Reduce on-premises footprint and associated  nancial and environmental impact

Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase

Expand services into Asia

Technical Requirements -

Assess key application for cloud suitability

Modify applications for the cloud

Move applications to a new infrastructure

Leverage managed services wherever feasible

Sunset 20% of capacity in existing data centers

Decrease latency in Asia

CEO Statement -

JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is

in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the

environment through green initiatives and policies.

CTO Statement -

The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a

public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.

CFO Statement -

Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to

outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak

periods and reduce costs.

Question

The JencoMart security team requires that all Google Cloud Platform infrastructure is deployed using a least privilege model with separation of

duties for administration between production and development resources.

What Google domain and project structure should you recommend?

A. Create two G Suite accounts to manage users: one for development/test/staging and one for production. Each account should contain one

project for every application

B. Create two G Suite accounts to manage users: one with a single project for all development applications and one with a single project for

all production applications

C. Create a single G Suite account to manage users with each stage of each application in its own project

D. Create a single G Suite account to manage users with one project for the development/test/staging environment and one project for the

production environment

Correct Answer: D

Note: The principle of least privilege and separation of duties are concepts that, although semantically different, are intrinsically related from

the standpoint of security. The intent behind both is to prevent people from having higher privilege levels than they actually need
? Principle of Least Privilege: Users should only have the least amount of privileges required to perform their job and no more. This reduces
authorization exploitation by limiting access to resources such as targets, jobs, or monitoring templates for which they are not authorized.
? Separation of Duties: Beyond limiting user privilege level, you also limit user duties, or the speci c jobs they can perform. No user should be
given responsibility for more than one related function. This limits the ability of a user to perform a malicious action and then cover up that

action.

Reference:

https://cloud.google.com/kms/docs/separation-of-duties

Community vote distribution

C (90%)

10%

? ?  Anjoy  Highly Voted ?  2áyears, 8ámonths ago

Here are the correct answers:
https://cloud.google.com/resource-manager/docs/creating-managing-folders
Refer to the diagram on top, different envs are created at the project level.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

504/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations
"A general recommendation is to have one project per application per environment. For example, if you have two applications, "app1" and
"app2", each with a development and production environment, you would have four projects: app1-dev, app1-prod, app2-dev, app2-prod.
This isolates the environments from each other, so changes to the development project do not accidentally impact production, and gives
you better access control, since you can (for example) grant all developers access to development projects but restrict production access
to your CI/CD pipeline."

The answer is C.
upvoted 50 times

? ?  JohnWick2020 2áyears, 2ámonths ago

Agreed, definitely C!

upvoted 2 times

? ?  francisco_guerra 2áyears, 7ámonths ago

Yes its right, its C for me.

upvoted 3 times

? ?  pepYash 2áyears, 7ámonths ago

the most logical link and explanation. Thank you!. It is 'C' indeed

upvoted 2 times

? ?  aadaisme  Highly Voted ?  3áyears, 2ámonths ago
i think C follows google's best practice?

upvoted 14 times

? ?  TheCloudGuruu  Most Recent ?  1ámonth, 1áweek ago

Selected Answer: C

C is best practice
upvoted 1 times

? ?  jaxclain 6ámonths, 3áweeks ago

Selected Answer: C

Ok, just to help all of use lol vote so everyone who sees this comment, will not reply or comment on this questions, why? because the case
study is very old so 100% guaranteed this question will not appear in the exam, any question from JencoMart and Dress4Win, maybe they
can place similar question without involving those 2 case studies but as for now, nobody has reported seeing those questions again so
stop replying or posting on this thread so it will no longer show at the top of the forum lol please..

upvoted 10 times

? ?  gonlafer 6ámonths, 3áweeks ago

Selected Answer: C

https://cloud.google.com/kms/docs/separation-of-duties

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is best practice as recommended by google
https://cloud.google.com/resource-manager/docs/creating-managing-folders

upvoted 1 times

? ?  6721sora 9ámonths, 3áweeks ago

Selected Answer: C

For segregation of applications and environments, C is the best reference architecture model

upvoted 1 times

? ?  riyer1 10ámonths ago

The case study should be removed as it is deprecated

upvoted 5 times

? ?  JohnnyBG 10ámonths, 3áweeks ago

This case study is deprecated, you (admin) should remove this content.

upvoted 10 times

? ?  H_S 1áyear ago

case study deprecated

upvoted 4 times

? ?  1289dev 1áyear, 1ámonth ago

Selected Answer: D

D is Right Answer

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

505/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

multi account is a overkill.One project per environment will ensure the segregation in enviorment henc C.

upvoted 2 times

? ?  Rajj98 1áyear, 3ámonths ago

Selected Answer: C

Its C - Google's best practice

upvoted 4 times

? ?  sjmsummer 1áyear, 5ámonths ago

Selected Answer: D

D is mostly seen in real world. I will choose it.

upvoted 2 times

? ?  sjmsummer 1áyear, 5ámonths ago

D. dev/test normally is in one function.

upvoted 1 times

? ?  Sekierer 1áyear, 5ámonths ago

Selected Answer: C

Vote C

upvoted 3 times

? ?  simbu1299 1áyear, 6ámonths ago

Answer is C

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

506/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company Overview -

Topic 2

JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One

of the company's core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon

output by 50% over the next 5 years.

Company Background -

JencoMart started as a general store in 1931, and has grown into one of the world's leading brands, known for great value and customer service.

Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart

has little presence in Asia, but considers that market key for future growth.

Solution Concept -

JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the

cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no

longer supported.

Existing Technical Environment -

JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.

JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.

Application: Customer loyalty portal

LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.

Database -

Oracle Database stores user pro les

- 20 TB

- Complex table structure

- Well maintained, clean data

- Strong backup strategy

PostgreSQL database stores user credentials

- Single-homed in US West

- No redundancy

- Backed up every 12 hours

- 100% uptime service level agreement (SLA)

- Authenticates all users

Compute -

30 machines in US West Coast, each machine has:

- Twin, dual core CPUs

- 32 GB of RAM

- Twin 250 GB HDD (RAID 1)

20 machines in US East Coast, each machine has:

- Single, dual-core CPU

- 24 GB of RAM

- Twin 250 GB HDD (RAID 1)

Storage -

Access to shared 100 TB SAN in each location

Tape backup every week

Business Requirements -

Optimize for capacity during peak periods and value during off-peak periods

Guarantee service availability and support

Reduce on-premises footprint and associated  nancial and environmental impact

Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase

Expand services into Asia

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

507/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Technical Requirements -

Assess key application for cloud suitability

Modify applications for the cloud

Move applications to a new infrastructure

Leverage managed services wherever feasible

Sunset 20% of capacity in existing data centers

Decrease latency in Asia

CEO Statement -

JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is

in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the

environment through green initiatives and policies.

CTO Statement -

The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a

public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.

CFO Statement -

Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to

outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak

periods and reduce costs.

Question

A few days after JencoMart migrates the user credentials database to Google Cloud Platform and shuts down the old server, the new database

server stops responding to SSH connections. It is still serving database requests to the application servers correctly.

What three steps should you take to diagnose the problem? (Choose three.)

A. Delete the virtual machine (VM) and disks and create a new one

B. Delete the instance, attach the disk to a new VM, and investigate

C. Take a snapshot of the disk and connect to a new machine to investigate

D. Check inbound  rewall rules for the network the machine is connected to

E. Connect the machine to another network with very simple  rewall rules and investigate

F. Print the Serial Console output for the instance for troubleshooting, activate the interactive console, and investigate

Correct Answer: CDF

D: Handling "Unable to connect on port 22" error message

Possible causes include:
? There is no  rewall rule allowing SSH access on the port. SSH access on port 22 is enabled on all Compute Engine instances by default. If
you have disabled access, SSH from the Browser will not work. If you run sshd on a port other than 22, you need to enable the access to that

port with a custom  rewall rule.
? The  rewall rule allowing SSH access is enabled, but is not con gured to allow connections from GCP Console services. Source IP
addresses for browser- based SSH sessions are dynamically allocated by GCP Console and can vary from session to session.

F: Handling "Could not connect, retrying..." error

You can verify that the daemon is running by navigating to the serial console output page and looking for output lines pre xed with the

accounts-from-metadata: string. If you are using a standard image but you do not see these output pre xes in the serial console output, the

daemon might be stopped. Reboot the instance to restart the daemon.

Reference:

https://cloud.google.com/compute/docs/ssh-in-browser

https://cloud.google.com/compute/docs/ssh-in-browser

Community vote distribution

CDF (100%)

? ?  Paul_DSouza  Highly Voted ?  3áyears ago

Assumption:- VM in production environment, cannot be taken down.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

508/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

based on above, A/B are out. E causes downtime to remove the VM from the old network and connect it to a new network for testing.

Only leaves CDF

C - Snapshotting is fine, it will reduce the performance for a short duration, but the database will still be up
D - Obvious place to check for firewall rules (if ssh ports are open or not)
F - Easy to see server messages on console (without downtime)

upvoted 33 times

? ?  H_S  Highly Voted ?  1áyear ago

CASE STUDY DEPRECATED PLEASE REMOVE

https://cloud.google.com/certification/guides/professional-cloud-architect

Review the case studies that may be used in the exam.
EHR Healthcare
Helicopter Racing League
Mountkirk Games
TerramEarth

upvoted 15 times

? ?  Mahmoud_E  Most Recent ?  8ámonths, 1áweek ago

Selected Answer: CDF

CDF best answers

upvoted 1 times

? ?  6721sora 9ámonths, 3áweeks ago

Selected Answer: CDF

D and F are ok and easy to conclude on.
Between E and C - C may cause disruptions to a running production VM.
E allows us to snapshot to a different VM and you can check the sshd configuration there. Also connect the snapshotted VM to the same
networks to troubleshoot in real time

upvoted 1 times

? ?  extopic01 1áyear, 1ámonth ago

CDF is correct.
Dcoumentation with regards to option C : https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-ssh#inspect_vm

upvoted 1 times

? ?  exam_war 1áyear, 6ámonths ago

go with DEF. C doesn't make sense, taking a snapshot for OS and connect to new machine, it won't tell you what's going on with ssh
session.

upvoted 4 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: CDF

vote CDF

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is CDF
upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

C. Take a snapshot of the disk and connect to a new machine to investigate
D. Check inbound firewall rules for the network the machine is connected to
F. Print the Serial Console output for the instance for troubleshooting, activate the interactive console, and investigate

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer are C,D, F

upvoted 1 times

? ?  Rathul 2áyears, 3ámonths ago

I will vote for CEF
upvoted 2 times

? ?  bnlcnd 2áyears, 4ámonths ago

You need to select 1 from A/B/C before you can have something to troubleshoot. You cannot troubleshoot on a running PORD server.
C is best.
D/E/F get 2, rule out E.

upvoted 3 times

? ?  sealvarezmx 2áyears, 5ámonths ago

C. SSH is related to the VM, not to the database.
Why would you create a snapshot of the disk and attach it to a NEW VMs? by creating a new VM you could fix the SSH problem without

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

509/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

even attaching the disk.
D. Doesnt make sense to print the output, but it does make sense to log into the console and troubleshoot SSH.
I'll go with DEF.
upvoted 3 times

? ?  _CloudTech_ 2áyears, 7ámonths ago

CDF is ok

upvoted 3 times

? ?  shaun_ko 2áyears, 8ámonths ago

I think DEF.. need to Snapshot? why?

upvoted 3 times

? ?  shaun_ko 2áyears, 8ámonths ago
sorry. I misunderstood. go CDF

upvoted 3 times

? ?  AshokC 2áyears, 9ámonths ago

Agree with CDF
upvoted 3 times

? ?  rehma017 3áyears ago

I like DEF - the problem with C is you are snapshotting an instance which is currently being queried by the application, you are definitely
going to cause an IOPS spike..

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

510/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company Overview -

Topic 2

JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One

of the company's core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon

output by 50% over the next 5 years.

Company Background -

JencoMart started as a general store in 1931, and has grown into one of the world's leading brands, known for great value and customer service.

Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart

has little presence in Asia, but considers that market key for future growth.

Solution Concept -

JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the

cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no

longer supported.

Existing Technical Environment -

JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.

JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.

Application: Customer loyalty portal

LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.

Database -

Oracle Database stores user pro les

- 20 TB

- Complex table structure

- Well maintained, clean data

- Strong backup strategy

PostgreSQL database stores user credentials

- Single-homed in US West

- No redundancy

- Backed up every 12 hours

- 100% uptime service level agreement (SLA)

- Authenticates all users

Compute -

30 machines in US West Coast, each machine has:

- Twin, dual core CPUs

- 32 GB of RAM

- Twin 250 GB HDD (RAID 1)

20 machines in US East Coast, each machine has:

- Single, dual-core CPU

- 24 GB of RAM

- Twin 250 GB HDD (RAID 1)

Storage -

Access to shared 100 TB SAN in each location

Tape backup every week

Business Requirements -

Optimize for capacity during peak periods and value during off-peak periods

Guarantee service availability and support

Reduce on-premises footprint and associated  nancial and environmental impact

Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase

Expand services into Asia

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

511/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Technical Requirements -

Assess key application for cloud suitability

Modify applications for the cloud

Move applications to a new infrastructure

Leverage managed services wherever feasible

Sunset 20% of capacity in existing data centers

Decrease latency in Asia

CEO Statement -

JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is

in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the

environment through green initiatives and policies.

CTO Statement -

The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a

public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.

CFO Statement -

Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to

outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak

periods and reduce costs.

Question

JencoMart has decided to migrate user pro le storage to Google Cloud Datastore and the application servers to Google Compute Engine (GCE).

During the migration, the existing infrastructure will need access to Datastore to upload the data.

What service account key-management strategy should you recommend?

A. Provision service account keys for the on-premises infrastructure and for the GCE virtual machines (VMs)

B. Authenticate the on-premises infrastructure with a user account and provision service account keys for the VMs

C. Provision service account keys for the on-premises infrastructure and use Google Cloud Platform (GCP) managed keys for the VMs

D. Deploy a custom authentication service on GCE/Google Kubernetes Engine (GKE) for the on-premises infrastructure and use GCP managed

keys for the VMs

Correct Answer: C

Migrating data to Google Cloud Platform

Let's say that you have some data processing that happens on another cloud provider and you want to transfer the processed data to Google

Cloud Platform. You can use a service account from the virtual machines on the external cloud to push the data to Google Cloud Platform. To

do this, you must create and download a service account key when you create the service account and then use that key from the external

process to call the Cloud Platform APIs.

Reference:

https://cloud.google.com/iam/docs/understanding-service-accounts#migrating_data_to_google_cloud_platform

Community vote distribution

C (100%)

? ?  Zarmi  Highly Voted ?  3áyears, 1ámonth ago

Answer: C.
https://cloud.google.com/iam/docs/understanding-service-accounts#migrating_data_to_google_cloud_platform

There are two types of service account keys:

GCP-managed keys. These keys are used by Cloud Platform services such as App Engine and Compute Engine. They cannot be
downloaded, and are automatically rotated and used for signing for a maximum of two weeks. The rotation process is probabilistic; usage
of the new key will gradually ramp up and down over the key's lifetime. We recommend caching the public key set for a service account for
at most 24 hours to ensure that you always have access to the current key set.

User-managed keys. These keys are created, downloadable, and managed by users. They expire 10 years from creation, and cease
authenticating successfully when they are deleted from the service account.

upvoted 27 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

512/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Carsonza 2áyears, 9ámonths ago

while that heading doesn't exist anymore the graphic that it is that doc speaks for itself.

upvoted 1 times

? ?  shashu07  Highly Voted ?  3áyears ago

Correct Answer : C
Where will the code that assumes the identity of the service account be running: on Google Cloud Platform or on-premises?
https://cloud.google.com/iam/docs/understanding-service-accounts

upvoted 8 times

? ?  Mahmoud_E  Most Recent ?  8ámonths, 1áweek ago

Selected Answer: C

C is the right answer https://cloud.google.com/iam/docs/understanding-service-accounts#migrating_data_to_google_cloud_platfor

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is C

upvoted 1 times

? ?  Yogikant 2áyears ago

Answer C.

Refer to first figure in https://cloud.google.com/iam/docs/understanding-service-accounts#migrating_data_to_google_cloud_platfor. It
mentions using User Managed Keys for on-premises usage of services accounts and GCP managed keys for code running in GCP.

Also in case study it emphasise using "managed services as much as possible". So this rules out A.

upvoted 4 times

? ?  victory108 2áyears, 1ámonth ago

C. Provision service account keys for the on-premises infrastructure and use Google Cloud Platform (GCP) managed keys for the VMs

upvoted 1 times

? ?  Koushick 2áyears, 1ámonth ago

Answer C as per https://cloud.google.com/iam/docs/understanding-service-accounts#migrating_data_to_google_cloud_platform

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is C

upvoted 1 times

? ?  pawel_ski 2áyears, 3ámonths ago

When you provision service account keys for the on-premises infrastructure you must then manage them. So if the kyes are managed by
GCP you must set up a process to rotate keys in the on-prem infrastructure. It is quite chalenging.
Therefore I prefer option B.

upvoted 1 times

? ?  AGG 2áyears, 4ámonths ago

I will go with A which is very similar to C but answer C suggest use Google Cloud Platform (GCP) managed keys for the VMs (there is no
word : "ONL" for the VMs) but it's suggestion (this is how I perceive it)
Answer A is copy/paste from link : https://cloud.google.com/iam/docs/understanding-service-
accounts#migrating_data_to_google_cloud_platform

A service account is a special type of Google account intended to represent a non-human user that needs to authenticate and be
authorized to access data in Google APIs.

Typically, service accounts are used in scenarios such as:

- Running workloads on virtual machines (VMs).(first part of answer A = and for the GCE virtual machines (VMs))
- Running workloads on on-premises workstations or data centers that call Google APIs. (Second part of answer A = Provision service
account keys for the on-premises infrastructure)

upvoted 4 times

? ?  ahmedemad3 2áyears, 4ámonths ago

Ans: C
make sense of the service account for infrastructure and managed key for VM

upvoted 1 times

? ?  bnlcnd 2áyears, 4ámonths ago

A /B / C are all playing with words. But the key points is who need service account key. no matter where the key is managed. GCP managed
or customer managed.
Only the on-prom resource need the service account key. so, only C is right.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

513/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  ybe_gcp_cert 2áyears, 6ámonths ago

In C, the vm part is wrong. A VM doesn't use key directly from a conf point of view. It uses a service account that is linked with a key pair.
the key could be managed by google or user managed.
https://cloud.google.com/iam/docs/service-accounts#service_account_keys

C is only playing with words...
I would go with A.

upvoted 1 times

? ?  _CloudTech_ 2áyears, 7ámonths ago

C is ok

upvoted 1 times

? ?  JCGO 2áyears, 7ámonths ago

Accessing something from on-premise to google cloud done by using service accounts this days. Datastore for example:
https://cloud.google.com/datastore/docs/activate Service account keys can be managed by google, or can be self-generated and public
key uploaded.
Question asks about provisioning service account keys during migration phase, when on-prem stuff needs access to datastore. C looks
good. A looks good also, but a involves provisioning service account keys for cloud VM's -> it is done another way. you could give
permissions to defsault compute service account per API, or create service account and give it appropriate premissions and choose while
creating cloud VM. I can not see any point bothering with service accout keys for cloud VM's here. So i choose C.

upvoted 1 times

? ?  akhadar2001 2áyears, 9ámonths ago

Answer C:
"Provision service account keys for the on-premises infrastructure": For code running on systems outside Google, you cannot use GCP-
managed keys. You need to create Service account for it and provision User-managed keys. These keys are created, downloadable, and
managed by users - This is solution for on-premises access to GCP datastore during migration

"use Google Cloud Platform (GCP) managed keys for the VMs" - this is solution for Application server migration since there is no external
access to GCP is required during the migration.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

514/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company Overview -

Topic 2

JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One

of the company's core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon

output by 50% over the next 5 years.

Company Background -

JencoMart started as a general store in 1931, and has grown into one of the world's leading brands, known for great value and customer service.

Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart

has little presence in Asia, but considers that market key for future growth.

Solution Concept -

JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the

cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no

longer supported.

Existing Technical Environment -

JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.

JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.

Application: Customer loyalty portal

LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.

Database -

Oracle Database stores user pro les

- 20 TB

- Complex table structure

- Well maintained, clean data

- Strong backup strategy

PostgreSQL database stores user credentials

- Single-homed in US West

- No redundancy

- Backed up every 12 hours

- 100% uptime service level agreement (SLA)

- Authenticates all users

Compute -

30 machines in US West Coast, each machine has:

- Twin, dual core CPUs

- 32 GB of RAM

- Twin 250 GB HDD (RAID 1)

20 machines in US East Coast, each machine has:

- Single, dual-core CPU

- 24 GB of RAM

- Twin 250 GB HDD (RAID 1)

Storage -

Access to shared 100 TB SAN in each location

Tape backup every week

Business Requirements -

Optimize for capacity during peak periods and value during off-peak periods

Guarantee service availability and support

Reduce on-premises footprint and associated  nancial and environmental impact

Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase

Expand services into Asia

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

515/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Technical Requirements -

Assess key application for cloud suitability

Modify applications for the cloud

Move applications to a new infrastructure

Leverage managed services wherever feasible

Sunset 20% of capacity in existing data centers

Decrease latency in Asia

CEO Statement -

JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is

in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the

environment through green initiatives and policies.

CTO Statement -

The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a

public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.

CFO Statement -

Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to

outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak

periods and reduce costs.

Question

JencoMart has built a version of their application on Google Cloud Platform that serves tra c to Asia. You want to measure success against their

business and technical goals.

Which metrics should you track?

A. Error rates for requests from Asia

B. Latency difference between US and Asia

C. Total visits, error rates, and latency from Asia

D. Total visits and average latency for users from Asia

E. The number of character sets present in the database

Correct Answer: D

From scenario:

Business Requirements include: Expand services into Asia

Technical Requirements include: Decrease latency in Asia

Community vote distribution

C (58%)

D (42%)

? ?  VASI  Highly Voted ?  3áyears, 4ámonths ago

Business Requirements include: Guarantee service availability and support. I would choose C

upvoted 36 times

? ?  Smart 3áyears, 4ámonths ago

I wonder how specific we have to be and how much common sense/best practices should we ignore.

upvoted 10 times

? ?  nitinz 2áyears, 3ámonths ago

D it meets both requirements. Error was never asked for in KPI.

upvoted 6 times

? ?  JohnWick2020  Highly Voted ?  2áyears, 2ámonths ago

Answer is C; more complete imo. Those aligning to D should note that average latency is not the only metric available to measure and is
too specific.

"Total visits" covers the business requirements:
- Optimize for capacity during peak periods and value during off-peak periods.
- Expand services into Asia.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

516/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

"Error rates" covers business requirement:
- Guarantee service availability and support. ** if service is unavailable, errors are reported!

"Latency" covers technical requirement:
- Decrease latency in Asia.

upvoted 17 times

? ?  Mahmoud_E  Most Recent ?  8ámonths, 1áweek ago

Selected Answer: D

D is the best answer

upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: C

C.
It says Guarantee service availability, we need to check for error rates to make sure our application is working perfectly fine.

upvoted 1 times

? ?  H_S 1áyear ago

CASE STUDY DEPRECATED PLEASE REMOVE
Review the case studies that may be used in the exam.
EHR Healthcare
Helicopter Racing League
Mountkirk Games
TerramEarth

upvoted 11 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

error and latency will cover technical and visits buissness hence C

upvoted 1 times

? ?  nqthien041292 1áyear, 6ámonths ago

Selected Answer: C

Vote C

upvoted 5 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 4 times

? ?  joe2211 1áyear, 6ámonths ago

Keywords, their business and technical goals are aimed to Asian users only
Business Requirements:
Expand services into Asia

Technical Requirements:
- Decrease latency in Asia

upvoted 1 times

? ?  AMohanty 10ámonths, 3áweeks ago

A part of their Goal is : Guarantee service availability and support
Errors doesn't cater well to their Service Availability.

upvoted 1 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 1 times

? ?  Urban_Life 1áyear, 11ámonths ago

guys trust me - it's C

upvoted 4 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is D

upvoted 2 times

? ?  victory108 2áyears, 1ámonth ago

C. Total visits, error rates, and latency from Asia

upvoted 4 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is D

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

517/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  pawel_ski 2áyears, 3ámonths ago

I prefer to see the latency change not only to have an average value.
I choose C.

upvoted 2 times

? ?  ebinv2 2áyears, 3ámonths ago

Success against business and and technical goals- should be C , as it check error rates also

upvoted 1 times

? ?  Fadhli 2áyears, 4ámonths ago

its D, based on the case, the technical requirement is to reduce latency

upvoted 2 times

? ?  bnlcnd 2áyears, 4ámonths ago

C is better. error rate is critical not just latency. sometime the latency can be very good but just because every request errored out and
finished faster.
"from asia" or "users in asia" ? if that matters, this is not a GCP test. it's a G2 English test for Asian schools.

upvoted 6 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

518/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company Overview -

Topic 2

JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One

of the company's core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon

output by 50% over the next 5 years.

Company Background -

JencoMart started as a general store in 1931, and has grown into one of the world's leading brands, known for great value and customer service.

Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart

has little presence in Asia, but considers that market key for future growth.

Solution Concept -

JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the

cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no

longer supported.

Existing Technical Environment -

JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.

JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.

Application: Customer loyalty portal

LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.

Database -

Oracle Database stores user pro les

- 20 TB

- Complex table structure

- Well maintained, clean data

- Strong backup strategy

PostgreSQL database stores user credentials

- Single-homed in US West

- No redundancy

- Backed up every 12 hours

- 100% uptime service level agreement (SLA)

- Authenticates all users

Compute -

30 machines in US West Coast, each machine has:

- Twin, dual core CPUs

- 32 GB of RAM

- Twin 250 GB HDD (RAID 1)

20 machines in US East Coast, each machine has:

- Single, dual-core CPU

- 24 GB of RAM

- Twin 250 GB HDD (RAID 1)

Storage -

Access to shared 100 TB SAN in each location

Tape backup every week

Business Requirements -

Optimize for capacity during peak periods and value during off-peak periods

Guarantee service availability and support

Reduce on-premises footprint and associated  nancial and environmental impact

Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase

Expand services into Asia

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

519/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Technical Requirements -

Assess key application for cloud suitability

Modify applications for the cloud

Move applications to a new infrastructure

Leverage managed services wherever feasible

Sunset 20% of capacity in existing data centers

Decrease latency in Asia

CEO Statement -

JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is

in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the

environment through green initiatives and policies.

CTO Statement -

The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a

public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.

CFO Statement -

Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to

outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak

periods and reduce costs.

Question

The migration of JencoMart's application to Google Cloud Platform (GCP) is progressing too slowly. The infrastructure is shown in the diagram.

You want to maximize throughput.

What are three potential bottlenecks? (Choose three.)

A. A single VPN tunnel, which limits throughput

B. A tier of Google Cloud Storage that is not suited for this task

C. A copy command that is not suited to operate over long distances

D. Fewer virtual machines (VMs) in GCP than on-premises machines

E. A separate storage layer outside the VMs, which is not suited for this task

F. Complicated internet connectivity between the on-premises infrastructure and GCP

Correct Answer: ACE

Community vote distribution

ACF (50%)

AC (36%)

14%

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

520/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  _CloudTech_  Highly Voted ?  2áyears, 7ámonths ago

Where are you Tartor?

upvoted 19 times

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

Think A,D,E. "Copy command not suited for long distances", what does it mean?

upvoted 18 times

? ?  kaush 3áyears ago

you need reliable agent software installed to copy files with retries ,copy command not sufficient

upvoted 1 times

? ?  dayody 2áyears, 10ámonths ago

the diagram does not show copy command.

upvoted 11 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 5ádays ago

I don't get why B isn't correct. Cloud Storage isn't a good replacement for SAN, especially when we have a database running on some of
the VMs.

upvoted 1 times

? ?  taer 2ámonths, 3áweeks ago

Selected Answer: ACF

I think ACF

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: ACE

ACE are the best asnwers

upvoted 2 times

? ?  6721sora 9ámonths, 3áweeks ago

Selected Answer: ACF

Single VPN tunnel limits throughput. Copying 20TB across long distances is a big bottleneck. VPN across internet cannot be relied upon for
high performance

upvoted 1 times

? ?  RGTest 1áyear ago

Selected Answer: ACF

ACF ...

upvoted 1 times

? ?  Joanale 1áyear, 3ámonths ago

Why "A" dudes? even if you put 20 tunnels connection the bandwidth of on premisses remains the same. I'm sure its B, D, F, who make's
migrations know google storage isn't made for migrations, i think D means that make some compression process or transformation and F
is right cause all connectivity goes trough internet. If for some reason the internet bw is shared this can be congestionated.

upvoted 1 times

? ?  mad314 1áyear, 2ámonths ago

https://cloud.google.com/network-connectivity/docs/vpn/quotas#limits
Bandwidth per VPN tunnel: Up to 3 Gbps for the sum of ingress and egress

upvoted 1 times

? ?  sjmsummer 1áyear, 5ámonths ago

Selected Answer: ACF

ACF, though there are a lot of room to imagine in answer B to make it a risk. Not sure what the test designer is implying in this case.

upvoted 4 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Looking at the figure is confusing.
The necessary information cannot be read from this figure.
Therefore, select the factors that deteriorate the throughput from the options.
I think A,C and F are factors that deteriorate the throughput.
Anyone pls tell me why choose E?

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: AC

vote ACF

upvoted 5 times

? ?  sudarchary 1áyear, 9ámonths ago
Correct answer is ACE only

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

521/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  MikeB19 1áyear, 10ámonths ago

There is just not enough data in this q to answer correctly. For instance it does not state they r using a copy command and we donÆt know
what the internet connection is. I think adf but this q needs more data points

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

A. A single VPN tunnel, which limits throughput
C. A copy command that is not suited to operate over long distances
E. A separate storage layer outside the VMs, which is not suited for this task

upvoted 2 times

? ?  dlzhang 2áyears ago

"E. A separate storage layer outside the VMs, which is not suited for this task". My question here is that there is no 'storage layer' in the
diagram given in the question.

upvoted 3 times

? ?  Manh 1áyear, 9ámonths ago

it's Cloud storage mouth to VM instance

upvoted 1 times

? ?  JohnWick2020 2áyears, 2ámonths ago

Answer is A,C,F.

Breakdown:
A. A single VPN tunnel, which limits throughput.
Recommended practice is to have two redundant connections, usually Dedicated Interconnect and VPN. VPNs suit low volume data
connections so won't cut it for this company.

C. A copy command that is not suited to operate over long distances.
Advisable to use GCP recommended tools that support multi-part, parallel composite and resumable uploads to Cloud Storage.

F. Complicated internet connectivity between the on-premises infrastructure and GCP.
With some existing VMs being dual-homed, firewalls, VPC and routing configs, things could get pretty complicated.

upvoted 16 times

? ?  rbarrote 2áyears, 1ámonth ago

But for C it mentions specifically the long distance, and the copy command problem would happen independent of distance, right?

upvoted 2 times

? ?  Ausias18 2áyears, 2ámonths ago

answer are A, C, F
upvoted 10 times

? ?  bnlcnd 2áyears, 4ámonths ago

A - everyone agrees
B - the VM should have persistent disk not cloud storage.
F - no other choices left :)

upvoted 6 times

? ?  amxexam 1áyear, 1ámonth ago

Not necessary a db vm

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

522/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company Overview -

Topic 2

JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One

of the company's core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon

output by 50% over the next 5 years.

Company Background -

JencoMart started as a general store in 1931, and has grown into one of the world's leading brands, known for great value and customer service.

Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart

has little presence in Asia, but considers that market key for future growth.

Solution Concept -

JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the

cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no

longer supported.

Existing Technical Environment -

JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed.

JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.

Application: Customer loyalty portal

LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.

Database -

Oracle Database stores user pro les

- 20 TB

- Complex table structure

- Well maintained, clean data

- Strong backup strategy

PostgreSQL database stores user credentials

- Single-homed in US West

- No redundancy

- Backed up every 12 hours

- 100% uptime service level agreement (SLA)

- Authenticates all users

Compute -

30 machines in US West Coast, each machine has:

- Twin, dual core CPUs

- 32 GB of RAM

- Twin 250 GB HDD (RAID 1)

20 machines in US East Coast, each machine has:

- Single, dual-core CPU

- 24 GB of RAM

- Twin 250 GB HDD (RAID 1)

Storage -

Access to shared 100 TB SAN in each location

Tape backup every week

Business Requirements -

Optimize for capacity during peak periods and value during off-peak periods

Guarantee service availability and support

Reduce on-premises footprint and associated  nancial and environmental impact

Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase

Expand services into Asia

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

523/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Technical Requirements -

Assess key application for cloud suitability

Modify applications for the cloud

Move applications to a new infrastructure

Leverage managed services wherever feasible

Sunset 20% of capacity in existing data centers

Decrease latency in Asia

CEO Statement -

JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is

in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the

environment through green initiatives and policies.

CTO Statement -

The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a

public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.

CFO Statement -

Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to

outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak

periods and reduce costs.

Question

JencoMart wants to move their User Pro les database to Google Cloud Platform.

Which Google Database should they use?

A. Cloud Spanner

B. Google BigQuery

C. Google Cloud SQL

D. Google Cloud Datastore

Correct Answer: D

Common workloads for Google Cloud Datastore:
? User pro les
? Product catalogs
? Game state
Reference:

https://cloud.google.com/storage-options/

https://cloud.google.com/datastore/docs/concepts/overview

Community vote distribution

D (50%)

A (50%)

? ?  JJu  Highly Voted ?  3áyears, 7ámonths ago
answer is D. Google Cloud Datastorage
Google Cloud Datastorage use:
* User profile
* game state
* product catalogs

upvoted 25 times

? ?  Ishu_awsguy 9ámonths, 2áweeks ago

They have a relational oracle DB with complex table structure. I dont think Data storw will work for them.
Answer should be A ( Cloud Spanner)

upvoted 3 times

? ?  Ishu_awsguy 9ámonths, 2áweeks ago

But yeah
it is debatable on how much moderization we take.
With Cloudspanner ( require less modernisation compared to Datastore)

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

524/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

With Cloud SQL ( Require less modernisation compared to Datastore)
With datastore - Best solution eventually - but the most tricky migration and modernisation.
very subjective question

upvoted 4 times

? ?  vvillar 2áyears, 2ámonths ago

datastore*

upvoted 1 times

? ?  dabrat  Highly Voted ?  3áyears, 7ámonths ago

oracle= Relational
+Gloabal = spanner =>A)

upvoted 15 times

? ?  DrLu 3áyears, 6ámonths ago
Technical Requirements
ò Assess key application for cloud suitability.
ò Modify application for the cloud. **(Which means possible to change the code or database when it migrate to GCP
)

upvoted 5 times

? ?  tartar 2áyears, 10ámonths ago

D is ok
https://cloud.google.com/datastore/docs/concepts/overview
Datastore is ideal for applications that rely on highly available structured data at scale. You can use Datastore to store and query all
of the following types of data:

Product catalogs that provide real-time inventory and product details for a retailer.
User profiles that deliver a customized experience based on the userÆs past activities and preferences.
Transactions based on ACID properties, for example, transferring funds from one bank account to another.

upvoted 10 times

? ?  nitinz 2áyears, 3ámonths ago
D, profiles go to datastore

upvoted 1 times

? ?  VishalB 1áyear, 11ámonths ago

User Profile not necessary would have relational database, Datastore is the best option for User Profile

upvoted 1 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 5ádays ago

A: Spanner retains the RDBMS compatibility and helps to reduce latency for any Asia resources

B: BigQuery no good for OLTP

C: CloudSQL, the closest option we have to Oracle

D: Significant architectural change but potentially a great alternative to RDBMS, document database typically a good fit for user data & as
others have said it can eliminate the complex RDBMS structure and queries

The problem I have with Spanner is the cost, it is extremely expensive compared to the other options, so is it really needed?

But when I review the requierments I don't see cost as being up there whilst latency to Asia it is, so whilst I intuitively picked Datastore I'll
revise to Spanner.

upvoted 1 times

? ?  BiddlyBdoyng 1áweek, 5ádays ago

I don't get why B isn't correct. Cloud Storage isn't a good replacement for SAN, especially when we have a database running on some of
the VMs.

upvoted 1 times

? ?  TheCloudGuruu 1ámonth, 1áweek ago

Selected Answer: D

Datastore is best for user profiles

upvoted 1 times

? ?  geekgirl007 2ámonths, 2áweeks ago

Selected Answer: D

says here https://cloud.google.com/datastore/docs/concepts/overview

upvoted 1 times

? ?  taer 2ámonths, 3áweeks ago

Selected Answer: A

Cloud Spanner is a fully managed, scalable, and globally distributed relational database service. It provides strong consistency, high
availability, and low-latency capabilities, which would be suitable for JencoMart's User Profiles database requirements.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

525/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  HD2023 3ámonths ago

Selected Answer: D

Option D

upvoted 1 times

? ?  Deb2293 3ámonths, 2áweeks ago

Selected Answer: A

Oracle (relational) + global => Spanner

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: A

This is pretty straightforward.
Relational + Global = Spanner.

DS not meant for TB size.

upvoted 1 times

? ?  zerg0 4ámonths, 2áweeks ago

Selected Answer: A

Spanner. - Global
upvoted 1 times

? ?  GopeshSahu 5ámonths ago

Selected Answer: A

Reference: https://cloud.google.com/solutions/migrate-oracle-workloads
Rewrite Strategy will apply as They have a relational oracle DB with complex table structure and business expanding globally.

Rewrite your application to take full advantage of cloud-native databases. If your application requires a relational database with global
scalability, you can migrate to Cloud Spanner, which provides scalability with an industry-leading high availability of 99.999% SLA.

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 2 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is D

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

A. Cloud Spanner
upvoted 1 times

? ?  victory108 1áyear, 5ámonths ago

D is correct --> Google Cloud Datastore

upvoted 1 times

? ?  jasim21 2áyears, 2ámonths ago

The only lift & shift option for Oracle DB is bare metal. To host on cloud spanner you need to rewrite the code. Since application
modification is allowed as per technical requirement why not rewrite to most suitable product of GCP for user profile i.e. Datastore.

The answer is D
Reference: https://cloud.google.com/solutions/migrate-oracle-workloads

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

526/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 3 - Testlet 10

Question #1

Introductory Info

Company overview -

Topic 3

Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and

several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the

races all over the world with live telemetry and predictions throughout each race.

Solution concept -

HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions.

Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and

recorded, closer to their users.

Existing technical environment -

HRL is a public cloud- rst company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and

editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and

local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public

cloud provider. Their existing technical environment is as follows:

Existing content is stored in an object storage service on their existing public cloud provider.

Video encoding and transcoding is performed on VMs created for each job.

Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.

Business requirements -

HRL's owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:

Support ability to expose the predictive models to partners.

Increase predictive capabilities during and before races:

?ùï Race results

?ùï Mechanical failures

?ùï Crowd sentiment

Increase telemetry and create additional insights.

Measure fan engagement with new predictions.

Enhance global availability and quality of the broadcasts.

Increase the number of concurrent viewers.

Minimize operational complexity.

Ensure compliance with regulations.

Create a merchandising revenue stream.

Technical requirements -

Maintain or increase prediction throughput and accuracy.

Reduce viewer latency.

Increase transcoding performance.

Create real-time analytics of viewer consumption patterns and engagement.

Create a data mart to enable processing of large volumes of race data.

Executive statement -

Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video

streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the

facility to support real-time predictions during races and the capacity to process season-long results.

Question

For this question, refer to the Helicopter Racing League (HRL) case study. Your team is in charge of creating a payment card data vault for card

numbers used to bill tens of thousands of viewers, merchandise consumers, and season ticket holders. You need to implement a custom card

tokenization service that meets the following requirements:

* It must provide low latency at minimal cost.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

527/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

* It must be able to identify duplicate credit cards and must not store plaintext card numbers.

* It should support annual key rotation.

Which storage approach should you adopt for your tokenization service?

A. Store the card data in Secret Manager after running a query to identify duplicates.

B. Encrypt the card data with a deterministic algorithm stored in Firestore using Datastore mode.

C. Encrypt the card data with a deterministic algorithm and shard it across multiple Memorystore instances.

D. Use column-level encryption to store the data in Cloud SQL.

Correct Answer: D

Community vote distribution

B (97%)

? ?  Neo_ACE  Highly Voted ?  1áyear, 7ámonths ago

Answer would be B

https://cloud.google.com/community/tutorials/pci-tokenizer

Deterministic output means that a given set of inputs (card number, expiration, and userID) will always generate the same token. This is
useful if you want to rely on the token value to deduplicate your token stores. You can simply match a newly generated token to your
existing catalog of tokens to determine whether the card has been previously stored. Depending on your application architecture, this can
be a very useful feature. However, this could also be accomplished using a salted hash of the input values.

https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss
Firestore is the next major version of Datastore. Firestore can run in Datastore mode, which uses the same API as Datastore and scales to
millions of writes per second,

upvoted 30 times

? ?  technodev  Highly Voted ?  1áyear, 5ámonths ago
Got this question in my exam, answered B

upvoted 17 times

? ?  sampon279  Most Recent ?  17áhours, 19áminutes ago

Selected Answer: B

Between B (firestore in datastore mode)and D (Cloud SQL) B is better solution since firestore is preferred for low latency queries, also since
firestore is in datastore mode (does not include real time capabilities supported in native mode - i.e mobile updates) it's cost effective.

upvoted 1 times

? ?  mimicha1 1áweek, 4ádays ago

Why not C ?

upvoted 1 times

? ?  BiddlyBdoyng 1áweek, 5ádays ago

From what I can work out column level encryption needs to be implemented by the client in Cloud SQL.

So both B & D are identical solutions except for the database type?

Cloud SQL seems to do a better job of the avoiding duplicates requirement & seems a better fit.

Don't see why B seems to be so popular, would have expect a bigger split on the vote. Am I missing something

upvoted 1 times

? ?  tdotcat 5ámonths, 2áweeks ago

Selected Answer: B

B fits the case
upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: B

B Is the Correct Answer

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

528/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

B as its clear in the example by google https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss

upvoted 3 times

? ?  aut0pil0t 9ámonths, 4áweeks ago

Selected Answer: B

B, but should be reworded as follows for clarify.

"B. Encrypt the card data with a deterministic algorithm and store in Firestore using Datastore mode."

https://cloud.google.com/architecture/tokenizing-sensitive-cardholder-data-for-pci-dss#a_service_for_handling_sensitive_information

upvoted 2 times

? ?  AzureDP900 11ámonths, 4áweeks ago

I would go with B.

upvoted 1 times

? ?  cpi_web 1áyear, 1ámonth ago

Hmmm. What is about the very first point low latency? Firefstore is not the one with best latency values...

https://cloud.google.com/architecture/building-scalable-apps-with-cloud-firestore#latency

upvoted 1 times

? ?  kapara 1áyear, 1ámonth ago

Selected Answer: D

ans is D

upvoted 1 times

? ?  mad314 1áyear, 2ámonths ago

Selected Answer: B

Had this question on my exam.

upvoted 5 times

? ?  slars2k 1áyear, 2ámonths ago

Considering low latency and minimal cost, will go with D.

upvoted 2 times

? ?  SAMBIT 1áyear, 3ámonths ago

Key rotation is a problem with column level encryption.. hence D is rejected

upvoted 3 times

? ?  jay9114 1áyear, 1ámonth ago

Agreed. Reading the key rotation section of https://cloud.google.com/bigquery/docs/column-key-encrypt it states, "It is not possible to
rotate a wrapped keyset using the KEYS.ROTATE_KEYSET function."

upvoted 1 times

? ?  victory108 1áyear, 5ámonths ago

B. Encrypt the card data with a deterministic algorithm stored in Firestore using Datastore mode.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

529/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company overview -

Topic 3

Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and

several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the

races all over the world with live telemetry and predictions throughout each race.

Solution concept -

HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions.

Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and

recorded, closer to their users.

Existing technical environment -

HRL is a public cloud- rst company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and

editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and

local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public

cloud provider. Their existing technical environment is as follows:

Existing content is stored in an object storage service on their existing public cloud provider.

Video encoding and transcoding is performed on VMs created for each job.

Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.

Business requirements -

HRL's owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:

Support ability to expose the predictive models to partners.

Increase predictive capabilities during and before races:

?ùï Race results

?ùï Mechanical failures

?ùï Crowd sentiment

Increase telemetry and create additional insights.

Measure fan engagement with new predictions.

Enhance global availability and quality of the broadcasts.

Increase the number of concurrent viewers.

Minimize operational complexity.

Ensure compliance with regulations.

Create a merchandising revenue stream.

Technical requirements -

Maintain or increase prediction throughput and accuracy.

Reduce viewer latency.

Increase transcoding performance.

Create real-time analytics of viewer consumption patterns and engagement.

Create a data mart to enable processing of large volumes of race data.

Executive statement -

Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video

streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the

facility to support real-time predictions during races and the capacity to process season-long results.

Question

For this question, refer to the Helicopter Racing League (HRL) case study. Recently HRL started a new regional racing league in Cape Town, South

Africa. In an effort to give customers in Cape Town a better user experience, HRL has partnered with the Content Delivery Network provider, Fastly.

HRL needs to allow tra c coming from all of the Fastly IP address ranges into their Virtual Private Cloud network (VPC network). You are a

member of the HRL security team and you need to con gure the update that will allow only the Fastly IP address ranges through the External

HTTP(S) load balancer. Which command should you use?

A.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

530/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B.

C.

D.

Correct Answer: A

Reference:

https://cloud.google.com/load-balancing/docs/https

? ?  technodev  Highly Voted ?  1áyear, 5ámonths ago
Got this question in my exam, answered D

upvoted 33 times

? ?  elrizos  Highly Voted ?  1áyear, 2ámonths ago

Is D:
In the GCP doc can see the same example
https://cloud.google.com/armor/docs/configure-security-policies#gcloud_11
"gcloud compute security-policies rules create 1000 \
--security-policy my-policy \
--expression "evaluatePreconfiguredExpr('sourceiplist-fastly')" \
--action "allow"
"

upvoted 22 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 5ádays ago

A. Looks like it opens to all IPs

B. Incorrect syntax "ACTION must be one of: allow, deny, goto_next."

C. Incorrect syntax "ACTION must be one of: allow, deny, goto_next."

D. Assuming the preconfigured expression is good then its right.

upvoted 1 times

? ?  LaxmanTiwari 4áweeks, 1áday ago

answer is D

upvoted 1 times

? ?  rr4444 3ámonths, 2áweeks ago

D, def not A

D is shown at https://cloud.google.com/armor/docs/configure-security-policies#use-console-gcloud

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

Why is no vote being allowed on this question?

upvoted 2 times

? ?  romandrigo 3ámonths, 3áweeks ago

Answer is D

upvoted 1 times

? ?  csestony 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

531/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Answers is D
I found the following useful
https://cloud.google.com/armor/docs/integrating-cloud-armor#https-vpc-firewall-rules

upvoted 6 times

? ?  csestony 6ámonths ago

"Google Cloud Armor security policies and VPC firewall rules have different functions:

Google Cloud Armor security policies provide edge security and act on client traffic to Google Front Ends (GFEs).
VPC firewall rules allow or deny traffic to and from your backends. You must create ingress allow firewall rules, whose targets are the
load-balanced backend VMs, and whose sources are IP ranges used by global external HTTP(S) load balancers or global external
HTTP(S) load balancer (classic)s. These rules allow GFEs and the health check systems to communicate with your backend VMs."

upvoted 3 times

? ?  omermahgoub 6ámonths ago

Option A, gcloud compute security-policies rules update 1000 --security-policy=from-fastly --src-ip-ranges=* --action=allow, would allow all
source IP ranges and is not specific to the Fastly IP address ranges.

Option B, gcloud compute firewall-rules update sourceiplist-fastly --priority=1000 --allow=tcp:443, allows traffic over TCP port 443 (HTTPS)
but does not specify any target tags or source IP ranges.

C. gcloud compute firewall-rules update hlr-policy --priority=1000 --target-tags=sourceiplist-fastly --allow=tcp:443. This command updates
a firewall rule with the name "hlr-policy" to allow traffic over TCP port 443 (HTTPS) from any instances with the tag "sourceiplist-fastly". The
priority of the rule is set to 1000, which means that it will be evaluated before any other rules with lower priority.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

The command gcloud compute security-policies rules update 1000 --security-policy=hlr-policy --
expression="evaluatePreconfigurationExpr(sourceiplist-fastly)" --action=allow updates a security policy rule with the name "1000" in
the security policy "hlr-policy". The rule allows traffic that matches the specified expression and has the action "allow".

In this particular case, the expression used in the rule is "evaluatePreconfigurationExpr(sourceiplist-fastly)". The
"evaluatePreconfigurationExpr" function is used to reference a preconfigured expression, in this case, the tag "sourceiplist-fastly". This
means that the rule will allow traffic from any instances that have the "sourceiplist-fastly" tag.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

HTTP(S) Load Balancer (HLB) is a load balancing service that distributes incoming traffic among the healthy instances of your
application. It routes traffic based on the IP address and port of the incoming request, and it can handle both HTTP and HTTPS
traffic. In this scenario, the HLB would be used to distribute traffic among the instances in the VPC network that are serving the
content for the HRL races.

Cloud Armor is a network security service that provides protection against DDoS attacks and other threats. It allows you to create
security policies that define rules for traffic coming to your resources, such as HLBs. In this scenario, Cloud Armor would be used to
enforce the security policy "hlr-policy" and allow traffic from the Fastly IP address ranges through the HLB.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

D is the correct answer

upvoted 1 times

? ?  jaxclain 6ámonths, 3áweeks ago

D for sure

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

D for sure

upvoted 1 times

? ?  bossdellacert 9ámonths, 3áweeks ago
The answer is D, for 2 reasons:
1. They want to apply a firewall rule at HTTPS LB level. This can be done only with cloud armor. Cloud armor does not work with classic
firewall; it works with security policies.
2. D is the only answer that points to a list of IPs to whitelist, via preconfigured expression. The others will open to too broad IPs.

upvoted 6 times

? ?  Nirca 9ámonths, 3áweeks ago

answer is D

upvoted 1 times

? ?  aut0pil0t 9ámonths, 4áweeks ago

D. This is a Cloud Armour question, and interestingly "sourceiplist-fastly" is actually one of the GCP's preconfigured expressions.

(project)$ gcloud compute security-policies list-preconfigured-expression-sets | grep fastly
EXPRESSION_SET: sourceiplist-fastly

https://cloud.google.com/armor/docs/configure-security-policies#list-preconfig-rules

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

532/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Crick76 10ámonths ago

Agree D

upvoted 1 times

? ?  chinmay7682 10ámonths, 1áweek ago

Answer is D

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

533/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company overview -

Topic 3

Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and

several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the

races all over the world with live telemetry and predictions throughout each race.

Solution concept -

HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions.

Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and

recorded, closer to their users.

Existing technical environment -

HRL is a public cloud- rst company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and

editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and

local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public

cloud provider. Their existing technical environment is as follows:

Existing content is stored in an object storage service on their existing public cloud provider.

Video encoding and transcoding is performed on VMs created for each job.

Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.

Business requirements -

HRL's owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:

Support ability to expose the predictive models to partners.

Increase predictive capabilities during and before races:

?ùï Race results

?ùï Mechanical failures

?ùï Crowd sentiment

Increase telemetry and create additional insights.

Measure fan engagement with new predictions.

Enhance global availability and quality of the broadcasts.

Increase the number of concurrent viewers.

Minimize operational complexity.

Ensure compliance with regulations.

Create a merchandising revenue stream.

Technical requirements -

Maintain or increase prediction throughput and accuracy.

Reduce viewer latency.

Increase transcoding performance.

Create real-time analytics of viewer consumption patterns and engagement.

Create a data mart to enable processing of large volumes of race data.

Executive statement -

Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video

streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the

facility to support real-time predictions during races and the capacity to process season-long results.

Question

For this question, refer to the Helicopter Racing League (HRL) case study. The HRL development team releases a new version of their predictive

capability application every Tuesday evening at 3 a.m. UTC to a repository. The security team at HRL has developed an in-house penetration test

Cloud Function called

Airwolf. The security team wants to run Airwolf against the predictive capability application as soon as it is released every Tuesday. You need to

set up Airwolf to run at the recurring weekly cadence. What should you do?

A. Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

534/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B. Set up a Cloud Logging sink and a Cloud Storage bucket that triggers a Cloud Function.

C. Con gure the deployment job to notify a Pub/Sub queue that triggers a Cloud Function.

D. Set up Identity and Access Management (IAM) and Con dential Computing to trigger a Cloud Function.

Correct Answer: A

Community vote distribution

C (81%)

Other

? ?  umashankar_a  Highly Voted ?  1áyear, 11ámonths ago

Answer C seems to be ok. Triggering Pub/Sub to invoke Cloud Functions seems to be relevant. Cloud Storage doesn't make any sense. It
would have been straight forward if Cloud Scheduler is mentioned in Option C instead of Deployment Job. But if you make a bit of
research on deployment jobs, it's pointing me to cron jobs which is making perfect sense.
https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml
https://cloud.google.com/scheduler/docs/tut-pub-sub

upvoted 48 times

? ?  elainexs 1áyear ago

Cannot understand why push CICD event to pub/sub... which is only one event, why need pub/sub

upvoted 5 times

? ?  stefanop 1áyear, 8ámonths ago

But the question requires a scheduled execution, not one triggered by the deployment job. ShouldnÆt A be the correct answer?

upvoted 5 times

? ?  Nimbus2021 1áyear, 6ámonths ago

I think no because question mentions "as soon as it is released every Tuesday."

upvoted 2 times

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is A

upvoted 18 times

? ?  nandoD 2ámonths, 1áweek ago

Please elaborate.
upvoted 1 times

? ?  sampon279  Most Recent ?  17áhours, 6áminutes ago

Selected Answer: C

Should be C. Cannot be A, to schedule cloud task you need to know when the deployment is complete, deployments usually are
unpredictable and do not meet scheduled time. With option C, CICD pipeline which deploys the code and publish a message to pub/sub to
trigger cloud function - better solution to trigger via http endpoint if that is an option. pub/sub is till okay.

upvoted 1 times

? ?  WinSxS 3ámonths, 2áweeks ago

Selected Answer: C

To run Airwolf against the predictive capability application as soon as it is released every Tuesday, you should configure the deployment
job to notify a Pub/Sub queue that triggers a Cloud Function.

upvoted 1 times

? ?  zerg0 4ámonths, 3áweeks ago

Selected Answer: A

Cloud task is supports scheduling

upvoted 1 times

? ?  tdotcat 5ámonths, 2áweeks ago

Selected Answer: C

c fits scenario
upvoted 1 times

? ?  main_street 6ámonths ago

Answer A seems correct since cloud tasks support scheduled delivery but pub/sub doesn't
see https://cloud.google.com/pubsub/docs/choosing-pubsub-or-cloud-tasks

upvoted 2 times

? ?  jlambdan 1áweek, 3ádays ago

https://cloud.google.com/tasks/docs/comp-tasks-sched
it seems to be scheduling of task ahead of time, not scheduling at fixed time interval.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

535/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  omermahgoub 6ámonths ago

To set up Airwolf to run at a recurring weekly cadence, the correct option would be C: Configure the deployment job to notify a Pub/Sub
queue that triggers a Cloud Function.

To set up Airwolf to run at the desired weekly cadence, you can configure the deployment job to send a notification to a Pub/Sub queue
when a new version of the predictive capability application is released. Then, you can set up a Cloud Function that is triggered by
messages in the Pub/Sub queue and runs the Airwolf penetration test. This way, the Cloud Function will be triggered every time a new
message is published to the queue, which will occur every Tuesday evening at 3 a.m. UTC when a new version of the application is
released.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

Option A, Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function, would not be the correct solution because
Cloud Tasks is a service for creating and managing asynchronous tasks that are executed later, but it does not support recurring
schedules.

Option B, Set up a Cloud Logging sink and a Cloud Storage bucket that triggers a Cloud Function, would not be the correct solution
because Cloud Logging is a service for collecting, viewing, and analyzing logs, but it does not support triggering Cloud Functions on a
recurring basis.

Option D, Set up Identity and Access Management (IAM) and Confidential Computing to trigger a Cloud Function, would not be the
correct solution because IAM is a service for managing access to Google Cloud resources and Confidential Computing is a service for
running sensitive workloads in hardware-isolated environments, but neither of these services can be used to trigger Cloud Functions
on a recurring basis.

upvoted 3 times

? ?  kat1969 5ámonths, 3áweeks ago

This conflicts with your earlier statements? Is this statement intended as a correction?

upvoted 1 times

? ?  nandoD 2ámonths, 1áweek ago

how I see it, the first post is the correct answer explanation, the second post is why the other 3 answers are wrong.

upvoted 1 times

? ?  thamaster 6ámonths ago

answer A does not make sense why put a cloud task and check a storage (which is never updated) for cloud function? If the release has
some late the task run for nothing.
Pub sub + cloud function is best practice

upvoted 2 times

? ?  amelm 4ámonths ago

That's what I taught too. "Why do I need Cloud Storage?"

upvoted 1 times

? ?  omermahgoub 6ámonths ago

The correct answer is A: Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function.

To set up Airwolf to run at a recurring weekly cadence, you should set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud
Function.

Cloud Tasks is a fully managed service that allows you to schedule and execute background jobs in a scalable and reliable way. You can use
Cloud Tasks to create a recurring task that runs at a specified interval (e.g., every week). When the task is triggered, it can send a message
to a Cloud Storage bucket, which can then trigger a Cloud Function to run the Airwolf penetration test.

Option B: Setting up a Cloud Logging sink and a Cloud Storage bucket would not allow you to schedule the task to run at a recurring
weekly cadence.

Option C: Configuring the deployment job to notify a Pub/Sub queue would not allow you to schedule the task to run at a recurring weekly
cadence.

Option D: Setting up Identity and Access Management (IAM) and Confidential Computing would not allow you to schedule the task to run
at a recurring weekly cadence.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C Is the Correct Answer

upvoted 1 times

? ?  Jackalski 7ámonths ago

Selected Answer: A

I vote on A
cloud task can trigger CF ... with limit to 30 days - here it is weekly - so far so good
however not sure why it would need any cloud storage .. potentially to store results of dony by CF

answer C - has no schedule option

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

536/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

example:
https://cloud.google.com/tasks/docs/tutorial-gcf

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  6721sora 9ámonths, 3áweeks ago

Selected Answer: C

C is ok
Needs to be triggered by the deployment and not on a schedule. Cloud storage doesn't seem relevant in the context of the question

upvoted 3 times

? ?  Kiroo 1ámonth, 3áweeks ago

Being honest, neither A or C seems entirely correct, to me the one that seems to be cheaper is C

upvoted 1 times

? ?  shekarcfc 9ámonths, 3áweeks ago

Selected Answer: C

IMHO, the question is not clear. Is it a git or object repository. If its git repository than there need to be a logging or webhook that triggers
the cloud function.. benefit of doubt goes to C.

upvoted 2 times

? ?  Nirca 9ámonths, 3áweeks ago

Selected Answer: C

C for sure

upvoted 2 times

? ?  kuboraam 9ámonths, 3áweeks ago

Selected Answer: B

A - NOK - is not reliable because Cloud Tasks could run before the repository is updated, there could be delays.
B - OK. Tie the two events together independently. https://cloud.google.com/logging/docs/export/configure_export_v2#supported-
destinations
C - NOK - requires the dev team to make changes to support the security team. WHy would they do that? keep things independent.
D - Nothing to do with this question.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

537/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company overview -

Topic 3

Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and

several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the

races all over the world with live telemetry and predictions throughout each race.

Solution concept -

HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions.

Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and

recorded, closer to their users.

Existing technical environment -

HRL is a public cloud- rst company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and

editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and

local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public

cloud provider. Their existing technical environment is as follows:

Existing content is stored in an object storage service on their existing public cloud provider.

Video encoding and transcoding is performed on VMs created for each job.

Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.

Business requirements -

HRL's owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:

Support ability to expose the predictive models to partners.

Increase predictive capabilities during and before races:

?ùï Race results

?ùï Mechanical failures

?ùï Crowd sentiment

Increase telemetry and create additional insights.

Measure fan engagement with new predictions.

Enhance global availability and quality of the broadcasts.

Increase the number of concurrent viewers.

Minimize operational complexity.

Ensure compliance with regulations.

Create a merchandising revenue stream.

Technical requirements -

Maintain or increase prediction throughput and accuracy.

Reduce viewer latency.

Increase transcoding performance.

Create real-time analytics of viewer consumption patterns and engagement.

Create a data mart to enable processing of large volumes of race data.

Executive statement -

Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video

streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the

facility to support real-time predictions during races and the capacity to process season-long results.

Question

For this question, refer to the Helicopter Racing League (HRL) case study. HRL wants better prediction accuracy from their ML prediction models.

They want you to use Google's AI Platform so HRL can understand and interpret the predictions. What should you do?

A. Use Explainable AI.

B. Use Vision AI.

C. Use Google Cloud's operations suite.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

538/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D. Use Jupyter Notebooks.

Correct Answer: A

Reference:

https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/preparing-metadata

Community vote distribution

A (100%)

? ?  umashankar_a  Highly Voted ?  1áyear, 11ámonths ago

Answer A
AI Explanations helps you understand your model's outputs for classification and regression tasks. Whenever you request a prediction on
AI Platform, AI Explanations tells you how much each feature in the data contributed to the predicted result. You can then use this
information to verify that the model is behaving as expected, recognize bias in your models, and get ideas for ways to improve your model
and your training data.
https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview

upvoted 26 times

? ?  technodev  Highly Voted ?  1áyear, 5ámonths ago
Got this question in my exam, answered A

upvoted 10 times

? ?  omermahgoub  Most Recent ?  6ámonths ago

The correct answer is A: Use Explainable AI.

To understand and interpret the predictions made by HRL's ML prediction models, you should use Explainable AI. Explainable AI, also
known as XAI, is a suite of tools and techniques that helps you understand and interpret the predictions made by machine learning
models. With Explainable AI, you can get insights into how the model made a particular prediction, which can help you understand the
underlying factors that influenced the prediction. This can help HRL improve the accuracy of their predictions and make more informed
decisions based on the output of their models.

Option B: Vision AI is a suite of tools and services that helps you build and deploy computer vision applications. It is not relevant to
understanding and interpreting the predictions made by HRL's ML prediction models.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

Option C: Google Cloud's operations suite is a set of tools and services that helps you monitor, troubleshoot, and optimize your Google
Cloud resources. It is not relevant to understanding and interpreting the predictions made by HRL's ML prediction models.

Option D: Jupyter Notebooks is an open-source web application that allows you to create and share documents that contain live code,
equations, visualizations, and narrative text. It is not relevant to understanding and interpreting the predictions made by HRL's ML
prediction models.

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  6721sora 9ámonths, 3áweeks ago

Selected Answer: A

Explainable AI now included with Vertex AI
https://cloud.google.com/explainable-ai

upvoted 2 times

? ?  AzureDP900 11ámonths, 4áweeks ago

Answer is A.

https://cloud.google.com/explainable-ai

upvoted 1 times

? ?  AzureDP900 11ámonths, 4áweeks ago

https://cloud.google.com/aiplatform/prediction/docs/ai-explanations/overview

upvoted 1 times

? ?  mad314 1áyear, 2ámonths ago

Selected Answer: A

Had this quection on my exam.

upvoted 2 times

? ?  esnecho 1áyear, 6ámonths ago

Selected Answer: A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

539/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is Correct

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 4 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

A. Use Explainable AI.

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 4 times

? ?  wilwong 1áyear, 11ámonths ago

A. Use Explainable AI.

upvoted 2 times

? ?  kopper2019 1áyear, 12ámonths ago
cloudstd 1 day, 9 hours ago
answer: A
upvoted 1 times
juccjucc 1 day, 12 hours ago
is it A?

upvoted 2 times

? ?  XDevX 1áyear, 12ámonths ago

I think A is correct - A helps to better understand your models and to improve them.
See for example the comments of the customers, particularly vivint and robot:
https://cloud.google.com/explainable-ai

upvoted 2 times

? ?  muhasinem 1áyear, 12ámonths ago

A. Use Explainable AI.
https://cloud.google.com/explainable-ai
Explainable AI is a set of tools and frameworks to help you understand and interpret predictions made by your machine learning models.
With it, you can debug and improve model performance, and help others understand your models' behavior. You can also generate
feature attributions for model predictions in AutoML Tables and AI Platform, and visually investigate model behavior using the What-If
Tool.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

540/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company overview -

Topic 3

Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and

several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the

races all over the world with live telemetry and predictions throughout each race.

Solution concept -

HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions.

Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and

recorded, closer to their users.

Existing technical environment -

HRL is a public cloud- rst company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and

editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and

local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public

cloud provider. Their existing technical environment is as follows:

Existing content is stored in an object storage service on their existing public cloud provider.

Video encoding and transcoding is performed on VMs created for each job.

Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.

Business requirements -

HRL's owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:

Support ability to expose the predictive models to partners.

Increase predictive capabilities during and before races:

?ùï Race results

?ùï Mechanical failures

?ùï Crowd sentiment

Increase telemetry and create additional insights.

Measure fan engagement with new predictions.

Enhance global availability and quality of the broadcasts.

Increase the number of concurrent viewers.

Minimize operational complexity.

Ensure compliance with regulations.

Create a merchandising revenue stream.

Technical requirements -

Maintain or increase prediction throughput and accuracy.

Reduce viewer latency.

Increase transcoding performance.

Create real-time analytics of viewer consumption patterns and engagement.

Create a data mart to enable processing of large volumes of race data.

Executive statement -

Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video

streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the

facility to support real-time predictions during races and the capacity to process season-long results.

Question

For this question, refer to the Helicopter Racing League (HRL) case study. HRL is looking for a cost-effective approach for storing their race data

such as telemetry. They want to keep all historical records, train models using only the previous season's data, and plan for data growth in terms

of volume and information collected. You need to propose a data solution. Considering HRL business requirements and the goals expressed by

CEO S. Hawke, what should you do?

A. Use Firestore for its scalable and  exible document-based database. Use collections to aggregate race data by season and event.

B. Use Cloud Spanner for its scalability and ability to version schemas with zero downtime. Split race data using season as a primary key.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

541/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Use BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season.

D. Use Cloud SQL for its ability to automatically manage storage increases and compatibility with MySQL. Use separate database instances

for each season.

Correct Answer: C

Reference:

https://cloud.google.com/bigquery/public-data

Community vote distribution

C (100%)

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is C

upvoted 16 times

? ?  Wonka 1áyear, 5ámonths ago

These questions are sounding too simple, are these really coming in exam or these are mocked up?

upvoted 3 times

? ?  ashrafh 7ámonths, 1áweek ago

In exam actually :)

upvoted 5 times

? ?  victory108  Highly Voted ?  1áyear, 11ámonths ago

C. Use BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season.

upvoted 6 times

? ?  examch  Most Recent ?  5ámonths, 1áweek ago

Selected Answer: C

C is the correct Answer,
We can use BigQuery for making Predictions with live and trained data with Cloud ML Engine, and BigQuery can handle large amounts of
data.

Refer to the Link for more details on Game Predictions,

https://cloud.google.com/blog/products/gcp/architecting-live-ncaa-predictions-from-archives-to-insights

upvoted 3 times

? ?  omermahgoub 6ámonths ago

For HRL's data storage needs, it is recommended to use BigQuery due to its scalability and ability to handle large amounts of data. By
partitioning the race data based on season, HRL can easily access and query specific seasons' data while also taking into consideration
future data growth. BigQuery also allows for the flexibility to add new columns to the schema as needed, making it easier to adapt to
changes in the data being collected. Additionally, BigQuery's pay-per-use pricing model allows HRL to only pay for the data storage and
querying they use, making it a cost-effective solution.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Nirca 9ámonths, 3áweeks ago

Selected Answer: C

C it is

upvoted 1 times

? ?  Nirca 11ámonths, 1áweek ago

c C C c

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

542/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  AzureDP900 11ámonths, 4áweeks ago

C most obvious when data is growing day by day.

upvoted 1 times

? ?  mad314 1áyear, 2ámonths ago

Selected Answer: C

Had this question on my exam.

upvoted 2 times

? ?  SAMBIT 1áyear, 3ámonths ago

Telemetry data in a relational table..what?? Why they gave firestore then

upvoted 3 times

? ?  technodev 1áyear, 5ámonths ago

Got this question in my exam, answered C

upvoted 3 times

? ?  Pime13 1áyear, 6ámonths ago

to me it's C -> https://cloud.google.com/architecture/mobile-gaming-analysis-telemetry

upvoted 4 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C seems to be the correct answer

upvoted 1 times

? ?  SamGCP 1áyear, 6ámonths ago

Selected Answer: C

https://cloud.google.com/architecture/mobile-gaming-analysis-telemetry

upvoted 1 times

? ?  vaibhav15 1áyear, 6ámonths ago

vote C

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

543/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company overview -

Topic 3

Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and

several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the

races all over the world with live telemetry and predictions throughout each race.

Solution concept -

HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions.

Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and

recorded, closer to their users.

Existing technical environment -

HRL is a public cloud- rst company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and

editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and

local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public

cloud provider. Their existing technical environment is as follows:

Existing content is stored in an object storage service on their existing public cloud provider.

Video encoding and transcoding is performed on VMs created for each job.

Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.

Business requirements -

HRL's owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:

Support ability to expose the predictive models to partners.

Increase predictive capabilities during and before races:

?ùï Race results

?ùï Mechanical failures

?ùï Crowd sentiment

Increase telemetry and create additional insights.

Measure fan engagement with new predictions.

Enhance global availability and quality of the broadcasts.

Increase the number of concurrent viewers.

Minimize operational complexity.

Ensure compliance with regulations.

Create a merchandising revenue stream.

Technical requirements -

Maintain or increase prediction throughput and accuracy.

Reduce viewer latency.

Increase transcoding performance.

Create real-time analytics of viewer consumption patterns and engagement.

Create a data mart to enable processing of large volumes of race data.

Executive statement -

Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video

streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the

facility to support real-time predictions during races and the capacity to process season-long results.

Question

For this question, refer to the Helicopter Racing League (HRL) case study. A recent  nance audit of cloud infrastructure noted an exceptionally

high number of

Compute Engine instances are allocated to do video encoding and transcoding. You suspect that these Virtual Machines are zombie machines

that were not deleted after their workloads completed. You need to quickly get a list of which VM instances are idle. What should you do?

A. Log into each Compute Engine instance and collect disk, CPU, memory, and network usage statistics for analysis.

B. Use the gcloud compute instances list to list the virtual machine instances that have the idle: true label set.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

544/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Use the gcloud recommender command to list the idle virtual machine instances.

D. From the Google Console, identify which Compute Engine instances in the managed instance groups are no longer responding to health

check probes.

Correct Answer: C

Reference:

https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations

Community vote distribution

C (100%)

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is C

upvoted 21 times

? ?  khadar 9ámonths, 3áweeks ago

I too got this question in 10-09-22 exam with similar option and result is pass

upvoted 7 times

? ?  mkhaired  Highly Voted ?  1áyear, 12ámonths ago

C is the Correct answer
C. Use the gcloud recommender command to list the idle virtual machine instances.
https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations

upvoted 17 times

? ?  tdotcat  Most Recent ?  5ámonths, 2áweeks ago

answer C
check here :
https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-
recommendations#viewing_idle_vm_instance_recommendations

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: C

C is the answer which is to use recommender.
https://www.youtube.com/watch?v=VBsLG4jCHJk

upvoted 1 times

? ?  ATANGA 10ámonths, 4áweeks ago

WhatsApp : +1(956)-520-4006 to obtain PMP, CISM, CCNA, CEH, PRINCE2, CISCO, ISTTQB, PRINCE2, AWS/Azure/Sale force/ITIL
Foundation/EC- COUNCIL...
Get Certified with 100% pass guarantee. Pay after exam.
all CISCO, ISACA & EC- COUNCIL certifications
For the Below certificates we offer 100% pass guarantee:
1. AWS Certification
2. Sales force
3. Scrum Master
4. Oracle Certification: OCA, OCP
5. Cisco Certification: CCNA, CCNP, CCIE
6. ITIL Foundation & Intermediate
7. Prince 2 Foundation and Practitioner
8. VMWARE Certification
9. Check Point Certification
10. EC-COUNCIL Certification (CEH V-9, CCISO, CND)
11. Cloud Certification
12. IBM Certification
13. HP Certification
14. Citrix Certification
15. Juniper certification
16. Azure
17.Skype 70-333/34
18.PMI (PMP/CAPM/ACP/PBA)

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

545/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

19.ISTQB
20.SAP
21.ISACA (CISA, CISM, CRISC, CGEIT, COBIT)

PAYMENT ONLY AFTER CERTIFICATION AND RESULT CONFIRMATION.
WhatsApp : +1(956)-520-4006

upvoted 1 times

? ?  satamex 11ámonths ago

Selected Answer: C

Some one who can explain me why A is an answer? is it deliberate?

upvoted 1 times

? ?  Mikado211 10ámonths, 4áweeks ago

Don't pay too much attention to the "correct answer", the "most voted" is much more reliable.

upvoted 5 times

? ?  deenee 11ámonths, 2áweeks ago

Selected Answer: C

C looks decent as identification has to be done quickly. Manually checking each machines will take lot of time. Moreover--- even option A
says "CPUs" and not GPUs
"Log into each Compute Engine instance and collect disk, CPU, memory, and network usage statistics for analysis."

upvoted 2 times

? ?  AzureDP900 11ámonths, 4áweeks ago

C is right

upvoted 1 times

? ?  Tillssatya12 1áyear ago

All the quesrions had come from this site , especially couple of case studies and answer is C

upvoted 3 times

? ?  mad314 1áyear, 2ámonths ago

Selected Answer: C

Had this question on my exam.

upvoted 5 times

? ?  technodev 1áyear, 5ámonths ago

Got this question in my exam, answered C

upvoted 4 times

? ?  hightech 1áyear, 5ámonths ago

Did all the questions in the exam come from this site?

upvoted 1 times

? ?  Pime13 1áyear, 6ámonths ago

Selected Answer: C

to me should be C -> https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations#before-you-
begin

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 2 times

? ?  SamGCP 1áyear, 6ámonths ago

Selected Answer: C

https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations

upvoted 2 times

? ?  SamGCP 1áyear, 6ámonths ago

Selected Answer: C

https://cloud.google.com/compute/docs/instances/viewing-and-applying-idle-vm-recommendations

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

546/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 4 - Testlet 11

Question #1

Introductory Info

Company overview -

Topic 4

EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a

service to multi- national medical o ces, hospitals, and insurance providers.

Solution concept -

Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They

need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their

software at a fast pace. Google

Cloud has been chosen to replace their current colocation facilities.

Existing technical environment -

EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.

Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored

in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).

EHR is hosting several legacy  le- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced

over the next several years. There is no plan to upgrade or move these systems at the current time.

Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and

are often ignored.

Business requirements -

* On-board new insurance providers as quickly as possible.

* Provide a minimum 99.9% availability for all customer-facing systems.

* Provide centralized visibility and proactive action on system performance and usage.

* Increase ability to provide insights into healthcare trends.

* Reduce latency to all customers.

* Maintain regulatory compliance.

* Decrease infrastructure administration costs.

* Make predictions and generate reports on industry trends based on provider data.

Technical requirements -

* Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.

* Provide a consistent way to manage customer-facing applications that are container-based.

* Provide a secure and high-performance connection between on-premises systems and Google Cloud.

* Provide consistent logging, log retention, monitoring, and alerting capabilities.

* Maintain and manage multiple container-based environments.

* Dynamically scale and provision new environments.

* Create interfaces to ingest and process data from new providers.

Executive statement -

Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different

systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of miscon gured

systems, inadequate capacity to manage spikes in tra c, and inconsistent monitoring practices. We want to use Google Cloud to leverage a

scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us

for future growth.

Question

For this question, refer to the EHR Healthcare case study. You are responsible for ensuring that EHR's use of Google Cloud will pass an upcoming

privacy compliance audit. What should you do? (Choose two.)

A. Verify EHR's product usage against the list of compliant products on the Google Cloud compliance page.

B. Advise EHR to execute a Business Associate Agreement (BAA) with Google Cloud.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

547/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Use Firebase Authentication for EHR's user facing applications.

D. Implement Prometheus to detect and prevent security breaches on EHR's web-based applications.

E. Use GKE private clusters for all Kubernetes workloads.

Correct Answer: BD

Community vote distribution

AB (100%)

? ?  raf2121  Highly Voted ?  1áyear, 10ámonths ago

My option it's A & B

A - OK (Google Cloud compliance page will give list of products those are HIPAA compliant
https://cloud.google.com/security/compliance/offerings?
skip_cache=true#/regions=USA&industries=Healthcare_and_life_sciences&focusArea=Privacy)
B - OK (BAA means HIPAA Business Associate amendment or Business Associate Agreement entered into between Google and Customer.
With EHR being a leading provider of health record software, this agreement is required. https://cloud.google.com/files/gcp-hipaa-
overview-guide.pdf?hl=en)
C - Eliminated (Firebase authentication provides backend services, easy-to-use SDKs and ready-made libraries to users on App.
https://firebase.google.com/docs/auth)
D - Eliminated (more of an observability platform)
E - Eliminated (Running distributed services in GKE private clusters gives enterprises both secure and reliable services. Not sure how this
may help with Private Compliance Audit)

upvoted 35 times

? ?  SoniaJacob521  Highly Voted ?  1áyear, 10ámonths ago

A& B https://cloud.google.com/security/compliance/hipaa

upvoted 17 times

? ?  surajkrishnamurthy  Most Recent ?  6ámonths, 2áweeks ago

Selected Answer: AB

A & B is the correct answer

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: AB

A & B is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: AB

AB is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: AB

A, B that's the only two things that I see related

upvoted 1 times

? ?  AzureDP900 11ámonths, 4áweeks ago

A, B is straight forward, I didnÆt even think too much before making my mind. You need to read all case studies understand throughly
before the exam. This whole set of case studies waste lot of time if you donÆt prepare in advance and trying go through during exam. My
approach is focus on key words..

upvoted 6 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: AB

Agree with raf 2121 A& B

upvoted 2 times

? ?  mbenhassine1986 1áyear, 4ámonths ago

A & B
https://cloud.google.com/security/compliance/hipaa#customer_responsibilities

upvoted 2 times

? ?  muky31dec 1áyear, 4ámonths ago

Ans is A and B
upvoted 2 times

? ?  Arjun1983 1áyear, 5ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

548/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A and B

https://cloud.google.com/security/compliance/hipaa
Essential best practices:
1. Execute a Google Cloud BAA. You can request a BAA directly from your account manager.
2. Disable or otherwise ensure that you do not use Google Cloud Products that are not explicitly covered by the BAA (see Covered
Products) when working with PHI.

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: AB

I chose A&B by Elimination method.

upvoted 2 times

? ?  Pime13 1áyear, 6ámonths ago

A and B https://cloud.google.com/security/compliance/hipaa

upvoted 1 times

? ?  PhilipKoku 1áyear, 6ámonths ago

Selected Answer: AB

https://cloud.google.com/security/compliance/offerings?
skip_cache=true#/regions=USA&industries=Healthcare_and_life_sciences&focusArea=Privacy

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: AB

AB is the correct answer

upvoted 3 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: AB

Vote AB

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: AB

vote AB

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

549/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company overview -

Topic 4

EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a

service to multi- national medical o ces, hospitals, and insurance providers.

Solution concept -

Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They

need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their

software at a fast pace. Google

Cloud has been chosen to replace their current colocation facilities.

Existing technical environment -

EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.

Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored

in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).

EHR is hosting several legacy  le- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced

over the next several years. There is no plan to upgrade or move these systems at the current time.

Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and

are often ignored.

Business requirements -

* On-board new insurance providers as quickly as possible.

* Provide a minimum 99.9% availability for all customer-facing systems.

* Provide centralized visibility and proactive action on system performance and usage.

* Increase ability to provide insights into healthcare trends.

* Reduce latency to all customers.

* Maintain regulatory compliance.

* Decrease infrastructure administration costs.

* Make predictions and generate reports on industry trends based on provider data.

Technical requirements -

* Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.

* Provide a consistent way to manage customer-facing applications that are container-based.

* Provide a secure and high-performance connection between on-premises systems and Google Cloud.

* Provide consistent logging, log retention, monitoring, and alerting capabilities.

* Maintain and manage multiple container-based environments.

* Dynamically scale and provision new environments.

* Create interfaces to ingest and process data from new providers.

Executive statement -

Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different

systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of miscon gured

systems, inadequate capacity to manage spikes in tra c, and inconsistent monitoring practices. We want to use Google Cloud to leverage a

scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us

for future growth.

Question

For this question, refer to the EHR Healthcare case study. You need to de ne the technical architecture for securely deploying workloads to Google

Cloud. You also need to ensure that only veri ed containers are deployed using Google Cloud services. What should you do? (Choose two.)

A. Enable Binary Authorization on GKE, and sign containers as part of a CI/CD pipeline.

B. Con gure Jenkins to utilize Kritis to cryptographically sign a container as part of a CI/CD pipeline.

C. Con gure Container Registry to only allow trusted service accounts to create and deploy containers from the registry.

D. Con gure Container Registry to use vulnerability scanning to con rm that there are no vulnerabilities before deploying the workload.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

550/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Correct Answer: AB

Community vote distribution

AD (44%)

AC (36%)

AB (15%)

5%

? ?  raf2121  Highly Voted ?  1áyear, 10ámonths ago

A & D

Binary Authorization to ensure only verified containers are deployed
To ensure deployment are secure and and consistent, automatically scan images for vulnerabilities with container analysis
(https://cloud.google.com/docs/ci-cd/overview?hl=en&skip_cache=true)

upvoted 38 times

? ?  cloudmon 1áyear, 2ámonths ago

Also see references to the combination of using binary authorization and vulnerability scanning here:
https://cloud.google.com/binary-authorization/docs/overview

upvoted 6 times

? ?  KillerGoogle  Highly Voted ?  1áyear, 10ámonths ago

IMHO its A&C

upvoted 26 times

? ?  mgm7 1áyear, 6ámonths ago

I see a lot of people answered D but I don't see how it answers the question. I can securely deploy complete junk code. There is no
contradiction in this phrase even if one obviously should avoid doing this.

upvoted 5 times

? ?  BeCalm 3ámonths, 3áweeks ago

Dude the same applies to C. Trusted service accounts can deploy junk too.

upvoted 4 times

? ?  medi01 2ámonths, 1áweek ago

But that's the goal: secure the deployment process.

upvoted 1 times

? ?  sampon279  Most Recent ?  16áhours, 45áminutes ago

Selected Answer: AB

Seems A & B are correct solution: https://github.com/grafeas/kritis. C = Service account we naturally use service account for CICD - hardly
anyone uses user accounts. Security scanning is required, option B Kritis ensure manages allowlistCVEs.

upvoted 1 times

? ?  claorden 2áweeks, 2ádays ago

Selected Answer: AD

https://cloud.google.com/blog/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services

upvoted 3 times

? ?  stfnz 1ámonth, 2áweeks ago

Selected Answer: AC

the question is not focused on vulnerability scanning (D), hence correct answers are A and C

upvoted 1 times

? ?  bk989123 2ámonths, 2áweeks ago

Selected Answer: AC

doesn't matter if it is redundant, the question asks for different options

upvoted 1 times

? ?  HD2023 3ámonths ago

Selected Answer: AC

Wording: "You also need to ensure that only verified containers are deployed using Google Cloud services."

A - makes sense.
D - does not do this ... but C does.

upvoted 2 times

? ?  JC0926 3ámonths ago

Selected Answer: AC

Option D suggests using vulnerability scanning to confirm that there are no vulnerabilities before deploying the workload. While this is an
important security consideration, it does not specifically address the requirement to ensure that only verified containers are deployed.

upvoted 1 times

? ?  telp 3ámonths, 1áweek ago

Selected Answer: AC

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

551/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

With the context of the questions for me AC. It depends on the context of the word "Securely " for me it's interpretation is "securely
deploying the containers".

upvoted 2 times

? ?  BeCalm 3ámonths, 2áweeks ago

Selected Answer: AD

A and C are somewhat redundant so it has to be AD.

upvoted 3 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: AD

A = control who does the deployment(trust)
D = address vulnerability(security)

The combination provides trust + security

upvoted 3 times

? ?  amelm 3ámonths, 4áweeks ago

Selected Answer: AC

"Securly" should be intepreted here as "securly deploying the containers". So AC

upvoted 1 times

? ?  RVivek 4ámonths, 2áweeks ago

Selected Answer: A

A -- to enbale binary authorziatiion
C- Prevent any one manually adding container image --Assign permission only to service account to create and deploy conatiner account

upvoted 2 times

? ?  tdotcat 5ámonths, 2áweeks ago

Selected Answer: AD

question is asking about how to verify containers, so AD

upvoted 1 times

? ?  main_street 5ámonths, 4áweeks ago

I'd go with A & C.
The key word to note is "define the technical architecture for securely deploying workloads to Google Cloud"

D does detect vulnerabilities but doesn't ensure the deployment is from a trusted source which by best practices is required for less
privilege access
upvoted 5 times

? ?  omermahgoub 6ámonths ago
Answer is A&C, here's why

A: Enable Binary Authorization on GKE, and sign containers as part of a CI/CD pipeline is a good option because Binary Authorization is a
security feature that ensures that only verified containers are deployed to GKE clusters. By signing containers as part of a CI/CD pipeline,
you can ensure that all containers deployed to GKE clusters are cryptographically signed, which allows Binary Authorization to verify the
authenticity of the containers.

C: Configure Container Registry to only allow trusted service accounts to create and deploy containers from the registry is a good option
because it allows you to control which service accounts can create and deploy containers from the registry. By only allowing trusted
service accounts to create and deploy containers, you can ensure that only authorized users are able to deploy workloads to Google
Cloud.

upvoted 5 times

? ?  omermahgoub 6ámonths ago

Why not D or B:
B: Configure Jenkins to utilize Kritis to cryptographically sign a container as part of a CI/CD pipeline is not necessary because Binary
Authorization provides a similar function for verifying the authenticity of containers.

D: Configure Container Registry to use vulnerability scanning to confirm that there are no vulnerabilities before deploying the workload
is not necessary for ensuring that only verified containers are deployed. Vulnerability scanning can help identify potential
vulnerabilities in containers, but it does not verify the authenticity of the containers.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

Container Registry does have a built-in vulnerability scanning feature, but it is not required for securing container deployments.

upvoted 1 times

? ?  thamaster 6ámonths ago

Selected Answer: AD

A is no brainer, D to use vulnerability scan as best practice

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

552/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company overview -

Topic 4

EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a

service to multi- national medical o ces, hospitals, and insurance providers.

Solution concept -

Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They

need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their

software at a fast pace. Google

Cloud has been chosen to replace their current colocation facilities.

Existing technical environment -

EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.

Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored

in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).

EHR is hosting several legacy  le- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced

over the next several years. There is no plan to upgrade or move these systems at the current time.

Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and

are often ignored.

Business requirements -

* On-board new insurance providers as quickly as possible.

* Provide a minimum 99.9% availability for all customer-facing systems.

* Provide centralized visibility and proactive action on system performance and usage.

* Increase ability to provide insights into healthcare trends.

* Reduce latency to all customers.

* Maintain regulatory compliance.

* Decrease infrastructure administration costs.

* Make predictions and generate reports on industry trends based on provider data.

Technical requirements -

* Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.

* Provide a consistent way to manage customer-facing applications that are container-based.

* Provide a secure and high-performance connection between on-premises systems and Google Cloud.

* Provide consistent logging, log retention, monitoring, and alerting capabilities.

* Maintain and manage multiple container-based environments.

* Dynamically scale and provision new environments.

* Create interfaces to ingest and process data from new providers.

Executive statement -

Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different

systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of miscon gured

systems, inadequate capacity to manage spikes in tra c, and inconsistent monitoring practices. We want to use Google Cloud to leverage a

scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us

for future growth.

Question

You need to upgrade the EHR connection to comply with their requirements. The new connection design must support business-critical needs and

meet the same network and security policy requirements. What should you do?

A. Add a new Dedicated Interconnect connection.

B. Upgrade the bandwidth on the Dedicated Interconnect connection to 100 G.

C. Add three new Cloud VPN connections.

D. Add a new Carrier Peering connection.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

553/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Correct Answer: D

Community vote distribution

A (100%)

? ?  GoReplyGCPExam  Highly Voted ?  1áyear, 2ámonths ago

Selected Answer: A

I will go A cause note in https://cloud.google.com/network-connectivity/docs/interconnect/how-to/dedicated/modifying-interconnects
says
" It is not possible to change the link type on an Interconnect connection circuit from 10 Gbps to 100 Gbps. If you want to migrate to 100
Gbps, you must first provision a new 100-Gbps Interconnect connection alongside your existing 10-Gbps connection, and then migrate the
traffic onto the 100-Gbps connection."

upvoted 23 times

? ?  LaxmanTiwari 1ámonth, 1áweek ago

spot on

upvoted 1 times

? ?  cloudmon 1áyear, 2ámonths ago

This is awesome! That answers it perfectly!

upvoted 2 times

? ?  rishab86  Highly Voted ?  1áyear, 9ámonths ago

Answer A ; 99.9% availability requires 2 interconnect

upvoted 14 times

? ?  surajkrishnamurthy  Most Recent ?  6ámonths, 2áweeks ago

Selected Answer: A

A is the correct answer

upvoted 3 times

? ?  ashrafh 7ámonths, 1áweek ago

Selected Answer: A

Answer is A

upvoted 2 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is best answer
upvoted 2 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: A

A.
In the text it mentions high connection - reliable, only dedicated interconnect could achieve that.

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: A

A is correct

upvoted 2 times

? ?  vaibhav15 1áyear, 6ámonths ago

Selected Answer: A

Vote A

upvoted 2 times

? ?  sapsant 1áyear, 7ámonths ago

Selected Answer: A

Dedicated Interconnect provides direct physical connections between your on-premises network and Google's network.

https://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview

upvoted 3 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 4 times

? ?  [Removed] 1áyear, 7ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

554/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is correct.
Marked D is wrong

upvoted 2 times

? ?  Ari_GCP 1áyear, 9ámonths ago

Says "Upgrade keeping same network requirements". Remember infrastructure costs need to be kept low. A new dedicated connection
will involve purchasing new installation equipment. Upgrading to a higher bandwidth is more cost-friendly. I go with B.

upvoted 2 times

? ?  MikeB19 1áyear, 9ámonths ago

A is correct. Carrier peering is dedicated connection to google workspace not gcp
https://cloud.google.com/network-connectivity/docs/carrier-peering

upvoted 2 times

? ?  aa_desh 1áyear, 9ámonths ago

Answer: A, In Note they suggest for new connection for migrating 10GB to 100gb
https://cloud.google.com/network-connectivity/docs/interconnect/how-to/dedicated/modifying-interconnects

upvoted 5 times

? ?  Sarin 1áyear, 10ámonths ago

My answer is A
upvoted 2 times

? ?  victory108 1áyear, 10ámonths ago

A. Add a new Dedicated Interconnect connection.

upvoted 2 times

? ?  raf2121 1áyear, 10ámonths ago

Answer : A
A - secured connection
B ruled out as EHR has chosen Google Cloud to replace their current colocation facilities

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

555/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company overview -

Topic 4

EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a

service to multi- national medical o ces, hospitals, and insurance providers.

Solution concept -

Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They

need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their

software at a fast pace. Google

Cloud has been chosen to replace their current colocation facilities.

Existing technical environment -

EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.

Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored

in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).

EHR is hosting several legacy  le- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced

over the next several years. There is no plan to upgrade or move these systems at the current time.

Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and

are often ignored.

Business requirements -

* On-board new insurance providers as quickly as possible.

* Provide a minimum 99.9% availability for all customer-facing systems.

* Provide centralized visibility and proactive action on system performance and usage.

* Increase ability to provide insights into healthcare trends.

* Reduce latency to all customers.

* Maintain regulatory compliance.

* Decrease infrastructure administration costs.

* Make predictions and generate reports on industry trends based on provider data.

Technical requirements -

* Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.

* Provide a consistent way to manage customer-facing applications that are container-based.

* Provide a secure and high-performance connection between on-premises systems and Google Cloud.

* Provide consistent logging, log retention, monitoring, and alerting capabilities.

* Maintain and manage multiple container-based environments.

* Dynamically scale and provision new environments.

* Create interfaces to ingest and process data from new providers.

Executive statement -

Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different

systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of miscon gured

systems, inadequate capacity to manage spikes in tra c, and inconsistent monitoring practices. We want to use Google Cloud to leverage a

scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us

for future growth.

Question

For this question, refer to the EHR Healthcare case study. You need to de ne the technical architecture for hybrid connectivity between EHR's on-

premises systems and Google Cloud. You want to follow Google's recommended practices for production-level applications. Considering the EHR

Healthcare business and technical requirements, what should you do?

A. Con gure two Partner Interconnect connections in one metro (City), and make sure the Interconnect connections are placed in different

metro zones.

B. Con gure two VPN connections from on-premises to Google Cloud, and make sure the VPN devices on-premises are in separate racks.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

556/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Con gure Direct Peering between EHR Healthcare and Google Cloud, and make sure you are peering at least two Google locations.

D. Con gure two Dedicated Interconnect connections in one metro (City) and two connections in another metro, and make sure the

Interconnect connections are placed in different metro zones.

Correct Answer: D

Community vote distribution

D (87%)

13%

? ?  raf2121  Highly Voted ?  1áyear, 10ámonths ago

Answer : D (based on the requirement of secure and high-performance connection between on-premises systems to Google Cloud)

Between A and D, picked D as with Direct Connect EHR can get the bandwidth of 10 GBS to 100GBS (VPN ruled out as traffic is over
internet and due to bandwidth. Direct Peering is more for Workspace rather than Google Cloud)

upvoted 29 times

? ?  jask  Highly Voted ?  1áyear, 9ámonths ago

If we notice this line in question - "Google's recommended practices for production-level applications" and then see overview of these 2
pages- https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/production-level-overview and
https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/non-critical-overview. It is clear answer should be D , which is
topology for production level applications recommended by Google

upvoted 23 times

? ?  cloudmon 1áyear, 2ámonths ago

^^^ This is the best explanation. Considering all of those factors, D looks best.

upvoted 5 times

? ?  moota 4ámonths, 3áweeks ago

Specifically this page https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/dedicated-creating-9999-
availability

upvoted 1 times

? ?  medi01 2ámonths, 1áweek ago

That's one more 9 than required.

upvoted 1 times

? ?  red_panda  Most Recent ?  4ádays, 3áhours ago

Selected Answer: D

For me is D.
By Technical requirement we need to establish a stable, low-latency connection between on-prem and cloud

upvoted 1 times

? ?  Wael216 6ámonths ago

Selected Answer: D

simple hint : in this EHR case study, whenever there is a network connection, it's a dedicated interconnect answer !

upvoted 11 times

? ?  omermahgoub 6ámonths ago

The recommended solution for hybrid connectivity between on-premises systems and Google Cloud is to configure two Dedicated
Interconnect connections in two different metros (cities). This ensures that EHR Healthcare has a redundant connection to Google Cloud,
with each connection providing a separate physical path. Placing the Interconnect connections in different metro zones also helps to
ensure that the connection is resilient to failures in a single geographic region. This solution meets the business requirement of providing
a secure and high-performance connection between on-premises systems and Google Cloud, as well as the technical requirement of
maintaining regulatory compliance. It also helps to meet the requirement of providing consistent logging, log retention, monitoring, and
alerting capabilities, as Dedicated Interconnect connections can be used in conjunction with Cloud Router to establish a connection
between on-premises networks and Google Cloud VPC networks.

upvoted 1 times

? ?  neokyle 6ámonths, 1áweek ago

Selected Answer: D

EHR is supposed to be massive in size. so the option of 100 GBps / Dedicated is warranted.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

557/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  exam9391 8ámonths, 2áweeks ago

Selected Answer: D

Business requirements for this case:

* Provide a minimum 99.9% availability for all customer-facing systems.
* Provide a secure and high-performance connection between on-premises systems and Google Cloud.

A. - builds us a 99.9% SLA partner interconnect, covering all business requirements.
B. - VPN is not suitable for the business requirements.
C. - Direct peering is used for workspace, instead of DMZ, again - not suitable.
D. - builds us a 99.99% SLA dedicated interconnect, covering all business requirements.

The answer to choosing A or D lies in the question, stating: "You want to follow Google's recommended practices for production-level
applications."

Google recommends using the 99.99% SLA interconnect (dedicated or partner) for production-level applications as stated here:
https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/production-level-overview

The answer is D.
upvoted 6 times

? ?  deepdowndave 9ámonths, 1áweek ago

Selected Answer: A

The case study requires 99.9% availability. Only the setup with two partner interconnects in two metro zones serves 99.9%
https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/partner-creating-999-availability
D is wrong since 99.9% availability does not require 4 interconnects in two metros and case study mentions to keep costs low.

upvoted 2 times

? ?  jahiye3916 8ámonths, 3áweeks ago

"Google's recommended practices for production-level applications"
https://cloud.google.com/network-connectivity/docs/interconnect/tutorials/production-level-overview

Google's recommended practice is to use 4 Interconnect connections split across two regions. Answer D mentions 2 Interconnect
connections in one metro/city (region) and another 2 Interconnect connections in another metro (region), which is clearly referring to
Google's recommended practice.

upvoted 2 times

? ?  kuboraam 9ámonths, 3áweeks ago

Selected Answer: A

two points to note:
- requirements says a minimum of 99.9% (not 99.99%)
- also, "Decrease infrastructure administration costs."

upvoted 2 times

? ?  rmahendra 10ámonths, 2áweeks ago

I think D is more suitable because it requires low latency and more spread out datacenter locations. Moreover, it is not necessarily the
location of the legacy datacenter support provider interconnect

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: D

D seems to be the correct answer

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 4 times

? ?  rishab86 1áyear, 8ámonths ago

case study says " Provide a minimum 99.9% availability for all customer-facing systems", i think minimum is the keyword , hence I would
go with D

upvoted 3 times

? ?  BrijMohan08 1áyear, 8ámonths ago

Both A and D will work, but they want the SLA 99.9% and keep the cost low, which is possible with A (cost low)

upvoted 1 times

? ?  BrijMohan08 1áyear, 8ámonths ago

OPEX low

upvoted 1 times

? ?  Ari_GCP 1áyear, 9ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

558/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Will go with A. Decreasing Infra administration costs is a business requirement - option D won't provide that. Also, 99.9% can be achieved
with partner interconnect in 2 zones in a metro which is the min requirement.

upvoted 2 times

? ?  kuboraam 9ámonths, 3áweeks ago

This was exactly my initial thought as well. 99.9% (not 99.99%), and infra cost savings..
i was surprised when I saw all votes for D. I'm going with A.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

559/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company overview -

Topic 4

EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a

service to multi- national medical o ces, hospitals, and insurance providers.

Solution concept -

Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They

need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their

software at a fast pace. Google

Cloud has been chosen to replace their current colocation facilities.

Existing technical environment -

EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.

Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored

in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).

EHR is hosting several legacy  le- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced

over the next several years. There is no plan to upgrade or move these systems at the current time.

Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and

are often ignored.

Business requirements -

* On-board new insurance providers as quickly as possible.

* Provide a minimum 99.9% availability for all customer-facing systems.

* Provide centralized visibility and proactive action on system performance and usage.

* Increase ability to provide insights into healthcare trends.

* Reduce latency to all customers.

* Maintain regulatory compliance.

* Decrease infrastructure administration costs.

* Make predictions and generate reports on industry trends based on provider data.

Technical requirements -

* Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.

* Provide a consistent way to manage customer-facing applications that are container-based.

* Provide a secure and high-performance connection between on-premises systems and Google Cloud.

* Provide consistent logging, log retention, monitoring, and alerting capabilities.

* Maintain and manage multiple container-based environments.

* Dynamically scale and provision new environments.

* Create interfaces to ingest and process data from new providers.

Executive statement -

Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different

systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of miscon gured

systems, inadequate capacity to manage spikes in tra c, and inconsistent monitoring practices. We want to use Google Cloud to leverage a

scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us

for future growth.

Question

For this question, refer to the EHR Healthcare case study. You are a developer on the EHR customer portal team. Your team recently migrated the

customer portal application to Google Cloud. The load has increased on the application servers, and now the application is logging many timeout

errors. You recently incorporated Pub/Sub into the application architecture, and the application is not logging any Pub/Sub publishing errors. You

want to improve publishing latency.

What should you do?

A. Increase the Pub/Sub Total Timeout retry value.

B. Move from a Pub/Sub subscriber pull model to a push model.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

560/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Turn off Pub/Sub message batching.

D. Create a backup Pub/Sub message queue.

Correct Answer: A

Community vote distribution

C (77%)

14%

9%

? ?  raf2121  Highly Voted ?  1áyear, 10ámonths ago

Answer : C (https://cloud.google.com/pubsub/docs/publisher?hl=en#batching)
Cost of Batching is latency for individual messages,. To minimize latency batching should be turned off

upvoted 26 times

? ?  gingerbeer  Highly Voted ?  1áyear, 9ámonths ago

C - The cost of batching is latency for individual messages, which are queued in memory until their corresponding batch is filled and ready
to be sent over the network. To minimize latency, batching should be turned off.
https://cloud.google.com/pubsub/docs/publisher?hl=en#batching

A incorrect. Application timeout because of publisher latency, nothing to do with timeout retry with publish request.
D does not make sense at all.
B is about receiver, not publisher.

upvoted 9 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 4ádays ago

I voted C but I think it has to be A. "Total timeout: the amount of time after a client library stops retrying publish requests."

"After each publish request, the request timeout increases by the request timeout multiplier, up to the maximum request timeout."

So it's increasing the timeout value on the retry which seems like the best solution.

upvoted 1 times

? ?  JC0926 2ámonths, 2áweeks ago

Selected Answer: B

B. Move from a Pub/Sub subscriber pull model to a push model.

Explanation:
Moving from a pull model to a push model in Google Cloud Pub/Sub can help improve the latency in your application. In a push model,
the messages are pushed from the Pub/Sub service to the subscriber application, reducing the time it takes for the application to receive
messages. This can help mitigate the timeout errors that you are experiencing due to increased load on the application servers.

upvoted 3 times

? ?  JC0926 2ámonths, 2áweeks ago

Option A, increasing the Pub/Sub Total Timeout retry value, would not address the latency issue directly; it would only increase the time
the publisher would wait for a response before considering it a failure.

Option C, turning off Pub/Sub message batching, might actually increase latency and decrease throughput, as batching can improve
the efficiency of message delivery.

Option D, creating a backup Pub/Sub message queue, would not solve the latency issue directly; it might provide a failover mechanism
but would not address the root cause of the problem.

upvoted 1 times

? ?  MaryMei 3ámonths, 2áweeks ago

Selected Answer: A

This is chatgpt's choice
I agree that Pub/Sub message batching can be a useful optimization for improving overall throughput and reducing the number of API
calls required to publish messages. However, in the context of addressing timeout errors during publishing, turning off message batching
may not be the most appropriate solution.

In cases where message batching is causing issues, such as network or system resource constraints, reducing the batch size or adjusting
the batch duration can help improve publishing latency. However, in the case of timeout errors, increasing the Total Timeout retry value
would be a more effective solution, as it allows more time for the message to be successfully published and reduces the likelihood of
encountering timeout errors.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

To improve publishing latency in this scenario, it is recommended to turn off Pub/Sub message batching. By turning off message
batching, you can send messages individually as soon as they are published, rather than waiting for a batch of messages to be created
before sending them. This can help to reduce the risk of timeout errors and improve the overall performance of the application. It is also a
good idea to monitor the application's performance and error logs to identify any other potential issues that may be contributing to the
timeout errors.
upvoted 4 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

561/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  Prashant2022 6ámonths, 2áweeks ago

but how is this related to turnoff batching??

upvoted 1 times

? ?  Prashant2022 6ámonths, 2áweeks ago

Let me ans: becuz we need to speed up the time to deliver the msgs to the app! and it waits and timesout..

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Tesla 8ámonths ago

But no where in the question or scenario it says they turned on Pub/Sub batching.

upvoted 1 times

? ?  BlankSong 5ámonths, 4áweeks ago

Batch messaging is enabled by default in a client library.
https://cloud.google.com/pubsub/docs/publisher?hl=en#batching

upvoted 3 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is better option, even though increasing total timeout would help reduce timeout errors but remember that that in this case we are
getting too many messages from the server since load increased and we need to reduce latency

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

Agreed with C
upvoted 2 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: C

I agree C.
Ty guys!

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer
https://cloud.google.com/pubsub/docs/publisher?hl=en#batching

upvoted 2 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: C

Vote C

upvoted 2 times

? ?  [Removed] 1áyear, 7ámonths ago

Selected Answer: C

Marked A is wrong.
C is correct.

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

562/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company overview -

Topic 4

EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a

service to multi- national medical o ces, hospitals, and insurance providers.

Solution concept -

Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They

need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their

software at a fast pace. Google

Cloud has been chosen to replace their current colocation facilities.

Existing technical environment -

EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.

Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored

in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).

EHR is hosting several legacy  le- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced

over the next several years. There is no plan to upgrade or move these systems at the current time.

Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and

are often ignored.

Business requirements -

* On-board new insurance providers as quickly as possible.

* Provide a minimum 99.9% availability for all customer-facing systems.

* Provide centralized visibility and proactive action on system performance and usage.

* Increase ability to provide insights into healthcare trends.

* Reduce latency to all customers.

* Maintain regulatory compliance.

* Decrease infrastructure administration costs.

* Make predictions and generate reports on industry trends based on provider data.

Technical requirements -

* Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.

* Provide a consistent way to manage customer-facing applications that are container-based.

* Provide a secure and high-performance connection between on-premises systems and Google Cloud.

* Provide consistent logging, log retention, monitoring, and alerting capabilities.

* Maintain and manage multiple container-based environments.

* Dynamically scale and provision new environments.

* Create interfaces to ingest and process data from new providers.

Executive statement -

Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different

systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of miscon gured

systems, inadequate capacity to manage spikes in tra c, and inconsistent monitoring practices. We want to use Google Cloud to leverage a

scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us

for future growth.

Question

For this question, refer to the EHR Healthcare case study. In the past, con guration errors put public IP addresses on backend servers that should

not have been accessible from the Internet. You need to ensure that no one can put external IP addresses on backend Compute Engine instances

and that external IP addresses can only be con gured on frontend Compute Engine instances. What should you do?

A. Create an Organizational Policy with a constraint to allow external IP addresses only on the frontend Compute Engine instances.

B. Revoke the compute.networkAdmin role from all users in the project with front end instances.

C. Create an Identity and Access Management (IAM) policy that maps the IT staff to the compute.networkAdmin role for the organization.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

563/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D. Create a custom Identity and Access Management (IAM) role named GCE_FRONTEND with the compute.addresses.create permission.

Correct Answer: D

Community vote distribution

A (100%)

? ?  rvopoqvmtlwdlzrqxr  Highly Voted ?  1áyear, 10ámonths ago

A - configuration by Organization policy service

upvoted 20 times

? ?  joe2211  Highly Voted ?  1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 8 times

? ?  surajkrishnamurthy  Most Recent ?  6ámonths, 2áweeks ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is the clear answer as per google recommendation

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I will choose A
upvoted 1 times

? ?  Snowball998877 9ámonths, 3áweeks ago

It's A.
Following is from Google page:
"Using an Organization Policy, you can restrict external IP addresses to specific VMs with constraints to control use of external IP
addresses for your VM instances within an organization or a project."

upvoted 4 times

? ?  chickennuggets 10ámonths, 2áweeks ago
Compute Network admin role info:
https://cloud.google.com/compute/docs/access/iam#compute.networkAdmin I think it may be B

upvoted 1 times

? ?  chickennuggets 10ámonths, 2áweeks ago

D cant be right - A is closet per https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip
There are some risks to not being able to create new MiG and GKE clusters

upvoted 1 times

? ?  andre123 1áyear, 1ámonth ago

the question say " to ensure no one can put external IP addresses on backend Compute Engine instances and that external IP addresses
can only be configured on frontend Compute Engine instances ". I think D. what do you think

upvoted 1 times

? ?  irraz 1áyear, 5ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I think A is correct.
https://cloud.google.com/blog/ja/products/identity-security/limiting-public-ips-google-cloud

upvoted 2 times

? ?  Pime13 1áyear, 6ámonths ago

Selected Answer: A

A ->
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

564/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  [Removed] 1áyear, 6ámonths ago

I'm not sure if A is correct. From the doc posted in the discussion (https://cloud.google.com/compute/docs/ip-addresses/reserve-static-
external-ip-address#disableexternalip); the organization policy only applies to created instances and won't apply if they're recreated. So it
doesn't seem like an option that *prevents* the creation of non-public instances.

upvoted 1 times

? ?  [Removed] 1áyear, 6ámonths ago

From that link:
"Specifications

You can only apply this list constraint to VM instances.
You cannot apply the constraint retroactively. All VM instances that have external IP addresses before the policy is enabled retain their
external IP address.
This constraint accepts either an allowedList or a deniedList but not both in the same policy.
It is up to you or an administrator with the required permissions to manage and maintain the instance lifecycle and integrity. The
constraint only verifies the instance's URI, and it does not prevent the allowlisted VMs from being altered, deleted, or recreated."

upvoted 2 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: A

Vote A

upvoted 1 times

? ?  [Removed] 1áyear, 8ámonths ago

A is right. D does not define any rule so it is not making any sense.

upvoted 3 times

? ?  gingerbeer 1áyear, 9ámonths ago

A
See reference:
https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#disableexternalip

upvoted 5 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

565/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #7

Introductory Info

Company overview -

Topic 4

EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a

service to multi- national medical o ces, hospitals, and insurance providers.

Solution concept -

Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They

need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their

software at a fast pace. Google

Cloud has been chosen to replace their current colocation facilities.

Existing technical environment -

EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire.

Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored

in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).

EHR is hosting several legacy  le- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced

over the next several years. There is no plan to upgrade or move these systems at the current time.

Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and

are often ignored.

Business requirements -

* On-board new insurance providers as quickly as possible.

* Provide a minimum 99.9% availability for all customer-facing systems.

* Provide centralized visibility and proactive action on system performance and usage.

* Increase ability to provide insights into healthcare trends.

* Reduce latency to all customers.

* Maintain regulatory compliance.

* Decrease infrastructure administration costs.

* Make predictions and generate reports on industry trends based on provider data.

Technical requirements -

* Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.

* Provide a consistent way to manage customer-facing applications that are container-based.

* Provide a secure and high-performance connection between on-premises systems and Google Cloud.

* Provide consistent logging, log retention, monitoring, and alerting capabilities.

* Maintain and manage multiple container-based environments.

* Dynamically scale and provision new environments.

* Create interfaces to ingest and process data from new providers.

Executive statement -

Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different

systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of miscon gured

systems, inadequate capacity to manage spikes in tra c, and inconsistent monitoring practices. We want to use Google Cloud to leverage a

scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us

for future growth.

Question

For this question, refer to the EHR Healthcare case study. You are responsible for designing the Google Cloud network architecture for Google

Kubernetes

Engine. You want to follow Google best practices. Considering the EHR Healthcare business and technical requirements, what should you do to

reduce the attack surface?

A. Use a private cluster with a private endpoint with master authorized networks con gured.

B. Use a public cluster with  rewall rules and Virtual Private Cloud (VPC) routes.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

566/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Use a private cluster with a public endpoint with master authorized networks con gured.

D. Use a public cluster with master authorized networks enabled and  rewall rules.

Correct Answer: C

Community vote distribution

A (59%)

C (41%)

? ?  jask  Highly Voted ?  1áyear, 9ámonths ago

It should be A.
Public endpoint access disabled is the most secure option as it prevents all internet access to the control plane. This is a good choice if you
have configured your on-premises network to connect to Google Cloud using Cloud Interconnect (EHR has enabled this) or Cloud VPN.
If you disable public endpoint access, then you must configure authorized networks for the private endpoint. If you don't do this, you can
only connect to the private endpoint from cluster nodes or VMs in the same subnet as the cluster.
Public endpoint access enabled, authorized networks enabled: This is a good choice if you need to administer the cluster from source
networks that are not connected to your cluster's VPC network using Cloud Interconnect or Cloud VPN (but EHR is already using
interconnect) So answer C is wrong.
Reference- https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept

upvoted 54 times

? ?  BalaGCPArch 7ámonths ago

"Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters"
This statement in the case study tells it needs to be Public, So i assume the answer should be A

upvoted 3 times

? ?  turbo8p 7ámonths, 2áweeks ago

Agreed with this answer but just one thing to point out. I can't find any info mention that "EHR is already using interconnect". So this
should not be use as the main factor to make a decision.

upvoted 1 times

? ?  bogdant 1áyear, 5ámonths ago

I agree with @Jask's answer above.
According to the documentation, answer A is the most secure and in my opinion correct: "Public endpoint access disabled: This is the
most secure option as it prevents all internet access to the control plane. This is a good choice if you have configured your on-premises
network to connect to Google Cloud using Cloud Interconnect or Cloud VPN."
I just don't understand why so many people voted C.
Ref: https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept#overview ,

upvoted 2 times

? ?  Begum 8ámonths, 3áweeks ago

Configure NAT as mater authorized networks

upvoted 1 times

? ?  victory108  Highly Voted ?  1áyear, 10ámonths ago

A. Use a private cluster with a private endpoint with master authorized networks configured.
--> Private clusters run nodes without external IP addresses, and optionally run their cluster control plane without a publicly-reachable
endpoint. Additionally, private clusters do not allow Google Cloud IP addresses to access the control plane endpoint by default. Using
private clusters with authorized networks makes your control plane reachable only by the allowed CIDRs, by nodes within your cluster's
VPC, and by Google's internal production jobs that manage your control plane.

upvoted 7 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 4ádays ago

A seems the most secure as it's the only option that makes access to the control plane private
using authorized networks work to limit access even further.

Although the nodes are private the pods can still be accessed via an externally exposed service

"An external client with a source IP address on the internet can connect to an external Service of type LoadBalancer"

upvoted 1 times

? ?  JC0926 2ámonths, 1áweek ago

Selected Answer: A

A. Use a private cluster with a private endpoint with master authorized networks configured.

Using a private cluster with a private endpoint and master authorized networks configured is the best way to reduce the attack surface in
Google Kubernetes Engine (GKE). A private cluster ensures that the nodes have private IP addresses, which are not accessible from the
internet. The private endpoint allows access to the GKE API server only within the same VPC or through a secure connection (e.g., VPN or
VPC peering). Configuring master authorized networks restricts access to the GKE control plane to specific CIDR blocks, further securing
the environment and adhering to EHR Healthcare's business and technical requirements.

upvoted 1 times

? ?  rr4444 3ámonths, 2áweeks ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

567/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C is correct. A is wrong because, despite what everyone is thinking, because you cannot have a private endpoint for the control plane
WITH authorised networks. It's a contradiction of ideas. The authorised networks are specifically to manage access to a public endpoint to
only a set RFC1918 addresses, for example. Which, ironically, is covered by the link that everyone is pasting referring to answer A
https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept

upvoted 5 times

? ?  pigracer 2ámonths, 2áweeks ago

from the link:
"If you disable public endpoint access, then you must configure authorized networks for the private endpoint. If you don't do this, you
can only connect to the private endpoint from cluster nodes or VMs in the same subnet as the cluster. With this setting, authorized
networks must be internal IP addresses."
I combed through the documentation to see what you were saying but couldn't find it and only found this. So I think it's A

upvoted 3 times

? ?  Jeena345 4ámonths, 2áweeks ago

Selected Answer: A

Public endpoint access disabled is the most secure option as it prevents all internet access to the control plane. This is a good choice if you
have configured your on-premises network to connect to Google Cloud using Cloud Interconnect (EHR has enabled this) or Cloud VPN.
If you disable public endpoint access, then you must configure authorized networks for the private endpoint. If you don't do this, you can
only connect to the private endpoint from cluster nodes or VMs in the same subnet as the cluster.
Public endpoint access enabled, authorized networks enabled: This would be a good choice if you need to administer the cluster from
source networks that are not connected to your cluster's VPC network (using Cloud Interconnect or Cloud VPN) but EHR is already using
interconnect!
Reference:

https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept

upvoted 1 times

? ?  RVivek 4ámonths, 2áweeks ago

Selected Answer: C

Private cluter will have only one end Private end point and It is not poosible to autherize any specific Master network

upvoted 1 times

? ?  tdotcat 5ámonths, 2áweeks ago

Selected Answer: A

A is good

upvoted 1 times

? ?  omermahgoub 6ámonths ago

To reduce the attack surface and follow Google's best practices for network architecture in Google Kubernetes Engine, you should use a
private cluster with a private endpoint and configure master authorized networks.

Private clusters allow you to create clusters with nodes that are not reachable from the public internet. This reduces the attack surface by
making it more difficult for an attacker to target the nodes. Additionally, by using a private endpoint and configuring master authorized
networks, you can further restrict access to the cluster to only authorized users and networks. This helps to ensure that only authorized
users and systems can access the cluster and helps to prevent unauthorized access.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

Using a public cluster with firewall rules and Virtual Private Cloud (VPC) routes may provide some level of security, but it does not
provide the same level of protection as a private cluster. Similarly, using a private cluster with a public endpoint and master authorized
networks can also provide some level of security, but it is not as secure as using a private cluster with a private endpoint and master
authorized networks. In summary, to reduce the attack surface and follow best practices, it is recommended to use a private cluster
with a private endpoint and configure master authorized networks.

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is the best answer https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept#overview

upvoted 1 times

? ?  abdelilahfa 8ámonths, 3áweeks ago

Selected Answer: C

Public endpoint access enabled, authorized networks enabled (recommended): This option provides restricted access to the control plane
from source IP addresses that you define. This is a good choice if you don't have existing VPN infrastructure or have remote users or
branch offices that connect over the public internet instead of the corporate VPN and Cloud Interconnect or Cloud VPN.

https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-
cluster#restrict_network_access_to_the_control_plane_and_nodes

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

568/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  jabrrJ68w02ond1 9ámonths, 4áweeks ago

Selected Answer: A

I'll go with A as it is the most secure option. C would be more cost-effective for when EHR has no plans for Cloud Interconnect / VPN
(which they do!).
upvoted 1 times

? ?  RitwickKumar 10ámonths, 1áweek ago

Selected Answer: A

Why would we need access to control plane from outside. It is better to keep everything private and expose the web/ui through an
external ingress.
upvoted 3 times

? ?  cdcollector 1áyear ago

Putting a Autn Network on a private endpoint is moot

Note: Authorized networks block untrusted IP addresses from outside Google Cloud. Addresses from inside Google Cloud (such as traffic
from Compute Engine virtual machines (VMs), Cloud Functions and Cloud Run) can reach your control plane using HTTPS, provided that
they have the necessary Kubernetes credentials.

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: A

Logically A by eliminerling all thing public and most securered

upvoted 3 times

? ?  Venket 1áyear, 1ámonth ago

C : Public endpoint access enabled, authorized networks enabled (recommended): This option provides restricted access to the control
plane from source IP addresses that you define. This is a good choice if you don't have existing VPN infrastructure or have remote users or
branch offices that connect over the public internet instead of the corporate VPN and Cloud Interconnect or Cloud VPN.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

569/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 5 - Testlet 2

Question #1

Introductory Info

Company Overview -

Topic 5

Mountkirk Games makes online, session-based, multiplayer games for the most popular mobile platforms. They build all of their games using

some server-side integration. Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

1. Dynamically scale up or down based on game activity

2. Connect to a managed NoSQL database service

3. Run customize Linux distro

Requirements for Game Analytics Platform

1. Dynamically scale up or down based on game activity

2. Process incoming data on the  y directly from the game servers

3. Process data that arrives late because of slow mobile networks

4. Allow SQL queries to access at least 10 TB of historical data

5. Process  les that are regularly uploaded by users' mobile devices

6. Use only fully managed services

CEO Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users.

CTO Statement -

Our current technology stack cannot provide the scale we need, so we want to replace MySQL and move to an environment that provides

autoscaling, low latency load balancing, and frees us up from managing physical servers.

CFO Statement -

We are not capturing enough user demographic data, usage metrics, and other KPIs. As a result, we do not engage the right users, we are not

con dent that our marketing is targeting the right users, and we are not selling enough premium Blast-Ups inside the games, which dramatically

impacts our revenue.

Question

Mountkirk Games wants you to design their new testing strategy. How should the test coverage differ from their existing backends on the other

platforms?

A. Tests should scale well beyond the prior approaches

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

570/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B. Unit tests are no longer required, only end-to-end tests

C. Tests should be applied after the release is in the production environment

D. Tests should include directly testing the Google Cloud Platform (GCP) infrastructure

Correct Answer: A

From Scenario:

A few of their games were more popular than expected, and they had problems scaling their application servers, MySQL databases, and

analytics tools.

Requirements for Game Analytics Platform include: Dynamically scale up or down based on game activity

Community vote distribution

A (65%)

D (35%)

? ?  Smart  Highly Voted ?  3áyears, 4ámonths ago

Tests should include directly "testing the Google Cloud Platform (GCP) infrastructure". I don't what does this mean? Testing their resources
running on GCP infrastructure or testing GCP services itself? Regardless, Option D suggests SLA requirements which is not the problem.
The problem is scalability that can be resolved through stress testing. Also, GCP services are indirectly tested through it (Option D). Hence,
I choose option A.
upvoted 31 times

? ?  HD2023 3ámonths ago

"How should the test coverage differ from their existing backends on the other platforms?"

Test coverage. They are already testing scale. Testing GPA isnÆt currently covered.

upvoted 1 times

? ?  ShadowLord 10ámonths ago

Can GCP Infrastructure configuration can be tested like Security, Firewall, Performance, Scaling, Failure ... Istion Failure Injections ...
A is too generic ... without any reference to what it means

upvoted 2 times

? ?  elainexs 1áyear ago

Same, no sense at all to test GCP infra

upvoted 1 times

? ?  Karthic  Highly Voted ?  3áyears, 6ámonths ago

Should be D, bcaz need to test GCP products too....

upvoted 13 times

? ?  ShadowLord 10ámonths ago

Configuration of GCP Platform, scaling, scale down, etc ,,,, so D ... A option is to generic .. Testing to scale beyond previous approaches

upvoted 1 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 14 times

? ?  tartar 2áyears, 10ámonths ago

" had problems scaling their global audience, application servers MySQL databases, and analytics tools."

upvoted 4 times

? ?  tartar 2áyears, 10ámonths ago

sorry, changing to D

upvoted 7 times

? ?  nitinz 2áyears, 3ámonths ago

It is D. New to GCP, migrated to GCP.... time to test if it works or not.

upvoted 7 times

? ?  sampon279  Most Recent ?  6áhours, 22áminutes ago

Selected Answer: A

Tests should scale well beyond the prior approaches seems correct, since they are migrating to new GCP infrastructure they should have
higher scalability in tests. D - Tests should include directly testing the Google Cloud Platform (GCP) infrastructure seems not required, if
they test the app in GCP env that makes sense, just testing GCP infrastructure without any app code in between is not needed.

upvoted 1 times

? ?  joesatriani 1áweek, 5ádays ago

Are these Mountkirk Games case questions still appearing on the exam?

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

571/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  taer 2ámonths, 3áweeks ago

Selected Answer: A

As they expect the new game to be very popular, their testing strategy should be designed to scale beyond their previous approaches to
ensure that the game can handle the increased traffic and user demands.

upvoted 3 times

? ?  HD2023 3ámonths ago

Selected Answer: D

How should the test coverage differ from their existing backends on the other platforms?" Test coverage. They are already testing scale.
Testing GPA isnÆt currently covered.

upvoted 1 times

? ?  JC0926 3ámonths ago

Selected Answer: D

DDDDDDDDD
upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

Wording in Answer Choice D is intended to confuse. What is being tested is the ability of GCP to handle the scale that Mountkirk needs as
the infra of the prior cloud provider could not scale.

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

Selected Answer: D

The previous "cloud" provider could not scale so the need here is to ensure that GCP scales. A is meaningless and generic.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

The correct answer is A: Tests should scale well beyond the prior approaches.

Mountkirk Games' new game is expected to be very popular, so it's important to have a testing strategy that can handle a high volume of
users and ensure that the game can scale well. This means that the test coverage for the new game's backend should be significantly
higher than the test coverage for their existing backends on other platforms.

Unit tests and end-to-end tests are both important for ensuring the quality and reliability of the game's backend, so both types of tests
should be included in the testing strategy. Testing the GCP infrastructure directly may not be necessary, as GCP provides managed
services that are expected to be reliable and well-maintained.

It's generally not recommended to perform testing only in the production environment, as this can potentially cause problems for live
users and result in lost revenue. Instead, it's important to test the game's backend thoroughly in a staging or testing environment before
deploying it to production.

upvoted 4 times

? ?  habros 6ámonths, 4áweeks ago

Selected Answer: A

A is essential as customer want a scalable system
B&C does not make sense from testing perspective.
D is remotely impossible as it might breach acceptable use policy conditionsà best to check with Google before doing so.

upvoted 2 times

? ?  BeCalm 3ámonths, 2áweeks ago

You don't need permission to stress test your app on GCP.

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 2 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A makes sense to me

upvoted 1 times

? ?  zellck 9ámonths, 1áweek ago

Selected Answer: A

A is my answer
upvoted 1 times

? ?  Nirca 9ámonths, 3áweeks ago

Selected Answer: A

A is the right one
upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

572/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  ShadowLord 10ámonths ago

Selected Answer: D

Can GCP Infrastructure configuration can be tested like Security, Firewall, Performance, Scaling, Failure ... Istion Failure Injections ...
A is too generic ... without any reference to what it means

upvoted 2 times

? ?  JohnPi 10ámonths, 1áweek ago

Selected Answer: D

Gcp infrastucture should be included

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

573/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company Overview -

Topic 5

Mountkirk Games makes online, session-based, multiplayer games for the most popular mobile platforms. They build all of their games using

some server-side integration. Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

1. Dynamically scale up or down based on game activity

2. Connect to a managed NoSQL database service

3. Run customize Linux distro

Requirements for Game Analytics Platform

1. Dynamically scale up or down based on game activity

2. Process incoming data on the  y directly from the game servers

3. Process data that arrives late because of slow mobile networks

4. Allow SQL queries to access at least 10 TB of historical data

5. Process  les that are regularly uploaded by users' mobile devices

6. Use only fully managed services

CEO Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users.

CTO Statement -

Our current technology stack cannot provide the scale we need, so we want to replace MySQL and move to an environment that provides

autoscaling, low latency load balancing, and frees us up from managing physical servers.

CFO Statement -

We are not capturing enough user demographic data, usage metrics, and other KPIs. As a result, we do not engage the right users, we are not

con dent that our marketing is targeting the right users, and we are not selling enough premium Blast-Ups inside the games, which dramatically

impacts our revenue.

Question

Mountkirk Games has deployed their new backend on Google Cloud Platform (GCP). You want to create a through testing process for new

versions of the backend before they are released to the public. You want the testing environment to scale in an economical way. How should you

design the process?

A. Create a scalable environment in GCP for simulating production load

B. Use the existing infrastructure to test the GCP-based backend at scale

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

574/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Build stress tests into each component of your application using resources internal to GCP to simulate load

D. Create a set of static environments in GCP to test different levels of load ?Ç" for example, high, medium, and low

Correct Answer: A

From scenario: Requirements for Game Backend Platform

1. Dynamically scale up or down based on game activity

2. Connect to a managed NoSQL database service

3. Run customize Linux distro

Community vote distribution

A (100%)

? ?  shandy  Highly Voted ?  3áyears, 7ámonths ago

A is correct because simulating production load in GCP can scale in an economical way.

B is not correct because one of the pain points about the existing infrastructure was precisely that the environment did not scale well. C is
not correct because it is a best practice to have a clear separation between test and production environments. Generating test load should
not be done from a production environment. D is not correct because Mountkirk Games wants the testing environment to scale as
needed. Defining several static environments for specific levels of load goes against this requirement.

upvoted 34 times

? ?  nitinz 2áyears, 3ámonths ago

A is correct.

upvoted 2 times

? ?  mlantonis  Highly Voted ?  2áyears, 12ámonths ago

The question is taken from Google Practice Exam
A: is correct because simulating production load in GCP can scale in an economical way.

B: is not correct because one of the pain points about the existing infrastructure was precisely that the environment did not scale well.

C: is not correct because it is a best practice to have a clear separation between test and production environments. Generating test load
should not be done from a production environment.

D: is not correct because Mountkirk Games wants the testing environment to scale as needed. Defining several static environments for
specific levels of load goes against this requirement.

upvoted 15 times

? ?  elainexs 1áyear ago

C option doesn't mention if that takes production load.

upvoted 2 times

? ?  BeCalm  Most Recent ?  3ámonths, 2áweeks ago

Not sure why C is not correct. That is the approach for testing an app in a non-prod environment.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

The correct answer is A: Create a scalable environment in GCP for simulating production load.
To create a thorough testing process for new versions of the backend before they are released to the public, it's important to simulate
production load in the testing environment. This will help ensure that the backend can handle the expected volume of users and scale
appropriately.

One way to do this is to create a scalable environment in GCP that can simulate production load. This could involve using GCP services
such as Google Compute Engine or Google Kubernetes Engine to spin up multiple virtual machines or containers to simulate a high
volume of requests and traffic. This approach will allow you to test the backend at scale and ensure that it can handle the expected load.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

Using the existing infrastructure to test the GCP-based backend at scale may not be feasible, as the existing infrastructure may not be
capable of simulating the expected production load. Building stress tests into each component of the application using internal GCP
resources could be time-consuming and may not provide a realistic simulation of production load. Creating a set of static environments
in GCP to test different levels of load may not be sufficient for testing the backend at scale, as the load may not be consistent with what
is expected in production.

upvoted 1 times

? ?  gonlafer 6ámonths, 4áweeks ago

Selected Answer: A

A is the right answer

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

575/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

ok for A

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I would like to choose A because it satisfy the given requirements

upvoted 1 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: A

A is correct because simulating production load in GCP can scale in an economical way.

upvoted 1 times

? ?  Nirca 9ámonths, 3áweeks ago

The question is taken from Google Practice Exam
A: is correct because simulating production load in GCP can scale in an economical way.

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 3 times

? ?  Yogikant 2áyears ago

Answer: A

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

A. Create a scalable environment in GCP for simulating production load

upvoted 2 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  LoganIsh 2áyears, 8ámonths ago

in reality we should have two environments 1 functional 2 stress test looks like this question is from functional perspective thus A is the
deal. however when you size the environment for 2 perspective you need to size for WEB/APP/Messaging/DB and Back end MF as well.

upvoted 1 times

? ?  AshokC 2áyears, 9ámonths ago

A is meaningful
upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

576/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company Overview -

Topic 5

Mountkirk Games makes online, session-based, multiplayer games for the most popular mobile platforms. They build all of their games using

some server-side integration. Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

1. Dynamically scale up or down based on game activity

2. Connect to a managed NoSQL database service

3. Run customize Linux distro

Requirements for Game Analytics Platform

1. Dynamically scale up or down based on game activity

2. Process incoming data on the  y directly from the game servers

3. Process data that arrives late because of slow mobile networks

4. Allow SQL queries to access at least 10 TB of historical data

5. Process  les that are regularly uploaded by users' mobile devices

6. Use only fully managed services

CEO Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users.

CTO Statement -

Our current technology stack cannot provide the scale we need, so we want to replace MySQL and move to an environment that provides

autoscaling, low latency load balancing, and frees us up from managing physical servers.

CFO Statement -

We are not capturing enough user demographic data, usage metrics, and other KPIs. As a result, we do not engage the right users, we are not

con dent that our marketing is targeting the right users, and we are not selling enough premium Blast-Ups inside the games, which dramatically

impacts our revenue.

Question

Mountkirk Games wants to set up a continuous delivery pipeline. Their architecture includes many small services that they want to be able to

update and roll back quickly. Mountkirk Games has the following requirements:
? Services are deployed redundantly across multiple regions in the US and Europe
? Only frontend services are exposed on the public internet
? They can provide a single frontend IP for their  eet of services
? Deployment artifacts are immutable
Which set of products should they use?

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

577/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A. Google Cloud Storage, Google Cloud Data ow, Google Compute Engine

B. Google Cloud Storage, Google App Engine, Google Network Load Balancer

C. Google Kubernetes Registry, Google Container Engine, Google HTTP(S) Load Balancer

D. Google Cloud Functions, Google Cloud Pub/Sub, Google Cloud Deployment Manager

Correct Answer: C

Community vote distribution

C (100%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

Correct answer is C.

upvoted 46 times

? ?  rishab86 1áyear, 9ámonths ago

Answer is C but options should have been Google Kubernetes Engine, Google Container Registry, Google HTTP(S) Load Balancer

upvoted 30 times

? ?  nitinz 2áyears, 3ámonths ago

C is correct

upvoted 4 times

? ?  kolcsarzs  Highly Voted ?  3áyears, 6ámonths ago

If the Question is erroneously formulated, and they mean Google Container Registry and Google Kubernetes Engine, then C is the right
answer

upvoted 30 times

? ?  Revedeep 2áyears, 11ámonths ago

Yeah Exactly! The choice was confusing.

upvoted 2 times

? ?  Kysmor 2áyears, 4ámonths ago

Maybe is not: they could have mispelled on purpose! ;)

upvoted 1 times

? ?  rr4444  Most Recent ?  3ámonths, 2áweeks ago

Selected Answer: C

Oooooh. Old skool wording

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

How is C the answer? No such thing as Google Kubernetes Registry and Google Container Engine

upvoted 2 times

? ?  omermahgoub 6ámonths ago

The correct answer is C: Google Kubernetes Registry, Google Container Engine, Google HTTP(S) Load Balancer.

To meet the requirements listed in the question, Mountkirk Games should use Google Kubernetes Registry to store their immutable
deployment artifacts, Google Container Engine to run their services in a Kubernetes cluster, and Google HTTP(S) Load Balancer to expose
their frontend services on the public internet and provide a single frontend IP for their fleet of services.

upvoted 4 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

ok for C

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is correct

upvoted 1 times

? ?  Nirca 9ámonths, 3áweeks ago

Selected Answer: C

Only C and we will see.

upvoted 3 times

? ?  Nirca 11ámonths, 1áweek ago

Selected Answer: C

HTTP(S) Load Balancer is GLOBAL !!!!

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

578/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  erika_vazquez 1áyear, 6ámonths ago

A is not correct because Mountkirk Games wants to set up a continuous delivery pipeline, not a data processing pipeline. Cloud Dataflow
is a fully managed service for creating data processing pipelines.

B is not correct because a Cloud Load Balancer distributes traffic to Compute Engine instances. App Engine and Cloud Load Balancer are
parts of different solutions.

C is correct because:
-Google Kubernetes Engine is ideal for deploying small services that can be updated and rolled back quickly. It is a best practice to
manage services using immutable containers. -Cloud Load Balancing supports globally distributed services across multiple regions. It
provides a single global IP address that can be used in DNS records. Using URL Maps, the requests can be routed to only the services that
Mountkirk wants to expose. -Container Registry is a single place for a team to manage Docker images for the services.

D is not correct because you cannot reserve a single frontend IP for cloud functions. When deployed, an HTTP-triggered cloud function
creates an endpoint with an automatically assigned IP.

upvoted 11 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 2 times

? ?  mudot 1áyear, 7ámonths ago

Selected Answer: C

"They can provide a single frontend IP for their fleet of services"

only C can do that

upvoted 3 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 2 times

? ?  ACE_ASPIRE 1áyear, 9ámonths ago

CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC

upvoted 1 times

? ?  PeppaPig 1áyear, 10ámonths ago

C is correct. "deployed redundantly across multiple regions" is the key point

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is C.

upvoted 3 times

? ?  Yogikant 2áyears ago

Assuming that this question was old, answer C.

Google Kubernetes Registry, Google Container Engine are old terminology.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

579/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company Overview -

Topic 5

Mountkirk Games makes online, session-based, multiplayer games for the most popular mobile platforms. They build all of their games using

some server-side integration. Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

1. Dynamically scale up or down based on game activity

2. Connect to a managed NoSQL database service

3. Run customize Linux distro

Requirements for Game Analytics Platform

1. Dynamically scale up or down based on game activity

2. Process incoming data on the  y directly from the game servers

3. Process data that arrives late because of slow mobile networks

4. Allow SQL queries to access at least 10 TB of historical data

5. Process  les that are regularly uploaded by users' mobile devices

6. Use only fully managed services

CEO Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users.

CTO Statement -

Our current technology stack cannot provide the scale we need, so we want to replace MySQL and move to an environment that provides

autoscaling, low latency load balancing, and frees us up from managing physical servers.

CFO Statement -

We are not capturing enough user demographic data, usage metrics, and other KPIs. As a result, we do not engage the right users, we are not

con dent that our marketing is targeting the right users, and we are not selling enough premium Blast-Ups inside the games, which dramatically

impacts our revenue.

Question

Mountkirk Games' gaming servers are not automatically scaling properly. Last month, they rolled out a new feature, which suddenly became very

popular. A record number of users are trying to use the service, but many of them are getting 503 errors and very slow response times. What

should they investigate  rst?

A. Verify that the database is online

B. Verify that the project quota hasn't been exceeded

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

580/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Verify that the new feature code did not introduce any performance bugs

D. Verify that the load-testing team is not running their tool against production

Correct Answer: B

503 is service unavailable error. If the database was online everyone would get the 503 error.

Community vote distribution

B (62%)

C (38%)

? ?  sri007  Highly Voted ?  3áyears, 5ámonths ago

Ans is B

upvoted 17 times

? ?  Ziegler  Highly Voted ?  3áyears ago

B is the correct answer
Error code starting like 5xx is something related to server
503 UNAVAILABLE Service unavailable. Typically the server is down.

upvoted 10 times

? ?  AGG 2áyears, 4ámonths ago

When server is down you will get timeout (503 - service unavailable - not server)

upvoted 2 times

? ?  red_panda  Most Recent ?  4ádays, 1áhour ago

Selected Answer: C

For me is C.
There is no reason to think about quota limit. Instead, is clearly that a new feature was released and from that there are errors.

upvoted 1 times

? ?  BiddlyBdoyng 1áweek, 4ádays ago

A is the only one I think can be 100% ruled out as it says "..and very slow reasponse times", I think a DB down would just get a 5xx.

B, C & D could all cause the problem being experienced. The only reason I can see B being the most correct is because it's easy to check
and maybe more probable than D.

upvoted 1 times

? ?  claorden 2áweeks, 1áday ago

Selected Answer: B

Concurrent quota: https://cloud.google.com/docs/quota

upvoted 1 times

? ?  stfnz 1ámonth, 2áweeks ago

Selected Answer: C

503 is not necessarily quota related, also question says _new_ feature was implemented

upvoted 1 times

? ?  zbyszek1 2ámonths, 1áweek ago
C. They use kubernetes engine.
What is Kubernetes Service 503 (Service Unavailable)
The 503 Service Unavailable error is an HTTP status code that indicates the server is temporarily unavailable and cannot serve the client
request. In a web server, this means the server is overloaded or undergoing maintenance. In Kubernetes, it means a Service tried to route
a request to a pod, but something went wrong along the way:

The Service could not find any pods matching its selector.
The Service found some pods matching the selector, but none of them were Running.
Pods are running but were removed from the Service endpoint because they did not pass the readiness probe.
Some other networking or configuration issue prevented the Service from connecting with the pods.

upvoted 1 times

? ?  JC0926 2ámonths, 2áweeks ago

Selected Answer: B

B. Verify that the project quota hasn't been exceeded

Given the sudden increase in popularity of the new feature and the resulting 503 errors and slow response times, Mountkirk Games
should first investigate if they have exceeded their project quota on Google Cloud Platform. Exceeding the project quota could prevent the
gaming servers from scaling properly to handle the increased traffic, causing the observed issues. Once they have verified or adjusted
their project quota, they can look into other potential causes if the problem persists, such as performance bugs, database issues, or load-
testing tools running against production.

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

581/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

"but many of them are getting 503 errors and very slow response times" --> the answer cannot be B else the game would be down for
everybody

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: C

Not following how B can be the right answer. Everybody should be getting a service not available error, not some.

upvoted 1 times

? ?  tdotcat 5ámonths, 2áweeks ago

Selected Answer: C

503 is not neccesarily quota related, also question says new feature

upvoted 2 times

? ?  thamaster 6ámonths ago

A. Verify that the database is online
B. Verify that the project quota hasn't been exceeded Most Voted
C. Verify that the new feature code did not introduce any performance bugs
D. Verify that the load-testing team is not running their tool against production

what is project quotat?

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

ok for B

upvoted 1 times

? ?  andras 8ámonths, 2áweeks ago

many of them experiencing 503... means some of the servers are still running, so this is a slow down -> answer is C

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is correct

upvoted 1 times

? ?  Nirca 9ámonths, 3áweeks ago

Selected Answer: B

B is the correct answer
Error code starting like 5xx is something related to server
503 UNAVAILABLE Service unavailable. Typically the server is down.

upvoted 2 times

? ?  Nirca 11ámonths, 1áweek ago

Selected Answer: B

B it is

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

582/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company Overview -

Topic 5

Mountkirk Games makes online, session-based, multiplayer games for the most popular mobile platforms. They build all of their games using

some server-side integration. Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

1. Dynamically scale up or down based on game activity

2. Connect to a managed NoSQL database service

3. Run customize Linux distro

Requirements for Game Analytics Platform

1. Dynamically scale up or down based on game activity

2. Process incoming data on the  y directly from the game servers

3. Process data that arrives late because of slow mobile networks

4. Allow SQL queries to access at least 10 TB of historical data

5. Process  les that are regularly uploaded by users' mobile devices

6. Use only fully managed services

CEO Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users.

CTO Statement -

Our current technology stack cannot provide the scale we need, so we want to replace MySQL and move to an environment that provides

autoscaling, low latency load balancing, and frees us up from managing physical servers.

CFO Statement -

We are not capturing enough user demographic data, usage metrics, and other KPIs. As a result, we do not engage the right users, we are not

con dent that our marketing is targeting the right users, and we are not selling enough premium Blast-Ups inside the games, which dramatically

impacts our revenue.

Question

Mountkirk Games needs to create a repeatable and con gurable mechanism for deploying isolated application environments. Developers and

testers can access each other's environments and resources, but they cannot access staging or production resources. The staging environment

needs access to some services from production.

What should you do to isolate development environments from staging and production?

A. Create a project for development and test and another for staging and production

B. Create a network for development and test and another for staging and production

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

583/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Create one subnetwork for development and another for staging and production

D. Create one project for development, a second for staging and a third for production

Correct Answer: D

Community vote distribution

D (50%)

A (45%)

5%

? ?  shandy  Highly Voted ?  3áyears, 7ámonths ago

Correct Answer is D.
https://cloud.google.com/appengine/docs/standard/php/creating-separate-dev-environments

upvoted 42 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 9 times

? ?  tartar 2áyears, 10ámonths ago

not D, A

upvoted 15 times

? ?  ACE_ASPIRE 1áyear, 9ámonths ago

hey man...it should be D...

upvoted 1 times

? ?  KOERA99 1áyear, 7ámonths ago

It's D!!!!

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

it is A

upvoted 4 times

? ?  Wonka 1áyear, 5ámonths ago

its standard but look at requirement given here

upvoted 1 times

? ?  euclid  Highly Voted ?  3áyears, 6ámonths ago

Correct is A

upvoted 40 times

? ?  walkwolf3 1áyear, 5ámonths ago

D

In the requirement, the staging environment needs access to production, not the other way around. Answer A could allow staging and
production to access each other. In answer D, staging and production are in different project, you can limit the access from either side.
So D is correct.
upvoted 18 times

? ?  hogtrough 1áyear, 5ámonths ago

End goal is to separate dev from staging/production. Putting staging/production in same project fits the requirements. Further
effort would be required to change access between Staging and Production projects that is out of scope of question.

It is not best practice, but fits requirements of question.

upvoted 6 times

? ?  Wonka 1áyear, 5ámonths ago

yes and there is no mention of test environment in option D.

upvoted 2 times

? ?  Ishu_awsguy 9ámonths, 2áweeks ago

Best approach is D.
A will also work based on question requirement

upvoted 3 times

? ?  AWS56 3áyears, 5ámonths ago

Agree with A

upvoted 4 times

? ?  TiagoM 2áyears, 2ámonths ago

"The staging environment needs access to some services from production"
Its not the best practice, but A has less effort

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

584/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 8 times

? ?  army234 2áyears, 2ámonths ago

Incorrect. Not a best practice to have Staging and Prod resources in the same project. D is correct

upvoted 8 times

? ?  Wonka 1áyear, 5ámonths ago

by standard it is absolutely incorrect but here it is requirement. will you still separate it out?

upvoted 1 times

? ?  sampon279  Most Recent ?  6áhours, 1áminute ago

Selected Answer: D

Best practice is to have separate projects for each env. "The staging environment needs access to some services from production" the
staging env can still access prod services if it is a storage bucket. They did not mention which services so assuming the services and be
easily accessed across projects.

upvoted 1 times

? ?  BiddlyBdoyng 1áweek, 4ádays ago

The best practice says " It's vital that these environments be completely isolated from one another"

"Vital", can't be anything other than D unless you want the sack.

upvoted 1 times

? ?  JC0926 2ámonths, 2áweeks ago

Selected Answer: B

B. Create a network for development and test and another for staging and production

To isolate development environments from staging and production while allowing developers and testers to access each other's
environments and resources, you should create separate Virtual Private Cloud (VPC) networks for development and test, and another for
staging and production. This configuration allows you to maintain separation between environments while providing the necessary access
controls. You can then set up the appropriate firewall rules and peering between the networks as needed, ensuring that the staging
environment has access to some services from production while keeping development and test environments isolated from staging and
production.

upvoted 2 times

? ?  HD2023 3ámonths ago

Selected Answer: A

Dev and test. Staging and production.

upvoted 2 times

? ?  telp 3ámonths, 1áweek ago

Selected Answer: D

D can work to respect separation of project and project can access ressource by configuration.

upvoted 1 times

? ?  rr4444 3ámonths, 2áweeks ago

Selected Answer: A

Apart from A dealing with the reqs (even if reqs could be better standard), D does not mention the test envs. So it CANNOT be D

upvoted 2 times

? ?  PST21 3ámonths, 2áweeks ago

the correct Ans should be B - create diff networks. for staging open the ports/service name which needs specifc access

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: D

Developers and testers ability to access both environments is intended to mislead. Best practice is a project per environment.

upvoted 1 times

? ?  Jeena345 4ámonths, 2áweeks ago

Selected Answer: A

Based on Google's best practices, "Create one project for development, a second for staging and a third for production" would be the best
option to pick. It follows the principles of "separation of duties" and "separation of concern". Anyway, it doesn't mention any test env.

End goal is to separate dev from staging/production. Putting staging/production in same project fits the requirements. Further effort
would be required to change access between Staging and Production projects but that is out of scope of question.

Hence, "Create a project for development and test and another for staging and production" is the correct answer, even if it is not the best
practice.

upvoted 2 times

? ?  RVivek 4ámonths, 2áweeks ago

Selected Answer: D

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

585/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Develpers and Tersters can access resources in each other domain. So one project for them
Staging should have limitted access to Production so One Staging project and one production project

upvoted 1 times

? ?  csestony 6ámonths ago

Clearly this question is poorly worded but after reading the near split consensus and reading the prompt again. I think the key is in this
line "Developers and testers can access each other's environments and resources, but they cannot access staging or production
resources."

"Developers and testers can access each other's environments " -> This implies developer and testers actually have different
environments. Caught me off guard because I was thinking dev and test are (development)
D does not give any clear answer for what they will do with the test environment.

upvoted 2 times

? ?  csestony 6ámonths ago

A is the answer
upvoted 2 times

? ?  thamaster 6ámonths ago

Selected Answer: D

for me it's D.
The staging environment needs access to some services from production. the production project will be host and staging will be service
project. you don't need staging to access all services in production that's why i get rid of answer A

upvoted 2 times

? ?  Smaks 6ámonths, 2áweeks ago

Selected Answer: D

the Q is : What should you do to isolate development environments from staging and production?
To isolate Dev (Test has not been mentioned) we need to: Create one project for development, a second for staging and a third for
production

upvoted 1 times

? ?  Kulwant85 7ámonths ago

Selected Answer: A

A is better approach as per the question/requirement

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

It's D because staging and production are in different projects

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

586/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company Overview -

Topic 5

Mountkirk Games makes online, session-based, multiplayer games for the most popular mobile platforms. They build all of their games using

some server-side integration. Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

1. Dynamically scale up or down based on game activity

2. Connect to a managed NoSQL database service

3. Run customize Linux distro

Requirements for Game Analytics Platform

1. Dynamically scale up or down based on game activity

2. Process incoming data on the  y directly from the game servers

3. Process data that arrives late because of slow mobile networks

4. Allow SQL queries to access at least 10 TB of historical data

5. Process  les that are regularly uploaded by users' mobile devices

6. Use only fully managed services

CEO Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users.

CTO Statement -

Our current technology stack cannot provide the scale we need, so we want to replace MySQL and move to an environment that provides

autoscaling, low latency load balancing, and frees us up from managing physical servers.

CFO Statement -

We are not capturing enough user demographic data, usage metrics, and other KPIs. As a result, we do not engage the right users, we are not

con dent that our marketing is targeting the right users, and we are not selling enough premium Blast-Ups inside the games, which dramatically

impacts our revenue.

Question

Mountkirk Games wants to set up a real-time analytics platform for their new game. The new platform must meet their technical requirements.

Which combination of Google technologies will meet all of their requirements?

A. Kubernetes Engine, Cloud Pub/Sub, and Cloud SQL

B. Cloud Data ow, Cloud Storage, Cloud Pub/Sub, and BigQuery

C. Cloud SQL, Cloud Storage, Cloud Pub/Sub, and Cloud Data ow

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

587/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D. Cloud Dataproc, Cloud Pub/Sub, Cloud SQL, and Cloud Data ow

E. Cloud Pub/Sub, Compute Engine, Cloud Storage, and Cloud Dataproc

Correct Answer: B

Ingest millions of streaming events per second from anywhere in the world with Cloud Pub/Sub, powered by Google's unique, high-speed private

network. Process the streams with Cloud Data ow to ensure reliable, exactly-once, low-latency data transformation. Stream the transformed

data into BigQuery, the cloud-native data warehousing service, for immediate analysis via SQL or popular visualization tools.

From scenario: They plan to deploy the game's backend on Google Compute Engine so they can capture streaming metrics, run intensive

analytics.

Requirements for Game Analytics Platform

1. Dynamically scale up or down based on game activity

2. Process incoming data on the  y directly from the game servers

3. Process data that arrives late because of slow mobile networks

4. Allow SQL queries to access at least 10 TB of historical data

5. Process  les that are regularly uploaded by users' mobile devices

6. Use only fully managed services

Reference:

https://cloud.google.com/solutions/big-data/stream-analytics/

Community vote distribution

B (100%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

Agree with B

upvoted 32 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 9 times

? ?  nitinz 2áyears, 3ámonths ago

it is B

upvoted 3 times

? ?  sri007  Highly Voted ?  3áyears, 5ámonths ago

Correct Answer B

Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery

A real time requires Stream / Messaging so Pub/Sub, Analytics by Big Query. Ingest millions of streaming events per second from
anywhere in the world with Cloud Pub/Sub, powered by Google's unique, high-speed private network. Process the streams with Cloud
Dataflow to ensure reliable, exactly-once, low-latency data transformation. Stream the transformed data into BigQuery, the cloud-native
data warehousing service, for immediate analysis via SQL or popular visualization tools

upvoted 29 times

? ?  someCloudUser  Most Recent ?  4ámonths, 1áweek ago

Selected Answer: B

Agree with B

upvoted 2 times

? ?  habros 6ámonths, 4áweeks ago

Selected Answer: B

Real-time analytics = OLAP = Bigquery

upvoted 2 times

? ?  ashrafh 7ámonths, 1áweek ago

Seems old case study, dosen't match with the below
https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf

upvoted 2 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I am confident is only B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

588/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  karmajuney 8ámonths, 3áweeks ago

b is ok

upvoted 1 times

? ?  Nirca 9ámonths, 3áweeks ago

Selected Answer: B

Correct Answer B

Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery

upvoted 2 times

? ?  Nirca 11ámonths, 1áweek ago

Selected Answer: B

b it is

upvoted 1 times

? ?  Pime13 1áyear, 6ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: B

B is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  rjupc 1áyear, 8ámonths ago

Aggree with B
upvoted 1 times

? ?  JustJack21 1áyear, 9ámonths ago

The question also says: "The new platform must meet their technical requirements". The case study says:
"Technical requirements:
1. Dynamically scale up or down based on game activity
2. Connect to a managed NoSQL database service
3. Run customize Linux distro"
Only E.áCloud Pub/Sub, Compute Engine, Cloud Storage, and Cloud Dataproc
seems to have all requirements. am I overthinking this?

upvoted 1 times

? ?  PleeO 1áyear, 9ámonths ago

look at the question part: Mountkirk Games wants to set up a real-time analytics platform for their new game -> BigQuery is always a
resolution

upvoted 2 times

? ?  Nik22 1áyear, 9ámonths ago

Are we seeing the older scenarios in new exam?

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

589/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 6 - Testlet 3

Question #1

Introductory Info

Company Overview -

Topic 6

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They build all of their games using some server-side

integration.

Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers, MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics, run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

Dynamically scale up or down based on game activity

Connect to a transactional database service to manage user pro les and game state

Store game activity in a timeseries database service for future analysis

As the system scales, ensure that data is not lost due to processing backlogs

Run hardened Linux distro

Requirements for Game Analytics Platform

Dynamically scale up or down based on game activity

Process incoming data on the  y directly from the game servers

Process data that arrives late because of slow mobile networks

Allow queries to access at least 10 TB of historical data

Process  les that are regularly uploaded by users' mobile devices

Executive Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users. Additionally, our current technology stack cannot provide the scale

we need, so we want to replace MySQL and move to an environment that provides autoscaling, low latency load balancing, and frees us up from

managing physical servers.

Question

For this question, refer to the Mountkirk Games case study. Mountkirk Games wants to migrate from their current analytics and statistics reporting

model to one that meets their technical requirements on Google Cloud Platform.

Which two steps should be part of their migration plan? (Choose two.)

A. Evaluate the impact of migrating their current batch ETL code to Cloud Data ow.

B. Write a schema migration plan to denormalize data for better performance in BigQuery.

C. Draw an architecture diagram that shows how to move from a single MySQL database to a MySQL cluster.

D. Load 10 TB of analytics data from a previous game into a Cloud SQL instance, and run test queries against the full dataset to con rm that

they complete successfully.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

590/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

E. Integrate Cloud Armor to defend against possible SQL injection attacks in analytics  les uploaded to Cloud Storage.

Correct Answer: AB

Community vote distribution

AB (100%)

? ?  sri007  Highly Voted ?  3áyears, 5ámonths ago

Correct Answer A, B

Evaluate the impact of migrating their current batch ETL code to Cloud Dataflow

Write a schema migration plan to denormalize data for better performance in BigQuery.

Stream processing (ETL) Dataflow and Reference https://cloud.google.com/bigquery/docs/loading-
data#loading_denormalized_nested_and_repeated_data

upvoted 30 times

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

agree AB

upvoted 12 times

? ?  tartar 2áyears, 10ámonths ago

AB is ok

upvoted 8 times

? ?  nitinz 2áyears, 3ámonths ago

it is AB

upvoted 1 times

? ?  megumin  Most Recent ?  7ámonths, 3áweeks ago

ok for AB

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A , B right choice
upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: AB

First remove non sense answers: D / E
Now A is a must, the ETL is definitely what Cloud Dataflow does.
Now between B / C. Its talking about a lot of data. we know that Cloud SQL is not the best for huge volume of data, plus not real time data.
Big Query is the best option

upvoted 4 times

? ?  AMohanty 10ámonths, 3áweeks ago

DeNormalization is an essential part of BigData - Agreed.
However we don't know how the Data is.
I would be inclined for A and C

upvoted 1 times

? ?  RVivek 4ámonths, 2áweeks ago

CloudSQL is not recommendad for DB size greater than 1 TB

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: AB

vote AB

upvoted 2 times

? ?  BrijMohan08 1áyear, 8ámonths ago

A and B

upvoted 1 times

? ?  hongha 1áyear, 11ámonths ago

Refer to updated Mountkik Games (post 01.05.2021) full analysis to prepare for your exams.
https://www.youtube.com/watch?v=1w1olPjlPZY&t=6s

upvoted 3 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A, B

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

591/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  victory108 2áyears, 1ámonth ago

A. Evaluate the impact of migrating their current batch ETL code to Cloud Dataflow.
B. Write a schema migration plan to denormalize data for better performance in BigQuery.

upvoted 3 times

? ?  Ausias18 2áyears, 2ámonths ago

Answers are A, B
upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

AB is ok

upvoted 1 times

? ?  bnlcnd 2áyears, 4ámonths ago

A
ETL --> Dataflow
B
https://cloud.google.com/solutions/bigquery-data-warehouse#managing_data

upvoted 2 times

? ?  noussy 2áyears, 5ámonths ago

why B ?

upvoted 3 times

? ?  AdityaGupta 2áyears, 8ámonths ago

Correct answer is AB
Cloud Dataflow -- Stream data (mobile devices)
BigQuery --------- Intensive Analytics + historic data

Both are FULLY MANAGED services.

upvoted 5 times

? ?  amolkekan2 2áyears, 8ámonths ago

AB is correct

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

592/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company Overview -

Topic 6

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They build all of their games using some server-side

integration.

Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers, MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics, run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

Dynamically scale up or down based on game activity

Connect to a transactional database service to manage user pro les and game state

Store game activity in a timeseries database service for future analysis

As the system scales, ensure that data is not lost due to processing backlogs

Run hardened Linux distro

Requirements for Game Analytics Platform

Dynamically scale up or down based on game activity

Process incoming data on the  y directly from the game servers

Process data that arrives late because of slow mobile networks

Allow queries to access at least 10 TB of historical data

Process  les that are regularly uploaded by users' mobile devices

Executive Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users. Additionally, our current technology stack cannot provide the scale

we need, so we want to replace MySQL and move to an environment that provides autoscaling, low latency load balancing, and frees us up from

managing physical servers.

Question

For this question, refer to the Mountkirk Games case study. You need to analyze and de ne the technical architecture for the compute workloads

for your company, Mountkirk Games. Considering the Mountkirk Games business and technical requirements, what should you do?

A. Create network load balancers. Use preemptible Compute Engine instances.

B. Create network load balancers. Use non-preemptible Compute Engine instances.

C. Create a global load balancer with managed instance groups and autoscaling policies. Use preemptible Compute Engine instances.

D. Create a global load balancer with managed instance groups and autoscaling policies. Use non-preemptible Compute Engine instances.

Correct Answer: D

Community vote distribution

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

593/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D (100%)

? ?  dabrat  Highly Voted ?  3áyears, 7ámonths ago

D) => KPI game stability = Use non-preemptible

upvoted 47 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 13 times

? ?  nitinz 2áyears, 3ámonths ago

has to be C, A & B does not meet SLA. D does not meet KPI.

upvoted 2 times

? ?  KNG  Highly Voted ?  3áyears, 5ámonths ago

Agree "D". Preemptible VM is suitable for app which is fault-tolerant. Termination of preemptive VM might affect gaming experience, so it
is not a good choice.

upvoted 18 times

? ?  someCloudUser  Most Recent ?  4ámonths, 1áweek ago

Selected Answer: D

D is correct in my opinion.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

ok for D

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

D is right

upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

"As the system scales, ensure that data is not lost due to processing backlogs"
Answer is clearly D.
Never mentions about cost saving / optimizing.
Plus ending instances will affect current active users.

upvoted 3 times

? ?  Nirca 9ámonths, 3áweeks ago

Selected Answer: D

"D". should be the right one.
Preemptible VM is suitable for app which is fault-tolerant.
Preemptive might lead to availability or services issues in ONLINE applications

upvoted 1 times

? ?  Nirca 11ámonths, 1áweek ago

Selected Answer: D

D it is!

upvoted 1 times

? ?  rsh3 1áyear, 6ámonths ago

I got the point that Answer should include non-preemptible option, but I am confused why not B?

upvoted 1 times

? ?  panqueca 1áyear, 1ámonth ago

They want to be everywhere around the globe, so u need a global load balancer to do this

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

594/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D û create globale LB with MIG and autoscaling policies. Use non-preemptible Compute Engine instances.
Standard vs Preemtible VMs is interesting dilemma for MountKirk Games. The diff in price is 4.5 times. E.g. 1 year price of n1-standard-8
(30 GB, 8 vCPUs) = 3329 $, and same preemptible = 740 $. If there are 10 servers running all time then saving in cost are 25890 $ per year.
Likely not super extra expense for company if they want preserve and extend users.
So, their choice should depend on user experience == their income.
Couple notes from Case Study reqs supporting UX / standard VMs:
1) Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game
2) Improve uptime - downtime is loss of players
3) Reduce latency to all customers
4) They plan to deploy the gameÆs backend on Google Compute Engine so they can capture streaming metrics, run intensive analytics, and
take advantage of its autoscaling server environment and integrate with a managed NoSQL database.

upvoted 3 times

? ?  MaxNRG 1áyear, 8ámonths ago

Last item about intestive analytics and streaming - is not appropriate task for preemptible VMs. And this is from GCP preemptible page:
1) If your applications are fault-tolerant and can withstand possible instance preemptions, then preemptible instances can reduce your
Compute Engine costs significantly. For example, batch processing jobs can run on preemptible instances.
2) Preemptible instances cannot live migrate to a regular VM instance, or be set to automatically restart when there is a maintenance
event.
3) Due to the above limitations, preemptible instances are not covered by any Service Level Agreement (and, for clarity, are excluded
from the Google Compute Engine SLA).
So, sounds as non-preemptible (standard) is a priority for Mountkirk...

upvoted 1 times

? ?  [Removed] 1áyear, 8ámonths ago

D is correct.
Non-premptive CE + Load Balancer + Autoscaling. Highly available solution

upvoted 1 times

? ?  BrijMohan08 1áyear, 8ámonths ago

I will go with D
upvoted 1 times

? ?  hongha 1áyear, 11ámonths ago

Refer to updated Mountkik Games (post 01.05.2021) full analysis to prepare for your exams.
https://www.youtube.com/watch?v=1w1olPjlPZY&t=6s

upvoted 1 times

? ?  mbrueck 1áyear, 11ámonths ago

Answer: D

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is D

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

595/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company Overview -

Topic 6

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They build all of their games using some server-side

integration.

Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers, MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics, run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

Dynamically scale up or down based on game activity

Connect to a transactional database service to manage user pro les and game state

Store game activity in a timeseries database service for future analysis

As the system scales, ensure that data is not lost due to processing backlogs

Run hardened Linux distro

Requirements for Game Analytics Platform

Dynamically scale up or down based on game activity

Process incoming data on the  y directly from the game servers

Process data that arrives late because of slow mobile networks

Allow queries to access at least 10 TB of historical data

Process  les that are regularly uploaded by users' mobile devices

Executive Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users. Additionally, our current technology stack cannot provide the scale

we need, so we want to replace MySQL and move to an environment that provides autoscaling, low latency load balancing, and frees us up from

managing physical servers.

Question

For this question, refer to the Mountkirk Games case study. Mountkirk Games wants to design their solution for the future in order to take

advantage of cloud and technology improvements as they become available. Which two steps should they take? (Choose two.)

A. Store as much analytics and game activity data as  nancially feasible today so it can be used to train machine learning models to predict

user behavior in the future.

B. Begin packaging their game backend artifacts in container images and running them on Google Kubernetes Engine to improve the ability to

scale up or down based on game activity.

C. Set up a CI/CD pipeline using Jenkins and Spinnaker to automate canary deployments and improve development velocity.

D. Adopt a schema versioning tool to reduce downtime when adding new game features that require storing additional player data in the

database.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

596/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

E. Implement a weekly rolling maintenance process for the Linux virtual machines so they can apply critical kernel patches and package

updates and reduce the risk of 0-day vulnerabilities.

Correct Answer: CE

Community vote distribution

AB (57%)

BC (17%)

13%

3%

? ?  dabrat  Highly Voted ?  3áyears, 7ámonths ago

A+B)
=>as well as other metrics that provide deeper insight into usage patterns so we can adapt the game to target users.

environment that provides autoscaling, low latency load balancing, and frees us up from managing physical servers.

upvoted 78 times

? ?  techalik 2áyears, 7ámonths ago

Enable CI/CD integration to improve deployment velocity, agility and reaction to change. is the right answer.

Having a CI/CD pipeline means you can deploy changes to environments faster. Does this help you take advantage of cloud and
technology improvements as they become available in the future? Yes. When new features become available, you can incorporate them
into your application and deploy them to test/production environments easily/efficiently and decrease the time to go live.

Store more data and use it as training data for machine learning. is the right answer.

The more data you, the better you can train your AI model. The better the trained model, the better the prediction service can perform.
By retaining as much real data as financially feasible, you are in the best position to take advantage of AI improvements in the future.

Ref: https://cloud.google.com/ai-platform

AC

upvoted 14 times

? ?  XDevX 1áyear, 12ámonths ago

Hi techalik,
I googled right now and could not finde " By retaining as much real data as financially feasible, you are in the best position to take
advantage of AI improvements in the future." - from an economic perspective that makes no sense or the wording in the question is
very "dirty". What is financially feasible? For me that means invest every cent you have into data - that cannot be the right approach.
I vote for B+C.
upvoted 7 times

? ?  kkhurana 1áyear, 5ámonths ago

I agree .Me too for BC

upvoted 1 times

? ?  tzKhalil 2áyears, 2ámonths ago

As they want to take advantage of cloud, it is better to choose Cloud Build which is from GCP's service to build CI/CD.
AB is the choice
upvoted 8 times

? ?  tartar 2áyears, 10ámonths ago

AB is ok

upvoted 21 times

? ?  kumarp6 2áyears, 8ámonths ago

AC make sense.
upvoted 7 times

? ?  nitinz 2áyears, 3ámonths ago

AC makes sense in respect to this question.

upvoted 5 times

? ?  AD2AD4  Highly Voted ?  3áyears, 1ámonth ago

Final Decision to go with Option AB

upvoted 24 times

? ?  ShadowLord 10ámonths ago

Say if there proposed feature are in BigQuery , DataFlow or some other GCP service... how will just doing B help ..... C can help

upvoted 1 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 4ádays ago

I can see why C & D are the given correct answers.
They objectively allow you to quickly incorporate tech improvements.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

597/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A. Would help with possible machine learning models in the future.
B. Makes use of an existing technology
D. Makes use of an existing technology

I chose A & E but I think C & E probably what Google are after.

upvoted 1 times

? ?  Atanu 2áweeks, 2ádays ago

Selected Answer: BC

B+C looks promising

upvoted 1 times

? ?  Murmure 1ámonth ago

Selected Answer: CE

Future is the key word.

upvoted 1 times

? ?  medi01 2ámonths, 1áweek ago

Selected Answer: AC

Higher velocity (C) = adopt new features fast.
A bodes well with the statement that they are missing a lot of data/insights.

upvoted 1 times

? ?  rr4444 3ámonths, 2áweeks ago

This is annoying
A, B and C seem correct.......... But only two......?

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

In a GCP exam, non GCP products(as per C) is rarely the correct answer!

upvoted 3 times

? ?  BeCalm 3ámonths, 2áweeks ago

Selected Answer: AB

A = ML = future stuff
B = containers = portability (since current platform runs on VM's)

upvoted 1 times

? ?  SandipGhosal 4ámonths ago

In technical and business requirement of the case study it mentioned about Analytics, not machine learning. So A seems not a correct
answer here. D is very generic as OS upgrade and patches could be done outside GCP. I would go with B and C.

upvoted 1 times

? ?  telp 4ámonths, 1áweek ago

Selected Answer: AB

Answer AB
Cloud Build is the Google Solution for CI/CD than other third party sofware can't be used for CI/CD in a certification examen.

upvoted 2 times

? ?  sameer2803 6ámonths ago

A&B is correct
rest of the improvements can be made without being on a cloud platform as well.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

A. Store as much analytics and game activity data as financially feasible today so it can be used to train machine learning models to
predict user behavior in the future. Storing as much data as possible will allow Mountkirk Games to use machine learning models to
analyze and predict user behavior, which can help them adapt the game to target users and improve the user experience.

B. Begin packaging their game backend artifacts in container images and running them on Google Kubernetes Engine to improve the
ability to scale up or down based on game activity. Containerization allows Mountkirk Games to package their game backend and
dependencies into a single unit that can be easily deployed and run on any platform. By using Google Kubernetes Engine (GKE), Mountkirk
Games can take advantage of GKE's autoscaling and load balancing features to ensure that their game backend can scale up or down
based on game activity. This will help Mountkirk Games improve uptime and reduce latency to all customers, as well as increase the
efficiency of their cloud resources.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: AB

A, B is the correct answer

upvoted 1 times

? ?  moustaoui 7ámonths ago

Ans : AB

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

598/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: AB

ok for AB

upvoted 1 times

? ?  muneebarshad 9ámonths, 1áweek ago

Selected Answer: AB

I think the answers are A & B.

A: It makes sense to collected as much data points in order to have adquate data for ML models
B: Google recommends microservices & having containerized images , if they already have containre images , it would be easier to push to
gcr & then run on GKE
C:Cloud Build is the Google Solution for CI/CD and requires Dockerfile , hence CI/CD piepline code for Jenkins
will not be useful in futire
D: schema versioning tool is not related for future google services
E: Google does the patching on all VMS so its not relevant

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

599/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company Overview -

Topic 6

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They build all of their games using some server-side

integration.

Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers, MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics, run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

Dynamically scale up or down based on game activity

Connect to a transactional database service to manage user pro les and game state

Store game activity in a timeseries database service for future analysis

As the system scales, ensure that data is not lost due to processing backlogs

Run hardened Linux distro

Requirements for Game Analytics Platform

Dynamically scale up or down based on game activity

Process incoming data on the  y directly from the game servers

Process data that arrives late because of slow mobile networks

Allow queries to access at least 10 TB of historical data

Process  les that are regularly uploaded by users' mobile devices

Executive Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users. Additionally, our current technology stack cannot provide the scale

we need, so we want to replace MySQL and move to an environment that provides autoscaling, low latency load balancing, and frees us up from

managing physical servers.

Question

For this question, refer to the Mountkirk Games case study. Mountkirk Games wants you to design a way to test the analytics platform's resilience

to changes in mobile network latency. What should you do?

A. Deploy failure injection software to the game analytics platform that can inject additional latency to mobile client analytics tra c.

B. Build a test client that can be run from a mobile phone emulator on a Compute Engine virtual machine, and run multiple copies in Google

Cloud Platform regions all over the world to generate realistic tra c.

C. Add the ability to introduce a random amount of delay before beginning to process analytics  les uploaded from mobile devices.

D. Create an opt-in beta of the game that runs on players' mobile devices and collects response times from analytics endpoints running in

Google Cloud Platform regions all over the world.

Correct Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

600/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Community vote distribution

A (71%)

C (18%)

12%

? ?  a66030  Highly Voted ?  2áyears, 8ámonths ago

The answer is A. The question asks - test the analytics platform's resilience to changes in mobile network latency.
Only A adds latency to mobile network.
C - adds delay at beginning of file processing. does not add delay/latency in mobile network.
One of the lines in the requirements for analytics: Process data that arrives late because of slow mobile networks

upvoted 34 times

? ?  ShadowLord 10ámonths ago

C is alright as well but just too specific to file upload

upvoted 2 times

? ?  sri007  Highly Voted ?  3áyears, 5ámonths ago

Correct Answer C

Add the ability to introduce a random amount of delay before beginning to process analytics files uploaded from mobile devices.

upvoted 27 times

? ?  Ani26 2áyears, 10ámonths ago

There is nothing mentioned about uploading analytical files from mobile devices - on analytical layer we need to perform resiliency test
on the latency changes..so A

upvoted 5 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 17 times

? ?  lkjhgfdsa 2áyears, 3ámonths ago

"If you're using a service mesh like Istio to manage your app services, you can inject faults at the application layer instead of
killing pods or machines, or you can inject corrupting packets at the TCP layer. You can introduce delays to simulate network
latency or an overloaded upstream system. You can also introduce aborts, which mimic failures in upstream systems." -
https://cloud.google.com/solutions/scalable-and-resilient-apps

upvoted 9 times

? ?  turbo8p 7ámonths, 2áweeks ago

I'm not quite sure how would you do testing at scale for option C.

It's likely that you need to test with your real users to perform testing at scale. But for Option A, you can control how many test clients
you want to simulate.

upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

A is the answer
upvoted 5 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 4ádays ago

I think C

I went for A think this was a K8 Istio platform but it's not so we'd need to do something special.

Mobile clients are uploading files and the question says to check reliability when there is delay in the upload. C seems to meet this
requirement perfectly.

B. I don't like as it will use the internet & not slow, flakey mobile signal

D. Seems like madness. Just put a create timestamp in the file & compare the difference to arrive timestamp, why need an opt in? Also this
isn't a test approach.

upvoted 1 times

? ?  rr4444 3ámonths, 2áweeks ago

Selected Answer: A

A

Use Istio

upvoted 1 times

? ?  BeCalm 3ámonths, 2áweeks ago

Selected Answer: A

A and C are both right but with no cost constraints, A is easier than C

upvoted 1 times

? ?  pepigeon 4ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

601/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is most likely correct: Istio (which is now Anthos Service Mesh on GCP) is capable of injecting delays:
https://istio.io/latest/docs/tasks/traffic-management/fault-injection/#injecting-an-http-delay-fault

upvoted 1 times

? ?  omermahgoub 6ámonths ago

A. Deploy failure injection software to the game analytics platform that can inject additional latency to mobile client analytics traffic.

In order to test the analytics platform's resilience to changes in mobile network latency, the best approach would be to use failure
injection software to intentionally introduce latency to the mobile client analytics traffic. This will allow Mountkirk Games to see how the
analytics platform responds to changes in network latency and identify any potential issues or bottlenecks that may arise. This approach
will also allow Mountkirk Games to test the analytics platform under realistic conditions, as it will be simulating the type of latency that
may occur in the real world.

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  gonlafer 6ámonths, 2áweeks ago

Selected Answer: C

I vote for C.
Random means chaos which is a good practice to test resiliency.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A seems right
upvoted 1 times

? ?  Nirca 9ámonths, 3áweeks ago

Selected Answer: A

A is ok to some extent.

upvoted 1 times

? ?  ShadowLord 10ámonths ago

Selected Answer: B

Question is on Mobile Network Latency between Mobile and GCP Endpoint ....
So the answer should be testing problem of latency between Mobile and GCP Edge / Endpoints
A. Is not fault injection on the platform , has nothing to do with Mobile Network Latency..
- Testing the latency is a better way ..... Emulator or Beta
- Beta version is too late on the process

To me Emulator makes more sense as this has to be done before Release

upvoted 2 times

? ?  midgoo 10ámonths ago

Selected Answer: C

C is how Istio does the delay test.
A does not make sense in practice.

upvoted 2 times

? ?  ShadowLord 10ámonths ago

Question is on Mobile Network Latency between Mobile and GCP Endpoint ....
- What would Istio do here ??/
- Testing the latency is a better way ..... Emulator or Beta
To me Emulator makes more sense as this has to be done before Release

upvoted 1 times

? ?  ShadowLord 10ámonths ago

Sorry B

upvoted 1 times

? ?  Ric350 11ámonths, 1áweek ago

"Deploy failure injection SOFTWARE to the game analytics platform" - that doesn't make sense to me. This reads to me as you're installing
additional software to game. Fault injection are tools GCP has to create rules for things such creating delay. See links below. I think it is C
and A is to throw you off. At least that's how I read it. Frustrating bc is it truly testing our knowledge or testing how carefully we can read
English and take a test? Think it's a terrible question.
https://cloud.google.com/anthos/service-mesh
https://istio.io/latest/docs/tasks/traffic-management/fault-injection/

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

602/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: A

A as told by a66030

upvoted 1 times

? ?  pddddd 1áyear, 5ámonths ago
A - in prod, I think no go
B - test setup and plan that makes sense.
C - in prod again...
D - what?

upvoted 2 times

? ?  Nirca 9ámonths, 3áweeks ago

Yet C is doing "realistic traffic." -> and we must have the option to add latency.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

603/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company Overview -

Topic 6

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They build all of their games using some server-side

integration.

Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers, MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics, run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

Dynamically scale up or down based on game activity

Connect to a transactional database service to manage user pro les and game state

Store game activity in a timeseries database service for future analysis

As the system scales, ensure that data is not lost due to processing backlogs

Run hardened Linux distro

Requirements for Game Analytics Platform

Dynamically scale up or down based on game activity

Process incoming data on the  y directly from the game servers

Process data that arrives late because of slow mobile networks

Allow queries to access at least 10 TB of historical data

Process  les that are regularly uploaded by users' mobile devices

Executive Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users. Additionally, our current technology stack cannot provide the scale

we need, so we want to replace MySQL and move to an environment that provides autoscaling, low latency load balancing, and frees us up from

managing physical servers.

Question

For this question, refer to the Mountkirk Games case study. You need to analyze and de ne the technical architecture for the database workloads

for your company, Mountkirk Games. Considering the business and technical requirements, what should you do?

A. Use Cloud SQL for time series data, and use Cloud Bigtable for historical data queries.

B. Use Cloud SQL to replace MySQL, and use Cloud Spanner for historical data queries.

C. Use Cloud Bigtable to replace MySQL, and use BigQuery for historical data queries.

D. Use Cloud Bigtable for time series data, use Cloud Spanner for transactional data, and use BigQuery for historical data queries.

Correct Answer: D

Community vote distribution

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

604/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D (100%)

? ?  misho  Highly Voted ?  3áyears ago

For the people who say it's C in Linux Academy, did you see the Technical requirements there? The old Technical Requirements have the
line "Connect to a managed NoSQL database service" but in the Technical Requirements in Google official site and in this question the line
is replaced if the following 2 lines "Connect to a transactional database service to manage user profiles and game state
Store game activity in a timeseries database service for future analysis". And for them definitely D is the answer!

upvoted 65 times

? ?  sri007  Highly Voted ?  3áyears, 5ámonths ago

Correct Answer D

Use Cloud Bigtable for time series data, use Cloud Spanner for transactional data, and use BigQuery for historical data queries.

Storing time-series data in Cloud Bigtable is a natural fit, Cloud Spanner scales horizontally and serves data with low latency while
maintaining transactional consistency and industry-leading 99.999% (five 9s) availability - 10x less downtime than four nines (<5 minutes
per year). Cloud Spanner helps future-proof your database backend. After you load your data into BigQuery, you can query the data in
your tables. BigQuery supports two types of queries: Interactive queries, Batch queries

upvoted 39 times

? ?  AdityaGupta 2áyears, 8ámonths ago

I agree with above explanation and choice

upvoted 8 times

? ?  omermahgoub  Most Recent ?  6ámonths ago

D. Use Cloud Bigtable for time series data, use Cloud Spanner for transactional data, and use BigQuery for historical data queries.

Based on the technical requirements provided, the best approach for analyzing and defining the technical architecture for the database
workloads for Mountkirk Games would be to use Cloud Bigtable for time series data, Cloud Spanner for transactional data, and BigQuery
for historical data queries.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

ok for D

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

D is right

upvoted 1 times

? ?  Nirca 9ámonths, 3áweeks ago

Selected Answer: D

D - GCP's classics best practice for tests

upvoted 1 times

? ?  Nirca 11ámonths, 1áweek ago

Selected Answer: D

DDDDD it is

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

C is worong.Bigtable is NOSQL.Bigtable cant be alternative for MySQL.
I think D is correct.
Biquery most fit historical data queries.

upvoted 1 times

? ?  zxcv1234 1áyear, 6ámonths ago

Selected Answer: D

C cannot be the answer

upvoted 1 times

? ?  ABO_Doma 1áyear, 6ámonths ago

Selected Answer: D

D Correct Answer D

upvoted 1 times

? ?  ABO_Doma 1áyear, 6ámonths ago

Selected Answer: D

D is the Answer
upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

605/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: D

Vote D

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 2 times

? ?  BrijMohan08 1áyear, 8ámonths ago

The answer is D
upvoted 1 times

? ?  MikeB19 1áyear, 10ámonths ago

I agree with D. There r 3 db needed.
1. Analytics - currently on MySQL
2. Nosql - this is a future requirement for the new game. This referenced in solution section
3. User profile - managed transactional db for user profiles. This reference in tech section.
User profiles can be addressed with scanner. The below article describes this use case
https://cloud.google.com/architecture/best-practices-cloud-spanner-gaming-database

upvoted 4 times

? ?  rm_2495 1áyear, 11ámonths ago

D is the answer
upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

606/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company Overview -

Topic 6

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They build all of their games using some server-side

integration.

Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers, MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics, run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

Dynamically scale up or down based on game activity

Connect to a transactional database service to manage user pro les and game state

Store game activity in a timeseries database service for future analysis

As the system scales, ensure that data is not lost due to processing backlogs

Run hardened Linux distro

Requirements for Game Analytics Platform

Dynamically scale up or down based on game activity

Process incoming data on the  y directly from the game servers

Process data that arrives late because of slow mobile networks

Allow queries to access at least 10 TB of historical data

Process  les that are regularly uploaded by users' mobile devices

Executive Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users. Additionally, our current technology stack cannot provide the scale

we need, so we want to replace MySQL and move to an environment that provides autoscaling, low latency load balancing, and frees us up from

managing physical servers.

Question

For this question, refer to the Mountkirk Games case study. Which managed storage option meets Mountkirk's technical requirement for storing

game activity in a time series database service?

A. Cloud Bigtable

B. Cloud Spanner

C. BigQuery

D. Cloud Datastore

Correct Answer: A

Community vote distribution

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

607/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A (100%)

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

@jcmoranp , that is incorrect.. https://cloud.google.com/bigtable/docs/schema-design-time-series it's A

upvoted 27 times

? ?  zbyszekz 1áyear, 9ámonths ago

It is not clear, read technical requirements: "Store game activity logs in structured files for future analysis." so I think that D is a good
option

upvoted 1 times

? ?  sri007  Highly Voted ?  3áyears, 5ámonths ago

Correct Answer A

Cloud Bigtable

Storing time series data in Cloud Bigtable https://cloud.google.com/bigtable/docs/schema-design-time-series

upvoted 17 times

? ?  anirban7172  Most Recent ?  1áweek, 5ádays ago

Selected Answer: A

Google Bigtable is a fully managed, scalable NoSQL database service for large analytical and operational workloads.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago
A can capture time series data

upvoted 1 times

? ?  jabrrJ68w02ond1 9ámonths, 3áweeks ago

Selected Answer: A

The key word is "time series" which ultimately leads to Bigtable. It is also used for collecting data from IoT devices.

upvoted 3 times

? ?  burner_1984 1áyear, 5ámonths ago

as per ACG its C. BigQuery

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  Rzla 1áyear, 9ámonths ago

Answer is A BigTable. Thats the best solution to process and store the data, BiqQuery can be used to analyse the data. Big Query not a
suitable target for time series.

upvoted 4 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 4 times

? ?  victory108 2áyears, 1ámonth ago

A. Cloud Bigtable
upvoted 4 times

? ?  gosi 2áyears, 2ámonths ago

C - BigQuery

BigTable is Wrong: BigTable is timeseries with "low latency" and they didnt mention anything about low-latency. They did mention about
future analysis so BQ is best.

DataStore is wrong Choice: DataStore is good for transactional data e.g. saving and updating game state as it chnages. In the question
they are talking about game activity which is a serious on insert only data based on time.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

608/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  Koushick 2áyears, 2ámonths ago

Google says we can store time series vehicle data in Bigtable which can later be used for analytical processing using BigQuery.
https://cloud.google.com/architecture/designing-connected-vehicle-platform#data_ingestion
Even though it says vehicle data and the question is for game data we eventually are storing time series type of data so I think this link is
relevant.
Based on Google explanation, I would choose A

upvoted 2 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

A is ok

upvoted 2 times

? ?  pawel_ski 2áyears, 3ámonths ago

Technical req:
"Connect to a transactional database service to manage user profiles and game state."
Bigtable is not trasactional. Datastore is trasactional. So D.

upvoted 2 times

? ?  lynx256 2áyears, 3ámonths ago

But question is about game ACTIVITY, not about game state

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

609/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #7

Introductory Info

Company Overview -

Topic 6

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They build all of their games using some server-side

integration.

Historically, they have used cloud providers to lease physical servers.

Due to the unexpected popularity of some of their games, they have had problems scaling their global audience, application servers, MySQL

databases, and analytics tools.

Their current model is to write game statistics to  les and send them through an ETL tool that loads them into a centralized MySQL database for

reporting.

Solution Concept -

Mountkirk Games is building a new game, which they expect to be very popular. They plan to deploy the game's backend on Google Compute

Engine so they can capture streaming metrics, run intensive analytics, and take advantage of its autoscaling server environment and integrate with

a managed NoSQL database.

Business Requirements -

Increase to a global footprint

Improve uptime " downtime is loss of players

Increase e ciency of the cloud resources we use

Reduce latency to all customers

Technical Requirements -

Requirements for Game Backend Platform

Dynamically scale up or down based on game activity

Connect to a transactional database service to manage user pro les and game state

Store game activity in a timeseries database service for future analysis

As the system scales, ensure that data is not lost due to processing backlogs

Run hardened Linux distro

Requirements for Game Analytics Platform

Dynamically scale up or down based on game activity

Process incoming data on the  y directly from the game servers

Process data that arrives late because of slow mobile networks

Allow queries to access at least 10 TB of historical data

Process  les that are regularly uploaded by users' mobile devices

Executive Statement -

Our last successful game did not scale well with our previous cloud provider, resulting in lower user adoption and affecting the game's reputation.

Our investors want more key performance indicators (KPIs) to evaluate the speed and stability of the game, as well as other metrics that provide

deeper insight into usage patterns so we can adapt the game to target users. Additionally, our current technology stack cannot provide the scale

we need, so we want to replace MySQL and move to an environment that provides autoscaling, low latency load balancing, and frees us up from

managing physical servers.

Question

For this question, refer to the Mountkirk Games case study. You are in charge of the new Game Backend Platform architecture. The game

communicates with the backend over a REST API.

You want to follow Google-recommended practices. How should you design the backend?

A. Create an instance template for the backend. For every region, deploy it on a multi-zone managed instance group. Use an L4 load balancer.

B. Create an instance template for the backend. For every region, deploy it on a single-zone managed instance group. Use an L4 load balancer.

C. Create an instance template for the backend. For every region, deploy it on a multi-zone managed instance group. Use an L7 load balancer.

D. Create an instance template for the backend. For every region, deploy it on a single-zone managed instance group. Use an L7 load balancer.

Correct Answer: C

Community vote distribution

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

610/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C (88%)

6%

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago
It's C. You need a L7 balancer and multi-zone

upvoted 63 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 15 times

? ?  bjuneja 2áyears, 6ámonths ago

L4 load balancing offers traffic management of transactions at the network protocol layer (TCP/UDP). ... L7 load balancing works at
the highest level of the OSI model. L7 bases its routing decisions on various characteristics of the HTTP/HTTPS header
Mountrik requirment is global so C is ok

upvoted 12 times

? ?  nitinz 2áyears, 3ámonths ago

It is C, you need L7 & mulit-region as its the ask.

upvoted 6 times

? ?  JJu  Highly Voted ?  3áyears, 6ámonths ago

I think answer is C.

This game type is mobile.
Check this link : https://cloud.google.com/solutions/gaming/cloud-game-infrastructure#dedicated_game_server
I recommend this section : æRequest/response based serversÆ
explain : In particular, however, mobile game servers, without a critical demand for real-time communication, have adopted HTTP request
and response semantics like those used in web hosting.

this game use HTTP load balancer. HTTP load balancer is L7.

upvoted 22 times

? ?  kratosmat  Most Recent ?  2ámonths, 3áweeks ago

Selected Answer: A

we don't need multi-zonal if we have multi-region. the traffic is internal, we could need an L7 LB if there are specific requirements, but I
don't see them.
upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: C

C gives best option
Key word "REST API " "L7 balancer" , "Multi Zone"

upvoted 3 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C perfectly make sense

upvoted 1 times

? ?  muneebarshad 9ámonths, 1áweek ago

Selected Answer: B

I think the answer is B and here is my reasoning.

Users never communicate with the Backend server (due to security reasons) there is always a Frontend Service that is open to the internet
and FE Service communicates with BE Server. The reason they called it Backend Serer is since it must have a Front end server refer to the
solution reference (https://reviewnprep.com/blog/gcp-how-to-work-on-mountkirk-games-case-study/)

the traffic flow should look like this...... User(mobile) -> FE Game Server (hosted in GCP) -> BE Server (hosted in GCP)

Since BE Service is not exposed to the internet therefore we would require a regional L4 Balancer since regional Load Balancer is NOT
internet facing and it's from VM to VM

upvoted 1 times

? ?  jabrrJ68w02ond1 9ámonths, 3áweeks ago

Multi-zone makes it high available. L7 balances load for e.g. HTTP traffic. So the answer is C. Why not A? L4 works at the network protocol
(TCP/UDP) which is not suitable for REST APIs.

upvoted 1 times

? ?  DrishaS4 11ámonths ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

611/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

L7 LB for RestAPI.

upvoted 2 times

? ?  Nirca 11ámonths, 1áweek ago

Selected Answer: C

C it is

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

C is correct

upvoted 1 times

? ?  SAMBIT 1áyear, 3ámonths ago

Out dated case study. DonÆt waste time

https://cloud.google.com/certification/guides/professional-cloud-architect

https://services.google.com/fh/files/blogs/master_case_study_mountkirk_games.pdf

upvoted 2 times

? ?  amanp 1áyear, 3ámonths ago

It is A, it is backend API and not frontend

upvoted 2 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: C

Ummm.I think the case must choose L7 LB for RestAPI.
Additional,the company require world wide scale.
So i chose C.

upvoted 2 times

? ?  zxcv1234 1áyear, 6ámonths ago

Selected Answer: C

C is correct. HTTP/S is layer 7

upvoted 1 times

? ?  ABO_Doma 1áyear, 6ámonths ago

C is the correct answer

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

612/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 7 - Testlet 4

Question #1

Introductory Info

Company overview -

Topic 7

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms

after successfully migrating their on-premises environments to Google Cloud.

Their most recent endeavor is to create a retro-style  rst-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-

speci c digital arena from multiple platforms and locations. A real-time digital banner will display a global leaderboard of all the top players

across every active arena.

Solution concept -

Mountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game's backend on Google

Kubernetes Engine so they can scale rapidly and use Google's global load balancer to route players to the closest regional game arenas. In order

to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.

Existing technical environment -

The existing environment was recently migrated to Google Cloud, and  ve games came across using lift-and-shift virtual machine migrations, with

a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions

and network policies. Legacy games with low tra c have been consolidated into a single project. There are also separate environments for

development and testing.

Business requirements -

Support multiple gaming platforms.

Support multiple regions.

Support rapid iteration of game features.

Minimize latency.

Optimize for dynamic scaling.

Use managed services and pooled resources.

Minimize costs.

Technical requirements -

Dynamically scale based on game activity.

Publish scoring data on a near real-time global leaderboard.

Store game activity logs in structured  les for future analysis.

Use GPU processing to render graphics server-side for multi-platform support.

Support eventual migration of legacy games to this new platform.

Executive statement -

Our last game was the  rst time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game

telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games

using cloud-native design principles.

Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top

priority, although cost management is the next most important challenge. As with our  rst cloud-based game, we have grown to expect the cloud

to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug  xes and new functionality.

Question

You need to optimize batch  le transfers into Cloud Storage for Mountkirk Games' new Google Cloud solution. The batch  les contain game

statistics that need to be staged in Cloud Storage and be processed by an extract transform load (ETL) tool. What should you do?

A. Use gsutil to batch move  les in sequence.

B. Use gsutil to batch copy the  les in parallel.

C. Use gsutil to extract the  les as the  rst part of ETL.

D. Use gsutil to load the  les as the last part of ETL.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

613/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Correct Answer: B

Reference:

https://cloud.google.com/storage/docs/gsutil/commands/cp

Community vote distribution

B (100%)

? ?  victory108  Highly Voted ?  1áyear, 11ámonths ago
B. Use gsutil to batch copy the files in parallel.

upvoted 13 times

? ?  kopper2019  Highly Voted ?  1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 11 times

? ?  surajkrishnamurthy  Most Recent ?  6ámonths, 1áweek ago

Selected Answer: B

B is the correct answer
Batch Copying in Parallel Saves time and an efficient option to use ( -m)

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  AHUI 8ámonths, 2áweeks ago

C is incorrect, gsutil does not do extract files

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

B is good

upvoted 1 times

? ?  Mikado211 10ámonths, 3áweeks ago

Selected Answer: B

When GCP PCA certification decide to become an english comprehension test instead of a computer science test ^^'

It's B !

upvoted 2 times

? ?  ATANGA 10ámonths, 4áweeks ago

WhatsApp : +1(956)-520-4006 to obtain PMP, CISM, CCNA, CEH, PRINCE2, CISCO, ISTTQB, PRINCE2, AWS/Azure/Sale force/ITIL
Foundation/EC- COUNCIL...
Get Certified with 100% pass guarantee. Pay after exam.
all CISCO, ISACA & EC- COUNCIL certifications
For the Below certificates we offer 100% pass guarantee:
1. AWS Certification
2. Sales force
3. Scrum Master
4. Oracle Certification: OCA, OCP
5. Cisco Certification: CCNA, CCNP, CCIE
6. ITIL Foundation & Intermediate
7. Prince 2 Foundation and Practitioner
8. VMWARE Certification
9. Check Point Certification
10. EC-COUNCIL Certification (CEH V-9, CCISO, CND)
11. Cloud Certification
12. IBM Certification
13. HP Certification
14. Citrix Certification
15. Juniper certification
16. Azure
17.Skype 70-333/34
18.PMI (PMP/CAPM/ACP/PBA)
19.ISTQB
20.SAP
21.ISACA (CISA, CISM, CRISC, CGEIT, COBIT)

PAYMENT ONLY AFTER CERTIFICATION AND RESULT CONFIRMATION.
WhatsApp : +1(956)-520-4006

upvoted 2 times

? ?  DrishaS4 11ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

614/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: B

Use gsutil to batch copy the files in parallel.

upvoted 1 times

? ?  Nirca 11ámonths, 1áweek ago

Selected Answer: B

B is it !

upvoted 1 times

? ?  muky31dec 1áyear, 4ámonths ago
My ans was B in the exam

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

Correct Answer: B
https://cloud.google.com/storage/docs/gsutil/commands/cp
If you have a large number of files to transfer, you can perform a parallel multi-threaded/multi-processing copy using the top-level gsutil -
m option (see gsutil help options):
gsutil -m cp -r dir gs://my-bucket

upvoted 5 times

? ?  riley5 1áyear, 11ámonths ago

B is the answer.
upvoted 5 times

? ?  milan74 1áyear, 11ámonths ago

Answer is B according to the documentation:
https://cloud.google.com/blog/products/gcp/optimizing-your-cloud-storage-performance-google-cloud-performance-atlas

upvoted 4 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 5 times

? ?  JeffClarke111 1áyear, 11ámonths ago

B is ok

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

615/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company overview -

Topic 7

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms

after successfully migrating their on-premises environments to Google Cloud.

Their most recent endeavor is to create a retro-style  rst-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-

speci c digital arena from multiple platforms and locations. A real-time digital banner will display a global leaderboard of all the top players

across every active arena.

Solution concept -

Mountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game's backend on Google

Kubernetes Engine so they can scale rapidly and use Google's global load balancer to route players to the closest regional game arenas. In order

to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.

Existing technical environment -

The existing environment was recently migrated to Google Cloud, and  ve games came across using lift-and-shift virtual machine migrations, with

a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions

and network policies. Legacy games with low tra c have been consolidated into a single project. There are also separate environments for

development and testing.

Business requirements -

Support multiple gaming platforms.

Support multiple regions.

Support rapid iteration of game features.

Minimize latency.

Optimize for dynamic scaling.

Use managed services and pooled resources.

Minimize costs.

Technical requirements -

Dynamically scale based on game activity.

Publish scoring data on a near real-time global leaderboard.

Store game activity logs in structured  les for future analysis.

Use GPU processing to render graphics server-side for multi-platform support.

Support eventual migration of legacy games to this new platform.

Executive statement -

Our last game was the  rst time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game

telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games

using cloud-native design principles.

Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top

priority, although cost management is the next most important challenge. As with our  rst cloud-based game, we have grown to expect the cloud

to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug  xes and new functionality.

Question

You are implementing Firestore for Mountkirk Games. Mountkirk Games wants to give a new game programmatic access to a legacy game's

Firestore database.

Access should be as restricted as possible. What should you do?

A. Create a service account (SA) in the legacy game's Google Cloud project, add a second SA in the new game's IAM page, and then give the

Organization Admin role to both SAs.

B. Create a service account (SA) in the legacy game's Google Cloud project, give the SA the Organization Admin role, and then give it the

Firebase Admin role in both projects.

C. Create a service account (SA) in the legacy game's Google Cloud project, add this SA in the new game's IAM page, and then give it the

Firebase Admin role in both projects.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

616/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D. Create a service account (SA) in the legacy game's Google Cloud project, give it the Firebase Admin role, and then migrate the new game to

the legacy game's project.

Correct Answer: C

Community vote distribution

C (100%)

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is C

upvoted 15 times

? ?  WFCheong  Most Recent ?  5ámonths, 4áweeks ago

I think it should not simply give out the Organization admin role so A and B is out. We should not migrate the new game to the lagacy
game's project and thus D is out. So remain C is the only choice.

upvoted 4 times

? ?  gonlafer 6ámonths, 4áweeks ago

Selected Answer: C

C is right

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 2 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is fine

upvoted 1 times

? ?  muky31dec 1áyear, 4ámonths ago

C is correct , I chose C in real exam

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

C is the correct answer

upvoted 2 times

? ?  ravisar 1áyear, 7ámonths ago

As per the best practice, we should have separate projects for every environment for each application. We should not add new
applications to an existing project. So D is out. Option C is the answer.

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 2 times

? ?  Nik22 1áyear, 9ámonths ago

I see many of the questions have older case studies. Are we still getting those in the exam?

upvoted 4 times

? ?  megumin 7ámonths, 2áweeks ago

Dress4Win and JencoMart no more present in the exam

upvoted 3 times

? ?  PeppaPig 1áyear, 10ámonths ago

C for sure. Cross-Projects Resource sharing via SA

upvoted 2 times

? ?  RamanathanPV 1áyear, 10ámonths ago

How do we get two projects here? Can someone pls. explain?

upvoted 1 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

617/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Create a service account (SA) in the legacy game?ÇÖs Google Cloud project, add this SA in the new game?ÇÖs IAM page, and then give it
the Firebase Admin role in both projects.

upvoted 2 times

? ?  JeffClarke111 1áyear, 11ámonths ago

C is ok

upvoted 3 times

? ?  kopper2019 1áyear, 12ámonths ago

It is D

upvoted 3 times

? ?  kopper2019 1áyear, 11ámonths ago

Sorry C

upvoted 5 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

618/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company overview -

Topic 7

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms

after successfully migrating their on-premises environments to Google Cloud.

Their most recent endeavor is to create a retro-style  rst-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-

speci c digital arena from multiple platforms and locations. A real-time digital banner will display a global leaderboard of all the top players

across every active arena.

Solution concept -

Mountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game's backend on Google

Kubernetes Engine so they can scale rapidly and use Google's global load balancer to route players to the closest regional game arenas. In order

to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.

Existing technical environment -

The existing environment was recently migrated to Google Cloud, and  ve games came across using lift-and-shift virtual machine migrations, with

a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions

and network policies. Legacy games with low tra c have been consolidated into a single project. There are also separate environments for

development and testing.

Business requirements -

Support multiple gaming platforms.

Support multiple regions.

Support rapid iteration of game features.

Minimize latency.

Optimize for dynamic scaling.

Use managed services and pooled resources.

Minimize costs.

Technical requirements -

Dynamically scale based on game activity.

Publish scoring data on a near real-time global leaderboard.

Store game activity logs in structured  les for future analysis.

Use GPU processing to render graphics server-side for multi-platform support.

Support eventual migration of legacy games to this new platform.

Executive statement -

Our last game was the  rst time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game

telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games

using cloud-native design principles.

Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top

priority, although cost management is the next most important challenge. As with our  rst cloud-based game, we have grown to expect the cloud

to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug  xes and new functionality.

Question

Mountkirk Games wants to limit the physical location of resources to their operating Google Cloud regions. What should you do?

A. Con gure an organizational policy which constrains where resources can be deployed.

B. Con gure IAM conditions to limit what resources can be con gured.

C. Con gure the quotas for resources in the regions not being used to 0.

D. Con gure a custom alert in Cloud Monitoring so you can disable resources as they are created in other regions.

Correct Answer: C

Community vote distribution

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

619/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A (87%)

13%

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is A

upvoted 29 times

? ?  muhasinem  Highly Voted ?  1áyear, 12ámonths ago

A is correct .
You can limit the physical location of a new resource with the Organization Policy Service resource locations constraint. You can use the
location property of a resource to identify where it is deployed and maintained by the service. For data-containing resources of some
Google Cloud services, this property also reflects the location where data is stored. This constraint allows you to define the allowed Google
Cloud locations where the resources for supported services in your hierarchy can be created.

After you define resource locations, this limitation will apply only to newly-created resources. Resources you created before setting the
resource locations constraint will continue to exist and perform their function.
https://cloud.google.com/resource-manager/docs/organization-policy/defining-locations

upvoted 25 times

? ?  Deb2293  Most Recent ?  3ámonths, 2áweeks ago

Selected Answer: A

Should be A.
Quotas are used to protect Google Cloud users from unforeseen spikes in usage

upvoted 1 times

? ?  omermahgoub 6ámonths ago

The correct answer is: A. Configure an organizational policy which constrains where resources can be deployed.

Google Cloud offers the ability to use organizational policies to constrain the deployment of resources to specific regions or zones. This
allows you to control where resources can be deployed within your organization, and ensure that they are only deployed in the regions
that are appropriate for your business needs. To configure an organizational policy to constrain the location of resources, you can use the
Cloud Resource Manager to create and apply a policy that specifies the allowed regions or zones for resource deployment.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A Org Policy would do

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I am going with A

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: A

A is correct

upvoted 1 times

? ?  cloudmon 1áyear, 2ámonths ago

Selected Answer: D

The confusing thing here is that GCP has renamed the same solution multiple times. The concept is "Multi Cluster Ingress (MCI)", and
kubemci was the original solution for setting this up. Then GCP released "Ingress for Anthos", which replaced kubemci. Now, they have
again renamed "Ingress for Anthos" to "Multi Cluster Ingress". If you see this question in the exam, it should no longer provide "Ingress
for Anthos" as an option, but instead will say something like "Multi Cluster Ingress". The answers can be found at these links:
https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress
https://cloud.google.com/kubernetes-engine/docs/how-to/multi-cluster-ingress

upvoted 2 times

? ?  chickennuggets 10ámonths, 2áweeks ago

You failed to read the case study. Region not just limited to GKE... Org policy is the ONLY way

upvoted 1 times

? ?  jaxclain 6ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

620/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

He just got confused and wrotte the comment in the incorrect question/subject lol but I know what question is he talking about and
at least on that question he is right, the answer was D lol
he meant to say D for the question 6 for topic 7.. :)
But in this question for sure the answer is A

upvoted 1 times

? ?  muky31dec 1áyear, 4ámonths ago

My Ans was A
upvoted 1 times

? ?  cdcollector 1áyear, 6ámonths ago

Selected Answer: A

https://cloud.google.com/resource-manager/docs/organization-policy/defining-locations#setting_the_organization_policy

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 2 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: A

Select A

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  [Removed] 1áyear, 9ámonths ago

C is right. It telling set the resources where not needed to 0.

upvoted 1 times

? ?  [Removed] 1áyear, 8ámonths ago

A gives more flexibility, so I would go with A as well. C is blunt setting to 0 suggestion

upvoted 1 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

621/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company overview -

Topic 7

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms

after successfully migrating their on-premises environments to Google Cloud.

Their most recent endeavor is to create a retro-style  rst-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-

speci c digital arena from multiple platforms and locations. A real-time digital banner will display a global leaderboard of all the top players

across every active arena.

Solution concept -

Mountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game's backend on Google

Kubernetes Engine so they can scale rapidly and use Google's global load balancer to route players to the closest regional game arenas. In order

to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.

Existing technical environment -

The existing environment was recently migrated to Google Cloud, and  ve games came across using lift-and-shift virtual machine migrations, with

a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions

and network policies. Legacy games with low tra c have been consolidated into a single project. There are also separate environments for

development and testing.

Business requirements -

Support multiple gaming platforms.

Support multiple regions.

Support rapid iteration of game features.

Minimize latency.

Optimize for dynamic scaling.

Use managed services and pooled resources.

Minimize costs.

Technical requirements -

Dynamically scale based on game activity.

Publish scoring data on a near real-time global leaderboard.

Store game activity logs in structured  les for future analysis.

Use GPU processing to render graphics server-side for multi-platform support.

Support eventual migration of legacy games to this new platform.

Executive statement -

Our last game was the  rst time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game

telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games

using cloud-native design principles.

Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top

priority, although cost management is the next most important challenge. As with our  rst cloud-based game, we have grown to expect the cloud

to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug  xes and new functionality.

Question

You need to implement a network ingress for a new game that meets the de ned business and technical requirements. Mountkirk Games wants

each regional game instance to be located in multiple Google Cloud regions. What should you do?

A. Con gure a global load balancer connected to a managed instance group running Compute Engine instances.

B. Con gure kubemci with a global load balancer and Google Kubernetes Engine.

C. Con gure a global load balancer with Google Kubernetes Engine.

D. Con gure Ingress for Anthos with a global load balancer and Google Kubernetes Engine.

Correct Answer: A

Community vote distribution

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

622/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D (47%)

C (25%)

B (20%)

8%

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago
IMHO d) is the correct answer, not a)
The game fulfills the business requirements as well as the technical requirements - so it is build upon an architecture that is multi regional.
https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress

upvoted 27 times

? ?  Begum 8ámonths, 3áweeks ago

Anthos is overkill! - As per the case study, there is no mention of hybrid requirement.

upvoted 6 times

? ?  Ishu_awsguy 9ámonths, 2áweeks ago

Why is anthos needed.
In the link shared above , a multi cluster ingress ( HTTPS LB ) is sufficient.
We should go with C.

upvoted 7 times

? ?  MikeB19 1áyear, 10ámonths ago

I think anthos would work but i donÆt think it is needed. Deploying anthos means they will maintain an on prem environment along with
gcp. Anthos will give them the ability to manage both environments from a single pane of glass.
I think b is correc. Kubemci provides global lb for multi gke clusters
https://cloud.google.com/blog/products/gcp/how-to-deploy-geographically-distributed-services-on-kubernetes-engine-with-kubemci

upvoted 9 times

? ?  MikeB19 1áyear, 9ámonths ago

From what i understand kubemci is now deprecated (although i have not found an official doc stating this). If this is the case then D
is correct

upvoted 2 times

? ?  gingerbeer 1áyear, 9ámonths ago

Official doc saying kubemci deprecated in here:
https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress
ôThis has now been deprecated in favor ofáIngress for Anthos. Ingress for Anthos is the recommended way to deploy multi-
cluster ingress.ö
upvoted 10 times

? ?  cotam 1áyear, 8ámonths ago

Interesting how they already, within this short period of time renamed 'Ingress for Anthos' to 'Multi Cluster Ingress'..

upvoted 2 times

? ?  Narinder 1áyear, 5ámonths ago

https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress

upvoted 2 times

? ?  taoj  Highly Voted ?  1áyear, 11ámonths ago

D for me.
since it's a multiple regions game.Need multi-GKE or multi-MIG.
To configure the ingress between multi-GKE. kubemci or Ahthos
kubemci has now been deprecated in favor of Ingress for Anthos. Ingress for Anthos is the recommended way to deploy multi-cluster
ingress.
https://github.com/GoogleCloudPlatform/k8s-multicluster-ingress

So. D

upvoted 20 times

? ?  red_panda  Most Recent ?  3ádays, 3áhours ago

Selected Answer: C

For me is C.
There is no necessity of Anthos or hybrid connectivity cluster. GKE and Global LB is enought

upvoted 1 times

? ?  BiddlyBdoyng 1áweek, 3ádays ago

Selected Answer: D

Needs ingress over multiple clusters which requires Multi Cluster Ingress or Antos for Ingress as it was previously known.

upvoted 1 times

? ?  Atanu 2áweeks, 3ádays ago

Selected Answer: A

A is good to go. Anthos is an overkill here

upvoted 1 times

? ?  nescafe7 2áweeks, 4ádays ago

Selected Answer: C

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

623/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C is sufficient.
upvoted 1 times

? ?  salim_ 1ámonth, 3áweeks ago

Selected Answer: A

I believe it the correct answer as it fulfills the technical requirement : Dynamically scale based on game activity. GKE will be used in the
backend.

upvoted 1 times

? ?  JC0926 2ámonths, 2áweeks ago

Selected Answer: C

C
The reason for choosing C is that configuring a global load balancer with Google Kubernetes Engine (GKE) meets the business and
technical requirements of Mountkirk Games. A global load balancer automatically routes players to the nearest regional game instances,
reducing latency and providing high performance. Moreover, GKE allows dynamic scaling based on game activity and takes advantage of
Google Cloud's managed services and resource pooling to minimize costs.

upvoted 2 times

? ?  JC0926 2ámonths, 2áweeks ago

Other options do not meet the requirements for the following reasons:

A. Configuring a global load balancer connected to a managed instance group running Compute Engine instances does not fully utilize
the dynamic scaling and managed services features of GKE.
B. Configuring kubemci with a global load balancer and Google Kubernetes Engine is not recommended as kubemci is deprecated and
no longer considered a best practice by Google.
D. Configuring Ingress for Anthos with a global load balancer and Google Kubernetes Engine is more suitable for multi-cluster
environments. In this scenario, deploying game instances in multiple regions is required, but a multi-cluster setup is not necessary.
Therefore, option C is the best solution.

upvoted 1 times

? ?  ralf_cc 2ámonths, 4áweeks ago

These games run on VMs folks... so it is A

upvoted 3 times

? ?  stfnz 1ámonth, 1áweek ago

Nope. Read the case study.
They plan to deploy the game's backend on Google Kubernetes Engine so they can scale rapidly and use Google's global load balancer

upvoted 1 times

? ?  CGS22 3ámonths ago

Selected Answer: C

C is the most appropriate solution for Mountkirk Games as it involves configuring a global load balancer with GKE, which allows for
dynamic scaling, minimizes latency, and optimizes costs. The use of a global load balancer ensures that players are routed to the closest
regional game arena, which helps to minimize latency.

In addition, using GKE allows for rapid iteration of game features, as well as the eventual migration of legacy games to the new platform.
The use of managed services and pooled resources also helps to minimize costs.

Option D is also not the best choice, as Anthos is not required for this scenario, as the game backend can be deployed directly on GKE.

upvoted 1 times

? ?  rr4444 3ámonths, 2áweeks ago

Selected Answer: B

B is kind of the right answer, only because it has been deprecated in favour of Multi Cluster Ingress (MCI), which is not Anthos specifically.

Anthos may be overkill. Not wrong. But much extra.....

upvoted 1 times

? ?  hoai_nam_1512 3ámonths, 2áweeks ago

Selected Answer: C

C for me, Anthos not meet solution

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: C

The answer is pretty explicitly stated in the Solution Concept para of the problem statement.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

Answer is D:
To implement a network ingress for a new game that meets the defined business and technical requirements, you should consider using
Ingress for Anthos. Ingress for Anthos is a solution that allows you to easily configure network ingress for applications running on Google
Kubernetes Engine (GKE) clusters, regardless of whether they are running on-premises or in Google Cloud.

To set up Ingress for Anthos, you can use a global load balancer to route traffic to your game instances, which can be located in multiple
Google Cloud regions. The global load balancer will distribute traffic across your game instances to ensure that they are able to handle

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

624/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

incoming requests efficiently. Additionally, you can use GKE to manage and deploy your game instances, which will allow you to easily
scale your game dynamically based on game activity, as required by the technical requirements.

upvoted 5 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: D

D is ok -> Anthos is necessary cause of Technical requirements: Support eventual migration of legacy games to this new platform.

upvoted 2 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Thinking that the ingress for the game front end which might be web servers deployed on MIGs in that vase answer might be A, just
thinking

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

625/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company overview -

Topic 7

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms

after successfully migrating their on-premises environments to Google Cloud.

Their most recent endeavor is to create a retro-style  rst-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-

speci c digital arena from multiple platforms and locations. A real-time digital banner will display a global leaderboard of all the top players

across every active arena.

Solution concept -

Mountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game's backend on Google

Kubernetes Engine so they can scale rapidly and use Google's global load balancer to route players to the closest regional game arenas. In order

to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.

Existing technical environment -

The existing environment was recently migrated to Google Cloud, and  ve games came across using lift-and-shift virtual machine migrations, with

a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions

and network policies. Legacy games with low tra c have been consolidated into a single project. There are also separate environments for

development and testing.

Business requirements -

Support multiple gaming platforms.

Support multiple regions.

Support rapid iteration of game features.

Minimize latency.

Optimize for dynamic scaling.

Use managed services and pooled resources.

Minimize costs.

Technical requirements -

Dynamically scale based on game activity.

Publish scoring data on a near real-time global leaderboard.

Store game activity logs in structured  les for future analysis.

Use GPU processing to render graphics server-side for multi-platform support.

Support eventual migration of legacy games to this new platform.

Executive statement -

Our last game was the  rst time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game

telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games

using cloud-native design principles.

Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top

priority, although cost management is the next most important challenge. As with our  rst cloud-based game, we have grown to expect the cloud

to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug  xes and new functionality.

Question

Your development teams release new versions of games running on Google Kubernetes Engine (GKE) daily. You want to create service level

indicators (SLIs) to evaluate the quality of the new versions from the user's perspective. What should you do?

A. Create CPU Utilization and Request Latency as service level indicators.

B. Create GKE CPU Utilization and Memory Utilization as service level indicators.

C. Create Request Latency and Error Rate as service level indicators.

D. Create Server Uptime and Error Rate as service level indicators.

Correct Answer: A

Community vote distribution

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

626/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C (100%)

? ?  XDevX  Highly Voted ?  1áyear, 12ámonths ago
IMHO c) is the correct answer, not a).
Reason is, that we have to take the users perspective (according to the given question and also to understand what the user expects from
us).
The question might be whether to choose c) or d). Considering that our requirement is to minimize the latency, we have to choose c) - that
means we are striving for no downtime of the service.

upvoted 32 times

? ?  medeis_jar 1áyear, 8ámonths ago

yeap, definition of SLI "While many numbers can function as an SLI, we generally recommend treating the SLI as the ratio of two
numbers: the number of good events divided by the total number of events" -> https://sre.google/workbook/implementing-
slos/#:~:text=or%20product%20manager).-,What%20to%20Measure%3A%20Using%20SLIs,-Once%20you%20agree

upvoted 3 times

? ?  MamthaSJ  Highly Voted ?  1áyear, 11ámonths ago

Answer is C

upvoted 8 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 3ádays ago

Selected Answer: C

CPU utilization doesn't tell us about the user experience except perhaps if it hits 100%. The errors & latency (driven party by CPU ) are
much better indicators

upvoted 1 times

? ?  alekonko 2ámonths, 1áweek ago

Selected Answer: C

Answer is C

upvoted 1 times

? ?  Deb2293 3ámonths, 1áweek ago

Selected Answer: C

Definitely C

upvoted 1 times

? ?  somchaikin 3ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  omermahgoub 6ámonths ago

The correct answer is: C. Create Request Latency and Error Rate as service level indicators.
Request Latency measures the time it takes for a request to be processed by the game, and is an important indicator of the
responsiveness of the game from the user's perspective. A high request latency can indicate that the game is experiencing performance
issues or is under heavy load, which may negatively impact the user experience.

Error Rate measures the percentage of requests that result in errors, such as HTTP errors or timeouts. A high error rate can indicate that
the game is experiencing technical issues or is not able to handle incoming requests effectively, which can also negatively impact the user
experience. By monitoring these SLIs, you can identify issues with the game's performance and take appropriate action to improve the
user experience.
upvoted 2 times

? ?  omermahgoub 6ámonths ago

You can use tools such as Stackdriver Monitoring or Cloud Monitoring to create custom metrics and alerts based on these SLIs, and
track the performance of your games over time. You can also use these tools to set up automated responses to alerts, such as
automatically scaling the number of game instances up or down based on the current load.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

C is good choice
upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

627/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  chickennuggets 10ámonths, 2áweeks ago

users perspective only way thats addressed is request latency and error rates..

upvoted 1 times

? ?  Nirca 11ámonths ago

Selected Answer: C

C it is a flute

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

C is better than A
upvoted 2 times

? ?  muky31dec 1áyear, 4ámonths ago

My ans in real teat was C

upvoted 1 times

? ?  Wonka 1áyear, 5ámonths ago

BTW are these questions coming in real exam and from what numbers really are still valid?

upvoted 1 times

? ?  brushek 1áyear, 6ámonths ago

Selected Answer: C

Answer is c

upvoted 3 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: C

Vote C

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

628/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company overview -

Topic 7

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms

after successfully migrating their on-premises environments to Google Cloud.

Their most recent endeavor is to create a retro-style  rst-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-

speci c digital arena from multiple platforms and locations. A real-time digital banner will display a global leaderboard of all the top players

across every active arena.

Solution concept -

Mountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game's backend on Google

Kubernetes Engine so they can scale rapidly and use Google's global load balancer to route players to the closest regional game arenas. In order

to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.

Existing technical environment -

The existing environment was recently migrated to Google Cloud, and  ve games came across using lift-and-shift virtual machine migrations, with

a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions

and network policies. Legacy games with low tra c have been consolidated into a single project. There are also separate environments for

development and testing.

Business requirements -

Support multiple gaming platforms.

Support multiple regions.

Support rapid iteration of game features.

Minimize latency.

Optimize for dynamic scaling.

Use managed services and pooled resources.

Minimize costs.

Technical requirements -

Dynamically scale based on game activity.

Publish scoring data on a near real-time global leaderboard.

Store game activity logs in structured  les for future analysis.

Use GPU processing to render graphics server-side for multi-platform support.

Support eventual migration of legacy games to this new platform.

Executive statement -

Our last game was the  rst time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game

telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games

using cloud-native design principles.

Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top

priority, although cost management is the next most important challenge. As with our  rst cloud-based game, we have grown to expect the cloud

to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug  xes and new functionality.

Question

Mountkirk Games wants you to secure the connectivity from the new gaming application platform to Google Cloud. You want to streamline the

process and follow

Google-recommended practices. What should you do?

A. Con gure Workload Identity and service accounts to be used by the application platform.

B. Use Kubernetes Secrets, which are obfuscated by default. Con gure these Secrets to be used by the application platform.

C. Con gure Kubernetes Secrets to store the secret, enable Application-Layer Secrets Encryption, and use Cloud Key Management Service

(Cloud KMS) to manage the encryption keys. Con gure these Secrets to be used by the application platform.

D. Con gure HashiCorp Vault on Compute Engine, and use customer managed encryption keys and Cloud Key Management Service (Cloud

KMS) to manage the encryption keys. Con gure these Secrets to be used by the application platform.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

629/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Correct Answer: A

Community vote distribution

A (100%)

? ?  muhasinem  Highly Voted ?  1áyear, 12ámonths ago

A is correct .
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
Workload Identity is the recommended way to access Google Cloud services from applications running within GKE due to its improved
security properties and manageability. For information about alternative ways to access Google Cloud APIs from GKE, refer to the
alternatives section below.

upvoted 30 times

? ?  dhamo_555  Highly Voted ?  1áyear, 11ámonths ago

A) - Because Mountkrik Game is going to use GKE clusters for its new deployment and so work load identity is the preferred way to
connect the apps running on GKE

upvoted 11 times

? ?  omermahgoub  Most Recent ?  6ámonths ago

The correct answer is: A. Configure Workload Identity and service accounts to be used by the application platform.

Workload Identity is a feature of Google Cloud that allows you to map identities from your on-premises or Google Cloud identity provider
to Google Cloud service accounts. By using Workload Identity, you can secure the connectivity of your application platform to Google
Cloud by using the service accounts to authenticate and authorize access to Google Cloud resources.

Service accounts are Google Cloud resources that represent non-human users that your application platform can use to authenticate and
authorize access to Google Cloud resources. By using service accounts, you can secure the connectivity of your application platform to
Google Cloud by controlling which resources the service accounts can access and what actions they can perform.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

To configure Workload Identity and service accounts, you will need to create a service account and bind it to the identity of your
workload. You can then use the service account to authenticate to Google Cloud APIs and access the resources needed by your
application platform. This will help to secure the connectivity from the platform to Google Cloud and streamline the process of
managing access and permissions.

upvoted 2 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is right

upvoted 1 times

? ?  muky31dec 1áyear, 4ámonths ago
I answered A in real exam

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity

upvoted 3 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

A. Workload Identity is the recommended way to access Google Cloud services from applications running within GKE due to its improved
security properties and manageability
https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity

upvoted 1 times

? ?  Ari_GCP 1áyear, 9ámonths ago

"Secure the connectivity" - gaming platform runs on GKE, and Workload Identity is the recommended way to connect to Google Cloud
services from GKE. Hence A.

upvoted 2 times

? ?  PeppaPig 1áyear, 10ámonths ago

A for sure if you are using GKE :)

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

630/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 3 times

? ?  victory108 1áyear, 11ámonths ago

A. Configure Workload Identity and service accounts to be used by the application platform.

upvoted 2 times

? ?  kopper2019 1áyear, 11ámonths ago

from my view looks like C

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 4 times

? ?  manmohan15 1áyear, 11ámonths ago

Encryption key stored in Secret Manager is for encrypting the data... whereas question is for connectivity/authentication for which
workload identity suites more. my vote is for workload identity A.

upvoted 3 times

? ?  kopper2019 1áyear, 12ámonths ago
cloudstd 1 day, 8 hours ago
answer: A

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

631/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #7

Introductory Info

Company overview -

Topic 7

Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms

after successfully migrating their on-premises environments to Google Cloud.

Their most recent endeavor is to create a retro-style  rst-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-

speci c digital arena from multiple platforms and locations. A real-time digital banner will display a global leaderboard of all the top players

across every active arena.

Solution concept -

Mountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game's backend on Google

Kubernetes Engine so they can scale rapidly and use Google's global load balancer to route players to the closest regional game arenas. In order

to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.

Existing technical environment -

The existing environment was recently migrated to Google Cloud, and  ve games came across using lift-and-shift virtual machine migrations, with

a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions

and network policies. Legacy games with low tra c have been consolidated into a single project. There are also separate environments for

development and testing.

Business requirements -

Support multiple gaming platforms.

Support multiple regions.

Support rapid iteration of game features.

Minimize latency.

Optimize for dynamic scaling.

Use managed services and pooled resources.

Minimize costs.

Technical requirements -

Dynamically scale based on game activity.

Publish scoring data on a near real-time global leaderboard.

Store game activity logs in structured  les for future analysis.

Use GPU processing to render graphics server-side for multi-platform support.

Support eventual migration of legacy games to this new platform.

Executive statement -

Our last game was the  rst time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game

telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games

using cloud-native design principles.

Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top

priority, although cost management is the next most important challenge. As with our  rst cloud-based game, we have grown to expect the cloud

to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug  xes and new functionality.

Question

Your development team has created a mobile game app. You want to test the new mobile app on Android and iOS devices with a variety of

con gurations. You need to ensure that testing is e cient and cost-effective. What should you do?

A. Upload your mobile app to the Firebase Test Lab, and test the mobile app on Android and iOS devices.

B. Create Android and iOS VMs on Google Cloud, install the mobile app on the VMs, and test the mobile app.

C. Create Android and iOS containers on Google Kubernetes Engine (GKE), install the mobile app on the containers, and test the mobile app.

D. Upload your mobile app with different con gurations to Firebase Hosting and test each con guration.

Correct Answer: C

Community vote distribution

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

632/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A (100%)

? ?  VishalB  Highly Voted ?  1áyear, 11ámonths ago

Correct Answer: A
- Firebase Test Lab is a cloud-based app testing infrastructure that lets you test your app on a range of devices and configurations, so you
can get a better idea of how it'll perform in the hands of live users.
- Firebase Test Lab Run tests on a wide range of Android and iOS devices hosted by Test Lab.

upvoted 33 times

? ?  Enzian  Highly Voted ?  1áyear, 12ámonths ago

A should be correct
B false - not really feasable
C false - cannot run Android or IOS on GKE
D false since that is what A is built to do

upvoted 11 times

? ?  omermahgoub  Most Recent ?  6ámonths ago

The correct answer is: A. Upload your mobile app to the Firebase Test Lab, and test the mobile app on Android and iOS devices.

The Firebase Test Lab is a cloud-based testing service that allows you to test your mobile app on a variety of physical devices running
Android and iOS. It provides a range of options for testing your app, including testing on different device models, screen sizes, and
operating system versions.

To use the Firebase Test Lab, you will need to upload your mobile app to the service and specify the devices and configurations you want
to test. The Test Lab will then run your tests on the specified devices and provide results, including performance metrics and crash
reports. This will allow you to test your mobile app on a variety of configurations efficiently and cost-effectively.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

In addition to providing performance metrics and crash reports, the Test Lab also allows you to record video of the tests being run, so
you can see how your app is behaving on each device. This can be particularly useful for identifying and debugging issues that may not
be immediately apparent from the test results.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 2áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is correct. Firebase is mobile testing

upvoted 1 times

? ?  muky31dec 1áyear, 4ámonths ago

Ans is A. Upload your mobile app to the Firebase Test Lab, and test the mobile app on Android and iOS devices ( got question in real exam)

upvoted 2 times

? ?  SamGCP 1áyear, 6ámonths ago

Selected Answer: A

https://firebase.google.com/docs/test-lab

upvoted 3 times

? ?  andeu 1áyear, 6ámonths ago

Selected Answer: A

Correct Answer: A

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: A

Vote A

upvoted 2 times

? ?  mudot 1áyear, 7ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

633/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

seems the given answer was chosen at random :-D

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  Gk21 1áyear, 7ámonths ago

I think Admin is forcing us to see the comment to find the correct answer.

upvoted 4 times

? ?  [Removed] 1áyear, 7ámonths ago

A is right. Firebase Lab is platform for mobile app testing.

upvoted 2 times

? ?  SuperNest 1áyear, 9ámonths ago

The given answer is ridiculous !!

upvoted 3 times

? ?  [Removed] 1áyear, 7ámonths ago

Not only this Q. Many others are marked wrong answers. Admin need to take an action and consolidate the right answers based on the
experts comments and explanations.

upvoted 4 times

? ?  pakilodi 1áyear, 6ámonths ago

iOS containers....a new way to debug iOS apps without a Mac :)

upvoted 1 times

? ?  dhamo_555 1áyear, 11ámonths ago

A) - https://firebase.google.com/docs/test-lab

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

634/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 8 - Testlet 5

Question #1

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

635/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

TerramEarth's CTO wants to use the raw data from connected vehicles to help identify approximately when a vehicle in the  eld will have a

catastrophic failure.

You want to allow analysts to centrally query the vehicle data.

Which architecture should you recommend?

A.

B.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

636/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

637/776

6/29/23, 1:52 PM

D.

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Correct Answer: A

The push endpoint can be a load balancer.

A container cluster can be used.

Cloud Pub/Sub for Stream Analytics

Reference:

https://cloud.google.com/pubsub/

https://cloud.google.com/solutions/iot/

https://cloud.google.com/solutions/designing-connected-vehicle-platform https://cloud.google.com/solutions/designing-connected-vehicle-

platform#data_ingestion http://www.eweek.com/big-data-and-analytics/google-touts-value-of-cloud-iot-core-for-analyzing-connected-car-data

https://cloud.google.com/solutions/iot/

? ?  hems4all  Highly Voted ?  2áyears, 7ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

638/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is correct

As described in the Designing a Connected Vehicle Platform on Cloud IoT Core case study,

1. Google Cloud Dataflow is essential to transform, enrich and then store telemetry data by using distributed data pipelines

2. Cloud Pub/Sub is essential to handle the streams of vehicle data while at the same time decoupling the specifics of the backend
processing implementation

It now comes down to a choice between

1. Cloud SQL vs BigQuery for analytics.

2. GKE (with or without Anthos) + Cloud Load balancing vs App Engine.

For the first point, there is no doubt that BigQuery is the preferred choice for analytics. Cloud SQL does not scale to this sort of data
volume (9TB/day + data coming through when vehicles are serviced).

For the second point, GKE with Cloud Load Balancing is a better fit than App Engine. App Engine is a regional service whereas, with the
other option, you can have multiple GKE clusters in different regions. And Cloud Load Balancing can send requests to the cluster in the
region that is closest to the vehicle. This option minimizes the latency and makes the feedback loop more real-time.

upvoted 44 times

? ?  MJK  Highly Voted ?  3áyears, 6ámonths ago

Ans should be A
upvoted 12 times

? ?  GCP_Azure 3áyears, 1ámonth ago

You will need to use (install) an FTP server that supports load balancing. Google Cloud does not offer an FTP server service or software
product

upvoted 3 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 9 times

? ?  bjuneja 2áyears, 6ámonths ago

How can you do analysis on SQL? A is ok

upvoted 5 times

? ?  bnlcnd 2áyears, 4ámonths ago

B missed the global LB. App Engine is regional only. You have to create App Engine in each region and use LB as the front door.

upvoted 1 times

? ?  alii 2áyears, 5ámonths ago

App engine is regional https://cloud.google.com/appengine/docs/locations. Case study says it's a global business. which rules out
app engine. so we are left with A.

"App Engine is regional, which means the infrastructure that runs your apps is located in a specific region, and Google manages it so
that it is available redundantly across all of the zones within that region."

upvoted 4 times

? ?  alii 2áyears, 5ámonths ago

As per case study: "They currently have over 500 dealers and service centers in 100 countries."

upvoted 1 times

? ?  Jphix 2áyears, 5ámonths ago

Agreed - A. For those with load balancing concerns re: FTP; transport protocol for ftp is TCP, albeit over two TCP ports. GCP Load
Balancing supports TCP.

upvoted 3 times

? ?  nitinz 2áyears, 3ámonths ago

It is A

upvoted 1 times

? ?  rr4444  Most Recent ?  3ámonths, 2áweeks ago

This question is a mess

FTP all over the place

And even into things that can't talk FTP in that way

Needs fixing

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

639/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  BeCalm 3ámonths, 3áweeks ago

What is a Google Container Engine?

upvoted 1 times

? ?  n_nana 3ámonths, 2áweeks ago
It is GKE in old google naming

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

A is correct

upvoted 1 times

? ?  JoeThach 1áyear ago

I vote for A - The company has customers in 100+ countries, while App Engine is regional (rule out B).

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

B is correct.
We dont have LB that support SFTP traffic.
Hence we eleminate A & C
We are talking arbore analyzing IoT data = Big Quavery = B.

upvoted 2 times

? ?  moota 4ámonths, 3áweeks ago

Google Cloud Load Balancing can also do TCP/SSL load balancing. Check out key features in https://cloud.google.com/load-balancing

upvoted 1 times

? ?  HD2023 2ámonths, 4áweeks ago

"FTP" doesnÆt appear once on that entire page. Answer: B

upvoted 1 times

? ?  jpco 1áyear, 4ámonths ago

Google Load Balancer doesn't support FTP protocol

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

vote A

upvoted 2 times

? ?  medeis_jar 1áyear, 8ámonths ago

Answer B, because of Bigquery (Analytics) and AppEngine Flex (custom runtime environments) and the fact that there is no such a thing as
Google Container Engine, there is Google Kubernetes Engine.

upvoted 3 times

? ?  Nik22 1áyear, 9ámonths ago

These are from old use case. Do we still need to do these?

upvoted 5 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 2 times

? ?  XDevX 1áyear, 12ámonths ago

IMHO it is b).
Background: We have in 22 hours 9 TB of ingest data. That makes 113 MB per second.
The question is: Why do we have to use for a "simple" transformation of FTP files into Cloud Pub/Sub GKE with multiple clusters around
the world? For me it seems to be sufficient to have App Engine Flexible in one region to handle that.
We have no highly interactive game where every ms counts concerning the customer experience - we have "only" some vehicles sending
data that has to be tranformed and then analysed.

upvoted 4 times

? ?  gatul28 2áyears, 1ámonth ago

how B? I do not see any LB there and App Engine doesn't support LBs beyond HTTP(s). Answer Is A but B's existence is confusing me much

upvoted 1 times

? ?  HD2023 2ámonths, 4áweeks ago

ultra low latency isnÆt required. Whereas A isnÆt possible because you canÆt use FTP with LB.

upvoted 1 times

? ?  victory108 2áyears, 1ámonth ago

A is correct

upvoted 2 times

? ?  ahmedemad3 2áyears, 2ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

640/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Ans: A
App Engine doesn't support anything beyond HTTP/HTTPS.

upvoted 4 times

? ?  HD2023 2ámonths, 4áweeks ago

Neither does LB. lol

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

641/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

642/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

The TerramEarth development team wants to create an API to meet the company's business requirements. You want the development team to

focus their development effort on business value versus creating a custom framework.

Which method should they use?

A. Use Google App Engine with Google Cloud Endpoints. Focus on an API for dealers and partners

B. Use Google App Engine with a JAX-RS Jersey Java-based framework. Focus on an API for the public

C. Use Google App Engine with the Swagger (Open API Speci cation) framework. Focus on an API for the public

D. Use Google Container Engine with a Django Python container. Focus on an API for the public

E. Use Google Container Engine with a Tomcat container with the Swagger (Open API Speci cation) framework. Focus on an API for dealers

and partners

Correct Answer: A

Develop, deploy, protect and monitor your APIs with Google Cloud Endpoints. Using an Open API Speci cation or one of our API frameworks,

Cloud Endpoints gives you the tools you need for every phase of API development.

From scenario:

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies ?Ç" especially with seed and fertilizer suppliers in the fast-growing agricultural business ?Ç"

to create compelling joint offerings for their customers.

Reference:

https://cloud.google.com/certi cation/guides/cloud-architect/casestudy-terramearth

Community vote distribution

A (90%)

10%

? ?  kvokka  Highly Voted ?  3áyears, 5ámonths ago

agree with A

upvoted 32 times

? ?  Vika  Highly Voted ?  2áyears, 3ámonths ago

Google offers Cloud Endpoint to develop, deploy and manage APIs on any google cloud backend.
https://cloud.google.com/endpoints

With Endpoints Frameworks, you don't have to deploy a third-party web server (such as Apache Tomcat or Gunicorn) with your application.
You annotate or decorate the code and deploy your application as you normally would to the App Engine standard environment.

Cloud Endpoints Frameworks for the App Engine standard environment : https://cloud.google.com/endpoints/docs/frameworks/about-
cloud-endpoints-frameworks

upvoted 10 times

? ?  VSMu  Most Recent ?  4ámonths, 3áweeks ago

Not sure why these questions are using the term Google Container Engine instead of Google Kubernetes Engine. That is so confusing

upvoted 1 times

? ?  n_nana 3ámonths, 2áweeks ago

These are old questions where GKE was named Google Container Engine

upvoted 1 times

? ?  KyubiBlaze 6ámonths, 4áweeks ago

Why you don't go for C, is cuz for partners, global speed will not be relevant. You can still serve globally via app engine

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

643/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A correct answer
upvoted 1 times

? ?  AzureDP900 12ámonths ago

Before even reading discussions I am fixed with my answer as A and everyone said same.

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: A

We should choose google components only. Here GCP Endpoint does the job so A.

upvoted 3 times

? ?  Nick89GR 1áyear, 2ámonths ago

Selected Answer: A

Definetely A

upvoted 1 times

? ?  cdcollector 1áyear, 6ámonths ago

Selected Answer: C

OAS adoption rules out framework development. Endpoints is only for indirection of the API and configuring other services which was not
part of the question

upvoted 1 times

? ?  cyqgz_36 1áyear, 5ámonths ago

API should be partner and dealer facing, not public as per BR

upvoted 1 times

? ?  Knerd 1áyear, 6ámonths ago

If this is A then how come the previous question answer is B (Container Engine) ?

upvoted 1 times

? ?  Knerd 1áyear, 6ámonths ago

Sorry typo there.. previous question answer is Container Engine while here we talk about App Engine. how come ?

upvoted 1 times

? ?  vartiklis 1áyear, 6ámonths ago

The previous question deals with analysis. This question focuses on creating an API for dealers and partners

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

A. Use Google App Engine with Google Cloud Endpoints. Focus on an API for dealers and partners

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 4 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

644/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

645/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

Your development team has created a structured API to retrieve vehicle data. They want to allow third parties to develop tools for dealerships that

use this vehicle event data. You want to support delegated authorization against this data.

What should you do?

A. Build or leverage an OAuth-compatible access control system

B. Build SAML 2.0 SSO compatibility into your authentication system

C. Restrict data access based on the source IP address of the partner systems

D. Create secondary credentials for each dealer that can be given to the trusted third party

Correct Answer: A

Delegate application authorization with OAuth2

Cloud Platform APIs support OAuth 2.0, and scopes provide granular authorization over the methods that are supported. Cloud Platform

supports both service- account and user-account OAuth, also called three-legged OAuth.

Reference:

https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#delegate_application_authorization_with_oauth2

https://cloud.google.com/appengine/docs/ exible/go/authorizing-apps

Community vote distribution

A (100%)

? ?  ravisar  Highly Voted ?  1áyear, 7ámonths ago

SAML is an authentication system.
OAuth is an authorization system.

Both can be used with SSO (Single sign on). SAML is for users and OAuth is more for applications.
Answer A

upvoted 33 times

? ?  huyhoang8344 10ámonths ago

SAML can do both authentication and authorization If I am not mistaken
But agree A should be the answer

upvoted 1 times

? ?  AD2AD4  Highly Voted ?  3áyears, 1ámonth ago

Final Decision to go with Option A.
Refer - https://cloud.google.com/docs/authentication
Good Read - https://cloud.google.com/blog/products/identity-security/identity-and-authentication-the-google-cloud-way

upvoted 25 times

? ?  megumin  Most Recent ?  7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: A

Delegate application authorization with OAuth2

upvoted 1 times

? ?  AzureDP900 12ámonths ago

OAuth Authorization is right. A is right!

upvoted 1 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: A

A is correct

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

646/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

A û O-Auth 2 access to system (clients would use APIs) https://cloud.google.com/docs/authentication/end-user
B û SAML 2.0 is redundant, not in requirements.

upvoted 1 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 3 times

? ?  victory108 1áyear, 11ámonths ago

A. Build or leverage an OAuth-compatible access control system

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 2 times

? ?  wzh5831 2áyears, 2ámonths ago

just query why there is not option for service account...

upvoted 1 times

? ?  poseidon24 1áyear, 11ámonths ago

Because OAuth 2.0 already take in count such flows (client credentials, that is service-to-service communication, meaning service
accounts).

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

A is good, they need auth not aunthentication.

upvoted 2 times

? ?  ahmedemad3 2áyears, 4ámonths ago

ANS: A
CHECK THIS LINK : https://developers.google.com/identity/protocols/oauth2/service-account

upvoted 3 times

? ?  bnlcnd 2áyears, 4ámonths ago

SAML is mostly for Single Sign On. O-Auth is better for delegation.

upvoted 4 times

? ?  AshokC 2áyears, 9ámonths ago

A is correct

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

647/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

648/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

TerramEarth plans to connect all 20 million vehicles in the  eld to the cloud. This increases the volume to 20 million 600 byte records a second for

40 TB an hour.

How should you design the data ingestion?

A. Vehicles write data directly to GCS

B. Vehicles write data directly to Google Cloud Pub/Sub

C. Vehicles stream data directly to Google BigQuery

D. Vehicles continue to write data using the existing system (FTP)

Correct Answer: C

Community vote distribution

B (100%)

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

It's Pub/Sub, too much data streaming for Bigquery...

upvoted 38 times

? ?  alexspam88 2áyears ago

Too much for pubsub either https://cloud.google.com/pubsub/quotas

upvoted 4 times

? ?  Bill831231 1áyear, 8ámonths ago

thanks for sharing the link, but seems pub/sub can handle more streaming data than bigquery. pub/sub 120,000,000 kB per minute
(2 GB/s) in large regions, bigquery is 1GB/s

upvoted 6 times

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago
Its B, it exceeds the streaming limit for BQ

upvoted 20 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 3ádays ago

So many people pointing out this breaks the BigQuery quota limit but very few pointing out it also breaks the Pub/Sub quote limit.......... So
the answer is either not bound by the quota limit (in which case why not BigQuery) both are wrong and we stick with FTP

upvoted 1 times

? ?  kapara 1ámonth ago

Selected Answer: B

it's B

upvoted 1 times

? ?  nunopires2001 5ámonths ago

I know it's B, however the sensors are probably legacy systems, that can not communicate to a pub/sub queue.
Ignoring how huge is to change or adapta 20 million devices is a mistake.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

To handle the volume of data that TerramEarth plans to ingest, it is recommended to use a scalable and reliable data ingestion solution
such as Google Cloud Pub/Sub. With Cloud Pub/Sub, the vehicles can stream data directly to the service, which can handle the high
volume of data and provide a buffer to absorb sudden spikes in traffic. The data can then be processed and stored in a data warehouse
such as BigQuery for analysis.

Option A (writing data directly to GCS) may not be suitable for handling high volumes of data in real-time and may result in data loss if the
volume exceeds the capacity of GCS.
Option C (streaming data directly to BigQuery) may not be suitable for handling high volumes of data in real-time as it may result in data
loss or ingestion delays.
Option D (continuing to write data using the existing system) may not be suitable as the current system may not be able to handle the
increased volume of data and may result in data loss or ingestion delays.

upvoted 7 times

? ?  sank8 6ámonths ago

correct. thanks for the explanation

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

649/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: B

B is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

ok for B

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

B is the correct answer, this similar question was in google simple questions

upvoted 1 times

? ?  AzureDP900 12ámonths ago

B is right!

upvoted 2 times

? ?  cdcollector 1áyear ago

Should be A - see next question on 80% cellular connectivity and Avro format files streamed directly to GCS

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: B

We need to buffer, the default limit of BigQuery is 100 API calls per second, till now this cannot be changed. Hence we should ease using
Pub/Sub so B.
upvoted 2 times

? ?  aronahl 1áyear, 2ámonths ago

Selected Answer: B

You can request limit increases to use BQ streaming for this load, but why pay to store data before ETL?

upvoted 1 times

? ?  AWS56 1áyear, 4ámonths ago

Selected Answer: B

B is right

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: B

B is the correct answer
https://cloud.google.com/bigquery/quotas#streaming_inserts

upvoted 1 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: B

Vote B

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

650/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

651/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

You analyzed TerramEarth's business requirement to reduce downtime, and found that they can achieve a majority of time saving by reducing

customer's wait time for parts. You decided to focus on reduction of the 3 weeks aggregate reporting time.

Which modi cations to the company's processes should you recommend?

A. Migrate from CSV to binary format, migrate from FTP to SFTP transport, and develop machine learning analysis of metrics

B. Migrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine learning analysis of metrics

C. Increase  eet cellular connectivity to 80%, migrate from FTP to streaming transport, and develop machine learning analysis of metrics

D. Migrate from FTP to SFTP transport, develop machine learning analysis of metrics, and increase dealer local inventory by a  xed factor

Correct Answer: C

The Avro binary format is the preferred format for loading compressed data. Avro data is faster to load because the data can be read in parallel,

even when the data blocks are compressed.

Cloud Storage supports streaming transfers with the gsutil tool or boto library, based on HTTP chunked transfer encoding. Streaming data lets

you stream data to and from your Cloud Storage account as soon as it becomes available without requiring that the data be  rst saved to a

separate  le. Streaming transfers are useful if you have a process that generates data and you do not want to buffer it locally before uploading

it, or if you want to send the result from a computational pipeline directly into Cloud Storage.

Reference:

https://cloud.google.com/storage/docs/streaming

https://cloud.google.com/bigquery/docs/loading-data

Community vote distribution

C (60%)

B (40%)

? ?  shandy  Highly Voted ?  3áyears, 7ámonths ago

C is right choice because using cellular connectivity will greatly improve the freshness of data used for analysis from where it is now,
collected when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop even
more. Machine learning is ideal for predictive maintenance workloads.

A is not correct because machine learning analysis is a good means toward the end of reducing downtime, but shuffling formats and
transport doesn't directly help at all. B is not correct because machine learning analysis is a good means toward the end of reducing
downtime, and moving to streaming can improve the freshness of the information in that analysis, but changing the format doesn't
directly help at all. D is not correct because machine learning analysis is a good means toward the end of reducing downtime, but the rest
of these changes don't directly help at all.

upvoted 30 times

? ?  nick_name_1 4ámonths ago

There are 20 million TerramEarth vehicles in operation ... Approximately 200,000 have cellular connectivity. So, you're saying for them
to keep cost low, increase cell phone bill from 0.01% connected to 80% connected? Statistical Analysis does not require such a large
sample size. C CANNOT BE RIGHT.

upvoted 2 times

? ?  nick_name_1 4ámonths ago

It's B.

upvoted 2 times

? ?  MrBog1  Highly Voted ?  3áyears, 6ámonths ago

A is not correct because machine learning analysis is a good means toward the end of reducing downtime, but shuffling formats and
transport doesn't directly help at all.

B is not correct because machine learning analysis is a good means toward the end of reducing downtime, and moving to streaming can
improve the freshness of the information in that analysis, but changing the format doesn't directly help at all.

C is correct because using cellular connectivity will greatly improve the freshness of data used for analysis from where it is now, collected
when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop even more.
Machine learning is ideal for predictive maintenance workloads.

D is not correct because machine learning analysis is a good means toward the end of reducing downtime, but the rest of these changes
don't directly help at all.

upvoted 19 times

? ?  WinSxS  Most Recent ?  3ámonths, 2áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

652/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: B

The most effective way to reduce the 3 weeks aggregate reporting time and achieve the business requirement of reducing downtime
would be to migrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine learning analysis of
metrics. This would significantly reduce the time it takes to collect and analyze data

upvoted 2 times

? ?  tdotcat 5ámonths, 1áweek ago

Selected Answer: B

binary format makes faster bigquery write
https://cloud.google.com/bigquery/docs/write-api#advantages

upvoted 2 times

? ?  foward 5ámonths, 2áweeks ago

Selected Answer: C

A is not correct because machine learning analysis is a good means toward the end of
reducing downtime, but shuffling formats and transport doesn't directly help at all.
B is not correct because machine learning analysis is a good means toward the end of
reducing downtime, and moving to streaming can improve the freshness of the
information in that analysis, but changing the format doesn't directly help at all.
C is correct because using cellular connectivity will greatly improve the freshness of data
used for analysis from where it is now, collected when the machines are in for
maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop
even more. Machine learning is ideal for predictive maintenance workloads.
D is not correct because machine learning analysis is a good means toward the end of
reducing downtime, but the rest of these changes don't directly help at all.

upvoted 1 times

? ?  thamaster 6ámonths ago

Selected Answer: C

This question is in the sample questions from google
A is not correct because machine learning analysis is a good means toward the end of reducing downtime, but shuffling formats and
transport doesn't directly help at all.

B is not correct because machine learning analysis is a good means toward the end of reducing downtime, and moving to streaming can
improve the freshness of the information in that analysis, but changing the format doesn't directly help at all.

C is correct because using cellular connectivity will greatly improve the freshness of data used for analysis from where it is now, collected
when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop even more.
Machine learning is ideal for predictive maintenance workloads.

D is not correct because machine learning analysis is a good means toward the end of reducing downtime, but the rest of these changes
don't directly help at all.

upvoted 3 times

? ?  Jackalski 7ámonths ago

Selected Answer: B

go for B
must go for streaming and faster processing (scalability on binary format)

option C makes no sense as there is no vehicle connectivity problem mentioned (no need to change cellular network)- delay is after data is
already received .

upvoted 3 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

ok for C

upvoted 1 times

? ?  AzureDP900 12ámonths ago

C is right!

upvoted 1 times

? ?  omodara 1áyear ago
C is the right answer

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

I agree B is better than A and D but C is better than B. Hence C

upvoted 1 times

? ?  Sekierer 1áyear, 5ámonths ago

Selected Answer: C

C is correct, taken from offiical Google practice Test:

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

653/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Official Explaination:
A is not correct because machine learning analysis is a good means toward the end of reducing downtime, but shuffling formats and
transport doesn't directly help at all.

B is not correct because machine learning analysis is a good means toward the end of reducing downtime, and moving to streaming can
improve the freshness of the information in that analysis, but changing the format doesn't directly help at all.

C is correct because using cellular connectivity will greatly improve the freshness of data used for analysis from where it is now, collected
when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop even more.
Machine learning is ideal for predictive maintenance workloads.

D is not correct because machine learning analysis is a good means toward the end of reducing downtime, but the rest of these changes
don't directly help at all.

upvoted 3 times

? ?  OrangeTiger 1áyear, 5ámonths ago

The commentary on the solution seems to say that B is the correct answer.
I'm lost in B and C.
B The speed of the analysis itself will be faster.
C Waiting time should be reduced by increasing the number of vehicles that can acquire information in real time.

upvoted 1 times

? ?  zxcv1234 1áyear, 6ámonths ago

Selected Answer: B

Not all vehicles are connected by cellular, C cannot be the answer. B is correct.

upvoted 1 times

? ?  hogtrough 1áyear, 5ámonths ago

This is sample question on GCP official training documentation. Answer is C.

upvoted 2 times

? ?  mudot 1áyear, 7ámonths ago

Selected Answer: C

A is not correct because machine learning analysis is a good means toward the end of reducing downtime, but shuffling formats and
transport doesn't directly help at all.

B is not correct because machine learning analysis is a good means toward the end of reducing downtime, and moving to streaming can
improve the freshness of the information in that analysis, but changing the format doesn't directly help at all.

C is correct because using cellular connectivity will greatly improve the freshness of data used for analysis from where it is now, collected
when the machines are in for maintenance. Streaming transport instead of periodic FTP will tighten the feedback loop even more.
Machine learning is ideal for predictive maintenance workloads.

D is not correct because machine learning analysis is a good means toward the end of reducing downtime, but the rest of these changes
don't directly help at all.

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 2 times

? ?  FERIN_02 1áyear, 7ámonths ago

A. Incorrect - There is nothing called SFTP Transport
B. Correct - CSV to Binary make big difference using ML
C. Incorrect. 80% Connectivity is illogical, It should be 100%
D. Incorrect . SFTP Transport is illogical

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

654/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

655/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

Which of TerramEarth's legacy enterprise processes will experience signi cant change as a result of increased Google Cloud Platform adoption?

A. Opex/capex allocation, LAN changes, capacity planning

B. Capacity planning, TCO calculations, opex/capex allocation

C. Capacity planning, utilization measurement, data center expansion

D. Data Center expansion, TCO calculations, utilization measurement

Correct Answer: B

Community vote distribution

B (90%)

10%

? ?  sri007  Highly Voted ?  3áyears, 5ámonths ago

Correct Answer B

Capacity planning, TCO calculations, opex/capex allocation

From the case study, it can conclude that Management (CXO) all concern rapid provision of resources (infrastructure) for growing as well
as cost management, such as Cost optimization in Infrastructure, trade up front capital expenditures (Capex) for ongoing operating
expenditures (Opex), and Total cost of ownership (TCO)

upvoted 26 times

? ?  nick_name_1 4ámonths ago

Only Issue I have w/ B is that they may currently be leasing owned compute, meaning that CapEx/OpEx considerations don't change.

upvoted 1 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 7 times

? ?  nitinz 2áyears, 3ámonths ago

B is correct.

upvoted 1 times

? ?  tdotcat  Most Recent ?  5ámonths, 1áweek ago

Selected Answer: A

sorry not B, I think A is right

upvoted 1 times

? ?  tdotcat 5ámonths, 1áweek ago

Selected Answer: B

TCO does not change as much as ownership of machinary is not changing

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

ok for B

upvoted 1 times

? ?  Nirca 11ámonths ago

Selected Answer: B

B it is !!!!

upvoted 1 times

? ?  AzureDP900 12ámonths ago

B is perfect!

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: B

B is correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

656/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  mudot 1áyear, 7ámonths ago

Selected Answer: B

A is not correct because LAN change management processes don't need to change significantly. TerramEarth can easily peer their on-
premises LAN with their Google Cloud Platform VPCs, and as devices and subnets move to the cloud, the LAN team's implementation will
change, but the change management process doesn't have to.

B is correct because all of these tasks are big changes when moving to the cloud. Capacity planning for cloud is different than for on-
premises data centers; TCO calculations are adjusted because TerramEarth is using services, not leasing/buying servers; OpEx/CapEx
allocation is adjusted as services are consumed vs. using capital expenditures.

C is not correct because measuring utilization can be done in the same way, often with the same tools (along with some new ones). Data
center expansion is not a concern for cloud customers; it is part of the undifferentiated heavy lifting that is taken care of by the cloud
provider.

D is not correct because data center expansion is not a concern for cloud customers; it is part of the undifferentiated heavy lifting that is
taken care of by the cloud provider. Measuring utilization can be done in the same way, often with the same tools (along with some new
ones).

upvoted 3 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 2 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 4 times

? ?  victory108 1áyear, 11ámonths ago

B. Capacity planning, TCO calculations, opex/capex allocation

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 3 times

? ?  go5 2áyears, 2ámonths ago

answer is B

upvoted 2 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B

upvoted 1 times

? ?  wiqi 2áyears, 10ámonths ago

B is correct.

upvoted 1 times

? ?  syu31svc 3áyears ago

GCP practice question confirms that B is correct

upvoted 4 times

? ?  gfhbox0083 3áyears ago

B, for sure

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

657/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #7

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

658/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

To speed up data retrieval, more vehicles will be upgraded to cellular connections and be able to transmit data to the ETL process. The current

FTP process is error-prone and restarts the data transfer from the start of the  le when connections fail, which happens often. You want to

improve the reliability of the solution and minimize data transfer time on the cellular connections.

What should you do?

A. Use one Google Container Engine cluster of FTP servers. Save the data to a Multi-Regional bucket. Run the ETL process using data in the

bucket

B. Use multiple Google Container Engine clusters running FTP servers located in different regions. Save the data to Multi-Regional buckets in

US, EU, and Asia. Run the ETL process using the data in the bucket

C. Directly transfer the  les to different Google Cloud Multi-Regional Storage bucket locations in US, EU, and Asia using Google APIs over

HTTP(S). Run the ETL process using the data in the bucket

D. Directly transfer the  les to a different Google Cloud Regional Storage bucket location in US, EU, and Asia using Google APIs over HTTP(S).

Run the ETL process to retrieve the data from each Regional bucket

Correct Answer: D

Community vote distribution

D (72%)

C (28%)

? ?  dabrat  Highly Voted ?  3áyears, 7ámonths ago

c)
Multi-Region Name Multi-Region Description
asia Data centers in Asia
eu Data centers in the European Union1
us Data centers in the United States

multi-region is a large geographic area, such as the United States, that contains two or more geographic places.

upvoted 38 times

? ?  JJu  Highly Voted ?  3áyears, 6ámonths ago

I think answer is C.
Use a multi-region when you want to serve content to data consumers that are outside of the Google network and distributed across large
geographic areas, or when you want the higher availability that comes with being geo-redundant.

upvoted 14 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 2ádays ago

Selected Answer: D

D, multi-region increases latency

upvoted 1 times

? ?  taer 2ámonths, 3áweeks ago

Selected Answer: C

By directly transferring the files to Google Cloud Multi-Regional Storage buckets using Google APIs over HTTP(S), you will improve the
reliability of the solution and minimize data transfer time on the cellular connections.

upvoted 1 times

? ?  n_nana 3ámonths, 2áweeks ago

Selected Answer: D

adding to other explanation, source https://cloud.google.com/storage/docs/locations#location_recommendations
It's pretty clear that multi-region is good for content serving and not analytics. Only regional and dual regional buckets are good storage
for analytics case which exist in the question

upvoted 6 times

? ?  YeJune 5ámonths ago
D is the correct answer

upvoted 1 times

? ?  ale_brd_ 6ámonths ago

Selected Answer: D

D is the correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

659/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  omermahgoub 6ámonths ago

To improve the reliability of the solution and minimize data transfer time on the cellular connections, the best option would be to directly
transfer the files to different Google Cloud Multi-Regional Storage bucket locations in US, EU, and Asia using Google APIs over HTTP(S).
Then, run the ETL process using the data in the bucket.

Using Google Cloud Multi-Regional Storage will allow the data to be stored in multiple locations, improving the reliability of the solution.
Using Google APIs over HTTP(S) will allow the data to be transferred directly to the storage buckets, minimizing the data transfer time on
the cellular connections.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

Option D, directly transferring the files to a different Google Cloud Regional Storage bucket location in US, EU, and Asia using Google
APIs over HTTP(S), would not be an effective solution as it would not improve the reliability of the solution. Using Google Cloud Multi-
Regional Storage, which stores the data in multiple locations, would be a more reliable solution.

upvoted 1 times

? ?  Kulwant85 6ámonths, 1áweek ago

Selected Answer: D

https://cloud.google.com/storage/docs/locations#considerations
Regional: 200 Gbps (per region, per project)
Multi-regional: 50 Gbps (per region, per project)

upvoted 9 times

? ?  n_nana 3ámonths, 2áweeks ago

Good point, in same source https://cloud.google.com/storage/docs/locations#location_recommendations
It's pretty clear that multi-region is good for content serving and not analytics. Only regional and dual regional buckets are good
storage for analytics case which exist in the question.

upvoted 3 times

? ?  medi01 2ámonths, 1áweek ago

Analytics have nothing to do with it, this is pure injection of the data.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D seems to be the correct answer
Sending Data to all Multi Region Buckets >> Incurs more cost , More Latency
Sending Data only to Regional Bucket >> Incurs less cost , Less Latency

upvoted 6 times

? ?  wisnu_ink 7ámonths, 1áweek ago

Selected Answer: C

C is the right ans
upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

ok for D

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

I would like to vote D

upvoted 1 times

? ?  ShadowLord 9ámonths, 3áweeks ago

Selected Answer: C

Having a Mutli Regional bucket makes the App Design for loading Simple
- Single Bucket name to load data from where ever
- Single Deployment of ETL to load data from Single Bucket to target DB

Overall management of Data sync across region is done on Google Backbone network .... anyway somewhere we have to coverge to a
single DB anyways

upvoted 1 times

? ?  JohnPi 10ámonths, 1áweek ago

Selected Answer: D

Use a region to help optimize latency and network bandwidth for data consumers, such as analytics pipelines, that are run in the same
region.
https://cloud.google.com/storage/docs/locations

upvoted 3 times

? ?  JohnPi 10ámonths, 1áweek ago

Selected Answer: D

D - see next question

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

660/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 3 times

? ?  DAYAGOWDA 11ámonths, 2áweeks ago

The next question says its regional buckets :-), so D might be the right answer?

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

661/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #8

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

662/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

TerramEarth's 20 million vehicles are scattered around the world. Based on the vehicle's location, its telemetry data is stored in a Google Cloud

Storage (GCS) regional bucket (US, Europe, or Asia). The CTO has asked you to run a report on the raw telemetry data to determine why vehicles

are breaking down after 100 K miles. You want to run this job on all the data.

What is the most cost-effective way to run this job?

A. Move all the data into 1 zone, then launch a Cloud Dataproc cluster to run the job

B. Move all the data into 1 region, then launch a Google Cloud Dataproc cluster to run the job

C. Launch a cluster in each region to preprocess and compress the raw data, then move the data into a multi-region bucket and use a Dataproc

cluster to  nish the job

D. Launch a cluster in each region to preprocess and compress the raw data, then move the data into a region bucket and use a Cloud

Dataproc cluster to  nish the job

Correct Answer: D

Community vote distribution

D (100%)

? ?  cetanx  Highly Voted ?  2áyears, 11ámonths ago
I will look at it from a different perspective;
A, B says "move all data" but analysis will try to reveal breaking down after 100K miles so there is no point of transferring data of the
vehicles with less than 100K milage.
Therefore, transferring all data is just waste of time and money.

There is one thing for sure here. If we move/copy data between continents it will cost us money therefore compressing the data before
copying to another region/continent makes sense.
Preprocessing also makes sense because we probably want to process smaller chunks of data first (remember 100K milage).
So now type of target bucket; multi-region or standard? multi-region is good for high-availability and low latency with a little more cost
however question doesn't require any of these features.
Therefore I think standard storage option is good to go given lower costs are always better.

So my answer would be D

upvoted 53 times

? ?  DiegoQ 2áyears, 9ámonths ago

I totally agree with you, and I think that what confuse people here is the "run a raw data", but preprocess doesn┤t mean to mandatory
transform raw data, it could be to only select the data that you need (as you said: vehicles with less than 100K milage)

upvoted 1 times

? ?  mrhege 2áyears, 2ámonths ago

You will need data from non-broken machines too for labelling.

upvoted 1 times

? ?  stfnz 1ámonth, 1áweek ago

yes, still you will be interested in 100K+ mileage, whether broken or not

upvoted 1 times

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

D is the most cost effective and DataProc is regional

upvoted 31 times

? ?  passnow 3áyears, 6ámonths ago

Dataproc can be use global end points too.

upvoted 1 times

? ?  passnow 3áyears, 6ámonths ago

Honestly, if we read the question well and factor in cost, D would be a better option

upvoted 2 times

? ?  vindahake 3áyears, 3ámonths ago

I think running additional compute regionally will be more expensive than data transfer charges and centrally processing them

upvoted 3 times

? ?  tartar 2áyears, 10ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

663/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D is ok

upvoted 11 times

? ?  Rafaa 3áyears ago

Hold on guys, you do not need to 'preprocess' the data. This rules out C,D.

upvoted 2 times

? ?  guid1984 2áyears, 4ámonths ago

why not it's a RAW data, so can be pre-processed for optimization

upvoted 2 times

? ?  nitinz 2áyears, 3ámonths ago

It is D.

upvoted 1 times

? ?  Jeena345  Most Recent ?  4ámonths, 3áweeks ago

Selected Answer: D

D should be fine
upvoted 1 times

? ?  omermahgoub 6ámonths ago

Answer is C
To run the report on all of the raw telemetry data for TerramEarth's vehicles in the most cost-effective way, it would be best to launch a
cluster in each region to preprocess and compress the raw data. This will allow you to process the data in place, which will minimize the
amount of data that needs to be transferred between regions. After the data has been preprocessed and compressed, you can then move
it into a multi-region bucket and use a Dataproc cluster to finish the job.

upvoted 2 times

? ?  omermahgoub 6ámonths ago

D, moving the data into a region bucket and using a Cloud Dataproc cluster to finish the job, would also not be as cost-effective as
moving the data into a multi-region bucket, as it would not take advantage of the lower costs of storing data in a multi-region bucket.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

ok for D

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

D seems better
upvoted 1 times

? ?  AMohanty 10ámonths, 3áweeks ago

What is the use of Multi-Regional DataProc if ur Storage Data is Regional

upvoted 1 times

? ?  AzureDP900 12ámonths ago

D is fine, There is no need of multi-region as mentioned in C. D is right in my opinion.

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: D

D is the correct answer. Regional bucket is required, since multi regional bucket will incur additional cost to transfer the data to a
centralized location.

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

D seems to be the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

D û Launch a cluster in each region to pre-process and compress the raw data, then move the data into a regional bucket and use Cloud
Dataproc cluster.
Egress rates are most important. It is free inside of region - so make sense to move all data into one region for processing/performance
(from all continents). Cross-region cost is 0.01$ per GB, and inter-continent 0.12$ per GB.
If to consider just option B (moving all raw data into one region) then just monthly volume would cost:
900 TB (all 20M units daily) 30 days 0.12 $ = 3.24 M $ (just for data transfer). So, it definitely makes sense to preprocess/compress data per
region, and then move all that data into one region for final analysis. That would save up to 10-100 times on egress costs. Also, important
aspect is processing time - running it in parallel on all regions accelerates overall analysis effort. Faster result - faster in-field

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

664/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

improvements.
Look this interesting video about price optimization in GCP (first 11.5 mins are about Storage/Network)
https://cloud.google.com/storage/docs/locations#considerations

upvoted 5 times

? ?  victory108 1áyear, 11ámonths ago

D. Launch a cluster in each region to preprocess and compress the raw data, then move the data into a region bucket and use a Cloud
Dataproc cluster to finish the job

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is D

upvoted 3 times

? ?  Yogikant 2áyears ago

Answer D:

moving data from one region to another region will incur network egress cost. By compressing data and then moving would reduce this
cost. Though running Dataproc for preprocessing in each region will incur additional cost but it will also reduce cost of running Dataproc
job on all pre-processed data will also reduce cost offsetting additional cost of Dataproc cluster at regional level.

upvoted 1 times

? ?  Pravin3c 2áyears, 2ámonths ago

Dataproc supports both a single "global" endpoint and regional endpoints based on Compute Engine zones.

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is D

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

665/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #9

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

666/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

TerramEarth has equipped all connected trucks with servers and sensors to collect telemetry data. Next year they want to use the data to train

machine learning models. They want to store this data in the cloud while reducing costs.

What should they do?

A. Have the vehicle's computer compress the data in hourly snapshots, and store it in a Google Cloud Storage (GCS) Nearline bucket

B. Push the telemetry data in real-time to a streaming data ow job that compresses the data, and store it in Google BigQuery

C. Push the telemetry data in real-time to a streaming data ow job that compresses the data, and store it in Cloud Bigtable

D. Have the vehicle's computer compress the data in hourly snapshots, and store it in a GCS Coldline bucket

Correct Answer: D

Storage is the best choice for data that you plan to access at most once a year, due to its slightly lower availability, 90-day minimum storage

duration, costs for data access, and higher per-operation costs. For example:

Cold Data Storage - Infrequently accessed data, such as data stored for legal or regulatory reasons, can be stored at low cost as Coldline

Storage, and be available when you need it.

Disaster recovery - In the event of a disaster recovery event, recovery time is key. Cloud Storage provides low latency access to data stored as

Coldline Storage.

Reference:

https://cloud.google.com/storage/docs/storage-classes

Community vote distribution

D (100%)

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

D is most cost effective as don't want to use until 'next year'

upvoted 31 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 8 times

? ?  nitinz 2áyears, 3ámonths ago
D is most cost effective

upvoted 2 times

? ?  HCL 2áyears, 3ámonths ago

Hourly snapshots in answer D does not make any sense.
The answer is B.
upvoted 1 times

? ?  Deb2293  Most Recent ?  3ámonths, 2áweeks ago

Selected Answer: D
If the words 'next year' wouldn't have been there then Big Table ? . But as it will be required next year so Coldline bucket would be the
most cost effective solution.

upvoted 1 times

? ?  omermahgoub 6ámonths ago

One option that TerramEarth could consider is storing the telemetry data in a Google Cloud Storage (GCS) Nearline bucket. This would
allow them to store the data in the cloud at a lower cost than other storage options, while still providing quick access to the data when
needed. By having the vehicle's computer compress the data in hourly snapshots, they can reduce the amount of storage needed and
further reduce costs.

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 2áweeks ago

Selected Answer: D

D is the correct answer
Clue is "next year they want to use the data"
Therefore moving the data to coldline storage makes more sense

upvoted 3 times

? ?  megumin 7ámonths, 3áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

667/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: D

ok for D

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  AzureDP900 12ámonths ago

I miss the point about cost optimization and I thought C is right. After reading the discussions I realized D is right answer. I am going with
D

upvoted 1 times

? ?  Aiffone 1áyear, 5ámonths ago

Big query does it. B...when it's long term storage, it costs same as coldline
https://cloud.google.com/bigquery/docs/best-practices-storage

upvoted 1 times

? ?  Wonka 1áyear, 5ámonths ago

It cost same as nearline when not accessed, but coldline is cheaper than BQ

upvoted 4 times

? ?  Aiffone 1áyear, 5ámonths ago

the highlight here is machine learning and not disaster recovery or data arhiving which is what coldline storages are for. You also dont pay
for datawarehousing in bigquery until you read from it for machine learning. So its cheap and good for ML. i go with B

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

D is the correct answer

upvoted 1 times

? ?  mgm7 1áyear, 6ámonths ago

D makes sense IF the "computer" on the vehicle can compress data and can take snapshots. Are we supposed to to assume that these
"computers" have snapshot capability though it is no stated anywhere in the question? Yet, if magically this was possible, this is the correct
answer. If this indeed is the correct answer then the only logical deduction is that the questions is stated horribly. I only can hope the real
exam isn't like this.

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

D û Have vehicleÆs computer compress data in hourly snapshots, and store in GCS Coldline bucket.
A û doesnÆt work, since Nearline is more expensive than Coldline in D (0.01$ vs 0.007$ GB/month).
B / C û stores compressed data in relational DB, which may not be possible. Even it is implemented, then B (BigQuery) is more expensive
than Cloud Storage Coldline (0.01$ vs 0.007$ GB/month)
C û Bigtable is most expensive option (0.026$ GB/month) and also it is not integrated with Cloud ML (Dataflow, BiqQuery and Cloud
Storage are integrated)
D û Coldline fits perfectly û blob storage, cheapest price, integration with ML

upvoted 3 times

? ?  victory108 1áyear, 11ámonths ago

D. Have the vehicle?ÇÖs computer compress the data in hourly snapshots, and store it in a GCS Coldline bucket

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is D

upvoted 2 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is D

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

IMO B and C are not correct because in both of them there is "[...] compress the data [...]" - and neiter BigQuery nor Bigtable are suitable
for "compressed data".
D is cheaper than A -- so I'll go with D .

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

668/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #10

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

669/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

Your agricultural division is experimenting with fully autonomous vehicles. You want your architecture to promote strong security during vehicle

operation.

Which two architectures should you consider? (Choose two.)

A. Treat every micro service call between modules on the vehicle as untrusted.

B. Require IPv6 for connectivity to ensure a secure address space.

C. Use a trusted platform module (TPM) and verify  rmware and binaries on boot.

D. Use a functional programming language to isolate code execution cycles.

E. Use multiple connectivity subsystems for redundancy.

F. Enclose the vehicle's drive electronics in a Faraday cage to isolate chips.

Correct Answer: AC

Community vote distribution

AC (100%)

? ?  kvokka  Highly Voted ?  3áyears, 5ámonths ago

AC is correct

upvoted 26 times

? ?  tartar 2áyears, 10ámonths ago

AC is ok

upvoted 10 times

? ?  nitinz 2áyears, 3ámonths ago

Agree with AC
upvoted 1 times

? ?  mlantonis  Highly Voted ?  3áyears ago

The question is taken from Google Practice Exam.
A and C are the correct.

The answer from Google (with different order)
A is not correct because this improves system durability, but it doesn't have any impact on the security during vehicle operation.

B is not correct because IPv6 doesn't have any impact on the security during vehicle operation, although it improves system scalability and
simplicity.

C is not correct because it doesn't have any impact on the security during vehicle operation, although it improves system durability.

D is not correct because merely using a functional programming language doesn't guarantee a more secure level of execution isolation.
Any impact on security from this decision would be incidental at best.

E is correct because this improves system security by making it more resistant to hacking, especially through man-in-the-middle attacks
between modules.

F is correct because this improves system security by making it more resistant to hacking, especially rootkits or other kinds of corruption
by malicious actors.

upvoted 7 times

? ?  nick_name_1  Most Recent ?  4ámonths ago

A.E. TPM relates to Cloud VM's and has nothing to do with the vehicle's self driving operation.

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: AC

AC is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: AC

A & C are correct

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

670/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

Agree with A , C
upvoted 1 times

? ?  AzureDP900 12ámonths ago

Agreed with all discussions with great detailed explanation as A, C as right ....

upvoted 1 times

? ?  tluu 1áyear, 3ámonths ago
Correct answer: A & C
B is not correct because IPv6 doesn't have any impact on the security during vehicle operation, although it improves system scalability and
simplicity.
D is not correct because merely using a functional programming language doesn't guarantee a more secure level of execution isolation.
Any impact on security from this decision would be incidental at best.
E is not correct because this improves system durability, but it doesn't have any impact on the security during vehicle operation.
F is not correct because it doesn't have any impact on the security during vehicle operation, although it improves system durability.

upvoted 6 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: AC

vote AC

upvoted 2 times

? ?  civilizador 1áyear, 9ámonths ago

I don't think it's AC. How hardware solution as TMP is related to the GCP exam. Also if you read the question it says : You want your
architecture to promote strong security during vehicle operation. During operation is the key word. So AE is the correct answer

upvoted 4 times

? ?  nick_name_1 4ámonths ago

A.E. "Self driving vehicle" most CERTAINLY REQUIRES CONNECTIVITY OR YOU WILL WRECK.

upvoted 1 times

? ?  passtest100 10ámonths ago

agree. Option E improvies HA, rather than the duability as others explained

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

A. Treat every micro service call between modules on the vehicle as untrusted.
C. Use a trusted platform module (TPM) and verify firmware and binaries on boot.

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A,C
upvoted 4 times

? ?  Ausias18 2áyears, 2ámonths ago

Answers are A, C
upvoted 1 times

? ?  bnlcnd 2áyears, 4ámonths ago

only A & C seemingly related. no other choices.

upvoted 2 times

? ?  wiqi 2áyears, 10ámonths ago
AC makes sense here.

upvoted 1 times

? ?  OnomeOkuma 2áyears, 11ámonths ago

AC is correct

upvoted 2 times

? ?  gfhbox0083 3áyears ago

A, C, for sure

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

671/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #11

Introductory Info

Company Overview -

Topic 8

TerramEarth manufactures heavy equipment for the mining and agricultural industries: about 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Company background -

TerramEarth was formed in 1946, when several small, family owned companies combined to retool after World War II. The company cares about

their employees and customers and considers them to be extended members of their family.

TerramEarth is proud of their ability to innovate on their core products and  nd new markets as their customers' needs change. For the past 20

years, trends in the industry have been largely toward increasing productivity by using larger vehicles with a human operator.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second with 22 hours of operation per day, Terram Earth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux-based systems that reside in a data center. These systems gzip CSV  les from the  eld

and upload via

FTP, transform and aggregate them, and place the data in their data warehouse. Because this process takes time, aggregated reports are based on

data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week, without increasing the cost of carrying surplus inventory

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers.

CEO Statement -

We have been successful in capitalizing on the trend toward larger vehicles to increase the productivity of our customers. Technological change is

occurring rapidly, and TerramEarth has taken advantage of connected devices technology to provide our customers with better services, such as

our intelligent farming equipment. With this technology, we have been able to increase farmers' yields by 25%, by using past trends to adjust how

our vehicles operate. These advances have led to the rapid growth of our agricultural product line, which we expect will generate 50% of our

revenues by 2020.

CTO Statement -

Our competitive advantage has always been in the manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

672/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

undergo the next wave of transformations in our industry. Unfortunately, our CEO doesn't take technology obsolescence seriously and he

considers the many new companies in our industry to be niche players. My goals are to build our skills while addressing immediate market needs

through incremental innovations.

Question

Operational parameters such as oil pressure are adjustable on each of TerramEarth's vehicles to increase their e ciency, depending on their

environmental conditions. Your primary goal is to increase the operating e ciency of all 20 million cellular and unconnected vehicles in the  eld.

How can you accomplish this goal?

A. Have you engineers inspect the data for patterns, and then create an algorithm with rules that make operational adjustments automatically

B. Capture all operating data, train machine learning models that identify ideal operations, and run locally to make operational adjustments

automatically

C. Implement a Google Cloud Data ow streaming job with a sliding window, and use Google Cloud Messaging (GCM) to make operational

adjustments automatically

D. Capture all operating data, train machine learning models that identify ideal operations, and host in Google Cloud Machine Learning (ML)

Platform to make operational adjustments automatically

Correct Answer: B

Community vote distribution

B (100%)

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

B is correct. only 200k vehicle's are connected so need to run updates locally

upvoted 34 times

? ?  exampanic 3áyears, 5ámonths ago

In my view, option B says "run locally" referring to the machine learning models. "Machine learning models" is the subject of the
sentence. Nowhere in the sentence says run "updates" locally. So running machine learning models would only make sense in Google's
ML platform, not locally. Because of this reason, I believe the correct answer should be "D".

upvoted 28 times

? ?  cetanx 2áyears, 11ámonths ago

Both B and D starts with "Capture all operating data, train machine learning models that identify ideal operations, ..." so they are
offering the same method for training the data.

The keypoint here is "make operational adjustments" such as adjusting the oil pressure so if we host in GCP-ML, how are we going
to instruct vehicles on field to adjust their oil pressure if they have no internet connection? There is no way to use GCP-ML model
generated parameters to command the "not connected" field vehicles to make operational adjustments automatically.

Therefore, I believe running it locally on the servers sitting in the vehicles is the only option.

My answer: B

upvoted 25 times

? ?  Vika 2áyears, 3ámonths ago

Making operational adjustments is an operational problem after recommendations are made by ML. In my mind, new data will
keep feeding and total operational data changes every day for model and which would impact model performance over time.
Monitoring model performance to achieve required efficiency levels would need some sort of centralization of efforts, as every
machine environment condition might be different and there might be a need to create multiple models and test and operate
them. (one shoe doesn't fit all).

upvoted 1 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 13 times

? ?  nick_name_1 4ámonths ago

B says "Capture all operating data". This is not right. You don't need ALL operating data to create an efficiency algorithm. The Answer is
A.

upvoted 1 times

? ?  techalik 2áyears, 7ámonths ago

What you think about D? there is a hint there :D
and host in "Google Cloud Machine Learning" (ML)

upvoted 4 times

? ?  nandoD 2ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

673/776

how will the automatic opeartional adjustments be done to the unconnected/offline vehicles?

6/29/23, 1:52 PM

p

j

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  kapa900 3ádays, 16áhours ago

end of day

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

B is correct.

upvoted 2 times

? ?  dabrat  Highly Voted ?  3áyears, 7ámonths ago
B)=> unconnected vehicles in the field.

upvoted 8 times

? ?  Rafaa 3áyears ago

Unconnected vehicles does not mean their data is not on GCP. you would still do ML on GCP and can use that to improve operational
performance via maintenance port.

upvoted 4 times

? ?  red_panda  Most Recent ?  5áhours, 52áminutes ago

Selected Answer: B

Also for me it's B.
Maybe it might not seem so straightforward when reading the answer, but personally I understood it as 'running the result of ML training
on devices'. Understanding it this way it certainly can only be B as from the scenario description we have most of the machines not
connected to a cellular network and therefore it is impossible to pass data to them.

upvoted 1 times

? ?  Arlima 5ámonths, 1áweek ago

B is correct. only 200k vehicle's are connected so need to run updates locally, means ML Edge

upvoted 1 times

? ?  ale_brd_ 6ámonths ago

Selected Answer: B

Answer is B

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

ok for B

upvoted 1 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: B

B is correct. only 200k vehicle's are connected so need to run updates locally

upvoted 1 times

? ?  AzureDP900 12ámonths ago

B is right , Agreed with detailed discussions !

upvoted 1 times

? ?  Danny2021 1áyear, 7ámonths ago

B. Train model in the cloud and deploy model to the edge for local prediction. This is typical in IoT.

upvoted 5 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 2 times

? ?  Nik22 1áyear, 9ámonths ago

from exam point of view, I doubt these questions would be part of new exam. Terram earth case study has changed.

upvoted 3 times

? ?  VishalB 1áyear, 11ámonths ago

Answer : B
Google has announced two new products aimed at helping customers develop and deploy intelligent connected devices at scale: Edge
TPU, a new hardware chip, and Cloud IoT Edge, a software stack that extends Google CloudÆs powerful AI capability to gateways and
connected devices. This lets you build and train ML models in the cloud, then run those models on the Cloud IoT Edge device through the
power of the Edge TPU hardware accelerator.

- https://cloud.google.com/blog/products/gcp/bringing-intelligence-edge-cloud-iot

upvoted 3 times

? ?  victory108 1áyear, 11ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

674/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B. Capture all operating data, train machine learning models that identify ideal operations, and run locally to make operational
adjustments automatically

upvoted 2 times

? ?  capitaine 1áyear, 11ámonths ago

Run Locally -> That's exactly what gcp ca, provide:
https://cloud.google.com/blog/products/gcp/bringing-intelligence-edge-cloud-iot

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 3 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

IMO - B is ok (because we have most vehicles unconnected)

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

675/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 9 - Testlet 6

Question #1

Introductory Info

Company Overview -

Topic 9

TerramEarth manufactures heavy equipment for the mining and agricultural industries. About 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second, with

22 hours of operation per day, TerramEarth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux and Windows-based systems that reside in a single U.S, west coast based data center.

These systems gzip CSV  les from the  eld and upload via FTP, and place the data in their data warehouse. Because this process takes time,

aggregated reports are based on data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers

Technical Requirements -

Expand beyond a single datacenter to decrease latency to the American midwest and east coast

Create a backup strategy

Increase security of data transfer from equipment to the datacenter

Improve data in the data warehouse

Use customer and equipment data to anticipate customer needs

Application 1: Data ingest -

A custom Python application reads uploaded data les from a single server, writes to the data warehouse.

Compute:

Windows Server 2008 R2

- 16 CPUs

- 128 GB of RAM

- 10 TB local HDD storage

Application 2: Reporting -

An off the shelf application that business analysts use to run a daily report to see what equipment needs repair. Only 2 analysts of a team of 10 (5

west coast, 5 east coast) can connect to the reporting application at a time.

Compute:

Off the shelf application. License tied to number of physical CPUs

- Windows Server 2008 R2

- 16 CPUs

- 32 GB of RAM

- 500 GB HDD

Data warehouse:

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

676/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A single PostgreSQL server

- RedHat Linux

- 64 CPUs

- 128 GB of RAM

- 4x 6TB HDD in RAID 0

Executive Statement -

Our competitive advantage has always been in our manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

undergo the next wave of transformations in our industry. My goals are to build our skills while addressing immediate market needs through

incremental innovations.

Question

For this question, refer to the TerramEarth case study. To be compliant with European GDPR regulation, TerramEarth is required to delete data

generated from its

European customers after a period of 36 months when it contains personal data. In the new architecture, this data will be stored in both Cloud

Storage and

BigQuery. What should you do?

A. Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use gsutil to enable

lifecycle management using a DELETE action with an Age condition of 36 months.

B. Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use gsutil to create a

SetStorageClass to NONE action when with an Age condition of 36 months.

C. Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud Storage, use

gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.

D. Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud Storage, use

gsutil to create a SetStorageClass to NONE action with an Age condition of 36 months.

Correct Answer: C

Community vote distribution

C (86%)

14%

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

I thought C was correct.
SetStorageClass could not be set to NONE. After data expired, data should be deleted not table.
any comment?
upvoted 41 times

? ?  mister 3áyears, 8ámonths ago

why not A

upvoted 2 times

? ?  KouShikyou 3áyears, 8ámonths ago

My understanding is table expiration is for table deletion.
Since we just want delete the old records in the table but not the entire table.
I thought what is the time-partitioned table for?
Any comment?
upvoted 8 times

? ?  Jambalaja 2áyears, 2ámonths ago

Thats correcr
upvoted 1 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 10 times

? ?  VishalB 1áyear, 11ámonths ago

bcoz you would land up creating a table for each day which is not a good practice

upvoted 2 times

? ?  Wonka 1áyear, 5ámonths ago

or rather it will delete the entire table and all the data in it i.e. records less than 36 months old

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

677/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 2 times

? ?  techalik 2áyears, 7ámonths ago

C
Enable a bucket lifecycle management rule to delete objects older than 36 months. Use partitioned tables in BigQuery and set the
partition expiration period to 36 months. is the right answer.

When you create a table partitioned by ingestion time, BigQuery automatically loads data into daily, date-based partitions that reflect
the data's ingestion or arrival time.

Ref: https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time

And Google recommends you configure the default table expiration for your datasets, configure the expiration time for your tables,
and configure the partition expiration for partitioned tables.

Ref: https://cloud.google.com/bigquery/docs/best-practices-
storage#use_the_expiration_settings_to_remove_unneeded_tables_and_partitions

If the partitioned table has a table expiration configured, all the partitions in it are deleted according to the table expiration settings.
For our specific requirement, we could set the partition expiration to 36 months so that partitions older than 36 months (and the data
within) are automatically deleted.

Ref: https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration

upvoted 20 times

? ?  AMohanty 10ámonths, 3áweeks ago

There is Nothing as Storage Class as NONE

upvoted 3 times

? ?  nitinz 2áyears, 3ámonths ago

C, partition the data and expire it in big query and use life cycle on GS bucket.

upvoted 3 times

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

answer C is the right choice here. Table expiration in BigQuery and life cycle management in GSC

upvoted 24 times

? ?  passnow 3áyears, 6ámonths ago

i vote C

upvoted 4 times

? ?  thamaster  Most Recent ?  6ámonths ago

Selected Answer: C

it's C, I'm sure at 100% the other answer are incorrect there is no None as storage class and you need to actually delete data

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: C

ok for C

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is correct

upvoted 1 times

? ?  AzureDP900 12ámonths ago

C is right

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

partion is way to go with big query hence B & C.
for block storage C isvtye waybto go.
hence C.

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: D

'Next year they want to use the data to train machine learning models.'
I agree with D.
upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I made a mistake in the question to post a comment

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

678/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  kimharsh 1áyear ago

you also screwed up the percentage of the correct answers now :P

upvoted 3 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

Correct Answer: C
A û doesnÆt work since there is no ôretention periodö for table, there is only ôexpiration timeö after which it is removed completely.
B/D û doesnÆt work, since no such storage class like NONE.

upvoted 1 times

? ?  [Removed] 1áyear, 8ámonths ago

C is correct.
Time-partioned tables AND DELETE data after 36 months using GCS life cycle management.

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

C. Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud
Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is C

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is C

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

IMO - C is ok (assuming DAY or lower level time-partitioning).
We want to delete only partitions older than 36 month not THE WHOLE tables when aged 36 months.

upvoted 1 times

? ?  okixavi 2áyears, 6ámonths ago

C is the right answer

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

679/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company Overview -

Topic 9

TerramEarth manufactures heavy equipment for the mining and agricultural industries. About 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second, with

22 hours of operation per day, TerramEarth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux and Windows-based systems that reside in a single U.S, west coast based data center.

These systems gzip CSV  les from the  eld and upload via FTP, and place the data in their data warehouse. Because this process takes time,

aggregated reports are based on data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers

Technical Requirements -

Expand beyond a single datacenter to decrease latency to the American midwest and east coast

Create a backup strategy

Increase security of data transfer from equipment to the datacenter

Improve data in the data warehouse

Use customer and equipment data to anticipate customer needs

Application 1: Data ingest -

A custom Python application reads uploaded data les from a single server, writes to the data warehouse.

Compute:

Windows Server 2008 R2

- 16 CPUs

- 128 GB of RAM

- 10 TB local HDD storage

Application 2: Reporting -

An off the shelf application that business analysts use to run a daily report to see what equipment needs repair. Only 2 analysts of a team of 10 (5

west coast, 5 east coast) can connect to the reporting application at a time.

Compute:

Off the shelf application. License tied to number of physical CPUs

- Windows Server 2008 R2

- 16 CPUs

- 32 GB of RAM

- 500 GB HDD

Data warehouse:

A single PostgreSQL server

- RedHat Linux

- 64 CPUs

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

680/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

- 128 GB of RAM

- 4x 6TB HDD in RAID 0

Executive Statement -

Our competitive advantage has always been in our manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

undergo the next wave of transformations in our industry. My goals are to build our skills while addressing immediate market needs through

incremental innovations.

Question

For this question, refer to the TerramEarth case study. TerramEarth has decided to store data  les in Cloud Storage. You need to con gure Cloud

Storage lifecycle rule to store 1 year of data and minimize  le storage cost.

Which two actions should you take?

A. Create a Cloud Storage lifecycle rule with Age: ?Ç30?Ç, Storage Class: ?ÇStandard?Ç, and Action: ?ÇSet to Coldline?Ç, and create a second

GCS life-cycle rule with Age: ?Ç365?Ç, Storage Class: ?ÇColdline?Ç, and Action: ?ÇDelete?Ç.

B. Create a Cloud Storage lifecycle rule with Age: ?Ç30?Ç, Storage Class: ?ÇColdline?Ç, and Action: ?ÇSet to Nearline?Ç, and create a second

GCS life-cycle rule with Age: ?Ç91?Ç, Storage Class: ?ÇColdline?Ç, and Action: ?ÇSet to Nearline?Ç.

C. Create a Cloud Storage lifecycle rule with Age: ?Ç90?Ç, Storage Class: ?ÇStandard?Ç, and Action: ?ÇSet to Nearline?Ç, and create a second

GCS life-cycle rule with Age: ?Ç91?Ç, Storage Class: ?ÇNearline?Ç, and Action: ?ÇSet to Coldline?Ç.

D. Create a Cloud Storage lifecycle rule with Age: ?Ç30?Ç, Storage Class: ?ÇStandard?Ç, and Action: ?ÇSet to Coldline?Ç, and create a second

GCS life-cycle rule with Age: ?Ç365?Ç, Storage Class: ?ÇNearline?Ç, and Action: ?ÇDelete?Ç.

Correct Answer: A

Community vote distribution

A (100%)

? ?  VishalB  Highly Voted ?  1áyear, 11ámonths ago

Answer A
o When Only Option A & D talks about deleting the file after 1 Year. In Option D at Age 30 the storage Class is set to Coldline and while
deleting they have used the condition Storage Class: "Nearline" which is incorrect.

upvoted 19 times

? ?  H_S 1áyear ago
thank you man
upvoted 2 times

? ?  ale_brd_  Most Recent ?  6ámonths ago

Selected Answer: A

A) is the correct answer

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is correct

upvoted 1 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: A

A û Create Cloud Storage lifecycle rule with Age: ô30ö, Storage Class: ôStandardö and Action: ôSet to Coldlineö;
and create a 2nd GCS life-cycle rule with age ô365ö, Storage Class: ôColdlineö and action ôDeleteö.

upvoted 2 times

? ?  AzureDP900 11ámonths, 4áweeks ago

A is right!

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is thee correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

681/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

vote A

upvoted 1 times

? ?  [Removed] 1áyear, 8ámonths ago

A is correct.

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

A û Create Cloud Storage lifecycle rule with Age: ô30ö, Storage Class: ôStandardö and Action: ôSet to Coldlineö;
and create a 2nd GCS life-cycle rule with age ô365ö, Storage Class: ôColdlineö and action ôDeleteö.
D û doesnÆt work since 2nd life-cyle rule requires ôNearlineö storage, while now data is in ôColdlineö.

upvoted 2 times

? ?  amxexam 1áyear, 9ámonths ago

The optimal answer is A, but is it Archival for 365 as per docs
https://cloud.google.com/storage/docs/storage-classes#available_storage_classes

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

A. Create a Cloud Storage lifecycle rule with Age: ?Ç30?Ç, Storage Class: ?ÇStandard?Ç, and Action: ?ÇSet to Coldline?Ç, and create a second
GCS life-cycle rule with Age: ?Ç365?Ç, Storage Class: ?ÇColdline?Ç, and Action: ?ÇDelete?Ç.

upvoted 2 times

? ?  JeffClarke111 1áyear, 11ámonths ago

A is ok

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 4 times

? ?  umashankar_a 1áyear, 11ámonths ago

Answer A
is the correct answer

upvoted 2 times

? ?  shaw2021 1áyear, 11ámonths ago

The correct answer is A

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

682/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company Overview -

Topic 9

TerramEarth manufactures heavy equipment for the mining and agricultural industries. About 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second, with

22 hours of operation per day, TerramEarth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux and Windows-based systems that reside in a single U.S, west coast based data center.

These systems gzip CSV  les from the  eld and upload via FTP, and place the data in their data warehouse. Because this process takes time,

aggregated reports are based on data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers

Technical Requirements -

Expand beyond a single datacenter to decrease latency to the American midwest and east coast

Create a backup strategy

Increase security of data transfer from equipment to the datacenter

Improve data in the data warehouse

Use customer and equipment data to anticipate customer needs

Application 1: Data ingest -

A custom Python application reads uploaded data les from a single server, writes to the data warehouse.

Compute:

Windows Server 2008 R2

- 16 CPUs

- 128 GB of RAM

- 10 TB local HDD storage

Application 2: Reporting -

An off the shelf application that business analysts use to run a daily report to see what equipment needs repair. Only 2 analysts of a team of 10 (5

west coast, 5 east coast) can connect to the reporting application at a time.

Compute:

Off the shelf application. License tied to number of physical CPUs

- Windows Server 2008 R2

- 16 CPUs

- 32 GB of RAM

- 500 GB HDD

Data warehouse:

A single PostgreSQL server

- RedHat Linux

- 64 CPUs

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

683/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

- 128 GB of RAM

- 4x 6TB HDD in RAID 0

Executive Statement -

Our competitive advantage has always been in our manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

undergo the next wave of transformations in our industry. My goals are to build our skills while addressing immediate market needs through

incremental innovations.

Question

For this question, refer to the TerramEarth case study. You need to implement a reliable, scalable GCP solution for the data warehouse for your

company,

TerramEarth.

Considering the TerramEarth business and technical requirements, what should you do?

A. Replace the existing data warehouse with BigQuery. Use table partitioning.

B. Replace the existing data warehouse with a Compute Engine instance with 96 CPUs.

C. Replace the existing data warehouse with BigQuery. Use federated data sources.

D. Replace the existing data warehouse with a Compute Engine instance with 96 CPUs. Add an additional Compute Engine preemptible

instance with 32 CPUs.

Correct Answer: A

Community vote distribution

A (88%)

13%

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

Bigquery partitioning, A. Federated makes no sense...

upvoted 40 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 9 times

? ?  nitinz 2áyears, 3ámonths ago

A is correct

upvoted 2 times

? ?  Ziegler  Highly Voted ?  3áyears, 1ámonth ago

A is the correct answer because the question was asking for a reliable way of improving the data warehouse. The reliable way is to have a
table partitioned and that can be well managed.
https://cloud.google.com/solutions/bigquery-data-warehouse
BigQuery supports partitioning tables by date. You enable partitioning during the table-creation process. BigQuery creates new date-
based partitions automatically, with no need for additional maintenance. In addition, you can specify an expiration time for data in the
partitions.
https://cloud.google.com/solutions/bigquery-data-warehouse#partitioning_tables
Federated is an option but not a reliable option.
You can run queries on data that exists outside of BigQuery by using federated data sources, but this approach has performance
implications. Use federated data sources only if the data must be maintained externally. You can also use query federation to perform ETL
from an external source to BigQuery. This approach allows you to define ETL using familiar SQL syntax.
https://cloud.google.com/solutions/bigquery-data-warehouse#external_sources

upvoted 20 times

? ?  szagarella  Most Recent ?  4ámonths ago

Selected Answer: A

A is the only correct answer

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

ok for A

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is fine

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

684/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: A

Bigquery partitioning, A. Federated makes no sense...

upvoted 1 times

? ?  ATANGA 10ámonths, 4áweeks ago

WhatsApp : +1(956)-520-4006 to obtain PMP, CISM, CCNA, CEH, PRINCE2, CISCO, ISTTQB, PRINCE2, AWS/Azure/Sale force/ITIL
Foundation/EC- COUNCIL...
Get Certified with 100% pass guarantee. Pay after exam.
all CISCO, ISACA & EC- COUNCIL certifications
For the Below certificates we offer 100% pass guarantee:
1. AWS Certification
2. Sales force
3. Scrum Master
4. Oracle Certification: OCA, OCP
5. Cisco Certification: CCNA, CCNP, CCIE
6. ITIL Foundation & Intermediate
7. Prince 2 Foundation and Practitioner
8. VMWARE Certification
9. Check Point Certification
10. EC-COUNCIL Certification (CEH V-9, CCISO, CND)
11. Cloud Certification
12. IBM Certification
13. HP Certification
14. Citrix Certification
15. Juniper certification
16. Azure
17.Skype 70-333/34
18.PMI (PMP/CAPM/ACP/PBA)
19.ISTQB
20.SAP
21.ISACA (CISA, CISM, CRISC, CGEIT, COBIT)

PAYMENT ONLY AFTER CERTIFICATION AND RESULT CONFIRMATION.
WhatsApp : +1(956)-520-4006

upvoted 1 times

? ?  DrishaS4 11ámonths ago

Selected Answer: A

Bigquery partitioning, A. Federated makes no sense...

upvoted 1 times

? ?  Nirca 11ámonths ago

Selected Answer: C

C! Expand beyond a single datacenter to decrease latency to the

upvoted 1 times

? ?  AzureDP900 11ámonths, 4áweeks ago

A is right.

upvoted 1 times

? ?  H_S 1áyear ago

Selected Answer: A

Bigquery partitioning, A. Federated makes no sense...

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

A û BigQuery in time-partitioned mode.
C û federated data source wonÆt be effective. It assumes that time-series data is stored in BigTable and BigQuery federates this table for
analytics. But, thatÆs expensive.
- BigTable charges for egress 0.08 $ GB/read (that adds charges in analytics mode)
- BigTable (HDD) û 0.026 $ GB/mo vs BigQuery 0.010 $ GB/mo (and first 10 GB are free monthly).
So, no point for BigTable at all. Stream everything to BiqQuery for storage and analytics. Also, BiqQuery can setup partitions expiration
period.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

685/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  VishalB 1áyear, 11ámonths ago

Answer A
o Existing Datawarehouse was hosted on single PostgreSQL server on with below configuration, replacing it with serverless Bigquery
using table partition is best recommended soltuion
(cid:0) RedHat Linux
(cid:0) 64 CPUs
(cid:0) 128 GB of RAM
(cid:0) 4x 6TB HDD in RAID 0

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

A. Replace the existing data warehouse with BigQuery. Use table partitioning.

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 4 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

686/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company Overview -

Topic 9

TerramEarth manufactures heavy equipment for the mining and agricultural industries. About 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second, with

22 hours of operation per day, TerramEarth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux and Windows-based systems that reside in a single U.S, west coast based data center.

These systems gzip CSV  les from the  eld and upload via FTP, and place the data in their data warehouse. Because this process takes time,

aggregated reports are based on data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers

Technical Requirements -

Expand beyond a single datacenter to decrease latency to the American midwest and east coast

Create a backup strategy

Increase security of data transfer from equipment to the datacenter

Improve data in the data warehouse

Use customer and equipment data to anticipate customer needs

Application 1: Data ingest -

A custom Python application reads uploaded data les from a single server, writes to the data warehouse.

Compute:

Windows Server 2008 R2

- 16 CPUs

- 128 GB of RAM

- 10 TB local HDD storage

Application 2: Reporting -

An off the shelf application that business analysts use to run a daily report to see what equipment needs repair. Only 2 analysts of a team of 10 (5

west coast, 5 east coast) can connect to the reporting application at a time.

Compute:

Off the shelf application. License tied to number of physical CPUs

- Windows Server 2008 R2

- 16 CPUs

- 32 GB of RAM

- 500 GB HDD

Data warehouse:

A single PostgreSQL server

- RedHat Linux

- 64 CPUs

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

687/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

- 128 GB of RAM

- 4x 6TB HDD in RAID 0

Executive Statement -

Our competitive advantage has always been in our manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

undergo the next wave of transformations in our industry. My goals are to build our skills while addressing immediate market needs through

incremental innovations.

Question

For this question, refer to the TerramEarth case study. A new architecture that writes all incoming data to BigQuery has been introduced. You

notice that the data is dirty, and want to ensure data quality on an automated daily basis while managing cost.

What should you do?

A. Set up a streaming Cloud Data ow job, receiving data by the ingestion process. Clean the data in a Cloud Data ow pipeline.

B. Create a Cloud Function that reads data from BigQuery and cleans it. Trigger the Cloud Function from a Compute Engine instance.

C. Create a SQL statement on the data in BigQuery, and save it as a view. Run the view daily, and save the result to a new table.

D. Use Cloud Dataprep and con gure the BigQuery tables as the source. Schedule a daily job to clean the data.

Correct Answer: D

Community vote distribution

D (84%)

A (16%)

? ?  Sj10  Highly Voted ?  3áyears, 4ámonths ago
Option D, as data needs to be cleaned ..
Dataprep has the capabilities to clean dirty data

upvoted 31 times

? ?  motty 3áyears ago

dataprep is GUI driven process to analyse adhoc data dumped on GCS, it has not place in this use case

upvoted 5 times

? ?  melono 8ámonths, 1áweek ago

looks like D
https://cloud.google.com/dataprep

upvoted 2 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 13 times

? ?  vindahake  Highly Voted ?  3áyears, 3ámonths ago

automated daily ... answer is D

upvoted 12 times

? ?  red_panda  Most Recent ?  5áhours, 42áminutes ago

Selected Answer: D

D without any doubt.
Dataflow is for data elaboration. Dataprep is for data preparation (and cleaning).

upvoted 1 times

? ?  RVivek 4ámonths, 2áweeks ago

Selected Answer: D

B & C does not make sense.
A is costly and in realtime
The question says on daily basis and cost effective hence D

upvoted 4 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: D

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

688/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

D is ok

upvoted 1 times

? ?  cbarg 9ámonths, 3áweeks ago

Selected Answer: D

Ans is D. Please refer to this example: https://medium.com/google-cloud/how-to-schedule-a-bigquery-etl-job-with-dataprep-
b1c314883ab9
upvoted 4 times

? ?  ShadowLord 9ámonths, 3áweeks ago

Selected Answer: A

Options should be A.
1. Cost in D would be higher. e.g. First load dirty data into DB and then run Data Prep Jobs to clean the data and load into some different
target Data . Overall cost of scanning the data and the loading is like double the cost. Then identifying already clean data and dirty data is
again a challenge on daily basis after the data growth is significant
2. Data Stream can be utilized to cleanse the data while loading

upvoted 3 times

? ?  dayody 9ámonths, 1áweek ago

you cannot clean data with Dataflow only with Dataprep

upvoted 2 times

? ?  DrishaS4 11ámonths ago

Selected Answer: D

automated daily ... answer is D

upvoted 2 times

? ?  AzureDP900 11ámonths, 4áweeks ago

D is perfect to cleanup the data daily!

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: D

D is the correct answer

upvoted 1 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: D

Vote D

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 1 times

? ?  gonzalopf94 1áyear, 7ámonths ago

Option is A, Dataprep uses a UI to perform the cleaning process and under the hood it is using Dataflow to perform the process, so I will
go with A.

upvoted 4 times

? ?  [Removed] 1áyear, 8ámonths ago

A and D are both will solve the purpose. A is more expensive and ask is daily basis clean-up of data. D is right choice.

upvoted 2 times

? ?  ZappsterB 1áyear, 8ámonths ago

Should be A. Dirty data may not be formatted to suit the table structure and then won't go in to be 'cleansed' later.

upvoted 2 times

? ?  MaxNRG 1áyear, 8ámonths ago

D û use Cloud Dataprep and configure the BigQuery tables as the source. Schedule daily jobs to clean the data.
Cloud Dataprep û is for fast exploration and anomaly detection.
It supports scheduling (as Q asks): ôSchedule the execution of recipes in your flows on a recurring or as-needed basis. When the scheduled
job successfully executes, you can collect the wrangled output in the specified output location, where it is available in the published form
you specifyö.
Also, Dataprep integrates naturally with BigQuery (and Cloud Storage, upload file directly). And it uses Dataflow under the hood.
A, B û are about real-time processing, which is not needed (per req on daily basis). Also, to find if data is dirty you may analyze several
adjacent rows, so real-time processing may not physically solve a problem;

upvoted 5 times

? ?  MaxNRG 1áyear, 8ámonths ago

C û is about just SQL processing, which is likely not enough to fix algorithmically data problems. Also, it is not automated, and requires
new table for cleaned data.
Pricing-wise, option A and D should be comparable, since Dataprep uses underneath Dataflow workers.
Dataprep pricing (0.056$ vCPU per hour û batch worker)

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

689/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Dataflow pricing (0.069$ x 4 vCPU per hour, for streaming worker)
In general, batch processing is more price-effective, since it eliminates potential data-waiting cycles. So, batched vCPUs should be 100%
busy by your work, though streaming worker can be idle, but you still need to pay for its time.

upvoted 1 times

? ?  ShadowLord 9ámonths, 3áweeks ago

How about Bigquery cost which would be double in afterwards scanning with Options D

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

690/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company Overview -

Topic 9

TerramEarth manufactures heavy equipment for the mining and agricultural industries. About 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second, with

22 hours of operation per day, TerramEarth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux and Windows-based systems that reside in a single U.S, west coast based data center.

These systems gzip CSV  les from the  eld and upload via FTP, and place the data in their data warehouse. Because this process takes time,

aggregated reports are based on data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers

Technical Requirements -

Expand beyond a single datacenter to decrease latency to the American midwest and east coast

Create a backup strategy

Increase security of data transfer from equipment to the datacenter

Improve data in the data warehouse

Use customer and equipment data to anticipate customer needs

Application 1: Data ingest -

A custom Python application reads uploaded data les from a single server, writes to the data warehouse.

Compute:

Windows Server 2008 R2

- 16 CPUs

- 128 GB of RAM

- 10 TB local HDD storage

Application 2: Reporting -

An off the shelf application that business analysts use to run a daily report to see what equipment needs repair. Only 2 analysts of a team of 10 (5

west coast, 5 east coast) can connect to the reporting application at a time.

Compute:

Off the shelf application. License tied to number of physical CPUs

- Windows Server 2008 R2

- 16 CPUs

- 32 GB of RAM

- 500 GB HDD

Data warehouse:

A single PostgreSQL server

- RedHat Linux

- 64 CPUs

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

691/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

- 128 GB of RAM

- 4x 6TB HDD in RAID 0

Executive Statement -

Our competitive advantage has always been in our manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

undergo the next wave of transformations in our industry. My goals are to build our skills while addressing immediate market needs through

incremental innovations.

Question

For this question, refer to the TerramEarth case study. Considering the technical requirements, how should you reduce the unplanned vehicle

downtime in GCP?

A. Use BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud Pub/Sub and Cloud

Data ow. Use Google Data Studio for analysis and reporting.

B. Use BigQuery as the data warehouse. Connect all vehicles to the network and upload gzip  les to a Multi-Regional Cloud Storage bucket

using gcloud. Use Google Data Studio for analysis and reporting.

C. Use Cloud Dataproc Hive as the data warehouse. Upload gzip  les to a Multi-Regional Cloud Storage bucket. Upload this data into BigQuery

using gcloud. Use Google Data Studio for analysis and reporting.

D. Use Cloud Dataproc Hive as the data warehouse. Directly stream data into partitioned Hive tables. Use Pig scripts to analyze data.

Correct Answer: A

Community vote distribution

A (100%)

? ?  balajee14  Highly Voted ?  3áyears, 2ámonths ago

Definitely A

upvoted 39 times

? ?  AdityaGupta 2áyears, 8ámonths ago

Once all the vehicle are connected to network, there is no need to use FTP; data can be ingested directly to BQ using Pub/Sub and
DataFlow.

upvoted 6 times

? ?  PRC  Highly Voted ?  3áyears, 2ámonths ago

A is good...simple streaming of data with managed services approach

upvoted 10 times

? ?  Aninina  Most Recent ?  6ámonths, 2áweeks ago

Selected Answer: A

A looks like the correct one

upvoted 1 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: A

A is ok

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is good

upvoted 1 times

? ?  cbarg 9ámonths, 3áweeks ago

Selected Answer: A

Ans is A.

upvoted 1 times

? ?  AzureDP900 11ámonths, 4áweeks ago

A is right, all other options doesn't make sense.

upvoted 1 times

? ?  H_S 1áyear ago

Selected Answer: A

Definitely A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

692/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  [Removed] 1áyear, 2ámonths ago

A should be better.

https://cloud.google.com/architecture/designing-connected-vehicle-platform#data_ingestion

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

A. Use BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud Pub/Sub and
Cloud Dataflow. Use Google Data Studio for analysis and reporting.

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 4 times

? ?  Ausias18 2áyears, 2ámonths ago

answer is A

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

A is ok

upvoted 1 times

? ?  sekhrivijay 2áyears, 4ámonths ago

Technical requirement : Create a backup strategy

Is bigquery a suitable system for data backup . Wouldn't a better system for backup be cloud storage.

Only B has that option

upvoted 1 times

? ?  OSNG 2áyears, 6ámonths ago

A is correct, using dataflow to clean and/or convert the data for analysis makes more sense.

B does not show any sign of how data will be loaded to bigquery (as gzip) or after conversion, it seems broken process to me.

upvoted 2 times

? ?  AdityaGupta 2áyears, 8ámonths ago

Once all the vehicle are connected to network, there is no need to use FTP; data can be ingested directly to BQ using Pub/Sub and
DataFlow.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

693/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company Overview -

Topic 9

TerramEarth manufactures heavy equipment for the mining and agricultural industries. About 80% of their business is from mining and 20% from

agriculture. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their

customers more productive.

Solution Concept -

There are 20 million TerramEarth vehicles in operation that collect 120  elds of data per second. Data is stored locally on the vehicle and can be

accessed for analysis when a vehicle is serviced. The data is downloaded via a maintenance port. This same port can be used to adjust

operational parameters, allowing the vehicles to be upgraded in the  eld with new computing modules.

Approximately 200,000 vehicles are connected to a cellular network, allowing TerramEarth to collect data directly. At a rate of 120  elds of data

per second, with

22 hours of operation per day, TerramEarth collects a total of about 9 TB/day from these connected vehicles.

Existing Technical Environment -

TerramEarth's existing architecture is composed of Linux and Windows-based systems that reside in a single U.S, west coast based data center.

These systems gzip CSV  les from the  eld and upload via FTP, and place the data in their data warehouse. Because this process takes time,

aggregated reports are based on data that is 3 weeks old.

With this data, TerramEarth has been able to preemptively stock replacement parts and reduce unplanned downtime of their vehicles by 60%.

However, because the data is stale, some customers are without their vehicles for up to 4 weeks while they wait for replacement parts.

Business Requirements -

Decrease unplanned vehicle downtime to less than 1 week

Support the dealer network with more data on how their customers use their equipment to better position new products and services

Have the ability to partner with different companies " especially with seed and fertilizer suppliers in the fast-growing agricultural business " to

create compelling joint offerings for their customers

Technical Requirements -

Expand beyond a single datacenter to decrease latency to the American midwest and east coast

Create a backup strategy

Increase security of data transfer from equipment to the datacenter

Improve data in the data warehouse

Use customer and equipment data to anticipate customer needs

Application 1: Data ingest -

A custom Python application reads uploaded data les from a single server, writes to the data warehouse.

Compute:

Windows Server 2008 R2

- 16 CPUs

- 128 GB of RAM

- 10 TB local HDD storage

Application 2: Reporting -

An off the shelf application that business analysts use to run a daily report to see what equipment needs repair. Only 2 analysts of a team of 10 (5

west coast, 5 east coast) can connect to the reporting application at a time.

Compute:

Off the shelf application. License tied to number of physical CPUs

- Windows Server 2008 R2

- 16 CPUs

- 32 GB of RAM

- 500 GB HDD

Data warehouse:

A single PostgreSQL server

- RedHat Linux

- 64 CPUs

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

694/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

- 128 GB of RAM

- 4x 6TB HDD in RAID 0

Executive Statement -

Our competitive advantage has always been in our manufacturing process, with our ability to build better vehicles for lower cost than our

competitors. However, new products with different approaches are constantly being developed, and I'm concerned that we lack the skills to

undergo the next wave of transformations in our industry. My goals are to build our skills while addressing immediate market needs through

incremental innovations.

Question

For this question, refer to the TerramEarth case study. You are asked to design a new architecture for the ingestion of the data of the 200,000

vehicles that are connected to a cellular network. You want to follow Google-recommended practices.

Considering the technical requirements, which components should you use for the ingestion of the data?

A. Google Kubernetes Engine with an SSL Ingress

B. Cloud IoT Core with public/private key pairs

C. Compute Engine with project-wide SSH keys

D. Compute Engine with speci c SSH keys

Correct Answer: B

Community vote distribution

B (100%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

Why not B?

upvoted 30 times

? ?  nitinz 2áyears, 3ámonths ago

It is B

upvoted 4 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 10 times

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

It's B:

https://cloud.google.com/iot-core/?utm_source=google&utm_medium=cpc&utm_campaign=emea-es-all-es-dr-bkws-all-all-trial-e-gcp-
1007176&utm_content=text-ad-none-any-DEV_c-CRE_253501321408-
ADGP_Hybrid+%7C+AW+SEM+%7C+BKWS+~+EXA_M:1_ES_ES_Cloud_IoT_IoT+Core_ES+Localisation-KWID_43700025425525852-kwd-
316837064614-userloc_20297&utm_term=KW_google%20iot%20cloud-
ST_google+iot+cloud&ds_rl=1242853&ds_rl=1245734&ds_rl=1245734&gclid=CjwKCAjw3c_tBRA4EiwAICs8CqQNooCVeuVB9Ki6V9e3PRdnJLa
7LcEce9arjbqUkeqo0jg5F0_Z5BoC3csQAvD_BwE

upvoted 18 times

? ?  dataqueen_3110  Most Recent ?  4ámonths, 3áweeks ago

Google Cloud IoT Core is being retired on August 16, 2023

upvoted 6 times

? ?  megumin 7ámonths, 3áweeks ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  AzureDP900 11ámonths, 4áweeks ago

IoT core is fine.. B is right!

upvoted 2 times

? ?  H_S 1áyear ago

Selected Answer: B

It's B

upvoted 2 times

? ?  Bobch 1áyear, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

695/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: B

B is correct

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: B

B is the correct answer

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  MaxNRG 1áyear, 8ámonths ago

B û Cloud IoT Core with public / private key pairs.
https://cloud.google.com/iot-core/
IoT Core was developed for connecting existing devices spread around the world to GCP. Also, it supports end-to-end security using
asymmetric key authentication over TLS 1.2. So, this is exact match for Q.

upvoted 3 times

? ?  victory108 1áyear, 11ámonths ago

B. Cloud IoT Core with public/private key pairs

upvoted 4 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 3 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

IMO B is ok

upvoted 1 times

? ?  ashish9_a 2áyears, 3ámonths ago

All options look okay but B tops them all.

upvoted 1 times

? ?  AD3 2áyears, 3ámonths ago

'B' is more correct even though the technical requirement doesn't clearly say about the new technology, the executive summary does say
it "transformation of technology".

upvoted 1 times

? ?  guid1984 2áyears, 4ámonths ago

Keywords "You are asked to design a new Architecture" Cloud IOT core is the way for this requirement

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

696/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 10 - Testlet 7

Question #1

Introductory Info

Company overview -

Topic 10

TerramEarth manufactures heavy equipment for the mining and agricultural industries. They currently have over 500 dealers and service centers in

100 countries.

Their mission is to build products that make their customers more productive.

Solution concept -

There are 2 million TerramEarth vehicles in operation currently, and we see 20% yearly growth. Vehicles collect telemetry data from many sensors

during operation. A small subset of critical data is transmitted from the vehicles in real time to facilitate  eet management. The rest of the sensor

data is collected, compressed, and uploaded daily when the vehicles return to home base. Each vehicle usually generates 200 to 500 megabytes

of data per day.

Existing technical environment -

TerramEarth's vehicle data aggregation and analysis infrastructure resides in Google Cloud and serves clients from all around the world. A growing

amount of sensor data is captured from their two main manufacturing plants and sent to private data centers that contain their legacy inventory

and logistics management systems. The private data centers have multiple network interconnects con gured to Google Cloud. The web frontend

for dealers and customers is running in

Google Cloud and allows access to stock management and analytics.

Business requirements -

* Predict and detect vehicle malfunction and rapidly ship parts to dealerships for just-in-time repair where possible.

* Decrease cloud operational costs and adapt to seasonality.

* Increase speed and reliability of development work ow.

* Allow remote developers to be productive without compromising code or data security.

* Create a  exible and scalable platform for developers to create custom API services for dealers and partners.

Technical requirements -

* Create a new abstraction layer for HTTP API access to their legacy systems to enable a gradual move into the cloud without disrupting

operations.

* Modernize all CI/CD pipelines to allow developers to deploy container-based workloads in highly scalable environments.

* Allow developers to run experiments without compromising security and governance requirements.

* Create a self-service portal for internal and partner developers to create new projects, request resources for data analytics jobs, and centrally

manage access to the API endpoints.

* Use cloud-native solutions for keys and secrets management and optimize for identity-based access.

* Improve and standardize tools necessary for application and network monitoring and troubleshooting.

Executive statement -

Our competitive advantage has always been our focus on the customer, with our ability to provide excellent customer service and minimize vehicle

downtimes.

After moving multiple systems into Google Cloud, we are seeking new ways to provide best-in-class online  eet management services to our

customers and improve operations of our dealerships. Our 5-year strategic plan is to create a partner ecosystem of new products by enabling

access to our data, increasing autonomous operation capabilities of our vehicles, and creating a path to move the remaining legacy systems to

the cloud.

Question

For this question, refer to the TerramEarth case study. You start to build a new application that uses a few Cloud Functions for the backend. One

use case requires a Cloud Function func_display to invoke another Cloud Function func_query. You want func_query only to accept invocations

from func_display. You also want to follow Google's recommended best practices. What should you do?

A. Create a token and pass it in as an environment variable to func_display. When invoking func_query, include the token in the request. Pass

the same token to func_query and reject the invocation if the tokens are different.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

697/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B. Make func_query 'Require authentication.' Create a unique service account and associate it to func_display. Grant the service account

invoker role for func_query. Create an id token in func_display and include the token to the request when invoking func_query.

C. Make func_query 'Require authentication' and only accept internal tra c. Create those two functions in the same VPC. Create an ingress

 rewall rule for func_query to only allow tra c from func_display.

D. Create those two functions in the same project and VPC. Make func_query only accept internal tra c. Create an ingress  rewall for

func_query to only allow tra c from func_display. Also, make sure both functions use the same service account.

Correct Answer: B

Community vote distribution

B (100%)

? ?  raf2121  Highly Voted ?  1áyear, 10ámonths ago

B

Authentication function to function calls. Add calling function service account as a member on the receiving function and grant that
member the cloud functions invoker
https://cloud.google.com/functions/docs/securing/authenticating

upvoted 16 times

? ?  MaxNRG  Highly Voted ?  1áyear, 8ámonths ago

B is correct. You need both service account (Authorization) and id token (Authentication)
When building services that connect multiple functions, it's a good idea to ensure that each function can only send requests to a specific
subset of your other functions. For instance, if you have a login function, it should be able to access the user profiles function, but it
probably shouldn't be able to access the search function.
To configure the receiving function to accept requests from a specific calling function, you need to grant the Cloud Functions Invoker
(roles/cloudfunctions.invoker) role to the calling function's service account on the receiving function.

upvoted 10 times

? ?  MaxNRG 1áyear, 8ámonths ago

Because it will be invoking the receiving function, the calling function must also provide a Google-signed ID token to authenticate. This
is a two step process:
1. Create a Google-signed ID token with the audience field (aud) set to the URL of the receiving function.
2. Include the ID token in an Authorization: Bearer ID_TOKEN header in the request to the function.
https://cloud.google.com/functions/docs/securing/authenticating#authenticating_function_to_function_calls
Authentication function to function calls. Add calling function service account as a member on the receiving function and grant that
member the cloud functions invoker.

upvoted 6 times

? ?  MaxNRG 1áyear, 8ámonths ago

Have the account you are using to access Cloud Functions assigned a role that contains the cloudfunctions.functions.invoke
permission. By default, the Cloud Functions Admin and Cloud Functions Developer roles have this permission.
https://cloud.google.com/functions/docs/securing/authenticating
Depending on who or what is invoking your cloud function the process for setting up authentication will vary, however there are two
requirements common to all types of authentication:
1. The person or service authorized to invoke the cloud function must be assigned the cloudfunctions.invoker role or some other
role with the cloudfunctions.invoke permission.
2. The person or service authorized to invoke the cloud function must send a token along with the HTTP request to prove that they
are authorized to invoke the cloud function.
https://dev.to/jakewitcher/setting-up-authorization-for-http-cloud-functions-in-gcp-45bc

upvoted 2 times

? ?  megumin  Most Recent ?  7ámonths, 1áweek ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Nirca 9ámonths, 2áweeks ago

Selected Answer: B

service account - B!

upvoted 1 times

? ?  satamex 11ámonths ago
Just checking why not A?

upvoted 1 times

? ?  AzureDP900 11ámonths, 4áweeks ago

B makes sense without too much thinking.. This is strict enforcement..

upvoted 1 times

? ?  mad314 1áyear, 2ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

698/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: B

Had this question on my exam.

upvoted 3 times

? ?  Pime13 1áyear, 6ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  victory108 1áyear, 10ámonths ago

B. Make func_query 'Require authentication.' Create a unique service account and associate it to func_display. Grant the service account
invoker role for func_query. Create an id token in func_display and include the token to the request when invoking func_query.

upvoted 7 times

? ?  SweetieS 1áyear, 10ámonths ago

B is OK

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

699/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company overview -

Topic 10

TerramEarth manufactures heavy equipment for the mining and agricultural industries. They currently have over 500 dealers and service centers in

100 countries.

Their mission is to build products that make their customers more productive.

Solution concept -

There are 2 million TerramEarth vehicles in operation currently, and we see 20% yearly growth. Vehicles collect telemetry data from many sensors

during operation. A small subset of critical data is transmitted from the vehicles in real time to facilitate  eet management. The rest of the sensor

data is collected, compressed, and uploaded daily when the vehicles return to home base. Each vehicle usually generates 200 to 500 megabytes

of data per day.

Existing technical environment -

TerramEarth's vehicle data aggregation and analysis infrastructure resides in Google Cloud and serves clients from all around the world. A growing

amount of sensor data is captured from their two main manufacturing plants and sent to private data centers that contain their legacy inventory

and logistics management systems. The private data centers have multiple network interconnects con gured to Google Cloud. The web frontend

for dealers and customers is running in

Google Cloud and allows access to stock management and analytics.

Business requirements -

* Predict and detect vehicle malfunction and rapidly ship parts to dealerships for just-in-time repair where possible.

* Decrease cloud operational costs and adapt to seasonality.

* Increase speed and reliability of development work ow.

* Allow remote developers to be productive without compromising code or data security.

* Create a  exible and scalable platform for developers to create custom API services for dealers and partners.

Technical requirements -

* Create a new abstraction layer for HTTP API access to their legacy systems to enable a gradual move into the cloud without disrupting

operations.

* Modernize all CI/CD pipelines to allow developers to deploy container-based workloads in highly scalable environments.

* Allow developers to run experiments without compromising security and governance requirements.

* Create a self-service portal for internal and partner developers to create new projects, request resources for data analytics jobs, and centrally

manage access to the API endpoints.

* Use cloud-native solutions for keys and secrets management and optimize for identity-based access.

* Improve and standardize tools necessary for application and network monitoring and troubleshooting.

Executive statement -

Our competitive advantage has always been our focus on the customer, with our ability to provide excellent customer service and minimize vehicle

downtimes.

After moving multiple systems into Google Cloud, we are seeking new ways to provide best-in-class online  eet management services to our

customers and improve operations of our dealerships. Our 5-year strategic plan is to create a partner ecosystem of new products by enabling

access to our data, increasing autonomous operation capabilities of our vehicles, and creating a path to move the remaining legacy systems to

the cloud.

Question

For this question, refer to the TerramEarth case study. You have broken down a legacy monolithic application into a few containerized RESTful

microservices.

You want to run those microservices on Cloud Run. You also want to make sure the services are highly available with low latency to your

customers. What should you do?

A. Deploy Cloud Run services to multiple availability zones. Create Cloud Endpoints that point to the services. Create a global HTTP(S) Load

Balancing instance and attach the Cloud Endpoints to its backend.

B. Deploy Cloud Run services to multiple regions. Create serverless network endpoint groups pointing to the services. Add the serverless

NEGs to a backend service that is used by a global HTTP(S) Load Balancing instance.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

700/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Deploy Cloud Run services to multiple regions. In Cloud DNS, create a latency-based DNS name that points to the services.

D. Deploy Cloud Run services to multiple availability zones. Create a TCP/IP global load balancer. Add the Cloud Run Endpoints to its backend

service.

Correct Answer: C

Community vote distribution

B (95%)

5%

? ?  fahad01hbti  Highly Voted ?  1áyear, 10ámonths ago

B
https://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts

upvoted 24 times

? ?  rishab86 1áyear, 8ámonths ago

B is correct

upvoted 4 times

? ?  MaxNRG  Highly Voted ?  1áyear, 8ámonths ago

B is correct.
Cloud Run is a regional service.
To serve global users you need to configure a Global HTTP LB and NEG as the backend.
Cloud Run services are deployed into individual regions and to route your users to different regions of your service, you need to configure
external HTTP(S) Load Balancing.
https://cloud.google.com/run/docs/multiple-regions
A network endpoint group (NEG) specifies a group of backend endpoints for a load balancer.
A serverless NEG is a backend that points to a Cloud Run, App Engine, or Cloud Functions service.
https://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts

upvoted 20 times

? ?  Shin9412  Most Recent ?  1ámonth, 3áweeks ago

Selected Answer: B

My guess is B
upvoted 1 times

? ?  omermahgoub 6ámonths ago

The correct answer is B. Deploying the microservices to multiple regions will ensure high availability, as it will allow the services to
continue running even if one region experiences an outage. Creating serverless network endpoint groups pointing to the services and
adding them to a backend service used by a global HTTP(S) Load Balancer will allow the load balancer to route traffic to the closest region,
reducing latency for customers.

upvoted 1 times

? ?  greyhats13 6ámonths, 1áweek ago

Why the answer is not A refer to this diagram
https://www.techgeeknext.com/google-cloud-architect/terramearth-case-study

upvoted 2 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: B

B is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: B

B is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: B

correct answer is B https://cloud.google.com/run/docs/multiple-regions

upvoted 1 times

? ?  zellck 9ámonths, 2áweeks ago

Selected Answer: B

B is the answer.

https://cloud.google.com/load-balancing/docs/negs/serverless-neg-concepts
A serverless NEG is a backend that points to a Cloud Run, App Engine, Cloud Functions, or API Gateway service.

upvoted 1 times

? ?  AzureDP900 11ámonths, 4áweeks ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

701/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B is fine.. I agree with others ..

upvoted 1 times

? ?  mad314 1áyear, 2ámonths ago

Selected Answer: B

Had this question on my exam.

upvoted 6 times

? ?  zxcv1234 1áyear, 6ámonths ago

Selected Answer: B

B is correct. https://cloud.google.com/run/docs/multiple-regions

upvoted 2 times

? ?  Bobch 1áyear, 6ámonths ago

Selected Answer: B

B is OK

upvoted 2 times

? ?  SamGCP 1áyear, 6ámonths ago

Selected Answer: B

https://cloud.google.com/run/docs/multiple-regions

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  Arad 1áyear, 6ámonths ago

Selected Answer: B

B is correct.

upvoted 1 times

? ?  [Removed] 1áyear, 6ámonths ago

Selected Answer: B

Marked C is wrong.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

702/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company overview -

Topic 10

TerramEarth manufactures heavy equipment for the mining and agricultural industries. They currently have over 500 dealers and service centers in

100 countries.

Their mission is to build products that make their customers more productive.

Solution concept -

There are 2 million TerramEarth vehicles in operation currently, and we see 20% yearly growth. Vehicles collect telemetry data from many sensors

during operation. A small subset of critical data is transmitted from the vehicles in real time to facilitate  eet management. The rest of the sensor

data is collected, compressed, and uploaded daily when the vehicles return to home base. Each vehicle usually generates 200 to 500 megabytes

of data per day.

Existing technical environment -

TerramEarth's vehicle data aggregation and analysis infrastructure resides in Google Cloud and serves clients from all around the world. A growing

amount of sensor data is captured from their two main manufacturing plants and sent to private data centers that contain their legacy inventory

and logistics management systems. The private data centers have multiple network interconnects con gured to Google Cloud. The web frontend

for dealers and customers is running in

Google Cloud and allows access to stock management and analytics.

Business requirements -

* Predict and detect vehicle malfunction and rapidly ship parts to dealerships for just-in-time repair where possible.

* Decrease cloud operational costs and adapt to seasonality.

* Increase speed and reliability of development work ow.

* Allow remote developers to be productive without compromising code or data security.

* Create a  exible and scalable platform for developers to create custom API services for dealers and partners.

Technical requirements -

* Create a new abstraction layer for HTTP API access to their legacy systems to enable a gradual move into the cloud without disrupting

operations.

* Modernize all CI/CD pipelines to allow developers to deploy container-based workloads in highly scalable environments.

* Allow developers to run experiments without compromising security and governance requirements.

* Create a self-service portal for internal and partner developers to create new projects, request resources for data analytics jobs, and centrally

manage access to the API endpoints.

* Use cloud-native solutions for keys and secrets management and optimize for identity-based access.

* Improve and standardize tools necessary for application and network monitoring and troubleshooting.

Executive statement -

Our competitive advantage has always been our focus on the customer, with our ability to provide excellent customer service and minimize vehicle

downtimes.

After moving multiple systems into Google Cloud, we are seeking new ways to provide best-in-class online  eet management services to our

customers and improve operations of our dealerships. Our 5-year strategic plan is to create a partner ecosystem of new products by enabling

access to our data, increasing autonomous operation capabilities of our vehicles, and creating a path to move the remaining legacy systems to

the cloud.

Question

For this question, refer to the TerramEarth case study. You are migrating a Linux-based application from your private data center to Google Cloud.

The

TerramEarth security team sent you several recent Linux vulnerabilities published by Common Vulnerabilities and Exposures (CVE). You need

assistance in understanding how these vulnerabilities could impact your migration. What should you do? (Choose two.)

A. Open a support case regarding the CVE and chat with the support engineer.

B. Read the CVEs from the Google Cloud Status Dashboard to understand the impact.

C. Read the CVEs from the Google Cloud Platform Security Bulletins to understand the impact.

D. Post a question regarding the CVE in Stack Over ow to get an explanation.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

703/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

E. Post a question regarding the CVE in a Google Cloud discussion group to get an explanation.

Correct Answer: AD

Community vote distribution

AC (90%)

10%

? ?  victory108  Highly Voted ?  1áyear, 9ámonths ago

A. Open a support case regarding the CVE and chat with the support engineer.
C. Read the CVEs from the Google Cloud Platform Security Bulletins to understand the impact.

upvoted 26 times

? ?  pakilodi  Highly Voted ?  1áyear, 6ámonths ago

Selected Answer: AC

A&C. Though if in real life we will do D :-)

upvoted 19 times

? ?  Risaa  Most Recent ?  1áweek, 5ádays ago

I think C&E

upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: AC

A , C is the correct answer

upvoted 2 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: AC

AC are ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: AC

best answers are A & C

upvoted 1 times

? ?  Nirca 11ámonths ago

Selected Answer: AC

we are OK with AC!

upvoted 2 times

? ?  binpan 11ámonths, 2áweeks ago

Selected Answer: AD

This is about vulnerabilities in the Linux based application of Terram Earth. You will not find anything in Google dashboards and bulletins
for these. Option A and D are correct as that would be the most logical steps.

upvoted 4 times

? ?  medi01 2ámonths, 1áweek ago

Crazy that nonsensical post like that got 4 upvotes...

upvoted 1 times

? ?  RitwickKumar 10ámonths, 1áweek ago

CVEs are known vulnerabilities for open source and are not specific to Terram Earth. The details are available in Google Cloud Platform
Security Bulletins:
https://cloud.google.com/support/bulletins

upvoted 3 times

? ?  AzureDP900 11ámonths, 4áweeks ago

A,C is right

upvoted 1 times

? ?  JoeyCASD 1áyear, 1ámonth ago

I laugh so hard when they reveal the answer including D
Vote A and C

upvoted 5 times

? ?  satamex 11ámonths ago

:P Stackoverflow in GCP PCA exam !! really?? !!

upvoted 3 times

? ?  mad314 1áyear, 2ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

704/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: AC

Had this question on my exam.

upvoted 4 times

? ?  cloudmon 1áyear, 2ámonths ago

The question does not provide sufficient details. Option A assumes that the customer has a support package that is more than just basic
support (because with basic support, engineers would only respond to billing requests). Customers with basic support are advised to
consult StackExchange or Google Discussion groups.
https://cloud.google.com/support
https://cloud.google.com/support/docs/community

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: AC

I vote A&C.

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: AC

Correct answer is A & C

upvoted 2 times

? ?  [Removed] 1áyear, 6ámonths ago

Selected Answer: AC

D marked is wrong

upvoted 2 times

? ?  mudot 1áyear, 7ámonths ago

Selected Answer: AC

seems .. whoever answered the questions always gets solutions from stackoverflow

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: AC

vote AC

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

705/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company overview -

Topic 10

TerramEarth manufactures heavy equipment for the mining and agricultural industries. They currently have over 500 dealers and service centers in

100 countries.

Their mission is to build products that make their customers more productive.

Solution concept -

There are 2 million TerramEarth vehicles in operation currently, and we see 20% yearly growth. Vehicles collect telemetry data from many sensors

during operation. A small subset of critical data is transmitted from the vehicles in real time to facilitate  eet management. The rest of the sensor

data is collected, compressed, and uploaded daily when the vehicles return to home base. Each vehicle usually generates 200 to 500 megabytes

of data per day.

Existing technical environment -

TerramEarth's vehicle data aggregation and analysis infrastructure resides in Google Cloud and serves clients from all around the world. A growing

amount of sensor data is captured from their two main manufacturing plants and sent to private data centers that contain their legacy inventory

and logistics management systems. The private data centers have multiple network interconnects con gured to Google Cloud. The web frontend

for dealers and customers is running in

Google Cloud and allows access to stock management and analytics.

Business requirements -

* Predict and detect vehicle malfunction and rapidly ship parts to dealerships for just-in-time repair where possible.

* Decrease cloud operational costs and adapt to seasonality.

* Increase speed and reliability of development work ow.

* Allow remote developers to be productive without compromising code or data security.

* Create a  exible and scalable platform for developers to create custom API services for dealers and partners.

Technical requirements -

* Create a new abstraction layer for HTTP API access to their legacy systems to enable a gradual move into the cloud without disrupting

operations.

* Modernize all CI/CD pipelines to allow developers to deploy container-based workloads in highly scalable environments.

* Allow developers to run experiments without compromising security and governance requirements.

* Create a self-service portal for internal and partner developers to create new projects, request resources for data analytics jobs, and centrally

manage access to the API endpoints.

* Use cloud-native solutions for keys and secrets management and optimize for identity-based access.

* Improve and standardize tools necessary for application and network monitoring and troubleshooting.

Executive statement -

Our competitive advantage has always been our focus on the customer, with our ability to provide excellent customer service and minimize vehicle

downtimes.

After moving multiple systems into Google Cloud, we are seeking new ways to provide best-in-class online  eet management services to our

customers and improve operations of our dealerships. Our 5-year strategic plan is to create a partner ecosystem of new products by enabling

access to our data, increasing autonomous operation capabilities of our vehicles, and creating a path to move the remaining legacy systems to

the cloud.

Question

For this question, refer to the TerramEarth case study. TerramEarth has a legacy web application that you cannot migrate to cloud. However, you

still want to build a cloud-native way to monitor the application. If the application goes down, you want the URL to point to a "Site is unavailable"

page as soon as possible. You also want your Ops team to receive a noti cation for the issue. You need to build a reliable solution for minimum

cost. What should you do?

A. Create a scheduled job in Cloud Run to invoke a container every minute. The container will check the application URL. If the application is

down, switch the URL to the "Site is unavailable" page, and notify the Ops team.

B. Create a cron job on a Compute Engine VM that runs every minute. The cron job invokes a Python program to check the application URL. If

the application is down, switch the URL to the "Site is unavailable" page, and notify the Ops team.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

706/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Create a Cloud Monitoring uptime check to validate the application URL. If it fails, put a message in a Pub/Sub queue that triggers a Cloud

Function to switch the URL to the "Site is unavailable" page, and notify the Ops team.

D. Use Cloud Error Reporting to check the application URL. If the application is down, switch the URL to the "Site is unavailable" page, and

notify the Ops team.

Correct Answer: A

Community vote distribution

C (100%)

? ?  raf2121  Highly Voted ?  1áyear, 10ámonths ago

C
Cloud monitoring for Uptime check to validate the application URL and leverage pub/sub to trigger Cloud Function to switch URL
https://cloud.google.com/monitoring/uptime-checks?hl=en

upvoted 25 times

? ?  fahad01hbti 1áyear, 10ámonths ago

will cloud monitoring work for on prem app without installing stackdriver agent ?

upvoted 2 times

? ?  MikeB19 1áyear, 10ámonths ago

Looks like cloud monitor will work on prem .. c looks correct
https://cloud.google.com/architecture/monitoring-on-premises-resources-with-blue-medora

upvoted 3 times

? ?  ashish_t 1áyear, 8ámonths ago

@raf2121,
You had submitted this question, so you should update the Suggested Answer accordingly.
Such wrong answer configurations creates confusion.
Anyways thanks for submitting the questions.

upvoted 2 times

? ?  BiddlyBdoyng  Most Recent ?  1áweek, 2ádays ago

If A is the correct answer what;s the point in the uptime checks in cloud monitoring? Why wouldn't we always use Clour Run for this
purpose?

upvoted 1 times

? ?  stfnz 1ámonth, 2áweeks ago

I understand now... the key is in the question "you still want to build a cloud-native way "... hence only option C, even if A is the cheapest, it
is not cloud-native way.

upvoted 1 times

? ?  stfnz 1ámonth, 2áweeks ago

though if you think about it really hard - Option A *does* the job and does it at *MINIMAL* cost... so it should be A?

upvoted 1 times

? ?  stfnz 1ámonth, 2áweeks ago

Selected Answer: C

should be C...
upvoted 1 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: C

C is the correct answer

upvoted 1 times

? ?  megumin 7ámonths, 1áweek ago

Selected Answer: C

C is ok

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: C

C is correct https://cloud.google.com/blog/products/management-tools/how-to-use-pubsub-as-a-cloud-monitoring-notification-channel

upvoted 1 times

? ?  AzureDP900 11ámonths, 4áweeks ago

C is right

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

707/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  mad314 1áyear, 2ámonths ago

Selected Answer: C

Had this question on my exam.

upvoted 4 times

? ?  brushek 1áyear, 5ámonths ago

Selected Answer: C

use cloud monitoring

upvoted 2 times

? ?  ttosl 1áyear, 6ámonths ago

Selected Answer: C

Cloud monitor uptime check can check http(s) outside of GCP. This is different from getting other matric like CPU, mem etc.

upvoted 2 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: C

C is the correct answer

upvoted 2 times

? ?  [Removed] 1áyear, 6ámonths ago

Selected Answer: C

Marked A is wrong.
Cloud Monitoring is right for uptime checks.

upvoted 2 times

? ?  pakilodi 1áyear, 6ámonths ago

Selected Answer: C

Select C

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 1 times

? ?  Firask 1áyear, 7ámonths ago

Ans. A is low cost solution as required in the question rather than add Pub/Sub that trigger cloud function.

upvoted 2 times

? ?  Examtaker2020 1áyear, 6ámonths ago

Creating a scheduled job in Cloud Run to invoke a container every minute. Is costly as this for every minute. Rather, triggering a cloud
function via Pub/Sub will only run when application URL fails. This will be the cheaper option.

C is the correct answer.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

708/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company overview -

Topic 10

TerramEarth manufactures heavy equipment for the mining and agricultural industries. They currently have over 500 dealers and service centers in

100 countries.

Their mission is to build products that make their customers more productive.

Solution concept -

There are 2 million TerramEarth vehicles in operation currently, and we see 20% yearly growth. Vehicles collect telemetry data from many sensors

during operation. A small subset of critical data is transmitted from the vehicles in real time to facilitate  eet management. The rest of the sensor

data is collected, compressed, and uploaded daily when the vehicles return to home base. Each vehicle usually generates 200 to 500 megabytes

of data per day.

Existing technical environment -

TerramEarth's vehicle data aggregation and analysis infrastructure resides in Google Cloud and serves clients from all around the world. A growing

amount of sensor data is captured from their two main manufacturing plants and sent to private data centers that contain their legacy inventory

and logistics management systems. The private data centers have multiple network interconnects con gured to Google Cloud. The web frontend

for dealers and customers is running in

Google Cloud and allows access to stock management and analytics.

Business requirements -

* Predict and detect vehicle malfunction and rapidly ship parts to dealerships for just-in-time repair where possible.

* Decrease cloud operational costs and adapt to seasonality.

* Increase speed and reliability of development work ow.

* Allow remote developers to be productive without compromising code or data security.

* Create a  exible and scalable platform for developers to create custom API services for dealers and partners.

Technical requirements -

* Create a new abstraction layer for HTTP API access to their legacy systems to enable a gradual move into the cloud without disrupting

operations.

* Modernize all CI/CD pipelines to allow developers to deploy container-based workloads in highly scalable environments.

* Allow developers to run experiments without compromising security and governance requirements.

* Create a self-service portal for internal and partner developers to create new projects, request resources for data analytics jobs, and centrally

manage access to the API endpoints.

* Use cloud-native solutions for keys and secrets management and optimize for identity-based access.

* Improve and standardize tools necessary for application and network monitoring and troubleshooting.

Executive statement -

Our competitive advantage has always been our focus on the customer, with our ability to provide excellent customer service and minimize vehicle

downtimes.

After moving multiple systems into Google Cloud, we are seeking new ways to provide best-in-class online  eet management services to our

customers and improve operations of our dealerships. Our 5-year strategic plan is to create a partner ecosystem of new products by enabling

access to our data, increasing autonomous operation capabilities of our vehicles, and creating a path to move the remaining legacy systems to

the cloud.

Question

For this question, refer to the TerramEarth case study. You are building a microservice-based application for TerramEarth. The application is based

on Docker containers. You want to follow Google-recommended practices to build the application continuously and store the build artifacts. What

should you do?

A. Con gure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build container images for each microservice, and tag

them using the code commit hash. Push the images to the Container Registry.

B. Con gure a trigger in Cloud Build for new source changes. The trigger invokes build jobs and build container images for the microservices.

Tag the images with a version number, and push them to Cloud Storage.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

709/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

C. Create a Scheduler job to check the repo every minute. For any new change, invoke Cloud Build to build container images for the

microservices. Tag the images using the current timestamp, and push them to the Container Registry.

D. Con gure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build one container image, and tag the image with the label

'latest.' Push the image to the Container Registry.

Correct Answer: B

Community vote distribution

A (100%)

? ?  meh_33  Highly Voted ?  1áyear, 10ámonths ago

https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_the_git_commit_hash
A is ok

upvoted 29 times

? ?  ashish_t 1áyear, 8ámonths ago

Just above that section, there is a section for the version number.
https://cloud.google.com/architecture/best-practices-for-building-containers#tagging_using_semantic_versioning

The difference between A and B is how it gets triggered.
A has "Invoke Cloud Build to build container images"
same with C and D.
B has "The trigger invokes build jobs"

Your pipeline should not have manual steps.
That's why I would choose B.
B is correct.

upvoted 7 times

? ?  cloudmon 1áyear, 2ámonths ago

B talks about pushing the images to Cloud Storage, which is not a best practice. A is correct

upvoted 9 times

? ?  KillerGoogle  Highly Voted ?  1áyear, 10ámonths ago

A, commit hash is required

upvoted 8 times

? ?  omermahgoub  Most Recent ?  6ámonths ago

A. Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build container images for each microservice, and tag
them using the code commit hash. Push the images to the Container Registry.

This option follows Google-recommended practices for building and storing the build artifacts for a microservice-based application. By
configuring a trigger in Cloud Build, you can automate the build process and ensure that the build artifacts are created whenever there
are new source changes. By tagging the images with the code commit hash, you can track the changes and have a record of the build
history. Finally, by storing the images in the Container Registry, you can manage and deploy the artifacts easily.

upvoted 3 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  Mahmoud_E 8ámonths, 1áweek ago

Selected Answer: A

A is correct

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is fine

upvoted 1 times

? ?  AzureDP900 11ámonths, 4áweeks ago

A is perfect

upvoted 2 times

? ?  omodara 1áyear ago

A is the correct answer. The question referred to Docker containers not cloud storage. https://cloud.google.com/architecture/best-
practices-for-building-containers#tagging_using_the_git_commit_hash

upvoted 2 times

? ?  amxexam 1áyear, 1ámonth ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

710/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

I will go with commit hash to tag inwages or time stamp A is better than C so A.

upvoted 2 times

? ?  mad314 1áyear, 2ámonths ago

Selected Answer: A

Had this question on my exam.

upvoted 3 times

? ?  sergaebi 1áyear, 2ámonths ago

Selected Answer: A

Vote for A

upvoted 2 times

? ?  Aiffone 1áyear, 5ámonths ago

I go with B rather than A because the trigger should invoke the build and versioning is a better way to tag rather than commit coments

upvoted 1 times

? ?  Bobch 1áyear, 6ámonths ago

Selected Answer: A

A is correct answer.
Google Cloud has two services for storing and managing container images such as Artifact Registry and Container Registry.
https://cloud.google.com/container-registry/docs/overview

upvoted 4 times

? ?  mesodan 1áyear, 4ámonths ago

Agree. B could be an option only if it wasn't for Cloud Storage which can't be used to store container images.

upvoted 3 times

? ?  SamGCP 1áyear, 6ámonths ago

Selected Answer: A

Correct answer is A since tagging using hash automates versioning. B cannot be the answer since it mentions Cloud Storage which is not
right.

upvoted 1 times

? ?  ttosl 1áyear, 6ámonths ago

Selected Answer: A

Tagging with latest will mess up pipeline when we deploy frequently.

upvoted 1 times

? ?  vincy2202 1áyear, 6ámonths ago

Selected Answer: A

A is the correct answer

upvoted 1 times

? ?  [Removed] 1áyear, 6ámonths ago

Selected Answer: A

A is correct. Tags needed on commit hash. Marked B is wrong

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

711/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company overview -

Topic 10

TerramEarth manufactures heavy equipment for the mining and agricultural industries. They currently have over 500 dealers and service centers in

100 countries.

Their mission is to build products that make their customers more productive.

Solution concept -

There are 2 million TerramEarth vehicles in operation currently, and we see 20% yearly growth. Vehicles collect telemetry data from many sensors

during operation. A small subset of critical data is transmitted from the vehicles in real time to facilitate  eet management. The rest of the sensor

data is collected, compressed, and uploaded daily when the vehicles return to home base. Each vehicle usually generates 200 to 500 megabytes

of data per day.

Existing technical environment -

TerramEarth's vehicle data aggregation and analysis infrastructure resides in Google Cloud and serves clients from all around the world. A growing

amount of sensor data is captured from their two main manufacturing plants and sent to private data centers that contain their legacy inventory

and logistics management systems. The private data centers have multiple network interconnects con gured to Google Cloud. The web frontend

for dealers and customers is running in

Google Cloud and allows access to stock management and analytics.

Business requirements -

* Predict and detect vehicle malfunction and rapidly ship parts to dealerships for just-in-time repair where possible.

* Decrease cloud operational costs and adapt to seasonality.

* Increase speed and reliability of development work ow.

* Allow remote developers to be productive without compromising code or data security.

* Create a  exible and scalable platform for developers to create custom API services for dealers and partners.

Technical requirements -

* Create a new abstraction layer for HTTP API access to their legacy systems to enable a gradual move into the cloud without disrupting

operations.

* Modernize all CI/CD pipelines to allow developers to deploy container-based workloads in highly scalable environments.

* Allow developers to run experiments without compromising security and governance requirements.

* Create a self-service portal for internal and partner developers to create new projects, request resources for data analytics jobs, and centrally

manage access to the API endpoints.

* Use cloud-native solutions for keys and secrets management and optimize for identity-based access.

* Improve and standardize tools necessary for application and network monitoring and troubleshooting.

Executive statement -

Our competitive advantage has always been our focus on the customer, with our ability to provide excellent customer service and minimize vehicle

downtimes.

After moving multiple systems into Google Cloud, we are seeking new ways to provide best-in-class online  eet management services to our

customers and improve operations of our dealerships. Our 5-year strategic plan is to create a partner ecosystem of new products by enabling

access to our data, increasing autonomous operation capabilities of our vehicles, and creating a path to move the remaining legacy systems to

the cloud.

Question

For this question, refer to the TerramEarth case study. TerramEarth has about 1 petabyte (PB) of vehicle testing data in a private data center. You

want to move the data to Cloud Storage for your machine learning team. Currently, a 1-Gbps interconnect link is available for you. The machine

learning team wants to start using the data in a month. What should you do?

A. Request Transfer Appliances from Google Cloud, export the data to appliances, and return the appliances to Google Cloud.

B. Con gure the Storage Transfer service from Google Cloud to send the data from your data center to Cloud Storage.

C. Make sure there are no other users consuming the 1Gbps link, and use multi-thread transfer to upload the data to Cloud Storage.

D. Export  les to an encrypted USB device, send the device to Google Cloud, and request an import of the data to Cloud Storage.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

712/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Correct Answer: A

Community vote distribution

A (85%)

B (15%)

? ?  SuperNest  Highly Voted ?  1áyear, 9ámonths ago

USB...where can I buy a 1 PB USB ...?

upvoted 42 times

? ?  MikeB19 1áyear, 9ámonths ago

Lol :)

upvoted 7 times

? ?  satamex 11ámonths ago

After this question I am sure why all answers are wrong, so the we come to the discussion and increase our knowledge levels by
reading from the high value notes shared by everyone.

upvoted 6 times

? ?  ACE_ASPIRE  Highly Voted ?  1áyear, 9ámonths ago

Who puts the answer here? how can you say that put the data in a USB....It should be A

upvoted 8 times

? ?  PleeO 1áyear, 9ámonths ago

definitely A is answer, why has someone placed a trap here?

upvoted 3 times

? ?  rottzy 1áyear, 8ámonths ago

;) ;) ;)

upvoted 1 times

? ?  prakata  Most Recent ?  5ámonths, 4áweeks ago
Why can't we use storage transfer service ?

upvoted 2 times

? ?  thamaster 6ámonths ago

Selected Answer: A

1 pb to move in one month with 1 gbps you need an appliance it will take 3 weeks.
Ans A

upvoted 2 times

? ?  pawan7869 6ámonths ago
jus purchased 1pb usb

upvoted 4 times

? ?  surajkrishnamurthy 6ámonths, 1áweek ago

Selected Answer: A

A Is the best answer
1 Pb Data Transfer with 1 Gbps speed takes 124 days to transfer data

upvoted 1 times

? ?  medi01 2ámonths, 1áweek ago

92, but doesn't matter.

upvoted 1 times

? ?  AzureDP900 8ámonths, 2áweeks ago

A is perfect for this use case

upvoted 2 times

? ?  abdelilahfa 8ámonths, 3áweeks ago

Selected Answer: A

It will take 123 days to transfer
The right answer is to use Transfer Appliance
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time

upvoted 2 times

? ?  desertlotus1211 10ámonths, 2áweeks ago

Answer is a A..
123 days to transfer 1PB of data using a 1GB link... this is from DC edge to GCP... this may be longer due to internal DC Fabric design

upvoted 2 times

? ?  AzureDP900 11ámonths, 4áweeks ago

USB is big joke whoever wrote this question they should have make it much better, Transfer Appliance is perfect for 1PB. A is right.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

713/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  Superr 1áyear, 1ámonth ago

Selected Answer: A

Transfer appliance can be used to transfer data more than 10 TB from on-prem

upvoted 3 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: A

There is no special use care to call for B like third party cloud.or any special massage of data required. A is the always way to go for private
sturge and peta byto of data .

A

upvoted 2 times

? ?  mad314 1áyear, 2ámonths ago

Selected Answer: A

Had this question on my exam.

upvoted 3 times

? ?  kimharsh 1áyear, 2ámonths ago

Selected Answer: A

IT's A,
Whoever answered B did you calculate how long it will take for the data to be transferred ? it have to be less than 1 month from the
question requirement
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time

upvoted 2 times

? ?  Skr6266 1áyear, 2ámonths ago

Selected Answer: A

Answer is A.
https://cloud.google.com/transfer-appliance/docs/4.0/overview#location-availability
With a typical network bandwidth of 100 Mbps, one petabyte of data takes about 3 years to upload. However, with Transfer Appliance, you
can receive the appliance and capture a petabyte of data in under 25 days. Your data can be accessed in Cloud Storage within another 25
days, all without consuming any outbound network bandwidth.
with 1GBps - online STS will take 124 days ..

upvoted 3 times

? ?  GauravLahoti 1áyear, 6ámonths ago

Correct Answer is A

upvoted 1 times

? ?  zxcv1234 1áyear, 6ámonths ago

Selected Answer: A

1PB USB drive is a joke.

upvoted 3 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

714/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 11 - Testlet 8

Question #1

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

715/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

The Dress4Win security team has disabled external SSH access into production virtual machines (VMs) on Google Cloud Platform (GCP).

The operations team needs to remotely manage the VMs, build and push Docker containers, and manage Google Cloud Storage objects.

What can they do?

A. Grant the operations engineer access to use Google Cloud Shell.

B. Con gure a VPN connection to GCP to allow SSH access to the cloud VMs.

C. Develop a new access request process that grants temporary SSH access to cloud VMs when an operations engineer needs to perform a

task.

D. Have the development team build an API service that allows the operations team to execute speci c remote procedure calls to accomplish

their tasks.

Correct Answer: A

Community vote distribution

A (100%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

I thought operations team doesn't need SSH access to manage VMs. All it needs is Cloud Shell with the Cloud SDK and gcloud tools.
Maybe A is correct answer.

upvoted 33 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 7 times

? ?  nitinz 2áyears, 3ámonths ago

A, you can do pretty much everything from cloud shell.

upvoted 3 times

? ?  JJu  Highly Voted ?  3áyears, 7ámonths ago

i think this answer is A.

I see similar Question.
Preparing for the Google Cloud Professional Cloud Architect Exam lecture of Coursera.

similar answer is A.

upvoted 10 times

? ?  Mahmoud_E  Most Recent ?  8ámonths, 1áweek ago

Selected Answer: A

A is OK

upvoted 1 times

? ?  Crick76 10ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

716/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: A

Old Case Study - Should be removed

upvoted 7 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 1 times

? ?  chorizama 1áyear, 7ámonths ago

We could misunderstand the question. It's not talking about SSH into the instance to deploy the images. The team only needs an
environment to build and publish to the repositories.

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

A. Grant the operations engineer access to use Google Cloud Shell.

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  VenV 2áyears, 3ámonths ago

how cloudshell works to login to vms if we block port 22 in the firewall rules for external access? try this in your environment and see if it
works.....not A. if we dont block external access, then cloudshell will be good option in this case.

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

I think we don't have to login to VMs; we only have to MANAGE them - which is quite different. The same for Docker containers and GCS
objects.
IMO A is the best...

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

Of course - you can still SSH to VMS but from Cloud Shell (NOT externally, as task states "security team has disabled external SSH
access).

upvoted 1 times

? ?  cert2020 2áyears, 4ámonths ago

Answer A - With Cloud Shell can manage your resources with its online terminal preloaded with utilities.

upvoted 1 times

? ?  aaabbbc1 2áyears, 4ámonths ago

A will be considered as the final decision, I promise

upvoted 1 times

? ?  CloudGenious 2áyears, 4ámonths ago

ans is A ..When the ops team login through cloud shell, the credential acc is there.

the ops team engineer typically has all the necessary permission required to manage system such as - build, push docker and manage.
when the team execute command from cloud shell the command will excecute through there credential acc and succed as log as they
have permission ehich they should as ops team. may not able to ssh but i am role let them to carry out action like start ,stop ,terminate all
don't need ssh .
upvoted 2 times

? ?  bnlcnd 2áyears, 4ámonths ago

A is wrong. how do you push a docker image from your on-prem server to GCP with cloud shell?
B is the only option.

upvoted 2 times

? ?  BobBui 2áyears, 4ámonths ago

My choice is A
upvoted 3 times

? ?  iamoct 2áyears, 5ámonths ago

This is the official answer. No more argue.

A. Grant operations team access to use Cloud Shell. ?

A - The operations team doesn't actually need SSH access to manage VMs. All it
needs is Cloud Shell with the Cloud SDK and gcloud tools.
Cloud Shell provides all the tools for managing Compute Engine instances. In this

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

717/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

case the assumption that SSH access is needed is incorrect.
Business requirement:
"Improve security by defining and adhering to a set of security and Identity and
Access Management (IAM) best practices for cloud."
B - A VPN is a way to connect from remote to the internal IP of an instance. If SSH is
blocked everywhere, this work-around won't help.
C - Developing an application that would use the Cloud API would be redundant with
the gcloud command line tool.
D - An application the provides temporary access to SSH is basically just violating the
security practices.

upvoted 9 times

? ?  okixavi 2áyears, 6ámonths ago

I'll go with A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

718/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

719/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

At Dress4Win, an operations engineer wants to create a tow-cost solution to remotely archive copies of database backup  les.

The database  les are compressed tar  les stored in their current data center.

How should he proceed?

A. Create a cron script using gsutil to copy the  les to a Coldline Storage bucket.

B. Create a cron script using gsutil to copy the  les to a Regional Storage bucket.

C. Create a Cloud Storage Transfer Service Job to copy the  les to a Coldline Storage bucket.

D. Create a Cloud Storage Transfer Service job to copy the  les to a Regional Storage bucket.

Correct Answer: A

Follow these rules of thumb when deciding whether to use gsutil or Storage Transfer Service:

* When transferring data from an on-premises location, use gsutil.

* When transferring data from another cloud storage provider, use Storage Transfer Service.

* Otherwise, evaluate both tools with respect to your speci c scenario.

Use this guidance as a starting point.

The speci c details of your transfer scenario will also help you determine which tool is more appropriate.

Community vote distribution

C (100%)

? ?  Ayzen  Highly Voted ?  3áyears, 2ámonths ago

Should be C: https://cloud.google.com/storage-transfer/docs/on-prem-overview
Especially, when Google docs explicitly states, that custom scripts are unreliable, slow, insecure, difficult to maintain and troubleshoot.

upvoted 34 times

? ?  cetanx 2áyears, 11ámonths ago

I would go with A

Storage Transfer Service has many valuable features but it comes with some dependencies such as;
- min 300-Mbps internet connection
- A docker engine on-prem (app runs inside a container)
https://cloud.google.com/storage-transfer/docs/on-prem-overview#what_requirements_does_have
... and these may not be available at Dress4Win (we have no data if D4W satisfies these requirements)

Based on the recommendations here: https://cloud.google.com/storage-transfer/docs/overview#gsutil
[# gsutil rsync] command seems to be a better option in a cron job with regular intervals as it will be much easier to implement
compared to setting up Storage Transfer Service.

upvoted 8 times

? ?  Jphix 2áyears, 5ámonths ago

We[re talking about potentially 100s of TBs of data based on the case study (at least 65TBs as that's how much they are using in their
NAS storage for backups/logs). I certainly hope they have the minimum 300-Mbps connection and a computer in their data center
that they can install docker on....

upvoted 5 times

? ?  SamirJ  Highly Voted ?  2áyears, 8ámonths ago

Answer should be C. As per the latest case study on google cloud website , they have DB storage of 1 PB out of which 600 TB is used. So
you get the size of the data.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

720/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

These are the thumb rules as per GCP documentation -

Transfer scenario Recommendation

Transferring from another cloud storage provider Use Storage Transfer Service
Transferring less than 1 TB from on-premises Use gsutil
Transferring more than 1 TB from on-premises Use Transfer service for on-premises data

https://cloud.google.com/storage-transfer/docs/overview

upvoted 15 times

? ?  AdityaGupta 2áyears, 8ámonths ago

I agree with Samir, when there is nothing mentioned about data size, refer the case study again. Storage appliance section mentioned
total size and available size. Which means we should be using storage transfer service. I will go with option C.

upvoted 1 times

? ?  jabrrJ68w02ond1  Most Recent ?  8ámonths ago

IMPORTANT: Dress4Win is not anymore part of the officially listed case studies:
https://cloud.google.com/certification/guides/professional-cloud-architect

upvoted 5 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: C

Answer is C.

upvoted 1 times

? ?  ramzez4815 10ámonths, 1áweek ago

Selected Answer: C

C is the correct answer

upvoted 2 times

? ?  Aiffone 1áyear, 5ámonths ago

I'd go with C, transfer service. gsutil is best used for transfer within GCS

upvoted 1 times

? ?  burner_1984 1áyear, 5ámonths ago

Storage Transfer Service is to be used when data is available online, not in physical datacenter

upvoted 1 times

? ?  GCPCloudArchitectUser 1áyear, 5ámonths ago

Dress4Win case is not listed as exam case study
https://cloud.google.com/certification/guides/professional-cloud-architect

upvoted 3 times

? ?  ravisar 1áyear, 6ámonths ago

Here are the guidelines from Google:
From Azure/AWS Transfer: Storage Transfer Service
Between two different bucket: Storage Transfer service
For less than 1 TB From Private datacenter to Google: gsutil
For more than 1 TB with enough bandwidth for Private datacenter to Google - Use Storage Transfer Service for on-premises data
Not enough bandwidth to meet project deadline for private data center to Google for more than 1 TB - Transfer Appliance. (Transfer
Appliance is recommended for data that exceeds 20 TB or would take more than a week to upload)
I assume the DB size will be more than 1 TB. (2 million TerramEarth vehicles each generate generates 200 to 500 megabytes of data per
day)
Since it is more than 1 TB, based on google guidelines, I will go with Storage Transfer Service Answer C
https://cloud.google.com/storage-transfer/docs/overview
https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets
Case: https://services.google.com/fh/files/blogs/master_case_study_terramearth.pdf

upvoted 1 times

? ?  GCPCloudArchitectUser 1áyear, 5ámonths ago

This question is for Dress4Win case study and you are referring different one

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 1 times

? ?  Amirso 1áyear, 9ámonths ago
IMO option A is correct.
According to the technical requirement;
- Support multiple VPN connections between the production data center and cloud
environment.
Cloud VPNátunnel can support up to 3 gigabits per second (Gbps).

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

721/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

There is no deadline for this usecase, And also by considering the industry I can say the database size wouldnÆt be bigger than 1TB; hence
gsutil is suitable for this case.

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

C. Create a Cloud Storage Transfer Service Job to copy the files to a Coldline Storage bucket.

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is C

upvoted 2 times

? ?  Pb55 2áyears, 2ámonths ago

Follow these rules of thumb when deciding whether to use gsutil or Storage Transfer Service:

Transfer scenario Recommendation
Transferring from another cloud storage provider Use Storage Transfer Service.
Transferring less than 1 TB from on-premises Use gsutil.
Transferring more than 1 TB from on-premises Use Transfer service for on-premises data.
Transferring less than 1 TB from another Cloud Storage region Use gsutil.
Transferring more than 1 TB from another Cloud Storage region Use Storage Transfer Service.
https://cloud.google.com/storage-transfer/docs/overview

upvoted 1 times

? ?  jasim21 2áyears, 2ámonths ago

Answer is C
Current DB disk size is 5 TB & backup size is 600 TB.
If size is more than 1 TB google recommend transfer service. regardless from other cloud/on-premise
https://cloud.google.com/storage-transfer/docs/overview#gsutil

upvoted 2 times

? ?  mrhege 2áyears, 2ámonths ago

"Fibre channel SAN - MySQL databases
- 1 PB total storage; 400 TB available"
Definitely a use-case for Storage Transfer Service. (C)

upvoted 1 times

? ?  azeqsd 2áyears, 2ámonths ago

https://cloud.google.com/storage-transfer-service
USE CASE
Database recovery, backup, and archival
With our data transfer services, you can schedule incremental syncs to enable disaster recovery for apps running in other clouds and on-
premises, to meet your recovery goals. Learn about all of our backup and disaster recovery options, or read about how to use Cloud
Storage for archiving your data.

I'll go with C

upvoted 2 times

? ?  hyordanov 2áyears, 2ámonths ago

but is not lowcost

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

722/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

723/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

Dress4Win has asked you to recommend machine types they should deploy their application servers to.

How should you proceed?

A. Perform a mapping of the on-premises physical hardware cores and RAM to the nearest machine types in the cloud.

B. Recommend that Dress4Win deploy application servers to machine types that offer the highest RAM to CPU ratio available.

C. Recommend that Dress4Win deploy into production with the smallest instances available, monitor them over time, and scale the machine

type up until the desired performance is reached.

D. Identify the number of virtual cores and RAM associated with the application server virtual machines align them to a custom machine type

in the cloud, monitor performance, and scale the machine types up until the desired performance is reached.

Correct Answer: C

Community vote distribution

D (58%)

A (42%)

? ?  MyPractice  Highly Voted ?  3áyears, 5ámonths ago

A - not correct. as its talking about Physical server size
B - not correct. as we its talking about Max spec
C - not correct. as its talking about the Smallest spec
D - is CORRECT. as its recommending to map with on premises app VM Size

upvoted 39 times

? ?  Ziegler 3áyears, 1ámonth ago

What if the on-premises workloads are oversized? C is the correct answer instead

upvoted 4 times

? ?  Rafaa 3áyears ago

They have scaling problem, hence decided to move to GCP.

upvoted 3 times

? ?  rottzy 1áyear, 8ámonths ago

Also, cost-cutting was a primary concern = while moving to cloud

upvoted 2 times

? ?  JMSTP 1áyear, 8ámonths ago

Agreed, but confused as question states they're moving Test/Dev first and C says "into Production". If they're looking to save money,
start small and grow until performance needs are met. Test/Dev is typically tolerant to these incremental changes.

upvoted 1 times

? ?  Smart 3áyears, 4ámonths ago

Agree. Starting with a minimal size at which your application can run efficiently/optimally is the best practice. This would be best
estimate from past usage so monitor closely and apply vertical and horizontal scaling.

upvoted 3 times

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

It's D. You can monitor machines to scale until necessary

upvoted 25 times

? ?  chiar 3áyears, 7ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

724/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Remember that the VM's CPU RAM and Disk are designed in GCP for optimized performance, I think C is the best

upvoted 3 times

? ?  MyPractice 3áyears, 5ámonths ago

the smallest instance avail is as low as 1CPU with 128MB RAM - Is that sufficient for prod run (assume currently its running with
24CPU with 128GB of ram in on premise)

upvoted 5 times

? ?  Rathish 3áyears, 1ámonth ago

I thought, only dev and testing environment migrated first and in that case, I prefer to start with smallest instance available. -
Dynamic scalability is the reason, we are moving to cloud.

upvoted 2 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 6 times

? ?  Pankonics 2áyears, 6ámonths ago

C is correct. In option D, it says No.of virtual cores and RAM associated... Which is not mentioned in Case study as well. This option just
trying to confuse that set. So will go with C.

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

C is the best answer. Thats the beauty of cloud. You can change machine type by just shutting it down. Also Google discourages using
custom VM sizing.

upvoted 2 times

? ?  techalik 2áyears, 7ámonths ago

D i think:
Start with the smallest instances and scale up to a larger machine type until the performance is of the desired standard. is the right
answer.

In continuation of the above explanation, although you could use a predefined machine type like e2-highmem-4/n2-highmem-4/n2d-
highmem-4 etc. if you need 4 VCPUs and 32GB memory, there's no guarantee that it performs similar to the existing VM in the data
centre. The networking fabric is different, the disk I/O is different, and the CPUs are different too. We don't know the exact
specifications of the data centre CPUs to draw a parallel to the processors offered by GCP. As you can see, the performance can vary a
lot depending on the frequency. (remember shelling out additional 500$ for upgrading CPU from 2.6GHz to 2.8GHz when buying your a
laptop?). You may realize that you need more vCPUs after migrating to Google Cloud or maybe less, but until you migrate and test it
out, there is no way to say which is the best machine type. So the recommendation should be to start small, increase the instance size
as needed until the performance is of an acceptable standard, and that is your machine type.

upvoted 3 times

? ?  techalik 2áyears, 7ámonths ago

I meant C :) explanation is for C

upvoted 3 times

? ?  luke19962023  Most Recent ?  2ámonths, 1áweek ago

Selected Answer: D

since they have VM's in current environment, I'll choose D. This lets you like for like replace the on prem footprint and scale from there.

I originally thought A, however, using custom machine image will prevent you from paying for running a larger machine image than
necessary.

upvoted 1 times

? ?  taer 2ámonths, 3áweeks ago

Selected Answer: D

By identifying the number of virtual cores and RAM associated with the application server virtual machines and aligning them to custom
machine types in the cloud, you can create an environment tailored to Dress4Win's specific needs. Monitoring performance and adjusting
the machine types accordingly ensures that the infrastructure is optimized for both performance and cost.

upvoted 1 times

? ?  BeCalm 3ámonths, 3áweeks ago

Selected Answer: A

D is an iterative approach to finding the right specs which is definitely not the way it works IRL.

upvoted 1 times

? ?  stevehlw 7ámonths, 1áweek ago

I'll be truly surprised if the correct answer is C.
Deploying the smallest instances into production will likely trigger a ton of error messages in real life.
If you're oversizing the VMs, just simply scale it down.

upvoted 1 times

? ?  jabrrJ68w02ond1 8ámonths ago

IMPORTANT: Dress4Win is not anymore part of the officially listed case studies:
https://cloud.google.com/certification/guides/professional-cloud-architect

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

725/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  AhmedH7793 8ámonths, 4áweeks ago

Selected Answer: D

It is D

upvoted 1 times

? ?  ShadowLord 9ámonths, 1áweek ago

Selected Answer: A

https://cloud.google.com/migrate/compute-engine/docs/4.9/concepts/planning-a-migration/cloud-instance-rightsizing

1. Performance-based recommendations: Recommends Compute Engine instances based on the CPU and RAM currently allocated to the
on-premises VM. This recommendation is the default.

upvoted 4 times

? ?  AlizCert 1ámonth, 1áweek ago

I agree with the thought process, but A is about the _physical_ HW that hosts the on-prem VMs, not about the VM requirements, so it
should be D.

upvoted 1 times

? ?  [Removed] 1áyear, 3ámonths ago

A should be better.

https://cloud.google.com/architecture/resource-mappings-from-on-premises-hardware-to-gcp

upvoted 3 times

? ?  Matalf 11ámonths, 1áweek ago

Resource Mapping: "In a scenario where your applications are running on bare-metal "

Rightsizing: "migrating a virtual machine to Compute Engine"

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: D

I vote D.
https://cloud.google.com/migrate/compute-engine/docs/4.9/concepts/planning-a-migration/cloud-instance-rightsizing

upvoted 2 times

? ?  MF2C 1áyear, 5ámonths ago

Selected Answer: D

vote D

upvoted 1 times

? ?  atlasga 1áyear, 6ámonths ago

Wow, these answers are so dumb I'm compelled to comment so new people don't mistakenly think this is how it works in the real world.
You're not supposed to map sizes one-to-one OR start with the smallest instance sizes available. The industry best practice is to right-size
based on actual utilization. I guess I'll pick C if this question comes up, because at least it will be cheaper than starting with big instances
(but it could cause production operational issues if the instances are undersized!).

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 1 times

? ?  PeppaPig 1áyear, 11ámonths ago

D is correct, first step of migration is the mapping of resources and choose right instance size that best fit

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

C. Recommend that Dress4Win deploy into production with the smallest instances available, monitor them over time, and scale the
machine type up until the desired performance is reached.

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is C

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

726/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

727/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

As part of Dress4Win's plans to migrate to the cloud, they want to be able to set up a managed logging and monitoring system so they can handle

spikes in their tra c load.

They want to ensure that:

* The infrastructure can be noti ed when it needs to scale up and down to handle the ebb and  ow of usage throughout the day

* Their administrators are noti ed automatically when their application reports errors.

* They can  lter their aggregated logs down in order to debug one piece of the application across many hosts

Which Google StackDriver features should they use?

A. Logging, Alerts, Insights, Debug

B. Monitoring, Trace, Debug, Logging

C. Monitoring, Logging, Alerts, Error Reporting

D. Monitoring, Logging, Debug, Error Report

Correct Answer: D

Community vote distribution

C (53%)

D (47%)

? ?  AWS56  Highly Voted ?  3áyears, 5ámonths ago

It is C.
With D you cannot achieve "Their administrators are notified automatically when their application reports errors." However with Error
Reporting(Log insights) - Notifies you when new errors are detected.
So I will go with C
upvoted 42 times

? ?  amxexam 1áyear, 9ámonths ago

There is no requirement for debugging

upvoted 4 times

? ?  Jos 3áyears, 5ámonths ago

You're wrong, with Reporting (https://cloud.google.com/error-reporting) you can: "...Opt in to receive email and mobile alerts on new
errors."

upvoted 4 times

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

D is correct

upvoted 21 times

? ?  TiagoM 2áyears, 2ámonths ago

Assuming this is an old question, Alert feature was a different section in Stackdriver and Debug is not mentioned in the requirements I
would pick C.
C in the current GCP setup doesnt make sence because Alerts are inside Monitoring. The only problem with D is that the Debug is not
requested like I said.

upvoted 6 times

? ?  salim_  Most Recent ?  1ámonth, 3áweeks ago

Selected Answer: D

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

728/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Cloud Logging:
ò Logging allows you to store, search, analyze, monitor, and alert on log data and events from GCP an AWS
ò 30 Days retention
Error Reporting:
ò Error Reporting counts, analyses, and aggregates the errors in your running Cloud services
Cloud Debugger:
ò To inspect the state of a running application in real time without stopping or slowing it.
ò Specifically, the debugger adds less than 10 milliseconds to the request latency when the application state is captured.
ò To understand the behavior of your code in production and analyze its state to locate those hard to find bugs.
Cloud Trace :
ò It's a distributed tracing system that collects latency data from your applications and displays it in the GCP console

upvoted 1 times

? ?  taer 2ámonths, 3áweeks ago

Selected Answer: C

Stackdriver Alerts can notify the infrastructure when it needs to scale up or down based on the traffic load.

upvoted 1 times

? ?  ACK_Topics 3ámonths, 3áweeks ago

Selected Answer: D

no alerts component on Stackdriver

upvoted 1 times

? ?  RVivek 4ámonths, 2áweeks ago

Selected Answer: D

StackDriver's features:
1.Stackdriver Monitoring
2.Stackdriver Logging and Error Reporting
3.Stackdriver Debugger
4.Stackdriver Trace
5.Stackdriver Profiler
https://versprite.com/blog/security-operations/google-
stackdriver/#:~:text=Stackdriver%20Logging%20allows%20you%20to,API%20to%20manage%20logs%20programmatically.

upvoted 1 times

? ?  gonlafer 6ámonths, 2áweeks ago

Selected Answer: D

Initially I was to say C, but Stackdriver services are:
Monitoring, Logging, Debug, Error Report ,Trace and Profiler
So D

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

As answered below

upvoted 1 times

? ?  nkit 1áyear, 2ámonths ago

Selected Answer: C

Debugging is not part of the request, hence I would not select D.

upvoted 1 times

? ?  [Removed] 1áyear, 3ámonths ago

Alerting gives timely awareness to problems in your cloud applications so you can resolve the problems quickly.
https://cloud.google.com/monitoring/alerts

Cloud Debugger solves the problem of isolating issues that occur only in production. By letting you inspect the state of a running
application in real time, without stopping or slowing it down, Debugger helps you solve problems that can be impossible to reproduce in a
local environment.
https://cloud.google.com/debugger/docs

C is better.

upvoted 1 times

? ?  [Removed] 1áyear, 3ámonths ago

sorry , ignored teh quesiton "Which Google StackDriver features should they use?"
StackDriver's feature:
Stackdriver Monitoring
Stackdriver Logging and Error Reporting
Stackdriver Debugger
Stackdriver Trace
Stackdriver Profiler

D is correct.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

729/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  sjmsummer 1áyear, 5ámonths ago

Selected Answer: D

initially select C, but reading all the comments I agree there is no feature called "alerting". thus select D.

upvoted 3 times

? ?  burner_1984 1áyear, 5ámonths ago
components of Stackdrvers are
Monitoring, Logging, Debug, Error Report ,Trace and Profiler

upvoted 2 times

? ?  ashehzad 1áyear, 5ámonths ago

Selected Answer: C

It is C.
With D you cannot achieve "Their administrators are notified automatically when their application reports errors." However with Error
Reporting(Log insights) - Notifies you when new errors are detected.
So I will go with C

upvoted 3 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I think D.
No 'Alert' in Stack Driver family....
Alert is one of setting such as logging.

upvoted 1 times

? ?  ABO_Doma 1áyear, 6ámonths ago

It is C.

upvoted 1 times

? ?  ravisar 1áyear, 6ámonths ago

D is correct

Following are the 7 operation services:
1. Monitoring - Monitoring agent collect CPU, Network and Process metrics. This creates alerts as well. We can also configure monitoring
agents for third party apps.
2. Debugger - Troubleshoot production application code issues.
3. Logging - Logs from Services, has query capability. Use router sing to send logs to GCS, Pubsub or big query
5. Profiler - monitor production for memory leak, reduce cost and code performance.
6. Trace - Troubleshoot latency issues
7. Error reporting - Send notification when error occurs from log files.

There is nothing called Alerts. Notification is incorporated with the above operation services.

upvoted 2 times

? ?  phanuphat 1áyear, 6ámonths ago

what about 4th operation services?

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

730/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

731/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

Dress4Win would like to become familiar with deploying applications to the cloud by successfully deploying some applications quickly, as is. They

have asked for your recommendation.

What should you advise?

A. Identify self-contained applications with external dependencies as a  rst move to the cloud.

B. Identify enterprise applications with internal dependencies and recommend these as a  rst move to the cloud.

C. Suggest moving their in-house databases to the cloud and continue serving requests to on-premise applications.

D. Recommend moving their message queuing servers to the cloud and continue handling requests to on-premise applications.

Correct Answer: C

Community vote distribution

A (80%)

C (20%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

A looks better for me.

upvoted 38 times

? ?  PRC  Highly Voted ?  3áyears, 2ámonths ago

A for me..Self contained application with no internal dependencies which means it does not need to integrate with any on-premise
systems. External dependencies are easier to manage through API based integration in Cloud..Other options have either dependencies, or
multiple hops between on-premise/cloud thereby causing latency issues,

upvoted 27 times

? ?  WinSxS  Most Recent ?  3ámonths, 2áweeks ago

Selected Answer: A

To become familiar with deploying applications to the cloud, it is recommended to start with simple, self-contained applications with
external dependencies that can be easily moved to the cloud. These applications are likely to have fewer dependencies on other
components in the infrastructure and can be migrated with minimal effort, helping the team to get comfortable with the cloud
deployment process. Once the team has gained experience with the cloud deployment process, they can gradually move more complex
applications with internal dependencies to the cloud.

upvoted 3 times

? ?  akhilesh_pundir 4ámonths, 4áweeks ago

Case study nowhere mentioned that Dress4Win has some self contained Application with no dependencies on Internal resources. And
databases are available out of the box and would be easy to migrate in my view.

upvoted 1 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: A

A. Identify self-contained applications with external dependencies as a first move to the cloud.

upvoted 1 times

? ?  shekarcfc 9ámonths, 3áweeks ago

Selected Answer: A

A for me

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

732/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: C

In std practice to move db first.

upvoted 2 times

? ?  szefco 11ámonths, 2áweeks ago

Moving database to cloud and letting that database serve apps hosted on prem? It doesn't make sense. A seems better option - move
self-contained application. External dependencies can be handled by APIs

upvoted 1 times

? ?  GoReplyGCPExam 1áyear, 3ámonths ago

Selected Answer: A

A for me

upvoted 1 times

? ?  OrangeTiger 1áyear, 5ámonths ago

I don't understand the meaning of 'self-contained applications with external dependencies'.
if just 'self-contained applications' ,then i understand....

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

A. Identify self-contained applications with external dependencies as a first move to the cloud.

upvoted 4 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 3 times

? ?  gosi 2áyears, 2ámonths ago

A - Every experienced IT resource will give a solution - pick something which is "low hanging fruit". Now which one is the low hanging fruit
?
Something which is independent of any internal dependencies. i.e. A.

upvoted 3 times

? ?  Pb55 2áyears, 2ámonths ago

A - self contained / external dependancies = no latency issues.

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  AD3 2áyears, 3ámonths ago

'C' can be correct. See their existing databases MySQL and Radis. The migration of DB is straight as-is. The connections to the DB servers
can be just the network dependency. No applications are changing. No code change so no QA. They just have to run the performance test.
'C' is more as-is.
upvoted 1 times

? ?  melono 8ámonths, 1áweek ago

Yup but its about migrating apps, right?

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

But moving data of large databases isn't so quick by itself....

upvoted 1 times

? ?  mrhege 2áyears, 2ámonths ago

They are looking for moving an _application_ as-is to the cloud, so C cannot be correct.

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

I'm not sure - B or C...
In the task there is text " [...] to become familiar with deploying APPLICATIONS to the cloud by successfully deploying some APPLICATIONS
quickly, as is. [...] ". Are the DATABASES (in ans. B) APPLICATIONS ???
In the other hand - alway when we want to migrate to the cloud, databases ought to be moved to the cloud first - especially the smaller
ones like MySQL for Dress4win (One server for user data, inventory, static data,), which have only 2xTB HDD (RAID 1).
But in the C they talk about databaseS (plural, many databases). Do they think about some of the Dress4win Storage appliances:
1. Fibre channel SAN - MySQL databases, 1 PB total storage; 400 TB available (IMO this coud be taken into account as a database)
2. NAS - image storage, logs, backups, 100 TB total storage; 35 TB available (IMO this coud NOT be taken into account as a database)

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

733/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

So - what do you think about - B or C ?

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

Sory, erratum: Are the DATABASES (in ans. C) APPLICATIONS ???

upvoted 1 times

? ?  lynx256 2áyears, 2ámonths ago
Sorry, I've changed my mind.
I had misunderstand "no internal dependencies" in opt. A.
If "no internal dependencies " means "the app does not dependent on anything inside the data center", I'll go with A.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

734/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

735/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

Dress4Win has asked you for advice on how to migrate their on-premises MySQL deployment to the cloud.

They want to minimize downtime and performance impact to their on-premises solution during the migration.

Which approach should you recommend?

A. Create a dump of the on-premises MySQL master server, and then shut it down, upload it to the cloud environment, and load into a new

MySQL cluster.

B. Setup a MySQL replica server/slave in the cloud environment, and con gure it for asynchronous replication from the MySQL master server

on-premises until cutover.

C. Create a new MySQL cluster in the cloud, con gure applications to begin writing to both on premises and cloud MySQL masters, and

destroy the original cluster at cutover.

D. Create a dump of the MySQL replica server into the cloud environment, load it into: Google Cloud Datastore, and con gure applications to

read/write to Cloud Datastore at cutover.

Correct Answer: B

Community vote distribution

B (100%)

? ?  chiar  Highly Voted ?  3áyears, 7ámonths ago

I think D it can't be, because you want to load a dump in a Cloud Datastore. If it were a Cloud Storage, it could be, but a Cloud datastore is
a nosql.
It's true that you hace to use a dump, but to create a replica server/slave to promote to Cloud SLQ. So I think it is B.

upvoted 30 times

? ?  AD2AD4  Highly Voted ?  3áyears, 1ámonth ago

Final Decision to go with Option B

upvoted 24 times

? ?  jabrrJ68w02ond1  Most Recent ?  8ámonths ago

IMPORTANT: Dress4Win is not anymore part of the officially listed case studies:
https://cloud.google.com/certification/guides/professional-cloud-architect

upvoted 3 times

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: B

Cloud datastore is a nosql.
It's true that you hace to use a dump, but to create a replica server/slave to promote to Cloud SLQ. So I think it is B.

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

B. Setup a MySQL replica server/slave in the cloud environment, and configure it for asynchronous replication from the MySQL master
server on-premises until cutover.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

736/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 4 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B

upvoted 2 times

? ?  lynx256 2áyears, 3ámonths ago

B is ok

upvoted 1 times

? ?  bnlcnd 2áyears, 4ámonths ago

https://cloud.google.com/sql/docs/mysql/replication/external-server
B is good.

upvoted 3 times

? ?  okixavi 2áyears, 6ámonths ago

B it is

upvoted 1 times

? ?  Chulbul_Pandey 2áyears, 6ámonths ago

B is correct

upvoted 1 times

? ?  Hjameel 2áyears, 7ámonths ago

If a database SLA or other requirements do not allow for an export-based migration, you should consider creating a replica of the
database in which the replica database is in the Google cloud. This configuration is referred to as primary/replica or leader/follower , and
in general it is the preferred migration method. Whenever there is a change to the primary or leader, the same change is made to the
replica or follower instance. Once the database has synchronized the data, database applications can be configured to point to the cloud
database.

Answer B

upvoted 1 times

? ?  AdityaGupta 2áyears, 8ámonths ago

I will go with answer B, because this will avoid any downtime and performance impact. And post cutover this database can be used a
master.

A -> Will cause downtime.
B -> Right choice
C -> Business impact, incosistency in data.
D -> Cloud DataStore is NoSQL DB

upvoted 3 times

? ?  homer_simpson 2áyears, 8ámonths ago

B is correct answer.
Datastore is no SQL database. And when we create a dupm and upload it we might lose some data during this process time that was
served in the promiss site

upvoted 1 times

? ?  bidibidiiii 2áyears, 9ámonths ago

It's B.
There's a similar question in the Linux Academy practice exam:
"Dress4Win is ready to migrate their on-premises MySQL deployment to the cloud. They want to reduce downtime and performance
impact to their on-premises solution during the migration. What should they do?"
Answer: "Set up a MySQL replica/slave in Google Cloud using Cloud SQL and configure it for asynchronous replication from the MySQL
master server on-premises until cutover."

upvoted 3 times

? ?  ESP_SAP 2áyears, 9ámonths ago

Correct Answer is (B):

Please, stop to confuse the people with crazy ideas.
Every migration we should try to smooth or near zero downtime in DB cases.
The question clearly mention move MySQL to the cloud, minimize downtime and performance impact.
How you can recommend "D" to accomplish the previous premises?

Then if you setup a MySQL replica server/slave will be easy the cutover, just shut down the replica to on-premise and change the role for
replica server from slave to primary. That is all! No impact, no service disruption, almost near zero downtime.

upvoted 8 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

737/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #7

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

738/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

Dress4Win has con gured a new uptime check with Google Stackdriver for several of their legacy services. The Stackdriver dashboard is not

reporting the services as healthy.

What should they do?

A. Install the Stackdriver agent on all of the legacy web servers.

B. In the Cloud Platform Console download the list of the uptime servers' IP addresses and create an inbound  rewall rule

C. Con gure their load balancer to pass through the User-Agent HTTP header when the value matches GoogleStackdriverMonitoring-

UptimeChecks (https:// cloud.google.com/monitoring)

D. Con gure their legacy web servers to allow requests that contain user-Agent HTTP header when the value matches

GoogleStackdriverMonitoring- UptimeChecks (https://cloud.google.com/monitoring)

Correct Answer: B

Community vote distribution

B (70%)

D (30%)

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

It's B. B must be done. For a health check on http (port 80) you don't need to configure nothing in the server (it makes a get o something
similar).

upvoted 33 times

? ?  tartar 2áyears, 10ámonths ago

B is ok

upvoted 6 times

? ?  GopiSivanathan 2áyears, 8ámonths ago

If the resource you are checking isn't publicly available, you must configure the resource's firewall to permit incoming traffic from
the uptime-check servers. See Getting IP addresses to download a list of the IP addresses
If the resource you are checking doesn't have an external IP address, uptime checks are unable to reach it.

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

ans is B

upvoted 3 times

? ?  gcp2019  Highly Voted ?  3áyears, 7ámonths ago

Correct answer is B
https://cloud.google.com/monitoring/uptime-checks/using-uptime-checks#monitoring_uptime_check_list_ips-console

upvoted 14 times

? ?  jabrrJ68w02ond1  Most Recent ?  8ámonths ago

IMPORTANT: Dress4Win is not anymore part of the officially listed case studies:
https://cloud.google.com/certification/guides/professional-cloud-architect

upvoted 10 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: B

It would be missing firewall rule that would be causing problem

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

739/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  DivAl272829 1áyear, 2ámonths ago

B: If the resource you are checking isn't publicly available, you must configure the resource's firewall to permit incoming traffic from the
uptime-check servers. See List uptime-check server IP addresses to download a list of the IP addresses.

upvoted 1 times

? ?  mesodan 1áyear, 4ámonths ago

Selected Answer: B

It's B. Read: https://cloud.google.com/monitoring/uptime-checks

upvoted 3 times

? ?  nagibator163 1áyear, 5ámonths ago

Selected Answer: D

I don't understand why everyone's saying it's B. The question talks about "legacy services". They are not on GCP, are they? So setting up
inbound rules on a firewall in GCP will have no effect.

upvoted 3 times

? ?  Andrea67 1áyear, 6ámonths ago

I think B is ok, "Your use of uptime checks is affected by any firewalls protecting your service." from
https://cloud.google.com/monitoring/uptime-checks

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 3 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 1 times

? ?  [Removed] 1áyear, 11ámonths ago

B) (if the answer talk about firewall of legacy services ant not firewall on GCP) and D) are valid solutions.
I prefer D) because with B) you have to modify firewall rules once per quarter as Ips can change
(https://cloud.google.com/monitoring/uptime-checks/using-uptime-checks#get-ips).

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

B. In the Cloud Platform Console download the list of the uptime servers' IP addresses and create an inbound firewall rule

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 2 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B

upvoted 1 times

? ?  VenV 2áyears, 3ámonths ago

B should be correct

upvoted 1 times

? ?  lazyme 2áyears, 3ámonths ago

its B.
https://cloud.google.com/monitoring/uptime-checks

upvoted 1 times

? ?  bnlcnd 2áyears, 4ámonths ago
Isn't D also including B?
I think D is better. You need to have the FW rule and also not blocking the uptime poll. D exactly cover both.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

740/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #8

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

741/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

As part of their new application experience, Dress4Wm allows customers to upload images of themselves.

The customer has exclusive control over who may view these images.

Customers should be able to upload images with minimal latency and also be shown their images quickly on the main application page when they

log in.

Which con guration should Dress4Win use?

A. Store image  les in a Google Cloud Storage bucket. Use Google Cloud Datastore to maintain metadata that maps each customer's ID and

their image  les.

B. Store image  les in a Google Cloud Storage bucket. Add custom metadata to the uploaded images in Cloud Storage that contains the

customer's unique ID.

C. Use a distributed  le system to store customers' images. As storage needs increase, add more persistent disks and/or nodes. Assign each

customer a unique ID, which sets each  le's owner attribute, ensuring privacy of images.

D. Use a distributed  le system to store customers' images. As storage needs increase, add more persistent disks and/or nodes. Use a Google

Cloud SQL database to maintain metadata that maps each customer's ID to their image  les.

Correct Answer: A

Community vote distribution

A (100%)

? ?  chiar  Highly Voted ?  3áyears, 7ámonths ago

I think it's A, because in the question says "The customer has exclusive control over who may view these images"
And I think it is easier to develop this feature having in cloud datastore a NOSQL database where you can manage the control of file's
viewer

upvoted 32 times

? ?  DrCoola  Highly Voted ?  3áyears, 4ámonths ago

A - using gsutil for this purpose makes querries on such metadata painful for application logic.

upvoted 8 times

? ?  joe2211  Most Recent ?  1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  PeppaPig 1áyear, 11ámonths ago

A is correct. The whole idea is simply build and maintain an external metadata service using NoSQL database to associate the GS object
key with its metadata, in order to facilitate object findings based on attributes you pre defined in metatdata

This AWS blog provides a solution in the context of AWS S3, but the idea behind is applicable to Google Storage as well
https://aws.amazon.com/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers/

upvoted 4 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

742/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  [Removed] 1áyear, 11ámonths ago

B) is not correct because we cannot search on bucket using metada (maybe in the future ...). So for now we have to get all files from the
bucket and fiter on metadata ( very bad performance).
The answer is A).
upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

A. Store image files in a Google Cloud Storage bucket. Use Google Cloud Datastore to maintain metadata that maps each customer's ID
and their image files.

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 3 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  Rightsaidfred 2áyears, 4ámonths ago

Between A & B. A will take quickest time. A is the answer.

upvoted 2 times

? ?  okixavi 2áyears, 6ámonths ago

A is the man

upvoted 2 times

? ?  Bijesh 2áyears, 6ámonths ago

The customer has exclusive control over who can view their image.
Is this possible by option B, by merely adding metadata on the object. I don't think so.
A is better suited.

upvoted 2 times

? ?  hems4all 2áyears, 7ámonths ago

A is correct

upvoted 2 times

? ?  AdityaGupta 2áyears, 8ámonths ago

I will go with A, Store the images in GCS is cost-optimized way of storing images/ objects. DataStore is best option to store user profiles,
which gives control to user.

upvoted 2 times

? ?  zzaric 2áyears, 11ámonths ago

A is correct

upvoted 2 times

? ?  mlantonis 3áyears ago

I agree with A
upvoted 2 times

? ?  motty 3áyears ago

A solves storage and access. What I do not like about Datastore that it requires application support and the use case is not clear how far
Application team in migration. But considering migration in GCP happening, Datastore is great database for this type of things

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

743/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #9

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

744/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

Dress4Win has end-to-end tests covering 100% of their endpoints.

They want to ensure that the move to the cloud does not introduce any new bugs.

Which additional testing methods should the developers employ to prevent an outage?

A. They should enable Google Stackdriver Debugger on the application code to show errors in the code.

B. They should add additional unit tests and production scale load tests on their cloud staging environment.

C. They should run the end-to-end tests in the cloud staging environment to determine if the code is working as intended.

D. They should add canary tests so developers can measure how much of an impact the new release causes to latency.

Correct Answer: B

Community vote distribution

B (100%)

? ?  examtaker11  Highly Voted ?  3áyears, 4ámonths ago

C- I would run the same test suite in cloud) to see what breaks

upvoted 24 times

? ?  Smart 3áyears, 4ámonths ago

Agree, however, I think running at production-scale would not only show what breaks but also when it breaks? I go with B

upvoted 6 times

? ?  mesodan 1áyear, 4ámonths ago

I would go with B. "Additional" seems to be the keyword here so adding unit tests and production scale load tests to the ones they
already have makes more sense.

upvoted 2 times

? ?  Jphix 2áyears, 5ámonths ago

Going with B. "Final answer" lol. C is a good answer except that they're asking for an additional testing method. Since they're already
testing endpoints specifically, you'd literally be running the exact same test after migration. That said, for B, I'm still at a loss of why
we'd need to do additional unit testing--best explanation is that some of the applications will have needed to be retooled for PaaS
offerings if they're doing more than a lift-and-shift, thereby actually changing the underlying code; but the production-level load testing
is like the most GCP thing you can do here

upvoted 9 times

? ?  FAB1010  Highly Voted ?  2áyears, 11ámonths ago

Question mention that "end-to-end tests covering 100% of their endpoints", "ensure that the move to the cloud does not introduce any
new bugs", and "additional testing methods should the *developers* employ to *prevent an outage*"

A - Not Correct. Developer can debug the problem, but cannot *prevent* the outage.
B - Correct. Developers are responsible for writing unit tests. They already have end-to-end tests for *endpoints* but nothing mentioned
about the unit tests. Cloud will auto-scale but you need to define your auto-scaling configuration (desired count, max count etc) and
production scale load test will help you to configure the auto-scaling policies
C - Not Correct. They already have end-to-end test. Running it on staging environment will not prevent an outage
D - Not Correct. Answers says "an impact the new release causes to latency" but question ask for preventing an outage and so this one is
ruled out

upvoted 19 times

? ?  RitwickKumar  Most Recent ?  10ámonths, 1áweek ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

745/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Selected Answer: B

Note the ask "prevent an outage".
One of the way to test outage scenarios is through load testing. Option B covers this where as option C only covers checking the intended
behaviour.

upvoted 2 times

? ?  [Removed] 1áyear, 3ámonths ago

Testing pipeline: Unit ---- Integration --- end-to-end
If choose B, add additional unit tests. It should be continue to do Integrationt test and end-to-end test.
But it just has a unit test, then go to produciton scale load test.

C should be better, becasue it has finished end-to-end tests before, so for move to the cloud, it should be test the end-to-end in cloud
preproduction enviroment to check whethere it's also working fine on cloud.

https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#end-to-end_tests

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: B

vote B

upvoted 2 times

? ?  rottzy 1áyear, 8ámonths ago

end-to-end is already present, go for additional tests

upvoted 1 times

? ?  amxexam 1áyear, 9ámonths ago

It should be B as for all those going with C if you do all staging you will still leave out the performance test that scales the application
which is covered in B that means even if the application works well but will not scale properly will lead to an outage, which we are asked to
prevent.

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

B. They should add additional unit tests and production scale load tests on their cloud staging environment.

upvoted 3 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 4 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B (but C... uff... is also possible, but as the question says end-to-end is already done)

upvoted 3 times

? ?  lynx256 2áyears, 3ámonths ago

B is ok

upvoted 1 times

? ?  ybe_gcp_cert 2áyears, 3ámonths ago

Question asks about "ADDITIONAL testing methods".

B adds production scale load tests.
C should also be executed in a new cloud env but this question dosn't ask for this.

In real life serious projects, B and C are mandatory (end to end and perf tests).

Should be B.

upvoted 2 times

? ?  guid1984 2áyears, 4ámonths ago

Should be B
Reasoning: They already had end-to-end contract tests coverage for all their service(s) endpoints. So, additionally they should add unit test
coverage and perform prod load tests in staging environment which will help find out performance related issues before deploying it to
production.

upvoted 1 times

? ?  Rightsaidfred 2áyears, 4ámonths ago

Obviously C. Yes they have end-to-end tests on prem with 100% coverage, however this hasn't been tested in the cloud yet.

upvoted 1 times

? ?  bnlcnd 2áyears, 4ámonths ago

B vs C
B sounds right other than additional "unit" test. C is nothing wrong but not mentioning load. I will throw a dime to decide which to
choose.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

746/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  okixavi 2áyears, 6ámonths ago

Answer is C

upvoted 1 times

? ?  OSNG 2áyears, 6ámonths ago

C is correct. As we want to test the code in the cloud for new bugs. (from question: They want to ensure that the move to the cloud does
not introduce any new bugs.)
Why not B? - Unit tests are used to check individual codes are working fine or not and scaling not properly impact Scalability not reliability.
Why not D? can be the closest one, but answer is checking for latency. Again increased latency does not impact reliability or availability. (in
question they are looking to prevent the outage).

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

747/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #10

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

748/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

You want to ensure Dress4Win's sales and tax records remain available for infrequent viewing by auditors for at least 10 years.

Cost optimization is your top priority.

Which cloud services should you choose?

A. Google Cloud Storage Coldline to store the data, and gsutil to access the data.

B. Google Cloud Storage Nearline to store the data, and gsutil to access the data.

C. Google Bigtabte with US or EU as location to store the data, and gcloud to access the data.

D. BigQuery to store the data, and a web server cluster in a managed instance group to access the data. Google Cloud SQL mirrored across

two distinct regions to store the data, and a Redis cluster in a managed instance group to access the data.

Correct Answer: A

Reference:

https://cloud.google.com/storage/docs/storage-classes

Community vote distribution

A (80%)

B (20%)

? ?  chiar  Highly Voted ?  3áyears, 7ámonths ago

I think it's A, because when you read documentation both of them (nearline and coldline) you can see the expresion infrecuent access. And
in this case, your priority is the cost, and you are going to sabe 10 years

upvoted 32 times

? ?  MyPractice  Highly Voted ?  3áyears, 5ámonths ago

its A - "Cold data storage - Infrequently accessed data, such as data stored for legal or regulatory reasons, can be stored at low cost as
Coldline Storage and be available when you need it"
https://cloud.google.com/storage/docs/storage-classes

upvoted 10 times

? ?  RVivek  Most Recent ?  4ámonths, 2áweeks ago

Selected Answer: B

Coldline Data retrival takes hours
If the data is infrequently accessed then Nearline is better.

upvoted 1 times

? ?  RVivek 4ámonths, 2áweeks ago

I change it to A. Just noticed the phrase " Cost optimization is top priority" in question

upvoted 1 times

? ?  NodummyIQ 5ámonths, 4áweeks ago

Option A is not a correct answer because Google Cloud Storage Coldline is not suitable for infrequent access to data. Coldline storage is
optimized for archival storage with a 90-day minimum storage duration and a retrieval period measured in hours. It is not suitable for
data that needs to be accessed frequently or within a short period of time.

Option B is a better choice for infrequent access to data. Google Cloud Storage Nearline is optimized for infrequent access with a 30-day
minimum storage duration and a retrieval period measured in seconds. It is more suitable for data that needs to be accessed infrequently
or within a short period of time, such as data that needs to be accessed by auditors.

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

749/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  alexandercamachop 9ámonths, 2áweeks ago

Selected Answer: A

Cost + 10 years, should be Archival, but since is not here. Lets go with ColdLine.

upvoted 2 times

? ?  gcpAMa 11ámonths ago

Answer: A

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

A. Google Cloud Storage Coldline to store the data, and gsutil to access the data.

upvoted 1 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 2 times

? ?  mbrueck 1áyear, 11ámonths ago

Answer: A

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 3 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

A is ok

upvoted 1 times

? ?  gu9singg 2áyears, 3ámonths ago
A - Coldline is cheapest one

upvoted 1 times

? ?  Joyrex 2áyears, 4ámonths ago

Today the answer would be Archive Storage instead of coldline, so keep in mind the options on the test may be updated.

upvoted 4 times

? ?  LoganIsh 2áyears, 8ámonths ago

A is the answer... The catch here is that 10 years archives to store thus coldline storage is the right pick.

upvoted 1 times

? ?  mlantonis 3áyears ago

I hate when we have to guess what infrequent actually means. I believe because "Cost optimization is your top priority" we should choose
Coldline.

A provides a more cost-effective solution.

upvoted 1 times

? ?  de nepi314 3áyears ago

Infrequent means not frequently, that is "less". You don't have to guess.

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

"Infrequent" is unclear same as "not frequently" :)
I think @mlantonis wants to say "I'd like more precision: twice a year, once a year or so on"

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

750/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #11

Introductory Info

Company Overview -

Topic 11

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a website and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a premium app model.

Company Background -

Dress4Win's application has grown from a few servers in the founder's garage to several hundred servers and appliances in a collocated data

center. However, the capacity of their infrastructure is now insu cient for the application's rapid growth. Because of this growth and the

company's desire to innovate faster,

Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is considering moving their development and test environments. They are also

considering building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of

their architecture they can migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location.

Databases:

- MySQL - user data, inventory, static data

- Redis - metadata, social graph, caching

Application servers:

- Tomcat - Java micro-services

- Nginx - static content

- Apache Beam - Batch processing

Storage appliances:

- iSCSI for VM hosts

- Fiber channel SAN - MySQL databases

- NAS - image storage, logs, backups

Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

MQ servers:

- Messaging

- Social noti cations

- Events

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Migrate fully to the cloud if all other requirements are met.

Technical Requirements -

Evaluate and choose an automation framework for provisioning resources in cloud.

Support failover of the production environment to cloud during an emergency.

Identify production services that can migrate to cloud to save capacity.

Use managed services whenever possible.

Encrypt data on the wire and at rest.

Support multiple VPN connections between the production data center and cloud environment.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

751/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

CEO Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a new

competitor could use a public cloud platform to offset their up-front investment and freeing them to focus on developing better features.

CTO Statement -

We have invested heavily in the current infrastructure, but much of the equipment is approaching the end of its useful life. We are consistently

waiting weeks for new gear to be racked before we can start new projects. Our tra c patterns are highest in the mornings and weekend evenings;

during other times, 80% of our capacity is sitting idle.

CFO Statement -

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years puts a cloud

strategy between 30 to 50% lower than our current model.

Question

The current Dress4Win system architecture has high latency to some customers because it is located in one data center.

As of a future evaluation and optimizing for performance in the cloud, Dresss4Win wants to distribute its system architecture to multiple locations

when Google cloud platform.

Which approach should they use?

A. Use regional managed instance groups and a global load balancer to increase performance because the regional managed instance group

can grow instances in each region separately based on tra c.

B. Use a global load balancer with a set of virtual machines that forward the requests to a closer group of virtual machines managed by your

operations team.

C. Use regional managed instance groups and a global load balancer to increase reliability by providing automatic failover between zones in

different regions.

D. Use a global load balancer with a set of virtual machines that forward the requests to a closer group of virtual machines as part of a

separate managed instance groups.

Correct Answer: A

Community vote distribution

A (100%)

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

Agree. A looks correct for me.

upvoted 29 times

? ?  kimharsh 1áyear ago

I thought A is talking about MIG , but if your read the question carefully you will see MIG's , which changed my answer from D to A

upvoted 1 times

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

I am not convinced with D. A sounds correct answer. Creating regional MIGs and connecting it to GLB. Anyone ?

upvoted 19 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 5 times

? ?  nitinz 2áyears, 3ámonths ago

A is correct

upvoted 3 times

? ?  NodummyIQ  Most Recent ?  5ámonths, 4áweeks ago

Answer D is correct. Answer A is not correct because it does not mention the aspect of distributing the system architecture to multiple
locations. A regional managed instance group can increase performance by allowing the group to grow instances in each region
separately based on traffic, but it does not address the issue of distributing the system architecture to multiple locations.

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: A

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

752/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

vote A

upvoted 3 times

? ?  Ari_GCP 1áyear, 9ámonths ago

Agree with A. It says optimize for performance, and multiple regional MIG's can definitely help you do that.

upvoted 1 times

? ?  PeppaPig 1áyear, 10ámonths ago

A is correct for sure
D is wrong. GLB is already capable of forwarding traffic to MIG in the closer region so why would you implement that again

upvoted 1 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

A. Use regional managed instance groups and a global load balancer to increase performance because the regional managed instance
group can grow instances in each region separately based on traffic.

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 2 times

? ?  Pb55 2áyears, 2ámonths ago

ItÆs A. Each region can have an instance group linked to a global load balancer. Instance groups do not need to be multi regional for this to
work.

upvoted 1 times

? ?  tzKhalil 2áyears, 2ámonths ago

A is not good, because a regional MIG, which deploys instances to multiple zones across the same region. This will not deploy instances in
multi regions.
D is good

upvoted 2 times

? ?  tzKhalil 2áyears, 2ámonths ago

Doc: https://cloud.google.com/compute/docs/instance-groups#types_of_managed_instance_groups

upvoted 1 times

? ?  taoj 1áyear, 11ámonths ago

your statement is right. But A was MIGs.

upvoted 2 times

? ?  jaguarrr 2áyears, 2ámonths ago

D is the correct answer.
With A it says "because the regional managed instance group can grow instances in each region separately based on traffic." A regional
instance group cannot grow instances in Multiple Regions, only in one.
With D, you have multiple separate Regional Instance Groups, which is what is missing in answer A.

upvoted 3 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

IMO - A.
We are gointg to use a few MIGs - one per region; each of them can scale independetly from others.

upvoted 1 times

? ?  gu9singg 2áyears, 3ámonths ago

A- because with Regional resources and global load balancer we can route traffic to nearest VM machine

upvoted 1 times

? ?  bnlcnd 2áyears, 4ámonths ago

https://cloud.google.com/load-balancing/docs/https/setting-up-https
Seems D is correct

upvoted 2 times

? ?  bnlcnd 2áyears, 4ámonths ago

"closer" group of virtual machines as part of a separate managed instance groups.
The key word closer in the answer D means routing the client request to a closer regional MIG. Anything wrong?
A seems not mentioning routing request to the MIG that is closer to the client.

upvoted 1 times

? ?  okixavi 2áyears, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

753/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

A is the correct answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

754/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Topic 12 - Testlet 9

Question #1

Introductory Info

Company Overview -

Topic 12

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a web app and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a freemium app model. The application has grown from a few servers in the founder's garage to several

hundred servers and appliances in a colocated data center. However, the capacity of their infrastructure is now insu cient for the application's

rapid growth. Because of this growth and the company's desire to innovate faster, Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is moving their development and test environments. They are also building a disaster

recovery site, because their current infrastructure is at a single location. They are not sure which components of their architecture they can

migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location. All servers run Ubuntu LTS v16.04.

Databases:

MySQL. 1 server for user data, inventory, static data:

- MySQL 5.8

- 8 core CPUs

- 128 GB of RAM

- 2x 5 TB HDD (RAID 1)

Redis 3 server cluster for metadata, social graph, caching. Each server is:

- Redis 3.2

- 4 core CPUs

- 32GB of RAM

Compute:

40 Web Application servers providing micro-services based APIs and static content.

"

- Tomcat

Java -

- Nginx

- 4 core CPUs

- 32 GB of RAM

20 Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

- 8 core CPUs

- 128 GB of RAM

- 4x 5 TB HDD (RAID 1)

3 RabbitMQ servers for messaging, social noti cations, and events:

- 8 core CPUs

- 32GB of RAM

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

- 8 core CPUs

- 32GB of RAM

Storage appliances:

iSCSI for VM hosts

Fiber channel SAN " MySQL databases

- 1 PB total storage; 400 TB available

NAS " image storage, logs, backups

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

755/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

- 100 TB total storage; 35 TB available

Business Requirements -

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Technical Requirements -

Easily create non-production environments in the cloud.

Implement an automation framework for provisioning resources in cloud.

Implement a continuous deployment process for deploying applications to the on-premises datacenter or cloud.

Support failover of the production environment to cloud during an emergency.

Encrypt data on the wire and at rest.

Support multiple private connections between the production data center and cloud environment.

Executive Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a competitor

could use a public cloud platform to offset their up-front investment and free them to focus on developing better features. Our tra c patterns are

highest in the mornings and weekend evenings; during other times, 80% of our capacity is sitting idle.

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years for a public

cloud strategy achieves a cost reduction between 30% and 50% over our current model.

Question

For this question, refer to the Dress4Win case study. Dress4Win is expected to grow to 10 times its size in 1 year with a corresponding growth in

data and tra c that mirrors the existing patterns of usage. The CIO has set the target of migrating production infrastructure to the cloud within the

next 6 months. How will you con gure the solution to scale for this growth without making major application changes and still maximize the ROI?

A. Migrate the web application layer to App Engine, and MySQL to Cloud Datastore, and NAS to Cloud Storage. Deploy RabbitMQ, and deploy

Hadoop servers using Deployment Manager.

B. Migrate RabbitMQ to Cloud Pub/Sub, Hadoop to BigQuery, and NAS to Compute Engine with Persistent Disk storage. Deploy Tomcat, and

deploy Nginx using Deployment Manager.

C. Implement managed instance groups for Tomcat and Nginx. Migrate MySQL to Cloud SQL, RabbitMQ to Cloud Pub/Sub, Hadoop to Cloud

Dataproc, and NAS to Compute Engine with Persistent Disk storage.

D. Implement managed instance groups for the Tomcat and Nginx. Migrate MySQL to Cloud SQL, RabbitMQ to Cloud Pub/Sub, Hadoop to

Cloud Dataproc, and NAS to Cloud Storage.

Correct Answer: D

Community vote distribution

D (71%)

C (29%)

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

Why do we need to put NAS data on persistant disk and not on GCS ? I would go with D!

upvoted 38 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 10 times

? ?  Jphix 2áyears, 5ámonths ago

Agreed. Looking to maximize ROI as well according to the question, and even the most expensive cloud storage is still going to be
half the price of cheapest Persistent Disk storage, and that's without even including your compute costs. D all the way.

upvoted 3 times

? ?  nitinz 2áyears, 3ámonths ago

ans is D

upvoted 4 times

? ?  techalik 2áyears, 7ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

756/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

1. Use Cloud Marketplace to provision Tomcat and Nginx on Google Compute Engine.

2. Replace MySQL with Cloud SQL for MySQL.

3. Use the Deployment Manager to provision Jenkins on Google Compute Engine. is the right answer.

As explained above, you would use Cloud SQL to replace MySQL. For the other requirements, i.e. Nginx/Tomcat and Jenkins, you can
deploy these through Cloud Deployment Manager by using custom images.

Ref: https://cloud.google.com/compute/docs/images

Using the same custom images every time ensures that your environments are "reliable and reproducible" and you achieve "rapid
provisioning".

D

upvoted 10 times

? ?  KouShikyou  Highly Voted ?  3áyears, 8ámonths ago

I prefer D.
Original NAS is for image, log, backup. GCS fits it perfectly.

upvoted 19 times

? ?  exampanic 3áyears, 5ámonths ago

I agree that GCS fits perfectly for storing images, log, backup. However, the question asks to avoid major application changes. GCS is
not NAS, meaning it does not provide SMB or NFS shares. Therefore moving the NAS files to Google Cloud Storage would require a
major application change in the way they access these files. I believe the correct answer would be C.

upvoted 8 times

? ?  poseidon24 1áyear, 11ámonths ago

It can, check on Cloud Storage FUSE. Buckets can be mounted as file systems.

upvoted 4 times

? ?  thamaster  Most Recent ?  6ámonths ago

Selected Answer: D

you don't need NAS to store archive and Image disk

upvoted 1 times

? ?  amxexam 1áyear, 1ámonth ago

Selected Answer: D

D is the correct chand equivalent mapping

upvoted 1 times

? ?  [Removed] 1áyear, 3ámonths ago

D is OK

https://cloud.google.com/architecture/filers-on-compute-engine?hl=en#managed_file_storage_solutions

upvoted 2 times

? ?  MF2C 1áyear, 5ámonths ago

SAN -> persistent disk, NAS -> Cloud Storage

upvoted 2 times

? ?  edilramos 1áyear, 6ámonths ago

Managed Instances With Tomcat and Nginx would bring the minimum necessary tweaking to the new environment.
Migrating from MySql to Cloud SQL does not require any syntax changes.
Moving from Rabbit MQ to Pub/Sub is relatively straightforward and has very complete documentation.
DataProc has Libraries and tools to ensure Apache Hadoop interoperability.
Without many changes in the environment, mainly keeping the original architecture, Datastorage will keep the presentation
characteristics of a shared area, mapped to the instances.

upvoted 2 times

? ?  phantomsg 1áyear, 7ámonths ago

Selected Answer: C

The answer should be C. 'A' and 'B' are ruled out as they introduce significant architecture changes. or irrelevant. 'D' is fine except
proposes to replace NAS with Cloud Storage. This will introduce major architectural changes. Instead, if the choice was to move 'NAS' to
'Cloud Filestore' then it would have made sense. Answer 'C' is the closest with the least amount of architectural changes involved in
migration.

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 3 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

757/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 1 times

? ?  anku15 1áyear, 9ámonths ago

I dont see the questions now. Did you remove it?

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

D. Implement managed instance groups for the Tomcat and Nginx. Migrate MySQL to Cloud SQL, RabbitMQ to Cloud Pub/Sub, Hadoop to
Cloud Dataproc, and NAS to Cloud Storage.

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is D

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is D

upvoted 1 times

? ?  pawel_ski 2áyears, 3ámonths ago

They expect ôto grow to 10 times its size in 1 year with a corresponding growth in data and trafficö
And ômigrating production infrastructure to the cloud within the next 6 monthsö.
The MySQL DB has now 5 TB of data. In 6 months, it will raise to 25 TB. In 1 year to 50 TB. The limit for MySQL DB size is 30 TB. So it is very
likely that when we launch the solution on PROD the MySQL DB will be very close to the limit. I would rather like to avoid the problem and
migrate MySQL DB to Datastore. Datastore is also transactional like MySQL, hence no change in application code.
Deploying RabbitMQ also will avoid the need to the code change.
Migration of the web application layer to App Engine and Hadoop: as is.
From the application code perspective, migration to option A requires only one change NAS -> GCS. And at the end we get and scalable
solution.
I go with A.

upvoted 2 times

? ?  lynx256 2áyears, 3ámonths ago
IMO you are wrong with A...
A states Migration MySQL to Cloud Datastore (which is NoSQL, object DB) - non sense.
IMO D is the right.

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

Also - Cloud SQL includes SQL Server (from about 6 months ago )

upvoted 1 times

? ?  mrhege 2áyears, 2ámonths ago

You almost convinced me with the MySQL scaling issue, but then I realized that the DB size will not grow proportionally with the
number of users as _past collected_ data is what makes it grow. Still, the issue should be on the radar and the company should start
working on plan-B, as it will likely cause issues in the next year.

upvoted 1 times

? ?  ahmedemad3 2áyears, 4ámonths ago

ans : D is the right answer

upvoted 1 times

? ?  bnlcnd 2áyears, 4ámonths ago

C vs D. NAS should go to Cloud Storage.
D is right.

upvoted 1 times

? ?  okixavi 2áyears, 6ámonths ago

D is the right answer

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

758/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #2

Introductory Info

Company Overview -

Topic 12

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a web app and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a freemium app model. The application has grown from a few servers in the founder's garage to several

hundred servers and appliances in a colocated data center. However, the capacity of their infrastructure is now insu cient for the application's

rapid growth. Because of this growth and the company's desire to innovate faster, Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is moving their development and test environments. They are also building a disaster

recovery site, because their current infrastructure is at a single location. They are not sure which components of their architecture they can

migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location. All servers run Ubuntu LTS v16.04.

Databases:

MySQL. 1 server for user data, inventory, static data:

- MySQL 5.8

- 8 core CPUs

- 128 GB of RAM

- 2x 5 TB HDD (RAID 1)

Redis 3 server cluster for metadata, social graph, caching. Each server is:

- Redis 3.2

- 4 core CPUs

- 32GB of RAM

Compute:

40 Web Application servers providing micro-services based APIs and static content.

"

- Tomcat

Java -

- Nginx

- 4 core CPUs

- 32 GB of RAM

20 Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

- 8 core CPUs

- 128 GB of RAM

- 4x 5 TB HDD (RAID 1)

3 RabbitMQ servers for messaging, social noti cations, and events:

- 8 core CPUs

- 32GB of RAM

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

- 8 core CPUs

- 32GB of RAM

Storage appliances:

iSCSI for VM hosts

Fiber channel SAN " MySQL databases

- 1 PB total storage; 400 TB available

NAS " image storage, logs, backups

- 100 TB total storage; 35 TB available

Business Requirements -

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

759/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Technical Requirements -

Easily create non-production environments in the cloud.

Implement an automation framework for provisioning resources in cloud.

Implement a continuous deployment process for deploying applications to the on-premises datacenter or cloud.

Support failover of the production environment to cloud during an emergency.

Encrypt data on the wire and at rest.

Support multiple private connections between the production data center and cloud environment.

Executive Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a competitor

could use a public cloud platform to offset their up-front investment and free them to focus on developing better features. Our tra c patterns are

highest in the mornings and weekend evenings; during other times, 80% of our capacity is sitting idle.

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years for a public

cloud strategy achieves a cost reduction between 30% and 50% over our current model.

Question

For this question, refer to the Dress4Win case study. Considering the given business requirements, how would you automate the deployment of

web and transactional data layers?

A. Deploy Nginx and Tomcat using Cloud Deployment Manager to Compute Engine. Deploy a Cloud SQL server to replace MySQL. Deploy

Jenkins using Cloud Deployment Manager.

B. Deploy Nginx and Tomcat using Cloud Launcher. Deploy a MySQL server using Cloud Launcher. Deploy Jenkins to Compute Engine using

Cloud Deployment Manager scripts.

C. Migrate Nginx and Tomcat to App Engine. Deploy a Cloud Datastore server to replace the MySQL server in a high-availability con guration.

Deploy Jenkins to Compute Engine using Cloud Launcher.

D. Migrate Nginx and Tomcat to App Engine. Deploy a MySQL server using Cloud Launcher. Deploy Jenkins to Compute Engine using Cloud

Launcher.

Correct Answer: A

Community vote distribution

A (100%)

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

It's A, "Cloud Datastore server" doesn't exist. A fits OK.

upvoted 26 times

? ?  cetanx 2áyears, 11ámonths ago

Also, GAE uses Jetty for http and servlet engine. Therefore Tomcat cannot be run on GAE (unless on flexible env.) - this rules out "C and
D"

upvoted 2 times

? ?  tartar 2áyears, 10ámonths ago

A is ok

upvoted 4 times

? ?  Jphix 2áyears, 5ámonths ago

agreed, A. For those saying C, the question is about "automating the deployment" in line with the business requirements. Going
from MySQL to datastore might be a good idea long term, but it won't make automating the deployment to the cloud any easier
or smoother. Automate the deployment to Cloud SQL because it's a natural fit, and once that's working, re-assess the
requirements to decide if it's worth the hefty lift of shifting from MySQL to a NoSQL Document DB.

upvoted 1 times

? ?  nitinz 2áyears, 3ámonths ago

A is the answer
upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

760/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  Eroc  Highly Voted ?  3áyears, 8ámonths ago

The requriements also specify:
"Easily create non-production environment in the cloud.
Implement an automation framework for provisioning resources in cloud.
Implement a continuous deployment process for deploying applications to the on-premises datacenter or cloud."
So A is better.

upvoted 11 times

? ?  SSQX 3áyears, 2ámonths ago

You can only deploy Jenkins with Cloud Launcher, not with Deployment manager

upvoted 2 times

? ?  Ayzen 3áyears, 2ámonths ago

Jenkins is just an app that should be run on a VM. You definitely can use Deployment Manager to set up a VM with needed image.

upvoted 2 times

? ?  joe2211  Most Recent ?  1áyear, 7ámonths ago

Selected Answer: A

vote A

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

A. Deploy Nginx and Tomcat using Cloud Deployment Manager to Compute Engine. Deploy a Cloud SQL server to replace MySQL. Deploy
Jenkins using Cloud Deployment Manager.

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is A

upvoted 2 times

? ?  gosi 2áyears, 2ámonths ago

D. With produciton parity, you cant replace MySQL with 128 GB of memory with Cloud SQL as there is no such image available. I have
checked it. MySQL has to go on GCE with PD
I would go for either go for B or D.
D is better because it is scalable better than B as B has no details if it is going to use MIG or just fleet of tomcat servers for web apps.

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is A

upvoted 1 times

? ?  vruizm 2áyears, 3ámonths ago

I think B is a valid response, please check:
https://cloud.google.com/blog/products/it-ops/google-cloud-launcher-simplifies-running-third-party-apps-in-the-cloud
and
https://medium.com/@PeetDenny/automated-provisioning-of-jenkins-on-google-cloud-c297b2e0be2

upvoted 2 times

? ?  bnlcnd 2áyears, 4ámonths ago

the question and the answers are so confusing. what is "Cloud Launcher"? Never heard of it.
Only A does not mention that launcher thingy. I can only choose A.

upvoted 2 times

? ?  Wira 2áyears, 3ámonths ago

its an old question - its cloud marketplace now

given size of mysql and type of data, the only valid choice is C for me

upvoted 1 times

? ?  pawel_ski 2áyears, 3ámonths ago

It's the previous name of GCP Marketplace.

upvoted 1 times

? ?  ybe_gcp_cert 2áyears, 5ámonths ago

A or B;
B doesn't tell which automation tool is used to deploy Cloud SQL. Cloud launcher generates Cloud Deployment Manager scripts. I would
go with B

upvoted 1 times

? ?  ybe_gcp_cert 2áyears, 5ámonths ago

Sorry A doesn't tell which tool is used to deploy Cloud SQL.
I would go with B.

upvoted 1 times

? ?  Mndwsk 2áyears, 6ámonths ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

761/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

B.
Only option that automates the deployment of all the tools mentioned.
Cloud Launcher creates a Deployment in Deployment Manager.

upvoted 1 times

? ?  SKSKSK 2áyears, 7ámonths ago

After reading the question more and kind of linking back to question one, i think it's asking how to "automate the deployment" of web and
transactional data layers". In that case, I think focus on deployment automation of existing technology might be a better than mapping
new cloud technology in this case? so, A might be a better fit?

upvoted 1 times

? ?  homer_simpson 2áyears, 8ámonths ago

the answer is A because datastore is nosql db and in business requirements it is clarly sais that improve bussiness agility and speed
innovation through rapid provisoning of new ressources

upvoted 1 times

? ?  brati_sankar 2áyears, 9ámonths ago
I believe this is D. Here is my logic.
In D we are using a MySQL from the Marketplace. Presently, on-prem the amount of data is 600 TB (1 PB SAN for MySQL of which 400 TB is
free) . This would not go in Cloud SQL which has a limit of 30 TB. Hence, we must go for MySQL on compute using Launcher/Marketplace.

upvoted 4 times

? ?  roastc 2áyears, 9ámonths ago

I don't think there is any automation mentioned while using Cloud Launcher. So the answer should be A

upvoted 2 times

? ?  Kabiliravi 2áyears, 10ámonths ago

A is correct

upvoted 1 times

? ?  pbrat 3áyears ago

Answer A

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

762/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #3

Introductory Info

Company Overview -

Topic 12

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a web app and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a freemium app model. The application has grown from a few servers in the founder's garage to several

hundred servers and appliances in a colocated data center. However, the capacity of their infrastructure is now insu cient for the application's

rapid growth. Because of this growth and the company's desire to innovate faster, Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is moving their development and test environments. They are also building a disaster

recovery site, because their current infrastructure is at a single location. They are not sure which components of their architecture they can

migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location. All servers run Ubuntu LTS v16.04.

Databases:

MySQL. 1 server for user data, inventory, static data:

- MySQL 5.8

- 8 core CPUs

- 128 GB of RAM

- 2x 5 TB HDD (RAID 1)

Redis 3 server cluster for metadata, social graph, caching. Each server is:

- Redis 3.2

- 4 core CPUs

- 32GB of RAM

Compute:

40 Web Application servers providing micro-services based APIs and static content.

"

- Tomcat

Java -

- Nginx

- 4 core CPUs

- 32 GB of RAM

20 Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

- 8 core CPUs

- 128 GB of RAM

- 4x 5 TB HDD (RAID 1)

3 RabbitMQ servers for messaging, social noti cations, and events:

- 8 core CPUs

- 32GB of RAM

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

- 8 core CPUs

- 32GB of RAM

Storage appliances:

iSCSI for VM hosts

Fiber channel SAN " MySQL databases

- 1 PB total storage; 400 TB available

NAS " image storage, logs, backups

- 100 TB total storage; 35 TB available

Business Requirements -

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

763/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Technical Requirements -

Easily create non-production environments in the cloud.

Implement an automation framework for provisioning resources in cloud.

Implement a continuous deployment process for deploying applications to the on-premises datacenter or cloud.

Support failover of the production environment to cloud during an emergency.

Encrypt data on the wire and at rest.

Support multiple private connections between the production data center and cloud environment.

Executive Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a competitor

could use a public cloud platform to offset their up-front investment and free them to focus on developing better features. Our tra c patterns are

highest in the mornings and weekend evenings; during other times, 80% of our capacity is sitting idle.

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years for a public

cloud strategy achieves a cost reduction between 30% and 50% over our current model.

Question

For this question, refer to the Dress4Win case study. Which of the compute services should be migrated as-is and would still be an optimized

architecture for performance in the cloud?

A. Web applications deployed using App Engine standard environment

B. RabbitMQ deployed using an unmanaged instance group

C. Hadoop/Spark deployed using Cloud Dataproc Regional in High Availability mode

D. Jenkins, monitoring, bastion hosts, security scanners services deployed on custom machine types

Correct Answer: A

Community vote distribution

C (100%)

? ?  Hemant_C  Highly Voted ?  3áyears ago

Question is about compute services to be migrated as ""is and would still be an optimized architecture for performance - Apache
Hadoop/Spark servers underline is compute and Hadoop/Spark deployed using Cloud Dataproc seems to be the correct answer.. Hence C
seems correct answer to me

upvoted 28 times

? ?  SAMBIT 1áyear, 3ámonths ago

They are not sure which components of their architecture they can migrate as is and which components they need to change before
migrating them.
upvoted 1 times

? ?  jcmoranp  Highly Voted ?  3áyears, 8ámonths ago

It's D. You cannot migrate to APP Engine "as-is"

upvoted 19 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 11 times

? ?  army234 2áyears, 2ámonths ago

C is correct

upvoted 7 times

? ?  akhilesh_pundir  Most Recent ?  4ámonths, 4áweeks ago

Read the previous questions ... they are going to use Managed instance groups with Tomcat &nginx installed on that so app engine is not
in picture. Hadoop workloads goes to dataproc as it is.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

764/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  OrangeTiger 1áyear, 5ámonths ago

Selected Answer: C

I agree with C.
'as-is'

upvoted 4 times

? ?  ABO_Doma 1áyear, 6ámonths ago

Google Cloud includes Dataproc, which is a managed Hadoop and Spark environment. You can use Dataproc to run most of your existing
jobs with minimal alteration, so you don't need to move away from all of the Hadoop tools you already know.

upvoted 2 times

? ?  ABO_Doma 1áyear, 6ámonths ago

Selected Answer: C

Answer is C

upvoted 2 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

C. Hadoop/Spark deployed using Cloud Dataproc Regional in High Availability mode

upvoted 6 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is C

upvoted 4 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is C

upvoted 2 times

? ?  hkmsn 2áyears, 4ámonths ago

A. Web applications deployed using App Engine standard environment - there are multiple web apps, seems project limit of 1 - and not
clear on the implications of Standard Env, with Nginx (there seems to be discussions) -- no not clear on this.
B. RabbitMQ - is always replaced by Pub/Sub - So No.
C. Hadoop/Spark - This is a well know Use Case
D. Jenkins, Etc, these duplicate GCP products so it can't be the answer.

My bet is C

upvoted 2 times

? ?  ahmedemad3 2áyears, 4ámonths ago

ans: C
compute services should be migrated as is and would still be an optimized architecture for performance in the cloud?

upvoted 1 times

? ?  bnlcnd 2áyears, 4ámonths ago

It's C. hardoop == dataproc. pretty much a cloud version.
D is "Jenkins, monitoring, bastion hosts, security scanners". How can you make them as-is to run in cloud? Monitoring? on-prem to cloud
no change? security scanner? no change?

upvoted 2 times

? ?  BobBui 2áyears, 4ámonths ago

I choose C

upvoted 1 times

? ?  okixavi 2áyears, 6ámonths ago

C is the correct answer. The question says: "...as is"

upvoted 1 times

? ?  practicioner 2áyears, 7ámonths ago

C and D make sense. However, "would still be an optimized architecture". In this case, I chose C because we can move our services as is
and we can get significant benefits from GCP

upvoted 1 times

? ?  gcparchitect007 2áyears, 8ámonths ago

C is correct answer.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

765/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #4

Introductory Info

Company Overview -

Topic 12

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a web app and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a freemium app model. The application has grown from a few servers in the founder's garage to several

hundred servers and appliances in a colocated data center. However, the capacity of their infrastructure is now insu cient for the application's

rapid growth. Because of this growth and the company's desire to innovate faster, Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is moving their development and test environments. They are also building a disaster

recovery site, because their current infrastructure is at a single location. They are not sure which components of their architecture they can

migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location. All servers run Ubuntu LTS v16.04.

Databases:

MySQL. 1 server for user data, inventory, static data:

- MySQL 5.8

- 8 core CPUs

- 128 GB of RAM

- 2x 5 TB HDD (RAID 1)

Redis 3 server cluster for metadata, social graph, caching. Each server is:

- Redis 3.2

- 4 core CPUs

- 32GB of RAM

Compute:

40 Web Application servers providing micro-services based APIs and static content.

"

- Tomcat

Java -

- Nginx

- 4 core CPUs

- 32 GB of RAM

20 Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

- 8 core CPUs

- 128 GB of RAM

- 4x 5 TB HDD (RAID 1)

3 RabbitMQ servers for messaging, social noti cations, and events:

- 8 core CPUs

- 32GB of RAM

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

- 8 core CPUs

- 32GB of RAM

Storage appliances:

iSCSI for VM hosts

Fiber channel SAN " MySQL databases

- 1 PB total storage; 400 TB available

NAS " image storage, logs, backups

- 100 TB total storage; 35 TB available

Business Requirements -

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

766/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Technical Requirements -

Easily create non-production environments in the cloud.

Implement an automation framework for provisioning resources in cloud.

Implement a continuous deployment process for deploying applications to the on-premises datacenter or cloud.

Support failover of the production environment to cloud during an emergency.

Encrypt data on the wire and at rest.

Support multiple private connections between the production data center and cloud environment.

Executive Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a competitor

could use a public cloud platform to offset their up-front investment and free them to focus on developing better features. Our tra c patterns are

highest in the mornings and weekend evenings; during other times, 80% of our capacity is sitting idle.

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years for a public

cloud strategy achieves a cost reduction between 30% and 50% over our current model.

Question

For this question, refer to the Dress4Win case study. To be legally compliant during an audit, Dress4Win must be able to give insights in all

administrative actions that modify the con guration or metadata of resources on Google Cloud.

What should you do?

A. Use Stackdriver Trace to create a Trace list analysis.

B. Use Stackdriver Monitoring to create a dashboard on the project's activity.

C. Enable Cloud Identity-Aware Proxy in all projects, and add the group of Administrators as a member.

D. Use the Activity page in the GCP Console and Stackdriver Logging to provide the required insight.

Correct Answer: D

Community vote distribution

D (100%)

? ?  MeasService  Highly Voted ?  3áyears, 8ámonths ago

D is the correct answer !
https://cloud.google.com/logging/docs/audit/

upvoted 49 times

? ?  chiar 3áyears, 8ámonths ago

I agree

upvoted 4 times

? ?  tartar 2áyears, 10ámonths ago

D is ok

upvoted 3 times

? ?  KouShikyou 3áyears, 8ámonths ago

Agree.

upvoted 4 times

? ?  Eroc 3áyears, 8ámonths ago

I agree

upvoted 3 times

? ?  newbie2020 3áyears, 5ámonths ago

Agree, Answer is D

upvoted 4 times

? ?  joe2211  Most Recent ?  1áyear, 7ámonths ago

Selected Answer: D

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

767/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

vote D

upvoted 2 times

? ?  Shahariargcppca 1áyear, 8ámonths ago

answer is d

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

D. Use the Activity page in the GCP Console and Stackdriver Logging to provide the required insight.

upvoted 1 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is D..

upvoted 3 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is D

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

IMO - D is ok

upvoted 2 times

? ?  pihuanshu 2áyears, 4ámonths ago

D should be

upvoted 2 times

? ?  bnlcnd 2áyears, 4ámonths ago

D for sure

upvoted 2 times

? ?  Chulbul_Pandey 2áyears, 6ámonths ago

D is the choice
upvoted 1 times

? ?  gcparchitect007 2áyears, 8ámonths ago

D is the right answer.

upvoted 1 times

? ?  homer_simpson 2áyears, 8ámonths ago

the answer is D

Admin Activity audit logs
Admin Activity audit logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of
resources. For example, these logs record when users create VM instances or change Identity and Access Management permissions.

To view these logs, you must have the IAM role Logging/Logs Viewer or Project/Viewer.

upvoted 1 times

? ?  Kabiliravi 2áyears, 10ámonths ago

D is correct

upvoted 1 times

? ?  wiqi 2áyears, 10ámonths ago

D is correct.

upvoted 1 times

? ?  mbiy 2áyears, 10ámonths ago
D is the correct option

upvoted 1 times

? ?  ry9280087 2áyears, 10ámonths ago

Seriously GCP must have written these answers as poison pills.

upvoted 2 times

? ?  mlantonis 3áyears ago
Yeah D is the correct

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

768/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #5

Introductory Info

Company Overview -

Topic 12

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a web app and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a freemium app model. The application has grown from a few servers in the founder's garage to several

hundred servers and appliances in a colocated data center. However, the capacity of their infrastructure is now insu cient for the application's

rapid growth. Because of this growth and the company's desire to innovate faster, Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is moving their development and test environments. They are also building a disaster

recovery site, because their current infrastructure is at a single location. They are not sure which components of their architecture they can

migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location. All servers run Ubuntu LTS v16.04.

Databases:

MySQL. 1 server for user data, inventory, static data:

- MySQL 5.8

- 8 core CPUs

- 128 GB of RAM

- 2x 5 TB HDD (RAID 1)

Redis 3 server cluster for metadata, social graph, caching. Each server is:

- Redis 3.2

- 4 core CPUs

- 32GB of RAM

Compute:

40 Web Application servers providing micro-services based APIs and static content.

"

- Tomcat

Java -

- Nginx

- 4 core CPUs

- 32 GB of RAM

20 Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

- 8 core CPUs

- 128 GB of RAM

- 4x 5 TB HDD (RAID 1)

3 RabbitMQ servers for messaging, social noti cations, and events:

- 8 core CPUs

- 32GB of RAM

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

- 8 core CPUs

- 32GB of RAM

Storage appliances:

iSCSI for VM hosts

Fiber channel SAN " MySQL databases

- 1 PB total storage; 400 TB available

NAS " image storage, logs, backups

- 100 TB total storage; 35 TB available

Business Requirements -

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

769/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Technical Requirements -

Easily create non-production environments in the cloud.

Implement an automation framework for provisioning resources in cloud.

Implement a continuous deployment process for deploying applications to the on-premises datacenter or cloud.

Support failover of the production environment to cloud during an emergency.

Encrypt data on the wire and at rest.

Support multiple private connections between the production data center and cloud environment.

Executive Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a competitor

could use a public cloud platform to offset their up-front investment and free them to focus on developing better features. Our tra c patterns are

highest in the mornings and weekend evenings; during other times, 80% of our capacity is sitting idle.

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years for a public

cloud strategy achieves a cost reduction between 30% and 50% over our current model.

Question

For this question, refer to the Dress4Win case study. You are responsible for the security of data stored in Cloud Storage for your company,

Dress4Win. You have already created a set of Google Groups and assigned the appropriate users to those groups. You should use Google best

practices and implement the simplest design to meet the requirements.

Considering Dress4Win's business and technical requirements, what should you do?

A. Assign custom IAM roles to the Google Groups you created in order to enforce security requirements. Encrypt data with a customer-

supplied encryption key when storing  les in Cloud Storage.

B. Assign custom IAM roles to the Google Groups you created in order to enforce security requirements. Enable default storage encryption

before storing  les in Cloud Storage.

C. Assign prede ned IAM roles to the Google Groups you created in order to enforce security requirements. Utilize Google's default encryption

at rest when storing  les in Cloud Storage.

D. Assign prede ned IAM roles to the Google Groups you created in order to enforce security requirements. Ensure that the default Cloud KMS

key is set before storing  les in Cloud Storage.

Correct Answer: C

Community vote distribution

C (100%)

? ?  JoeShmoe  Highly Voted ?  3áyears, 7ámonths ago

C is the simplest
upvoted 34 times

? ?  AWS56 3áyears, 5ámonths ago

I am a bit confused "You should use Google best practices and implement the simplest design to meet the requirements." ---> Simplest
-- agree with D, but for googles best practice I will go with A

upvoted 3 times

? ?  AWS56 3áyears, 5ámonths ago

Ignore my comment, Agree C is the simple -- https://cloud.google.com/compute/docs/disks/customer-supplied-encryption

upvoted 4 times

? ?  tartar 2áyears, 10ámonths ago

C is ok

upvoted 5 times

? ?  rockstar9622 3áyears, 5ámonths ago

c is correct - going by simplest design whereas google manages the encrytion though by default and thats sufficient

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

770/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  nitinz 2áyears, 3ámonths ago

ans is C

upvoted 3 times

? ?  kimharsh 1áyear, 2ámonths ago

how come it's C , and for best practice we need to use Custom Roles

upvoted 1 times

? ?  newbie2020  Highly Voted ?  3áyears, 5ámonths ago

There 2 requirements
1) best practices = least privilege = custom role
2) simplest = default encryption as
: If you use customer-supplied encryption keys or client-side encryption, you must securely manage your keys and ensure that they are
not lost. If you lose your keys, you are no longer able to read your data, and you continue to be charged for storage of your objects until
you delete them.
upvoted 12 times

? ?  Dannyygcp 3áyears, 4ámonths ago

What about option B..default encryption[which is simple to manage] + Custom role[which is secure compared to predefined and not
difficult to create]

upvoted 3 times

? ?  sivass 3áyears, 1ámonth ago
I agrre. I will go with B.

upvoted 5 times

? ?  GCP_Azure 3áyears, 1ámonth ago

It has to be B
upvoted 4 times

? ?  Rafaa 3áyears ago

there is no option to 'enable default encyption' as such! It is provided by default if you dont do anything.

upvoted 2 times

? ?  Vika 2áyears, 2ámonths ago

Check out this link - https://cloud.google.com/iam/docs/using-iam-securely

Basic roles include thousands of permissions across all Google Cloud services. In production environments, do not grant basic roles
unless there is no alternative. Instead, grant the most limited predefined roles or custom roles that meet your needs.

upvoted 1 times

? ?  SAMBIT  Most Recent ?  1áyear, 3ámonths ago
B custom IAM & out of box encryption

upvoted 1 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: C

vote C

upvoted 2 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 1 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 1 times

? ?  victory108 1áyear, 11ámonths ago

C. Assign predefined IAM roles to the Google Groups you created in order to enforce security requirements. Utilize Google?ÇÖs default
encryption at rest when storing files in Cloud Storage.

upvoted 2 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is B

upvoted 1 times

? ?  wilwong 1áyear, 11ámonths ago

C is correct

upvoted 1 times

? ?  Pb55 2áyears, 2ámonths ago

C. Best practice is predefined not custom. Only use custom when predefined to broard.

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

771/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

? ?  ansh0692 2áyears, 2ámonths ago

From "Google's best practices and simplest design" Answer should be C

upvoted 1 times

? ?  Skeeter 2áyears, 2ámonths ago

Cloud storage encryption is enabled by default. Why would you need to enable it as stated in B? Answer is A, use CSEK and specify a .boto
file during upload with gsutil, simple!

upvoted 2 times

? ?  Ausias18 2áyears, 2ámonths ago

it says simple, what you say is not as easy as possible... default encryption is easier

upvoted 1 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is B

upvoted 1 times

? ?  lynx256 2áyears, 3ámonths ago

IMO - C is ok.
Simplest --> predefined roles + default encryption

upvoted 2 times

? ?  Rightsaidfred 2áyears, 4ámonths ago
C is the 'Google' answer here :)

upvoted 1 times

? ?  bnlcnd 2áyears, 4ámonths ago

although most people seems agree with C. But, in real life for major companies with public names, who don't provide their own
encryption keys? I never heard of.
Go with A.

upvoted 1 times

? ?  ahmedemad3 2áyears, 5ámonths ago

ans:C
going by simplest design whereas google manages the encrytion though by default and thats sufficient

upvoted 1 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

772/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Question #6

Introductory Info

Company Overview -

Topic 12

Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a web app and mobile application.

The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through

advertising, e-commerce, referrals, and a freemium app model. The application has grown from a few servers in the founder's garage to several

hundred servers and appliances in a colocated data center. However, the capacity of their infrastructure is now insu cient for the application's

rapid growth. Because of this growth and the company's desire to innovate faster, Dress4Win is committing to a full migration to a public cloud.

Solution Concept -

For the  rst phase of their migration to the cloud, Dress4Win is moving their development and test environments. They are also building a disaster

recovery site, because their current infrastructure is at a single location. They are not sure which components of their architecture they can

migrate as is and which components they need to change before migrating them.

Existing Technical Environment -

The Dress4Win application is served out of a single data center location. All servers run Ubuntu LTS v16.04.

Databases:

MySQL. 1 server for user data, inventory, static data:

- MySQL 5.8

- 8 core CPUs

- 128 GB of RAM

- 2x 5 TB HDD (RAID 1)

Redis 3 server cluster for metadata, social graph, caching. Each server is:

- Redis 3.2

- 4 core CPUs

- 32GB of RAM

Compute:

40 Web Application servers providing micro-services based APIs and static content.

"

- Tomcat

Java -

- Nginx

- 4 core CPUs

- 32 GB of RAM

20 Apache Hadoop/Spark servers:

- Data analysis

- Real-time trending calculations

- 8 core CPUs

- 128 GB of RAM

- 4x 5 TB HDD (RAID 1)

3 RabbitMQ servers for messaging, social noti cations, and events:

- 8 core CPUs

- 32GB of RAM

Miscellaneous servers:

- Jenkins, monitoring, bastion hosts, security scanners

- 8 core CPUs

- 32GB of RAM

Storage appliances:

iSCSI for VM hosts

Fiber channel SAN " MySQL databases

- 1 PB total storage; 400 TB available

NAS " image storage, logs, backups

- 100 TB total storage; 35 TB available

Business Requirements -

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

773/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Build a reliable and reproducible environment with scaled parity of production.

Improve security by de ning and adhering to a set of security and Identity and Access Management (IAM) best practices for cloud.

Improve business agility and speed of innovation through rapid provisioning of new resources.

Analyze and optimize architecture for performance in the cloud.

Technical Requirements -

Easily create non-production environments in the cloud.

Implement an automation framework for provisioning resources in cloud.

Implement a continuous deployment process for deploying applications to the on-premises datacenter or cloud.

Support failover of the production environment to cloud during an emergency.

Encrypt data on the wire and at rest.

Support multiple private connections between the production data center and cloud environment.

Executive Statement -

Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a competitor

could use a public cloud platform to offset their up-front investment and free them to focus on developing better features. Our tra c patterns are

highest in the mornings and weekend evenings; during other times, 80% of our capacity is sitting idle.

Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we

expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next 5 years for a public

cloud strategy achieves a cost reduction between 30% and 50% over our current model.

Question

For this question, refer to the Dress4Win case study. You want to ensure that your on-premises architecture meets business requirements before

you migrate your solution.

What change in the on-premises architecture should you make?

A. Replace RabbitMQ with Google Pub/Sub.

B. Downgrade MySQL to v5.7, which is supported by Cloud SQL for MySQL.

C. Resize compute resources to match prede ned Compute Engine machine types.

D. Containerize the micro-services and host them in Google Kubernetes Engine.

Correct Answer: C

Community vote distribution

D (100%)

? ?  chiar  Highly Voted ?  3áyears, 7ámonths ago

Be careful, because in the case study that you can find in google website MySQL version is 5.7
https://cloud.google.com/certification/guides/cloud-architect/casestudy-dress4win-rev2

upvoted 20 times

? ?  jasim21 2áyears, 2ámonths ago
cloud SQL support MySQL 5.7
https://cloud.google.com/sql/docs/mysql/db-versions

Answer is D

upvoted 8 times

? ?  crypt0  Highly Voted ?  3áyears, 8ámonths ago

I would tend to answer B "Second Generation instances support MySQL 5.6 or 5.7,"
https://cloud.google.com/sql/docs/mysql/features

upvoted 14 times

? ?  chiar 3áyears, 8ámonths ago
I agree, the answer is B

upvoted 6 times

? ?  addy007 3áyears, 6ámonths ago

Downgrading is not supported. Seems C is the right choice.
https://dev.mysql.com/doc/refman/8.0/en/downgrading.html

upvoted 4 times

? ?  xps 3áyears, 1ámonth ago

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

774/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

Downgrade is possible from 5.7 to 5.6, which is in the same major version. So downgrade from 5.8 to 5.7 makes sense. Containerize
the java applications require to build kubernetes infrastructures in the on-premise environment, it's not in the plan. So the answer
goes to B.

upvoted 3 times

? ?  SAMBIT  Most Recent ?  1áyear, 3ámonths ago
Replace RabbitMQ with pub sub àA
https://docs.devicewise.com/Content/Products/GatewayDevelopersGuide/CloudConnectors/GoogleCloud/GoogleCloudPlatform.htm

upvoted 2 times

? ?  ABO_Doma 1áyear, 6ámonths ago

Selected Answer: D

Containerizing the existing applications ensures efficient use of resources. This activity the business requirement ôoptimize architecture
for performance in the cloudö. As a precursor to Cloud migration, you could convert the microservices to containers and host them on GKE
on-prem: https://cloud.google.com/anthos/gke/docs/on-prem/overview which also makes it very easy for you to migrate to Cloud. GKE on-
prem is hybrid cloud software that brings Google Kubernetes Engine (GKE) to on-premises data centres. With GKE on-prem, you can
create, manage, and upgrade Kubernetes clusters in your on-premises environment.

upvoted 8 times

? ?  didek1986 1áyear, 6ámonths ago

D read business req.

upvoted 2 times

? ?  kvenkatasudhakar 1áyear, 6ámonths ago

Cloud SQL supports MySQL 5.6, 5.7 and 8.0 and the current onprem version is MySQL 5.8. So downgrade from 5.8 to 5.7 is the right
answer (D).

upvoted 1 times

? ?  phantomsg 1áyear, 7ámonths ago

Selected Answer: D

'D' as it matches the requirement - Improve business agility and speed of innovation through rapid provisioning of new resources. Among
the choices, containerizing microservices will allow the company to deploy and scale services independantly.

upvoted 4 times

? ?  joe2211 1áyear, 7ámonths ago

Selected Answer: D

vote D

upvoted 1 times

? ?  kopper2019 1áyear, 11ámonths ago

hey guys new Qs posted as of July 12th, 2021, All 21 new Qs in Question #152

upvoted 3 times

? ?  vishwassahu 1áyear, 9ámonths ago

it seems 21 new questions are deleted

upvoted 2 times

? ?  victory108 1áyear, 11ámonths ago

D. Containerize the micro-services and host them in Google Kubernetes Engine.

upvoted 6 times

? ?  MamthaSJ 1áyear, 11ámonths ago

Answer is D

upvoted 4 times

? ?  wilwong 1áyear, 11ámonths ago

Answer is D

upvoted 2 times

? ?  getzsagar 2áyears, 2ámonths ago

One important Update related to case study - Date - 27-04-2021
Right answer is option D ---
Case Study for Dress4win is updated, MYSQL version 5.7 is mentioned in the case study and not 5.8 as given in here. This makes option B
invalid. Right answer is option D. In the exam I saw option B was still given to confuse people, but in the case study they mentioned the
MYSQL version 5.7 and not 5.8. So despite of being confident about the case studies, ensure that you read it thoroughly even during the
exam. Time is given sufficient enough.

upvoted 7 times

? ?  ansh0692 2áyears, 2ámonths ago

Answer is D
A: If you are expecting to "install" pub/sub locally and not connect to the hosted GCP service, you cannot use pub/sub, if it is okay for you
to use hosted pub/sub and then do the rest of the processing on prem but you won't get a readily available system or you'll have to
integrate a lot of things.
B: 5.8 is basically 8.0 and it is supported
C: Best practice is to find the predefined types on GCP as close to the physical server spec and not the other way around.

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

775/776

6/29/23, 1:52 PM

Professional Cloud Architect Exam û Free Actual Q&As, Page 1 | ExamTopics

upvoted 4 times

? ?  Ausias18 2áyears, 2ámonths ago

Answer is D

upvoted 5 times

? ?  lynx256 2áyears, 3ámonths ago

IMO - D is ok

upvoted 4 times

? ?  VenV 2áyears, 3ámonths ago
D is make more sense

upvoted 2 times

https://www.examtopics.com/exams/google/professional-cloud-architect/custom-view/

776/776`)

